Authors,Title,DOI,Link,Abstract,Author Keywords,Index Keywords
"Aalst W.M.P.","The Data Science Revolution: How Learning Machines Changed the Way We Work and Do Business","10.1007/978-3-030-64246-4_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097338394&doi=10.1007%2f978-3-030-64246-4_2&partnerID=40&md5=03361cc1c313349e2c5ccab08736e435","Data science technology is rapidly changing the role of information technology in society and all economic sectors. Artificial Intelligence (AI) and Machine Learning (ML) are at the forefront of attention. However, data science is much broader and also includes data extraction, data preparation, data exploration, data transformation, storage and retrieval, computing infrastructures, other types of mining and learning, presentation of explanations and predictions, and the exploitation of results taking into account ethical, social, legal, and business aspects. This paper provides an overview of the field of data science also showing the main developments, thereby focusing on (1) the growing importance of learning from data (rather than modeling or programming), (2) the transfer of tasks from humans to (software) robots, and (3) the risks associated with data science (e.g., privacy problems, unfair or nontransparent decision making, and the market dominance of a few platform providers). © 2020, IFIP International Federation for Information Processing.","Artificial Intelligence; Big data; Data science; Machine learning; Responsible data science","Data Science; Decision making; Digital storage; Machine learning; Privacy by design; Professional aspects; Robot programming; Business aspects; Computing infrastructures; Data exploration; Data transformation; Learning machines; Science revolution; Science technologies; Storage and retrievals; Metadata"
"Aarset M.V., Johannessen L.K.","On Distributed Cognition While Designing an AI System for Adapted Learning","10.3389/frai.2022.910630","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135239557&doi=10.3389%2ffrai.2022.910630&partnerID=40&md5=5008056f113bca0e5ed1c0274334123c","When analyzing learning, focus has traditionally been on the teacher, but has in the recent decades slightly moved toward the learner. This is also reflected when supporting systems, both computer-based and more practical equipment, has been introduced. Seeing learning as an integration of both an internal psychological process of acquisition and elaboration, and an external interaction process between the learner and the rest of the learning environment though, we see the necessity of expanding the vision and taking on a more holistic view to include the whole learning environment. Specially, when introducing an AI (artificial intelligence) system for adapting the learning process to an individual learner through machine learning, this AI system should take into account both the learner and the other agents and artifacts being part of this extended learning system. This paper outlines some lessons learned in a process of developing an electronic textbook adapting to a single learner through machine learning, to the process of extracting input from and providing feedback both to the learner, the teacher, the learning institution, and the learning resources provider based on a XAI (explainable artificial intelligence) system while also taking into account characteristics with respect to the learner's peers. Copyright © 2022 Aarset and Johannessen.","adaptive learning; artificial intelligence; distributed cognition and learning; distributed situational awareness; stochastic processes",
"Aas K., Jullum M., Løland A.","Explaining individual predictions when features are dependent: More accurate approximations to Shapley values","10.1016/j.artint.2021.103502","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104452069&doi=10.1016%2fj.artint.2021.103502&partnerID=40&md5=2b180d6d1243e9227bf8188756da65f8","Explaining complex or seemingly simple machine learning models is an important practical problem. We want to explain individual predictions from such models by learning simple, interpretable explanations. Shapley value is a game theoretic concept that can be used for this purpose. The Shapley value framework has a series of desirable theoretical properties, and can in principle handle any predictive model. Kernel SHAP is a computationally efficient approximation to Shapley values in higher dimensions. Like several other existing methods, this approach assumes that the features are independent. Since Shapley values currently suffer from inclusion of unrealistic data instances when features are correlated, the explanations may be very misleading. This is the case even if a simple linear model is used for predictions. In this paper, we extend the Kernel SHAP method to handle dependent features. We provide several examples of linear and non-linear models with various degrees of feature dependence, where our method gives more accurate approximations to the true Shapley values. © 2021 The Authors","Dependence; Feature attribution; Kernel SHAP; Shapley values","Forecasting; Game theory; Machinery; Predictive analytics; Computationally efficient; Higher dimensions; Individual prediction; Linear modeling; Machine learning models; Non-linear model; Practical problems; Predictive modeling; Learning systems"
"Aasmets O., Lüll K., Lang J.M., Pan C., Kuusisto J., Fischer K., Laakso M., Lusis A.J., Org E.","Machine learning reveals time-varying microbial predictors with complex effects on glucose regulation","10.1128/MSYSTEMS.01191-20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102535413&doi=10.1128%2fMSYSTEMS.01191-20&partnerID=40&md5=faad4a4fae74964484d4d268c731c33e","The incidence of type 2 diabetes (T2D) has been increasing globally, and a growing body of evidence links type 2 diabetes with altered microbiota composition. Type 2 diabetes is preceded by a long prediabetic state characterized by changes in various metabolic parameters. We tested whether the gut microbiome could have predictive potential for T2D development during the healthy and prediabetic disease stages. We used prospective data of 608 well-phenotyped Finnish men collected from the population-based Metabolic Syndrome in Men (METSIM) study to build machine learning models for predicting continuous glucose and insulin measures in a shorter (1.5year) and longer (4year) period. Our results show that the inclusion of the gut microbiome improves prediction accuracy for modeling T2D-associated parameters such as glycosylated hemoglobin and insulin measures. We identified novel microbial biomarkers and described their effects on the predictions using interpretable machine learning techniques, which revealed complex linear and nonlinear associations. Additionally, the modeling strategy carried out allowed us to compare the stability of model performance and biomarker selection, also revealing differences in short-term and long-term predictions. The identified microbiome biomarkers provide a predictive measure for various metabolic traits related to T2D, thus providing an additional parameter for personal risk assessment. Our work also highlights the need for robust modeling strategies and the value of interpretable machine learning. IMPORTANCE Recent studies have shown a clear link between gut microbiota and type 2 diabetes. However, current results are based on cross-sectional studies that aim to determine the microbial dysbiosis when the disease is already prevalent. In order to consider the microbiome as a factor in disease risk assessment, prospective studies are needed. Our study is the first study that assesses the gut microbiome as a predictive measure for several type 2 diabetes-associated parameters in a longitudinal study setting. Our results revealed a number of novel microbial biomarkers that can improve the prediction accuracy for continuous insulin measures and glycosylated hemoglobin levels. These results make the prospect of using the microbiome in personalized medicine promising. Copyright © 2021 Aasmets et al.","Gut microbiome; Gut microbiome; Machine learning; Prediction analysis; T2D; Type 2 diabetes","biological marker; double stranded DNA; glucose; hemoglobin A1c; insulin; adult; aged; Alistipes; Article; Asteroleplasma; Christensenellaceae; Clostridiales; comparative study; controlled study; cross-sectional study; diagnostic accuracy; diagnostic test accuracy study; glucose blood level; glycemic control; hemoglobin blood level; human; hypertension; impaired glucose tolerance; insulin blood level; intestine flora; longitudinal study; machine learning; major clinical study; male; Muribaculaceae; non insulin dependent diabetes mellitus; obesity; Paraprevotella; phenotype; predictive value; Prevotellaceae; prospective study; random forest; Rhodospirillales; Ruminiclostridium; Shuttleworthia; Subdoligranulum"
"Abacha A.B., Zweigenbaum P.","Medical question answering: Translating medical questions into SPARQL queries","10.1145/2110363.2110372","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857758460&doi=10.1145%2f2110363.2110372&partnerID=40&md5=448b796c0c246b4d48ac506c2714f59b","Designing question answering systems requires efficient and deep analysis of natural language questions. A key process for this task is to translate the semantic relations expressed in the question into a machine-readable representation. In this paper we tackle question analysis in the medical field. More precisely, we study how to translate a natural language question into a machine-readable representation. The underlying transformation process requires determin- ing three key points: (i) What are the main characteris- tics of medical questions? (ii) Which methods are the most fitted for the extraction of these characteristics? and (iii) how to translate the extracted information into a machine- understandable representation? We present a complete question analysis approach includ- ing medical entity recognition, semantic relation extraction and automatic translation to SPARQL queries. Our study supports the fact that SPARQL can represent a wide range of natural language questions in a question-answering per- spective. Experiments on a corpus of real questions show that we obtain encouraging results in medical entity recog- nition and relation extraction. The obtained results also show that the output SPARQL queries correctly represent more than 60% of the original questions. Copyright © 2012 ACM.","Information extraction; Machine learning; Medical question analysis; Question answering; RDF; SPARQL","Information Extraction; Machine-learning; Medical question analysis; Question Answering; RDF; SPARQL; Semantic Web; Semantics; Natural language processing systems"
"Abadía J.J.P., Fritz H., Dadoulis G., Dragos K., Smarsly K.","Automated decision making in structural health monitoring using explainable artificial intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134227634&partnerID=40&md5=56788345ff3823d918d79e042d04f0a0","The need for processing large amounts of data from modern structural health monitoring (SHM) systems has been fostering interdisciplinary SHM strategies employing artificial intelligence (AI) algorithms for detecting damage. However, the opacity of several AI algorithms hinders their widespread adoption in SHM practice. To enhance the trust of practitioners in AI algorithms, this paper proposes an explainable artificial intelligence (XAI) approach for SHM. The approach builds upon the capabilities of unsupervised learning algorithms for detecting outliers indicative of structural damage in structural response data. Moreover, features in the data governing outlier detection are “explained” to the user, thus ensuring transparency in decision making. The XAI-SHM approach is validated via simulations of a pedestrian bridge that may or may not include damage. Results show that the XAI-SHM approach is capable of distinguishing between damage and random fluctuations of structural properties, while decisions made by the XAI-SHM approach are clearly explained. © 2021 Universitätsverlag der Technischen Universität Berlin. All Rights Reserved.",,"Artificial intelligence; Damage detection; Decision making; Statistics; Artificial intelligence algorithms; Automated decision making; Large amounts of data; Monitoring approach; Monitoring strategy; Response data; Structural damages; Structural health monitoring systems; Structural response; Unsupervised learning algorithms; Structural health monitoring"
"Abakarim Y., Lahby M., Attioui A.","Towards An Efficient Real-time Approach to Loan Credit Approval Using Deep Learning","10.1109/ISIVC.2018.8709173","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065975861&doi=10.1109%2fISIVC.2018.8709173&partnerID=40&md5=9416bd8089fddbb8cedbdbfa44f187f8","The last decade has seen an important rise of data gathering, especially in the financial sectors. Banks are indeed one of the biggest producers of big data, as a matter of fact no other company than the bank has so much data gathered on its customers. Gathering and analyzing this data is a key feature for decision making, particularly in banking sector. One of the most important and frequent decision banks has to make, is loan approval. The challenge is to know how to build a proactive, powerful, responsible and ethical exploitation of personal data, to make loan applicant proposals more relevant and personalized. Machine learning is a promising solution to deal with this problem. Therefor, in the last years, many algorithms based on machine learning have been proposed to solve loan approval issue. However, these algorithms have not taken into consideration Real-time paradigm during processing. In this paper, we propose a Real-Time Binary classification model to deal with loan approval. Our proposed model is based on a deep neural network, and it permits to classify loan applicant as good or bad risk. Experimental results prove that our proposed Real-Time model, based on deep neural network, outperforms typical binary classifiers, in terms of precision recall and accuracy. © 2018 IEEE.","Deep Learning; Loan approval; Machine learning; Real-time data","Decision making; Deep learning; Deep neural networks; Learning systems; Technology transfer; Banking sectors; Binary classification; Binary classifiers; Data gathering; Financial sectors; Loan approval; Real time modeling; Real-time data; Machine learning"
"Abaker A.A., Saeed F.A.","A comparative analysis of machine learning algorithms to build a predictive model for detecting diabetes complications","10.31449/inf.v45i1.3111","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105116735&doi=10.31449%2finf.v45i1.3111&partnerID=40&md5=0055c3fce3c6e5274ed12bb7d6e5e07c","Diabetes complications have a significant impact on patients’ quality of life. The objective of this study was to predict which patients were more likely to be in a complicated health condition at the time of admission to allow for the early introduction of medical interventions. The data were 644 electronic health records from Alsukari Hospital collected from January 2018 to April 2019. We used the following machine learning methods: logistic regression, random forest, and k-nearest neighbor (KNN). The logistic regression algorithm performed better than the other algorithms achieving an accuracy of 81%, recall of 81%, and F1 score of 75%. Also, attributes such as infection years, swelling, diabetic ketoacidosis, and diabetic septic foot were significant in predicting diabetes complications. This model can be useful for the identification of patients requiring additional care to limit the complications and help practitioners in making decisions on whether the patient should be hospitalized or sent home. Furthermore, we used the sequential feature selection (SFS) algorithm which reduced the features to six, which is fewer than any model built before to predict diabetes complications. The primary goal of this study was achieved. The model had fewer attributes which means we have a simple and understandable model in addition to, it has a better performance. © 2021 Slovene Society Informatika. All rights reserved.","Diabetes complications; Electronic health records; Feature selection; Logistic regression; Machine learning","Decision trees; Forecasting; Ketones; Logistic regression; Machine learning; Nearest neighbor search; Patient treatment; Predictive analytics; Comparative analysis; Electronic health record; K nearest neighbor (KNN); Logistic regression algorithms; Machine learning methods; Medical intervention; Predictive modeling; Sequential feature selections; Learning algorithms"
"Abarca-Alvarez F.J., Fernandez-Avidad A.","Generation of downtown planning-ordinances using self organizing maps",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961377744&partnerID=40&md5=c4be2b9372e9a420f44278d6f821050b","Facing the stability of typical urban ordinances, which all too frequently become outdated at the moment of their formulation, a new method of work is currently being explored. This innovation is based on feedback from the normative body via the dynamic introduction of preexistences and transformations generated by the same framework as the ordinance. The Ordinance, as a link between the urban project and architecture, should compromise with the purely urban concepts as well as the purely architectural, establishing relationship or network pattern between them. It will be that definition of relational or network pattern that we will call Network- Ordinance. And its adaptation or approximation to the same ordinance, by means of the architectural-urban proposal, will be the normative framework generated from the global, in contrast to the particular and accidental. It will be, therefore, the search for coherencies formalized in pattern that come together and display the most representative ideals and values of the selected area. By means of the objective numeric representation of the values considered useful or representative, as well as those whose interests are unknown, and their application through an artificial neuronal network, with unsupervised and competitive learning of the same type as the SOM (Self- Organizing Map) and, more concretely, Kohonen's network, the underlying structure of the same data will be discovered and represented in a comprehensible manner. The results obtained are easily interpretable, permitting the recognition of the grouping, in the form of pattern, of the architectural objects represented, allowing the verification of the integration (or not) of a new building or object into those pattern, which conform to the Network-Ordinance and what we have come to call Ecotype. The new objects allowed by this rule will then form part of the same ordinance, becoming integrated in the body of a new neuronal network, and thus acquiring feedback from the ordinance. As a verification, it is proposed that a Network-Ordinance is created for a sector of the historical center of a city in the province of Granada, in order to compare the obtained results with the proposed Ordinance by the city planning.","Downtown; Ecotype; GIS-SOM; Network; Network-ordinance; Neural network; Ordinance; Pattern; Self organizing map; SOM; Typology","Artificial intelligence; Conformal mapping; Decision support systems; Networks (circuits); Neurons; Self organizing maps; Signal encoding; Downtown; Ecotype; Ordinance; Pattern; SOM; Typology; Neural networks"
"Abavisani M., Patel V.M.","Deep Sparse Representation-Based Classification","10.1109/LSP.2019.2913022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065600933&doi=10.1109%2fLSP.2019.2913022&partnerID=40&md5=e34d4506a04c469bdb2b254274c32585","We present a transductive deep learning-based formulation for the sparse representation-based classification (SRC) method. The proposed network consists of a convolutional autoencoder along with a fully connected layer. The role of the autoencoder network is to learn robust deep features for classification. On the other hand, the fully connected layer, which is placed in between the encoder and the decoder networks, is responsible for finding the sparse representation. The estimated sparse codes are then used for classification. Various experiments on three different datasets show that the proposed network leads to sparse representations that give better classification results than state-of-the-art SRC methods. The source code is available at: github.com/mahdiabavisani/DSRC. © 1994-2012 IEEE.","Deep learning; deep sparse representation-based classification; sparse representation-based classification","Classification (of information); Auto encoders; Classification results; Source codes; Sparse codes; Sparse representation; Sparse representation based classifications; State of the art; Deep learning"
"Abbas A., O’Byrne C., Fu D.J., Moraes G., Balaskas K., Struyven R., Beqiri S., Wagner S.K., Korot E., Keane P.A.","Evaluating an automated machine learning model that predicts visual acuity outcomes in patients with neovascular age-related macular degeneration","10.1007/s00417-021-05544-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124270619&doi=10.1007%2fs00417-021-05544-y&partnerID=40&md5=9f04a85fb6f5e06d4c94ceb79e987c6d","Purpose: Neovascular age-related macular degeneration (nAMD) is a major global cause of blindness. Whilst anti-vascular endothelial growth factor (anti-VEGF) treatment is effective, response varies considerably between individuals. Thus, patients face substantial uncertainty regarding their future ability to perform daily tasks. In this study, we evaluate the performance of an automated machine learning (AutoML) model which predicts visual acuity (VA) outcomes in patients receiving treatment for nAMD, in comparison to a manually coded model built using the same dataset. Furthermore, we evaluate model performance across ethnic groups and analyse how the models reach their predictions. Methods: Binary classification models were trained to predict whether patients’ VA would be ‘Above’ or ‘Below’ a score of 70 one year after initiating treatment, measured using the Early Treatment Diabetic Retinopathy Study (ETDRS) chart. The AutoML model was built using the Google Cloud Platform, whilst the bespoke model was trained using an XGBoost framework. Models were compared and analysed using the What-if Tool (WIT), a novel model-agnostic interpretability tool. Results: Our study included 1631 eyes from patients attending Moorfields Eye Hospital. The AutoML model (area under the curve [AUC], 0.849) achieved a highly similar performance to the XGBoost model (AUC, 0.847). Using the WIT, we found that the models over-predicted negative outcomes in Asian patients and performed worse in those with an ethnic category of Other. Baseline VA, age and ethnicity were the most important determinants of model predictions. Partial dependence plot analysis revealed a sigmoidal relationship between baseline VA and the probability of an outcome of ‘Above’. Conclusion: We have described and validated an AutoML-WIT pipeline which enables clinicians with minimal coding skills to match the performance of a state-of-the-art algorithm and obtain explainable predictions. © 2022, The Author(s).","Anti-VEGF; Artificial intelligence; Automated machine learning; Model interpretability; Neovascular age-related macular degeneration; OCT","aflibercept; ranibizumab; angiogenesis inhibitor; vasculotropin A; aged; area under the curve; Article; artificial intelligence; Asian; binary classification; controlled study; ethnicity; female; human; machine learning; major clinical study; male; outcome assessment; performance indicator; very elderly; visual acuity; wet macular degeneration; intravitreal drug administration; machine learning; macular degeneration; retrospective study; treatment outcome; visual acuity; wet macular degeneration; Angiogenesis Inhibitors; Humans; Intravitreal Injections; Machine Learning; Macular Degeneration; Ranibizumab; Retrospective Studies; Treatment Outcome; Vascular Endothelial Growth Factor A; Visual Acuity; Wet Macular Degeneration"
"Abbas A.K., Al-haideri N.A., Bashikh A.A.","Implementing artificial neural networks and support vector machines to predict lost circulation","10.1016/j.ejpe.2019.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068443808&doi=10.1016%2fj.ejpe.2019.06.006&partnerID=40&md5=76f152804a7793da887f20ff049bd701","Lost circulation is one of the major challenges encountered during drilling operations. The events related to the lost circulation can be responsible for losses of hundreds of millions of dollars each year. This paper presents a study on the application of artificial neural networks (ANNs) and support vector machine (SVM) to develop a robust system that can be used to predict the lost circulation occurrence. In the first step, field dataset, including drilling operation parameters, formation type, and lithology of the rock, as well as the drilling fluid characteristics, were collected from 385 wells drilled in southern Iraq from different fields. Then, the user-controlled parameters for ANNs (e.g., training function, number of hidden layers, transferring function, and number of neurons in each hidden layer) and SVMs (e.g., regularization factor, the type of kernel function, and its specific parameters) were optimized using the most common conventional performance criteria. Finally, the best-proposed models were examined using a few examples of real lost circulation cases from the field. The results of the analysis have revealed that both ANNs and SVM approaches can be of great use, with the SVM results being more promising. The application of the machine learning methods could assist drilling engineers in modifying drilling parameters to minimize the likelihood of lost circulation. © 2019 Egyptian Petroleum Research Institute","Artificial neural networks; Lost circulation; Support vector machine",
"Abbas A.K., Almubarak H., Abbas H., Dawood J.","Application of machine learning approach for intelligent prediction of pipe sticking","10.2118/197396-ms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086056467&doi=10.2118%2f197396-ms&partnerID=40&md5=359927110c0f1d7b3d1bc1d5393cd348","Stuck pipe has been recognized as one of the serious problems in drilling operations that has a significant impact on drilling efficiency and well costs. The events related to the stuck pipe can be responsible for losses of hundreds of millions of dollars each year in the drilling industry. This paper presents a study on the application of machine learning methodologies to predict the stuck pipe occurrence which can be utilized to modify drilling variables to minimize the likelihood of sticking. The new models were developed to predict the stuck pipe incidence for vertical and deviated wells using artificial neural networks (ANNs) and a support vector machine (SVM). The proposed models were examined using a few examples of real stick pipe cases from the field. The results of the analysis have revealed that both ANNs and SVM approaches can be of great use, with the SVM results being more promising. The present analysis supplies knowledge that can be used during well pre-planning and developmental phases to make informed decisions that will avoid pipe sticking problems and essentially optimize drilling performance. The risk of pipe sticking can then be minimized and the costs associated with its occurrence will be reduced. © 2019, Society of Petroleum Engineers",,"Forecasting; Gasoline; Infill drilling; Neural networks; Support vector machines; Deviated wells; Drilling efficiency; Drilling industry; Drilling operation; Drilling performance; Informed decision; Intelligent prediction; Machine learning approaches; Learning systems"
"Abbas A.N., Chasparis G.C., Kelleher J.D.","Interpretable Input-Output Hidden Markov Model-Based Deep Reinforcement Learning for the Predictive Maintenance of Turbofan Engines","10.1007/978-3-031-12670-3_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135914203&doi=10.1007%2f978-3-031-12670-3_12&partnerID=40&md5=cc778f82970c45c0204df7faa28ff947","An open research question in deep reinforcement learning is how to focus the policy learning of key decisions within a sparse domain. This paper emphasizes on combining the advantages of input-output hidden Markov models and reinforcement learning. We propose a novel hierarchical modeling methodology that, at a high level, detects and interprets the root cause of a failure as well as the health degradation of the turbofan engine, while at a low level, provides the optimal replacement policy. This approach outperforms baseline deep reinforcement learning (DRL) models and has performance comparable to that of a state-of-the-art reinforcement learning system while being more interpretable. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Deep Reinforcement Learning (DRL); Input-Output Hidden Markov Model (IOHMM); Interpretable AI; Predictive maintenance","Deep learning; Hidden Markov models; Learning systems; Turbofan engines; Deep reinforcement learning; Input-output hidden markov model; Input/output hidden markov models; Interpretable AI; Model learning; Model-based OPC; Policy learning; Predictive maintenance; Reinforcement learnings; Research questions; Reinforcement learning"
"Abbas F., Afzaal H., Farooque A.A., Tang S.","Crop yield prediction through proximal sensing and machine learning algorithms","10.3390/AGRONOMY10071046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092294715&doi=10.3390%2fAGRONOMY10071046&partnerID=40&md5=64d5806f8941785b15d197670ee4ac72","Proximal sensing techniques can potentially survey soil and crop variables responsible for variations in crop yield. The full potential of these precision agriculture technologies may be exploited in combination with innovative methods of data processing such as machine learning (ML) algorithms for the extraction of useful information responsible for controlling crop yield. Four ML algorithms, namely linear regression (LR), elastic net (EN), k-nearest neighbor (k-NN), and support vector regression (SVR), were used to predict potato (Solatium tuberosum) tuber yield from data of soil and crop properties collected through proximal sensing. Six fields in Atlantic Canada including three fields in Prince Edward Island (PE) and three fields in New Brunswick (NB) were sampled, over two (2017 and 2018) growing seasons, for soil electrical conductivity, soil moisture content, soil slope, normalized-difference vegetative index (NDVI), and soil chemistry. Data were collected from 39-40 30 x 30 m2 locations in each field, four times throughout the growing season, and yield samples were collected manually at the end of the growing season. Four datasets, namely PE-2017, PE-2018, NB-2017, and NB-2018, were then formed by combing data points from three fields to represent the province data for the respective years. Modeling techniques were employed to generate yield predictions assessed with different statistical parameters. The SVR models outperformed all other models for NB-2017, NB-2018, PE-2017, and PE-2018 dataset with RMSE of 5.97, 4.62, 6.60, and 6.17 t/ha, respectively. The performance of k-NN remained poor in three out of four datasets, namely NB-2017, NB-2018, and PE-2017 with RMSE of 6.93, 5.23, and 6.91 t/ha, respectively. The study also showed that large datasets are required to generate useful results using either model. This information is needed for creating site-specific management zones for potatoes, which form a significant component for food security initiatives across the globe. © 2020 MDPI AG. All rights reserved.","Elastic net; K-nearest neighbor; Precision agriculture; Support vector regression; Yield modeling",
"Abbasi Jannat Abadi E., Sahu H., Javadpour S.M., Goharimanesh M.","Interpretable machine learning for developing high-performance organic solar cells","10.1016/j.mtener.2022.100969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126981469&doi=10.1016%2fj.mtener.2022.100969&partnerID=40&md5=e489d7664a7fb1c1a0dd92b2bbdd66cd","Rapidly screening the underlying relationships between organic photovoltaics (OPVs) and their chemical structures remains an open challenge due to their complex interconnectivity. In this study, a new methodology for structure-property mappings of OPVs and device performances prediction is designed by combining the machine learning (ML) approach with the Taguchi Design of Experiments (TDOE). The established structure-property relationships are built up with the ML models from 240 data points of small molecule OPV systems and ten important microscopic features of OPVs. The quite remarkable performance of the ML model (Pearson's coefficient = 0.79) depicts its ability to extract hidden physical principles of OPVs. The TDOE model shows that molecular orbitals other than the highest and the lowest ones that are not frequently considered in the designing process of OPVs play quite essential roles in developing promising OPV materials. Moreover, strategies to boost the design of high-performing devices with different values of the considered features are also extracted from the model with the DOE approach. These results reveal that ML combined with DOE is an impressive package for guiding the design process effectively and efficiently. © 2022 Elsevier Ltd","Machine learning; Optimization; Organic photovoltaics; Signal to Noise Ratio; Taguchi design of experiments","Design of experiments; Molecular orbitals; Organic solar cells; Signal to noise ratio; Solar power generation; Interconnectivity; Machine learning models; Optimisations; Organic devices; Organic photovoltaics; Performance; Photovoltaic performance; Properties mappings; Structure property; Taguchi design of experiment; Machine learning"
"Abbasi T., Abbasi T., Luithui C., Abbasi S.A.","Modelling methane and nitrous oxide emissions from rice paddy wetlands in India using Artificial Neural Networks (ANNs)","10.3390/w11102169","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074361567&doi=10.3390%2fw11102169&partnerID=40&md5=172e4f85d17e07dcb173ad65381bfd41","Paddy fields, which are shallow man-made wetlands, are estimated to be responsible for ~11% of the total methane emissions attributed to anthropogenic sources. The role of water use in driving these emissions, and the apportioning of the emissions to individual countries engaged in paddy cultivation, are aspects that have been mired in controversy and disagreement. This is largely due to the fact that methane (CH4) emissions not only change with the cultivar type but also regions, climate, soil type, soil conditions, manner of irrigation, type and quantity of fertilizer added-to name a few. The factors which can influence these aspects also encompass a wide range, and have origins in causes which can be physical, chemical, biological, and combinations of these. Exceedingly complex feedback mechanisms, exerting different magnitudes and types of influences on CH4 emissions under different conditions, are operative. Similar is the case of nitrous oxide (N2O); indeed, the present level of understanding of the factors which influence the quantum of its emission is still more patchy. This makes it difficult to even understand precisely the role of the myriad factors, less so model them. The challenge is made even more daunting by the fact that accurate and precise data on most of these aspects is lacking. This makes it nearly impossible to develop analytical models linking causes with effects vis a vis CH4 and N2O emissions from paddy fields. For situations like this the bioinspired artificial intelligence technique of artificial neural network (ANN), which can model a phenomenon on the basis of past data and without the explicit understanding of the mechanism phenomena, may prove useful. However, no such model for CH4 or N2O has been developed so far. Hence the present work was undertaken. It describes ANN-based models developed by us to predict CH4 and N2O emissions using soil characteristics, fertilizer inputs, and rice cultivar yield as inputs. Upon testing the predictive ability of the models with sets of data not used in model development, it was seen that there was excellent agreement between model forecasts and experimental findings, leading to correlations coefficients of 0.991 and 0.96, and root mean square error (RMSE) of 11.17 and 261.3, respectively, for CH4 and N2O emissions. Thus, the models can be used to estimate CH4 and N2O emissions from all those continuously flooded paddy wetlands for which data on total organic carbon, soil electrical conductivity, applied nitrogen, phosphorous and potassium, NPK, and grain yield is available. © 2019 by the authors.","Artificial neural networks; Greenhouse gas emissions; Methane; Nitrous oxide; Oryza sativa; Paddy; Rice","Ability testing; Climate change; Fertilizers; Greenhouse gases; Mean square error; Methane; Neural networks; Nitrogen oxides; Organic carbon; Soils; Wetlands; Artificial intelligence techniques; Nitrous oxide; Nitrous oxide emissions; Oryza sativa; Paddy; Rice; Root mean square errors; Soil electrical conductivity; Gas emissions; agricultural emission; agricultural land; anthropogenic source; artificial intelligence; artificial neural network; greenhouse gas; methane; nitrous oxide; rice; source apportionment; water use; Oryza sativa"
"Abbasi W.A., Minhas F.U.A.A.","Issues in performance evaluation for host-pathogen protein interaction prediction","10.1142/S0219720016500116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960194655&doi=10.1142%2fS0219720016500116&partnerID=40&md5=41da0fa73a040c75e4b14861c1deba7b","The study of interactions between host and pathogen proteins is important for understanding the underlying mechanisms of infectious diseases and for developing novel therapeutic solutions. Wet-lab techniques for detecting protein-protein interactions (PPIs) can benefit from computational predictions. Machine learning is one of the computational approaches that can assist biologists by predicting promising PPIs. A number of machine learning based methods for predicting host-pathogen interactions (HPI) have been proposed in the literature. The techniques used for assessing the accuracy of such predictors are of critical importance in this domain. In this paper, we question the effectiveness of K-fold cross-validation for estimating the generalization ability of HPI prediction for proteins with no known interactions. K-fold cross-validation does not model this scenario, and we demonstrate a sizable difference between its performance and the performance of an alternative evaluation scheme called leave one pathogen protein out (LOPO) cross-validation. LOPO is more effective in modeling the real world use of HPI predictors, specifically for cases in which no information about the interacting partners of a pathogen protein is available during training. We also point out that currently used metrics such as areas under the precision-recall or receiver operating characteristic curves are not intuitive to biologists and propose simpler and more directly interpretable metrics for this purpose. © 2016 World Scientific Publishing Europe Ltd.","cross-validation; host-pathogen interactions; machine learning; Performance evaluation; protein-protein interactions","Human immunodeficiency virus protein; viral protein; Adenoviridae; area under the curve; host pathogen interaction; human; machine learning; metabolism; molecular evolution; pathogenicity; procedures; protein analysis; protein database; Adenoviridae; Area Under Curve; Databases, Protein; Evolution, Molecular; Host-Pathogen Interactions; Human Immunodeficiency Virus Proteins; Humans; Machine Learning; Protein Interaction Mapping; Viral Proteins"
"Abbass H.A., Hunjet R.A.","Smart Shepherding: Towards Transparent Artificial Intelligence Enabled Human-Swarm Teams","10.1007/978-3-030-60898-9_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135567908&doi=10.1007%2f978-3-030-60898-9_1&partnerID=40&md5=d969d8707f3ca6c48489a8a00e467a38","The aim of this chapter is to uncover the beauty and complexity in the world of shepherding as we view it through the lens of Artificial Intelligence (AI) and Autonomous Systems (AS). In the pursuit of imitating human intelligence, AI researchers have made significant and vast contributions over decades. Yet even with such interest and activity from within industry and the academic community, general AI remains out of our reach. By comparison, this book aims for a less ambitious goal in trying to recreate the intelligence of a sheepdog. As our efforts display, even with this seemingly modest goal, there is a plethora of research opportunities where AI and AS still have a long way to go. Let us start this journey by asking the basic questions: what is shepherding and what makes shepherding an interesting problem? How does one design a smart shepherd for swarm guidance? What AI algorithms are required and how are they organised in a cognitive architecture to enable a smart shepherd? How does one design transparent AI for smart shepherding? © 2021, Springer Nature Switzerland AG.","Explainable artificial intelligence; Interpretable artificial intelligence; Shepherding; Swarm control; Swarm guidance; Swarm ontology; Swarm tactics; Transparent artificial intelligence",
"Abbass H.A., Elsawah S., Petraki E., Hunjet R.","Machine Education: Designing semantically ordered and ontologically guided modular neural networks","10.1109/SSCI44817.2019.9003083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080908160&doi=10.1109%2fSSCI44817.2019.9003083&partnerID=40&md5=f1d896a2f9129a8c8acb8a9e921a844c","The literature on machine teaching, machine education, and curriculum design for machines is in its infancy with sparse papers on the topic primarily focusing on data and model engineering factors to improve machine learning. In this paper, we first discuss selected attempts to date on machine teaching and education. We then bring theories and methodologies together from human education to structure and mathematically define the core problems in lesson design for machine education and the modelling approaches required to support the steps for machine education. Last, but not least, we offer an ontology-based methodology to guide the development of lesson plans to produce transparent and explainable modular learning machines, including neural networks. © 2019 IEEE.","curriculum design; machine education; machine teaching; transparent and explainable modular neural networks.","Computation theory; Curricula; Machine learning; Ontology; Core problems; Curriculum designs; Lesson plans; Model engineering; Modular learning; Modular neural networks; On-machines; Ontology-based; Semantic Web"
"Abbassi H., Monsefi R., Sadoghi Yazdi H.","Constraint excluded classifier","10.1109/AISP.2012.6313707","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869156224&doi=10.1109%2fAISP.2012.6313707&partnerID=40&md5=32dd0db31208a97f1b268f43fc6de160","Linear classifiers have the generalization property while lacking the power of classifying complex patterns. A simple and effective idea is to somehow exclude the complexity of the data such that it can be classified using a linear classifier. In this paper a new classifier system called Constraint Excluded Classifier is proposed that classifies most of the input patterns using a simple, e.g., a linear classifier. The classification is composed of an iterative three step loop. In the Construction step, several sub-classifiers are constructed which are responsible for linearly classifying parts of input patterns. Sub-classifiers are merged together in the Fusion step. The Evaluation step tests and fine tunes the construction of sub-classifiers. The comparison of the new classifier with famous classifiers is also presented. © 2012 IEEE.","Classifier Boosting; Constraint Classification; Linear Classification; Multiple Classifier System","Classifier systems; Complex pattern; Constraint classification; Construction steps; Generalization properties; Input patterns; Linear classification; Linear classifiers; Multiple classifier systems; Step test; Signal processing; Artificial intelligence"
"Abbona F., Vanneschi L., Giacobini M.","Towards a Vectorial Approach to Predict Beef Farm Performance","10.3390/app12031137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123094159&doi=10.3390%2fapp12031137&partnerID=40&md5=4dbe2fc14e609f8ec7c6c3eae1cb9b07","Accurate livestock management can be achieved by means of predictive models. Critical factors affecting the welfare of intensive beef cattle husbandry systems can be difficult to be detected, and Machine Learning appears as a promising approach to investigate the hundreds of variables and temporal patterns lying in the data. In this article, we explore the use of Genetic Programming (GP) to build a predictive model for the performance of Piemontese beef cattle farms. In particular, we investigate the use of vectorial GP, a recently developed variant of GP, that is particularly suitable to manage data in a vectorial form. The experiments conducted on the data from 2014 to 2018 confirm that vectorial GP can outperform not only the standard version of GP but also a number of state-of-the-art Machine Learning methods, such as k-Nearest Neighbors, Generalized Linear Models, feed-forward Neural Networks, and long-and short-term memory Recurrent Neural Networks, both in terms of accuracy and generalizability. Moreover, the intrinsic ability of GP in performing an automatic feature selection, while generating interpretable predictive models, allows highlighting the main elements influencing the breeding performance. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Cattle breeding; Evolutionary algorithms; Genetic programming; Machine learning; Piemontese bovines; Precision livestock farming; Vector-based representation",
"Abbood S.H., Abdull Hamed H.N., Mohd Rahim M.S.","Automatic Classification of Diabetic Retinopathy Through Segmentation Using CNN","10.1007/978-3-030-99197-5_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127904745&doi=10.1007%2f978-3-030-99197-5_9&partnerID=40&md5=428f6216252dc5b2738a094446e8e834","The process division of Diabetes Retinopathy (DR) has been considered as a significant step in diabetic retinopathy assessment and treatment. Different levels of microstructures like microaneurysm, rough exudates as well as neovascularization could take place on the retina area due to disruption to the retinal blood vessels triggered by elevated blood glucose levels. This is one of the primary causes of the prevalent visual impairment/blindness due to diabetes. Image segmentation, region merging, and Convolutional Neural Network (CNN) used in the paper for automated classification of high-resolution photographs of the retinal fundus in five stages of the DR. High heterogeneity is a significant problem for fundus image recognition for diabetic retinopathy, whereby new blood vessel proliferation including retinal detachment occurs. Therefore, careful examination of the retinal vessels is important to obtain accurate results which, through retinal segmentation could be achieved. We also highlight the difficulties in the development and learning of powerful, efficient, and reliable deep learning models for different DR diagnostic problems. The system was able to classify various DR stages with an average accuracy of around 94.2%, a sensitivity of 97%, and a specificity of 96%. There appears to be a genuine necessity for a steady interpretable classification system for DR and diabetic macular edema supported with solid confirmation. The suggested interpretable categorization systems allow diabetic retinopathy and macular edema to be properly classified. These technologies are expected to be beneficial in increasing diabetes screening and communication and discussion among those who care for these patients. © 2022, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.","Artificial intelligence; Computer vision; Deep learning; Diabetic retinopathy; Image classification","Blood; Blood vessels; Computer vision; Convolutional neural networks; Deep learning; Diagnosis; Image classification; Image recognition; Image segmentation; Ophthalmology; Automatic classification; Blood glucose level; Convolutional neural network; Deep learning; Diabetic retinopathy; Images classification; Macular edema; Microaneurysms; Neo-vascularization; Retinal blood vessels; Eye protection"
"Abbott R.","The reasonable computer: Disrupting the paradigm of tort liability",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046156385&partnerID=40&md5=77e463a74c2846ad085d14a0fd2bab98","Artificial intelligence is part of our daily lives. Whether working as chauffeurs, accountants, or police, computers are taking over a growing number of tasks once performed by people. As this occurs, computers will also cause the injuries inevitably associated with these activities. Accidents happen, and now computer-generated accidents happen. The recent fatality involving Tesla's autonomous driving software is just one example in a long series of ""computergenerated torts."" Yet hysteria over such injuries is misplaced. In fact, machines are, or at least have the potential to be, substantially safer than people. Self-driving cars will cause accidents, but they will cause fewer accidents than human drivers. Because automation will result in substantial safety benefits, tort law should encourage its adoption as a means of accident prevention. Under current legal frameworks, suppliers of computer tortfeasors are likely strictly responsible for their harms. This Article argues that where a supplier can show that an autonomous computer, robot, or machine is safer than a reasonable person, the supplier should be liable in negligence rather than strict liability. The negligence test would focus on the computer's act instead of its design, and in a sense, it would treat a computer tortfeasor as a person rather than a product. Negligence-based liability would incentivize automation when doing so would reduce accidents, and it would continue to reward suppliers for improving safety. More importantly, principles of harm avoidance suggest that once computers become safer than people, human tortfeasors should no longer be measured against the standard of the hypothetical reasonable person that has been employed for hundreds of years. Rather, individuals should be judged against computers. To appropriate the immortal words of Justice Holmes, we are all ""hasty and awkward"" compared to the reasonable computer. © 2018 George Washington University. All Rights Reserved.",,
"Abboud A., Jaber A.H., Cances J.-P., Meghdadi V.","Indoor Massive MIMO: Uplink Pilot Mitigation Using Channel State Information Map","10.1109/CSE-EUC-DCABES.2016.223","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026631088&doi=10.1109%2fCSE-EUC-DCABES.2016.223&partnerID=40&md5=df91909e2c5a7dc308d3270af62d2911","Massive MIMO brings both motivations and challenges to develop the 5th generation mobile wireless technology. The promising number of users and the high bitrate offered per unit area are challenged by uplink pilot contamination due to pilot reuse and a limited number of orthogonal pilot sequences. This paper proposes a solution to mitigate uplink pilot contamination in an indoor scenario where multi-cell share the same pool of pilot sequences, that are supposed to be less than the number of users. This can be done by reducing uplink pilots using Channel State Information (CSI) prediction. The proposed method is based on machine learning approach, where a quantized version of Channel State Information (QCSI) is learned during estimation session and stored at the Base Station (BS) to be exploited for future CSI prediction. The learned QCSI are represented by a weighted directed graph, which is responsible to monitor and predict the CSI of User Terminals (UTs) in the local cell. We introduce an online learning algorithm to create and update this graph which we call CSI map. Simulation results show an increase in the downlink sum-rate and a significant feedback reduction. © 2016 IEEE.","Channel State Information Map; Machine Learning; Massive MIMO; Pilot Contamination","Artificial intelligence; Communication channels (information theory); Directed graphs; Distributed computer systems; Forecasting; Learning algorithms; Learning systems; MIMO systems; Ubiquitous computing; Wireless telecommunication systems; Csi predictions; Mobile wireless; Online learning algorithms; Pilot contaminations; Pilot sequences; Quantized version; User terminals; Weighted directed graph; Channel state information"
"Abburu S., Golla S.B.","Ontology and NLP support for building disaster knowledge base","10.1109/CESYS.2017.8321236","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047146733&doi=10.1109%2fCESYS.2017.8321236&partnerID=40&md5=a612de972b5dd2d264dc29a664ef5daa","The data from pre, post and during disasters play a very important and vital role while immediate decision making in emergency management. Data availability, analysis, and exchange build up a critical knowledge base that helps in taking the right decision at the right time. Voluminous data is available through various sources in heterogeneous formats. It is a very big challenge for utilization of such massive heterogeneous data in disaster management situations. Disaster data is widely available in various documents and formats. Past disasters, anticipated disasters and during the disasters, a large amount of data is getting exchanged in various documents between different agencies. Effective techniques to extract and gather the data from heterogeneous documents helps in right data extraction and build up a knowledge base to formulate ease of analysis and modeling. Ontology based semantic technology is a powerful mechanism that supports information integration, exchange, share, reuse and build a knowledge base from heterogeneous information sources. The current research work focuses on three aspects of knowledge base system for effective disaster management. The current research work implemented a mechanism to extract relevant information from semi-structured and structured heterogeneous documents using NLP, representing extracted information in homogeneous and machine understandable format using RDF and map the RDF triples to appropriate concepts of the disaster management domain ontologies. This is very novel and effective means of data usage mechanism to help and guide policy makers, disaster mitigation agencies and society at large. © 2017 IEEE.","Disaster Management; Information Extraction; Information Integration; Ontology; RDF; Semantic Query and Visualization; Semantic Technology","Decision making; Disaster prevention; Disasters; Information retrieval; Knowledge based systems; Knowledge management; Natural language processing systems; Ontology; Risk management; Semantic Web; Semantics; Analysis and modeling; Disaster management; Heterogeneous documents; Heterogeneous information sources; Information integration; Knowledge base system; Semantic query; Semantic technologies; Data mining"
"Abd El Hamid M.M., Omar Y.M.K., Shaheen M., Mabrouk M.S.","Discovering epistasis interactions in Alzheimer's disease using deep learning model","10.1016/j.genrep.2022.101673","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137694006&doi=10.1016%2fj.genrep.2022.101673&partnerID=40&md5=f905079bb0eac83625c71f5d4d186f79","Alzheimer's disease (AD) is the most common form of dementia. Single Nucleotide Polymorphisms (SNPs) are single nucleotide alterations that can be used as genomic markers disclosing susceptibility to complex diseases like AD. Epistasis has long been significant for recognizing the function of genetic pathways and the evolutionary dynamics of difficult genetic systems. Discovering epistasis interactions holds a vital key to personalized medicine (PM). PM needs a better understanding of the relationship between human genetic data and complex diseases. In this proposed work, a deep neural network (DNN) is applied using SHapley Additive exPlanations (SHAP) to get top 20, 100, 300, and 500 ranking SNPs responsible for AD risk through epistasis interactions. Multi-locus interaction analysis is performed on these identified SNPs using Multifactor Dimensionality Reduction (MDR). This constructive induction algorithm is integrated with DNN for discovering epistasis interactions in a computationally effective method. The proposed framework is applied to Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. The best accuracies are achieved using the top 500 SNPs, and the classification accuracies varied between 0.860 and 0.874 in the five-way interaction model. However, the classification accuracies of 2-way, 3-way, 4-way models varied between 0.663 and 0.670, 0.718 and 0.727, and 0.793 and 0.803, respectively. The results revealed that the reported accuracy scores of the proposed framework outperform the referenced literature work. The proposed framework presents high-ranked risk genes and promising epistasis interactions that may help in explaining the risk of AD. © 2022 Elsevier Inc.","Alzheimer's disease; Deep learning model; Epistasis interactions; Personalized medicine; SHAP","Alzheimer disease; Article; classification algorithm; controlled study; deep learning; deep neural network; diagnostic accuracy; DNA sequence; epistasis; genetic association; genetic predisposition; genetic profile; genetic variability; human; multifactor dimensionality reduction; neuroimaging; personalized medicine; phenotypic variation; risk assessment; sensitivity and specificity; single nucleotide polymorphism"
"Abdalla M., Abdalla M.","The Grey Hoodie Project: Big Tobacco, Big Tech, and the Threat on Academic Integrity","10.1145/3461702.3462563","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112448575&doi=10.1145%2f3461702.3462563&partnerID=40&md5=3a0aab6062d83e20432877651b5ec457","As governmental bodies rely on academics' expert advice to shape policy regarding Artificial Intelligence, it is important that these academics not have conflicts of interests that may cloud or bias their judgement. Our work explores how Big Tech can actively distort the academic landscape to suit its needs. By comparing the well-studied actions of another industry (Big Tobacco) to the current actions of Big Tech we see similar strategies employed by both industries. These strategies enable either industry to sway and influence academic and public discourse. We examine the funding of academic research as a tool used by Big Tech to put forward a socially responsible public image, influence events hosted by and decisions made by funded universities, influence the research questions and plans of individual scientists, and discover receptive academics who can be leveraged. We demonstrate how Big Tech can affect academia from the institutional level down to individual researchers. Thus, we believe that it is vital, particularly for universities and other institutions of higher learning, to discuss the appropriateness and the tradeoffs of accepting funding from Big Tech, and what limitations or conditions should be put in place. © 2021 ACM.","conflicts of interest; research funding","Philosophical aspects; Tobacco; Academic integrity; Academic research; Conflicts of interest; Expert advice; Higher learning; Public image; Research questions; Artificial intelligence"
"Abdallah A.A.","Designing of Artificial Intelligence Methods for Manufacturing Industries Quality Control","10.6688/JISE.202111_37(6).0005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122295648&doi=10.6688%2fJISE.202111_37%286%29.0005&partnerID=40&md5=5d12681820a755cf6dfb68faf856ac0a","There is good amount of potential for implementing ANN tool for quality control in manufacturing. In any manufacturing process, there are many input parameters, which are responsible for introduction of variability in the product. If this exceeds acceptance limits specified in the drawing, the component is rejected. However, within acceptance limit guided by the dimensions, tolerances, surface finish or other specifications the variability exists, governing normal distribution. The objective of proposed work is to study such process variables, identify their extent of contribution in product acceptance, Design & Develop ANN network model, for such application. Train the network by using training sets defined by domain expert. Validate the results and compare the same by using suitable statistical tools and analysis of the findings for such typical application. Selection of machining parameters is an important task for a specific component. A process engineer (domain expert) who traditionally perform this task manually applies the knowledge that he acquired by learning the mapping between input patterns, consisting of feature being machined (Such as hole, external step) and attributes like (size, tolerance, surface finish etc.) of the part and output pattern, consisting of machining operations to apply to these parts. © 2021 Institute of Information Science. All rights reserved.","Artificial intelligence; Manufacturing; Neural networks; Orthogonal arrays; Quality control","Artificial intelligence; Finishing; Machining centers; Normal distribution; Product design; Quality assurance; Statistical mechanics; Acceptance limits; Artificial intelligence methods; Dimension tolerances; Domain experts; Input parameter; Manufacturing industries; Manufacturing process; Neural-networks; Orthogonal array; Surface finishes; Quality control"
"Abdallah W., Kanzari D., Sallami D., Madani K., Ghedira K.","A deep reinforcement learning based decision-making approach for avoiding crowd situation within the case of Covid'19 pandemic","10.1111/coin.12516","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126049215&doi=10.1111%2fcoin.12516&partnerID=40&md5=7639abd676d8e8d16fea9e1eb4d9bb5e","Individuals' flow's fluidifcation in the same way as the thinning of the population's concentration remains among major concerns within the context of the pandemic crisis situations. The recent COVID-19 pandemic crisis is a typical example of the aforementioned where on despite of the containment phases that radically isolate the population but are not applicable persistently, people have to adapt their behavior to new daily-life situations tempering Individuals' stream, avoiding tides, and watering down population's concentration. Crowd evacuation is one of the well-known research domains that can play a pertinent role to face the challenge of the COVID-19 pandemic. In fact, considering the population's concentration thinning within the slant of the “crowd evacuation” paradigm allows managing the flow of the population, and consequently, decreasing the probable number of infected cases. In other words, crowd evacuation modeling and simulation with the aim of better-exploiting individuals' flow allow the study and analysis of different possible outcomes for designing population's concentration thinning strategies. In this article, a new decision-making approach is proposed in order to cope with the aforesaid challenges, which relies on an independent Deep Q Network with an improved SIR model (IDQN-I-SIR). The machine-learning component (i.e., IDQN) is in charge of the agent's movements control and I-SIR (improved “susceptible-infected-recovered” individuals) model is responsible to control the virus spread. We demonstrate the effectiveness of IDQN-I-SIR through a case-study of individuals' flow's management with infected cases' avoidance in an emergency department (often overcrowded in context of a pandemic crisis). © 2022 Wiley Periodicals LLC.","COVID'19; crowd situation; decision making; deep reinforcement learning; multiagent reinforcement learning; SIR model","Behavioral research; Deep learning; Reinforcement learning; Viruses; COVID'19; Crises situations; Crowd evacuation; Crowd situation; Daily-life situations; Decisions makings; Multi-agent reinforcement learning; Research domains; SIR model; Thinnings; Decision making"
"Abdar M., Zomorodi-Moghadam M., Zhou X.","An Ensemble-Based Decision Tree Approach for Educational Data Mining","10.1109/BESC.2018.8697318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065228049&doi=10.1109%2fBESC.2018.8697318&partnerID=40&md5=dc57a76aadc868b5b3eccd91cd0cf7dc","Nowadays, data mining and machine learning techniques are applied to a variety of different topics (e. g., healthcare and disease, security, decision support, sentiment analysis, education, etc.). Educational data mining investigates the performance of students and gives solutions to enhance the quality of education. The aim of this study is to use different data mining and machine learning algorithms on actual data sets related to students. To this end, we apply two decision tree methods. The methods can create several simple and understandable rules. Moreover, the performance of a decision tree is optimized by using an ensemble technique named Rotation Forest algorithm. Our findings indicate that the Rotation Forest algorithm can enhance the performance of decision trees in terms of different metrics. In addition, we found that the size of tree generated by decision trees ensemble were bigger than simple ones. This means that the proposed methodology can reveal more information concerning simple rules. © 2018 IEEE.","Data mining; Decision tree; Educational data mining; Ensemble techninuqe; Rotaion forest algorithm","Decision support systems; Decision trees; Forestry; Learning algorithms; Machine learning; Sentiment analysis; Students; Decision supports; Decision tree method; Educational data mining; Ensemble techninuqe; Ensemble techniques; Information concerning; Machine learning techniques; Quality of education; Data mining"
"Abdar M., Yen N.Y.","Understanding regional characteristics through crowd preference and confidence mining in P2P accommodation rental service","10.1108/LHT-01-2017-0030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044320832&doi=10.1108%2fLHT-01-2017-0030&partnerID=40&md5=f56e326d94257255e510656aa31ce596","Purpose: This research intends to look at the regional characteristics through an analysis of crowd preference and confidence, and investigates how regional characteristics are going to affect human beings at all aspects in a scenario of sharing economy. The purpose of this paper is to introduce an approach to provide an understandable rating score. Furthermore, the paper aims to find the relationships between different features classified in this study by using machine learning methods. Furthermore, due to the importance of performance of methods, the performance of the features is also improved. Design/methodology/approach: The Rating Matching Rate (RMRate) approach is proposed to provide score in terms of simplicity and understandability for all features. The relationships between features can be extracted from accommodation data set using decision tree (DT) algorithms (J48, HoeffdingTree, and REPTree). Usability of these methods was evaluated using different metrics. Two techniques, “ClassBalancer” and “SpreadSubsample,” are applied to improve the performance of algorithms. Findings: Experimental outcomes using the RMRate approach show that the scores are very easy to understand. Three property types are very popular almost in all of selected countries in this study (“apartment”, “house”, and “bed and breakfast”). The findings also indicate that “Entire home/apt” is the most common room-type and 4.5 and 5 star-rating are the most given star-rating by users. The proposed DT algorithms can find the relationships between features significantly. In addition, applied CB and SS techniques could improve the performance of algorithms efficiently. Originality/value: This study gives precise details about the guests’ preferences and hosts’ preferences. The proposed techniques can effectively improve the performance in predicting the behavior of users in sharing economy. The findings can also help group decision making in P2P platforms efficiently. © 2017, Emerald Publishing Limited.","Airbnb; Decision tree; Feature discovery; Optimization; Rating Matching Rate (RMRate); Sharing economy",
"Abdar M., Kalhori S.R.N., Sutikno T., Subroto I.M.I., Arji G.","Comparing performance of data mining algorithms in prediction heart diseses","10.11591/ijece.v5i6.pp1569-1576","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947447863&doi=10.11591%2fijece.v5i6.pp1569-1576&partnerID=40&md5=2bedb5fb4b26cf8c5c8a6e92def0ae23","Heart diseases are among the nation's leading couse of mortality and moribidity. Data mining teqniques can predict the likelihood of patients getting a heart disease. The purpose of this study is comparison of different data mining algorithm on prediction of heart diseases. This work applied and compared data mining techniques to predict the risk of heart diseases.After feature analysis, models by six algorithms including decision tree, neural network, support vector machine and k-nearest neighborhood developed and validated. C5.0 Decision tree has been able to build a model with greatest accuracy 93.02%, KNN, SVM, Neural network have been 88.37%, 86.05% and 80.23% respectively. Produced results of decision tree can be simply interpretable and applicable; their rules can be understood easily by different clinical practitioner. © 2015 Institute of Advanced Engineering and Science. All rights reserved.","C5.0 Algorithm; Data Mining; Heart Disease; Neural Network",
"Abdelaal T., van Unen V., Höllt T., Koning F., Reinders M.J.T., Mahfouz A.","Predicting Cell Populations in Single Cell Mass Cytometry Data","10.1002/cyto.a.23738","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062964563&doi=10.1002%2fcyto.a.23738&partnerID=40&md5=54e3fcdd789c9a162b1b31c00c3e9110","Mass cytometry by time-of-flight (CyTOF) is a valuable technology for high-dimensional analysis at the single cell level. Identification of different cell populations is an important task during the data analysis. Many clustering tools can perform this task, which is essential to identify “new” cell populations in explorative experiments. However, relying on clustering is laborious since it often involves manual annotation, which significantly limits the reproducibility of identifying cell-populations across different samples. The latter is particularly important in studies comparing different conditions, for example in cohort studies. Learning cell populations from an annotated set of cells solves these problems. However, currently available methods for automatic cell population identification are either complex, dependent on prior biological knowledge about the populations during the learning process, or can only identify canonical cell populations. We propose to use a linear discriminant analysis (LDA) classifier to automatically identify cell populations in CyTOF data. LDA outperforms two state-of-the-art algorithms on four benchmark datasets. Compared to more complex classifiers, LDA has substantial advantages with respect to the interpretable performance, reproducibility, and scalability to larger datasets with deeper annotations. We apply LDA to a dataset of ~3.5 million cells representing 57 cell populations in the Human Mucosal Immune System. LDA has high performance on abundant cell populations as well as the majority of rare cell populations, and provides accurate estimates of cell population frequencies. Further incorporating a rejection option, based on the estimated posterior probabilities, allows LDA to identify previously unknown (new) cell populations that were not encountered during training. Altogether, reproducible prediction of cell population compositions using LDA opens up possibilities to analyze large cohort studies based on CyTOF data. © 2019 The Authors. Cytometry Part A published by Wiley Periodicals, Inc. on behalf of International Society for Advancement of Cytometry. © 2019 The Authors. Cytometry Part A published by Wiley Periodicals, Inc. on behalf of International Society for Advancement of Cytometry.","cell population prediction; machine learning; mass cytometry; single cell","algorithm; animal; bone marrow cell; classification; cluster analysis; cytology; flow cytometry; human; information processing; metabolism; mouse; procedures; reproducibility; single cell analysis; Algorithms; Animals; Bone Marrow Cells; Cluster Analysis; Datasets as Topic; Flow Cytometry; Humans; Mice; Reproducibility of Results; Single-Cell Analysis"
"Abdel-Basset M., Hawash H., Moustafa N., Elkomy O.M.","Two-Stage Deep Learning Framework for Discrimination between COVID-19 and Community-Acquired Pneumonia from Chest CT scans","10.1016/j.patrec.2021.10.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118559761&doi=10.1016%2fj.patrec.2021.10.027&partnerID=40&md5=e8596d04a16da5ec80fad7c3b2c13dac","COVID-19 stay threatening the health infrastructure worldwide. Computed tomography (CT) was demonstrated as an informative tool for the recognition, quantification, and diagnosis of this kind of disease. It is urgent to design efficient deep learning (DL) approach to automatically localize and discriminate COVID-19 from other comparable pneumonia on lung CT scans. Thus, this study introduces a novel two-stage DL framework for discriminating COVID-19 from community-acquired pneumonia (CAP) depending on the detected infection region within CT slices. Firstly, a novel U-shaped network is presented to segment the lung area where the infection appears. Then, the concept of transfer learning is applied to the feature extraction network to empower the network capabilities in learning the disease patterns. After that, multi-scale information is captured and pooled via an attention mechanism for powerful classification performance. Thirdly, we propose an infection prediction module that use the infection location to guide the classification decision and hence provides interpretable classification decision. Finally, the proposed model was evaluated on public datasets and achieved great segmentation and classification performance outperforming the cutting-edge studies. © 2021",,"Biological organs; Classification (of information); Deep learning; Diagnosis; Classification decision; Classification performance; Community-acquired pneumonia; Computed tomography scan; Disease patterns; Features extraction; Learning approach; Learning frameworks; Network capability; U-shaped; Computerized tomography"
"Abdel-Basset M., Chang V., Hawash H., Chakrabortty R.K., Ryan M.","Deep-IFS: Intrusion Detection Approach for Industrial Internet of Things Traffic in Fog Environment","10.1109/TII.2020.3025755","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111918190&doi=10.1109%2fTII.2020.3025755&partnerID=40&md5=552418652ad79234c1610bb66e2dd188","The extensive propagation of industrial Internet of Things (IIoT) technologies has encouraged intruders to initiate a variety of attacks that need to be identified to maintain the security of end-user data and the safety of services offered by service providers. Deep learning (DL), especially recurrent approaches, has been applied successfully to the analysis of IIoT forensics but their key challenge of recurrent DL models is that they struggle with long traffic sequences and cannot be parallelized. Multihead attention (MHA) tried to address this shortfall but failed to capture the local representation of IIoT traffic sequences. In this article, we propose a forensics-based DL model (called Deep-IFS) to identify intrusions in IIoT traffic. The model learns local representations using local gated recurrent unit (LocalGRU), and introduces an MHA layer to capture and learn global representation (i.e., long-range dependencies). A residual connection between layers is designed to prevent information loss. Another challenge facing the current IIoT forensics frameworks is their limited scalability, limiting performance in handling Big IIoT traffic data produced by IIoT devices. This challenge is addressed by deploying and training the proposed Deep-IFS in a fog computing environment. The intrusion identification becomes scalable by distributing the computation and the IIoT traffic data across worker fog nodes for training the model. The master fog node is responsible for sharing training parameters and aggregating worker node output. The aggregated classification output is subsequently passed to the cloud platform for mitigating attacks. Empirical results on the Bot-IIoT dataset demonstrate that the developed distributed Deep-IFS can effectively handle Big IIoT traffic data compared with the present centralized DL-based forensics techniques. Further, the results validate the robustness of the proposed Deep-IFS across various evaluation measures. © 2005-2012 IEEE.","Deep learning (DL); Forensics; Industrial Internet of Things (IIoT); Intrusion detection","Accident prevention; Deep learning; Digital forensics; Fog; Fog computing; Intrusion detection; Computing environments; Evaluation measures; Global representation; Intrusion detection approaches; Limiting performance; Long-range dependencies; Service provider; Training parameters; Industrial internet of things (IIoT)"
"AbdelFattah A.M.H., Zakaria W., Abdelghaffar N., Abdelmoneim N., Schneider S., Kühnberger K.","A preliminary assessment of the role of conceptual salience in automatic sketching",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029153201&partnerID=40&md5=5f51e7ae82bfc15badc455d6f34b620b","We discuss and empirically assess the abilities of the normal person to both recognise conceptual salience of objects and generate freehand sketches of these objects, investigating some underlying cognitive mechanisms that seem to be mainly responsible for these abilities. The ultimate goal is to employ human-in-the-loop results to implement a general-purpose automatic sketch recogniser that is guided by the way people operate on sketches to perform the same tasks. The aim of the article in hand is to contribute to answering two particular questions in this regard: does conceptual salience affect object recognition (when humans identify objects sketched by others)? and do specific parts of a sketch play more significant roles than others in generating a sketch of this object?","Concepts representation; Sketch recognition; Spatial relations","Artificial intelligence; Object recognition; Cognitive mechanisms; Concepts representation; Freehand sketch; Human-in-the-loop; Preliminary assessment; Sketch recognition; Spatial relations; Automatic guided vehicles"
"Abdeljaber H.A.M., Ahmad S., Alharbi A., Kumar S.","XAI-Based Reinforcement Learning Approach for Text Summarization of Social IoT-Based Content","10.1155/2022/7516832","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136067396&doi=10.1155%2f2022%2f7516832&partnerID=40&md5=70978ca480022cb2e0c28d4c7173442c","The purpose of automatic text summarising technology is to condense a given text while properly portraying the main information in the original text in a summary. To present generative text summarising approaches, on the other hand, restructure the original language and introduce new words when constructing summary sentences, which can easily lead to incoherence and poor readability. This research proposes a XAI (explainable artificial intelligence)-based Reinforcement Learning-based Text Summarization of Social IoT-Based Content using Reinforcement Learning. Furthermore, standard supervised training based on labelled data to improve the coherence of summary sentences has substantial data costs, which restricts practical applications. In order to do this, a ground-truth-dependent text summarization (generation) model (XAI-RL) is presented for coherence augmentation. On the one hand, based on the encoding result of the original text, a sentence extraction identifier is generated, and the screening process of the vital information of the original text is described. Following the establishment of the overall benefits of the two types of abstract writings, the self-judgment approach gradient assists the model in learning crucial sentence selection and decoding the selected key phrases, resulting in a summary text with high sentence coherence and good content quality. Experiments show that the proposed model's summary content index surpasses text summarising ways overall, even when there is no pre-annotated summary ground-truth; information redundancy, lexical originality, and abstract perplexity also outperform the current methods. © 2022 Hikmat A. M. Abdeljaber et al.",,"Abstracting; Internet of things; Text processing; Data costs; Encodings; Ground truth; Labeled data; Reinforcement learning approach; Reinforcement learnings; Sentence extraction; Supervised trainings; Text Summarisation; Text summarizing; Reinforcement learning"
"Abdellah N.A.A., Thangadurai N.","Real Time Application of IoT for the Agriculture in the Field along with Machine Learning Algorithm","10.1109/ICCCEEE49695.2021.9429606","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107162198&doi=10.1109%2fICCCEEE49695.2021.9429606&partnerID=40&md5=ac691196791a99c5afafe338b0427473","With the daily increase of Internet of Things(IoT) devices, which have reached tens of billions these days. The term IoT has become popular and available in our daily life even if we sometimes don't know and feel that. This work presented a friendly IoT system to help farmers, especially in the rural areas to visualize their farm data remotely, results in saving time, increasing crops productivity, and irrigating precisely. Everyone is capable to cultivate with the help of this system, contributing in solving issues like farmers leaving agriculture for mining. The design is done by using Blynk IoT platform to connect the physical devices in the field with the user mobile application, which makes the farmer visualizing the data. Raspberry pi 3 is the controller that is responsible for all processes such as sending and receiving the data with the help of sensors of temperature and humidity, soil moisture, pH, Passive Infrared (PIR), and camera, in addition to water pump as an actuator. This system capable to perform three operations, firstly auto irrigation, which will help in watering crops precisely and saving water. Secondly, suggesting fertilizers based on the soil'spH level helping farmers in determining the suitable fertilizers. Finally, objects detection, if there is any motion in the field, the system directly informs the user with a notification in the mobile application, and simultaneously the camera captures objects and the machine learning algorithm responsible for detecting the objects and tells the user via mobile application exactly which type of an object. © 2021 IEEE.","Auto Irrigation; Blynk IoT; component; IoT Agricultural Applications; Fertilizers; Object Detection; pH sensor","Agricultural robots; Cameras; Crops; Fertilizers; Humidity control; Learning algorithms; Machine learning; Mobile computing; Object detection; Soil moisture; Daily lives; Internet of Things (IOT); Mobile applications; Objects detection; Passive infrared; Physical devices; Real-time application; Temperature and humidities; Internet of things"
"Abdellaoui M., Douik A.","Human action recognition in video sequences using deep belief networks","10.18280/ts.370105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082491207&doi=10.18280%2fts.370105&partnerID=40&md5=988ed01ad234c678937bb7c6b1292c4f","For the last several decades, Human Activity Recognition (HAR) has been an intriguing topic in the domain of artificial intelligence research, since it has applications in many areas, such as image and signal processing. Generally, every recognition system can be either an end-to-end system or including two phases: Feature extraction and classification. In order to create an optimal HAR system that offers a better quality of classification prediction, in this paper we propose a new approach within two-phase recognition system paradigm. Probabilistic generative models, known as Deep Belief Networks (DBNs), are introduced. These DBNs comprise a series of Restricted Boltzmann Machines (RBMs) and are responsible for data reconstruction, feature construction and classification. We tested our approach on the KTH and UIUC human action datasets. The results obtained are very promising, with the recognition accuracy outperforming the recent state-of-the-art. © 2020 Lavoisier. All rights reserved.","Deep belief network; Deep learning; Human action recognition; Restricted Boltzmann machine","Bayesian networks; Deep learning; Feature extraction; Signal processing; Artificial intelligence research; Classification prediction; Deep belief networks; Feature construction; Feature extraction and classification; Human activity recognition; Human-action recognition; Restricted boltzmann machine; Classification (of information)"
"Abdellatif M., Chamoin J., Defer D.","Data-driven predictive control method for building heating systems: experimental validation","10.1109/SEST53650.2022.9898412","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140846077&doi=10.1109%2fSEST53650.2022.9898412&partnerID=40&md5=46b8a829a6b7230ff21135beff4a3b26","As the most energy-intensive economic sector, the building industry offers a significant potential of energy savings. Heating systems are responsible for the most important part of the energy consumed in buildings. The principal function of heating in buildings is to compensate heat losses through ventilation, building's envelope, and user's activity more widely. Generally, heating systems in buildings are regulated according to schedules with one or more temperature set points defined according to the occupancy of the building. One of the problems of the efficiency of heating systems lies in their control mode which often does not allow to anticipate the possible disturbing events. The conventional control method, which is the most widely used, regulates the heating by studying the response time of the building. However, it is not able to anticipate other phenomena such as meteorological variations (e.g., variation of the outside temperature) and to use the thermal inertia of the building to avoid overconsumption or uncomfortable situations. This paper proposes a data-driven predictive control method for building heating systems in order to improve thermal comfort and energy efficiency. Thereafter, to validate this method, the heating of an experimental building was controlled over a period of 21 days. © 2022 IEEE.","Artificial Intelligence; Energy management; Heating control; Multiple Linear Regression; Smart Building","Buildings; Construction industry; Energy efficiency; Energy management; Heating; Heating equipment; Information management; Building heating; Data driven; Economic sectors; Energy; Experimental validations; Heating control; Heating system; In-buildings; Multiple linear regressions; Predictive control methods; Multiple linear regression"
"Abd-Elmagid M.A., Dhillon H.S., Pappas N.","A Reinforcement Learning Framework for Optimizing Age of Information in RF-Powered Communication Systems","10.1109/TCOMM.2020.2991992","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090204068&doi=10.1109%2fTCOMM.2020.2991992&partnerID=40&md5=4cef8fd7c48a45286198c75749bd2dca","In this paper, we study a real-time monitoring system in which multiple source nodes are responsible for sending update packets to a common destination node in order to maintain the freshness of information at the destination. Since it may not always be feasible to replace or recharge batteries in all source nodes, we consider that the nodes are powered through wireless energy transfer (WET) by the destination. For this system setup, we investigate the optimal online sampling policy (referred to as the age-optimal policy) that jointly optimizes WET and scheduling of update packet transmissions with the objective of minimizing the long-term average weighted sum of Age of Information (AoI) values for different physical processes (observed by the source nodes) at the destination node, referred to as the sum-AoI. To solve this optimization problem, we first model this setup as an average cost Markov decision process (MDP) with finite state and action spaces. Due to the extreme curse of dimensionality in the state space of the formulated MDP, classical reinforcement learning algorithms are no longer applicable to our problem even for reasonable-scale settings. Motivated by this, we propose a deep reinforcement learning (DRL) algorithm that can learn the age-optimal policy in a computationally-efficient manner. We further characterize the structural properties of the age-optimal policy analytically, and demonstrate that it has a threshold-based structure with respect to the AoI values for different processes. We extend our analysis to characterize the structural properties of the policy that maximizes average throughput for our system setup, referred to as the throughput-optimal policy. Afterwards, we analytically demonstrate that the structures of the age-optimal and throughput-optimal policies are different. We also numerically demonstrate these structures as well as the impact of system design parameters on the optimal achievable average weighted sum-AoI. © 2020 IEEE.","Age of Information; Markov Decision Process; Reinforcement learning; RF energy harvesting","Data communication systems; Deep learning; Energy transfer; Learning algorithms; Markov processes; Structural properties; Computationally efficient; Curse of dimensionality; Markov Decision Processes; Optimization problems; Packet transmissions; Real time monitoring system; Throughput optimal policies; Wireless energy transfers; Reinforcement learning"
"Abdelrahman O., Keikhosrokiani P.","Assembly line anomaly detection and root cause analysis using machine learning","10.1109/ACCESS.2020.3029826","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102841434&doi=10.1109%2fACCESS.2020.3029826&partnerID=40&md5=49d5da8fec2ce61707a0e949d962bccf","Anomaly detection is becoming widely used in Manufacturing Industry to enhance product quality. At the same time, it plays a great role in several other domains due to the fact that anomaly may reveal rare but represent an important phenomenon. The objective of this paper is to detect anomalies and identify the possible variables that caused these anomalies on historical assembly data for two series of products. Multiple anomaly detection techniques were performed; HBOS, IForest, KNN, CBLOF, OCSVM, LOF, and ABOD. Moreover, we used AUROC and Rank Power as performance metrics, followed by Boosting ensemble learning method to ensure the best anomaly detectors robustness. The techniques that gave the highest performance are KNN, ABOD for both product series datasets with 0.95 and 0.99 AUROC respectively. Finally, we applied a statistical root cause analysis on the detected anomalies with the use of Pareto chart to visualize the frequency of the possible causes and its cumulative occurrence. The results showed that there are seven rejection causes for both product series, whereas the first three causes are responsible for 85% of the rejection rates. Besides, assembly machines engineers reported a significant reduction in the rejection rates in both assembly machines after tuning the specification limits of the rejection causes identified by this research results. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Anomaly detection; Assembly lines; Big data; Machine learning; Manufacturing industries; Root cause analysis; Unsupervised learning","Assembly; Assembly machines; Machine learning; Anomaly detector; Boosting ensembles; Manufacturing industries; Performance metrics; Rejection rates; Research results; Root cause analysis; Specification limit; Anomaly detection"
"Abdelsamea M.M., Zidan U., Senousy Z., Gaber M.M., Rakha E., Ilyas M.","A survey on artificial intelligence in histopathology image analysis","10.1002/widm.1474","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135113816&doi=10.1002%2fwidm.1474&partnerID=40&md5=9c38997515418acbe7e286892939cfc0","The increasing adoption of the whole slide image (WSI) technology in histopathology has dramatically transformed pathologists' workflow and allowed the use of computer systems in histopathology analysis. Extensive research in Artificial Intelligence (AI) with a huge progress has been conducted resulting in efficient, effective, and robust algorithms for several applications including cancer diagnosis, prognosis, and treatment. These algorithms offer highly accurate predictions but lack transparency, understandability, and actionability. Thus, explainable artificial intelligence (XAI) techniques are needed not only to understand the mechanism behind the decisions made by AI methods and increase user trust but also to broaden the use of AI algorithms in the clinical setting. From the survey of over 150 papers, we explore different AI algorithms that have been applied and contributed to the histopathology image analysis workflow. We first address the workflow of the histopathological process. We present an overview of various learning-based, XAI, and actionable techniques relevant to deep learning methods in histopathological imaging. We also address the evaluation of XAI methods and the need to ensure their reliability on the field. This article is categorized under: Application Areas > Health Care. © 2022 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.","Actionability; artificial intelligence; deep learning; histopathology; image analysis; machine learning","Data mining; Deep learning; Learning systems; Surveys; Actionability; Artificial intelligence algorithms; Deep learning; Effective algorithms; Histopathology; Image technology; Image-analysis; Machine-learning; Whole slide images; Work-flows; Image analysis"
"Abdelwahab S., Ojha V.K., Abraham A.","Neuro-fuzzy risk prediction model for computational grids","10.1007/978-3-319-29504-6_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958231773&doi=10.1007%2f978-3-319-29504-6_13&partnerID=40&md5=dd07e50d96b7708c655c080ed2d4fdb4","Prediction of risk assessment is demanding because it is one of the most important contributory factors towards grid computing. Hence, researchers were motivated for developing and deploying grids on diverse computers, which is responsible for spreading resources across administrative domains so that resource sharing becomes effective. Risk assessment in grid computing can analyses possible risks, that is, the risk of growing computational requirements of an organization. Thus, risk assessment helps in determining these risks. In this, we present an adaptive neuro-fuzzy inference system that can provide an insight of predicting the risk environment. The main goal of this paper is to obtain empirical results with an illustration of high performance and accurate results. We used data mining tools to determine the contributing attributes so that we can obtain the risk prediction accurately. © Springer International Publishing Switzerland 2016.","Adaptive neuro fuzzy system; Prediction; Risk assessment","Data mining; Forecasting; Fuzzy inference; Fuzzy systems; Grid computing; Intelligent agents; Risk analysis; Adaptive neuro fuzzy system; Adaptive neuro-fuzzy inference system; Computational grids; Computational requirements; Contributory factors; Data-mining tools; Risk environment; Risk predictions; Risk assessment"
"Abdelwahed G.A., Al-Haglaa K.S., Saadallah D.M.","Tourist-trail design: The interpretation qualities of built heritage as a motivating force for tourist's route choice behaviour, Turkish Town, Alexandria, Egypt Case",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031088169&partnerID=40&md5=d313cd4e62cdb9fe7297ea7227a321fd","Cultural tourism is one of the largest and fastest-growing global tourism markets that reveal new political, economic and social opportunities for the development of cities. The central challenge in linking culture and tourism lies in reconstructing the past in the through 'interpretation'. It is defined as a process of communicating to people the significance of a place through stimulating their understanding. Different approaches could be used to achieve this goal. A Tourist trail is one of the convenient approaches for realizing interpretation. It is an essential tool for enhancing tourists' experience and appreciation of old heritage by revealing the quality of a place. Both physical and non-physical aspects of cultural areas play a key role in supporting the interpretation qualities of the heritage trail. However, the design of tourist trails is directly affected by the qualities of the built environment which determine the choice of pedestrian route. This is considered to be the main concern of this research. This paper poses the question of how to design tourist trail based on examining the relation between the built heritage qualities and the tourists' route choice behaviour. It applies its approach to the Turkish Town-a historical part of the City of Alexandria-as a well-defined conservation area in the City. To answer the paper's question, a hypothesis was designed, taking in consideration the qualities that is responsible for enhancing interpretation, and managing tourist's experiences of place. Thus, the research follows a methodology comprised of a number of stages. The first stage investigates the qualities of the built heritage that have a direct effect on tourist's route choices. This is done to extract a number of criteria used as a benchmark for the analysis process. The second stage applies these criteria to the selected historical urban pattern. This is achieved through two analytical methods used during the evaluation process. The first method is the Space Syntax Theory to measure spatial configuration of urban spaces using both angular segment analysis and visibility graph analysis (VGA). The second method is a detailed field survey to examine each street segment's characteristics, objectively, in terms of its physical features. The research ends by performing data manipulations and analysis to propose optimal routes with interpretation qualities that facilitate tourists' navigation in the area as well as manage their experience. The preliminary findings imply the importance of using computer aided analytical tools as decision support systems for designing and manipulating tourist trails based on the built heritage qualities.","Built environment; Cultural tourism; Route choice behaviour; Street configuration; Tourist trail; Turkish Town","Artificial intelligence; Decision support systems; Syntactics; Built environment; Cultural tourism; Route choice behaviour; Tourist trail; Turkish town; Quality control"
"Abdi A.H., Luong C., Tsang T., Allan G., Nouranian S., Jue J., Hawley D., Fleming S., Gin K., Swift J., Rohling R., Abolmaesumi P.","Automatic Quality Assessment of Echocardiograms Using Convolutional Neural Networks: Feasibility on the Apical Four-Chamber View","10.1109/TMI.2017.2690836","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021447339&doi=10.1109%2fTMI.2017.2690836&partnerID=40&md5=5132f52a39f87d8848a2b56a43fd0252","Echocardiography (echo) is a skilled technical procedure that depends on the experience of the operator. The aim of this paper is to reduce user variability in data acquisition by automatically computing a score of echo quality for operator feedback. To do this, a deep convolutional neural network model, trained on a large set of samples, was developed for scoring apical four-chamber (A4C) echo. In this paper, 6,916 end-systolic echo images were manually studied by an expert cardiologist and were assigned a score between 0 (not acceptable) and 5 (excellent). The images were divided into two independent training-validation and test sets. The network architecture and its parameters were based on the stochastic approach of the particle swarm optimization on the training-validation data. The mean absolute error between the scores from the ultimately trained model and the expert's manual scores was 0.71 ± 0.58. The reported error was comparable to the measured intra-rater reliability. The learned features of the network were visually interpretable and could be mapped to the anatomy of the heart in the A4C echo, giving confidence in the training result. The computation time for the proposed network architecture, running on a graphics processing unit, was less than 10 ms per frame, sufficient for real-time deployment. The proposed approach has the potential to facilitate the widespread use of echo at the point-of-care and enable early and timely diagnosis and treatment. Finally, the approach did not use any specific assumptions about the A4C echo, so it could be generalizable to other standard echo views. © 2017 IEEE.","apical four-chamber; Convolutional neural network; deep learning; echocardiography; quality assessment; swarm optimization","Computer graphics; Convolution; Data acquisition; Deep learning; Deep neural networks; Echocardiography; Graphics processing unit; Medical imaging; Neural networks; Particle swarm optimization (PSO); Personnel training; Program processors; Stochastic systems; apical four-chamber; Convolutional neural network; Deep convolutional neural networks; Mean absolute error; Operator feedback; Quality assessment; Stochastic approach; Swarm optimization; Apical four-chamber; Computation time; Network architecture; anatomy; cardiologist; diagnosis; echocardiography; feasibility study; human; information processing; intrarater reliability; learning; nervous system; running; stochastic model; validation process; artificial neural network; reproducibility; Echocardiography; Humans; Neural Networks (Computer); Reproducibility of Results"
"Abdi J., Hadipoor M., Hadavimoghaddam F., Hemmati-Sarapardeh A.","Estimation of tetracycline antibiotic photodegradation from wastewater by heterogeneous metal-organic frameworks photocatalysts","10.1016/j.chemosphere.2021.132135","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114149107&doi=10.1016%2fj.chemosphere.2021.132135&partnerID=40&md5=d3746083bac73b22099cd06ee309d0c0","In this work, the potential ability of various modern and powerful machine learning methods such as Categorical Boosting (CatBoost), Light Gradient Boosting Machine (LightGBM), Extreme Gradient Boosting (XGBoost), Adaptive Boosting (AdaBoost), Gradient-Boosted Decision Trees (GBDT), Extra Tree (ET), Decision Trees (DT), and Random Forest (RF) were investigated to estimate tetracycline (TC) photodegradation from wastewater by 10 different metal-organic frameworks (MOFs). A comprehensive databank was gathered, including 374 data points from the photodegradation percentage of MOFs in various practical conditions. The inputs of the employed models were chosen as catalyst dosage, antibiotic concentration, Illumination time, solution pH, and specific surface area and pore volume of the investigated MOFs, and the output was TC degradation efficiency. Different statistical criteria were calculated for the validation of the developed models. Average absolute percent relative error (AAPRE) and standard deviation error (STD) values of 1.19% and 0.0431, 3.07% and 0.0628, 2.88% and 0.0751, 2.86% and 0.1304, 8.73% and 0.2751, 4.24% and 0.1024, 2.83% and 0.0934, and 11.56% and 0.4459 were obtained for CatBoost, LightGBM, XGBoost, AdaBoost, GBDT, ET, DT, and RF approaches, respectively. Among all implemented models, the CatBoost was found to be the most trustable model. Moreover, this model followed the expected trends of the TC degradation process with variation of catalyst dosage, initial TC concentration, and reaction pH. The developed CatBoost model predicted the removal of TC by MOFs accurately, which proved the capability of this approach in solving complex problems with numerous data points and its straightforwardness and cost-effectiveness for environmental applications. © 2021 Elsevier Ltd","Categorical boosting model; Metal-organic framework; Modeling; Photodegradation; Tetracycline","Adaptive boosting; Catalysts; Cost effectiveness; Decision trees; Learning systems; Organic polymers; Organometallics; Photodegradation; Boosted decision trees; Categorical boosting model; Datapoints; Extra-trees; Gradient boosting; Light gradients; Metalorganic frameworks (MOFs); Modeling; Photo degradation; Random forests; Antibiotics; metal organic framework; tetracycline; antiinfective agent; tetracycline; catalysis; catalyst; detection method; experimental study; machine learning; photodegradation; wastewater; wastewater treatment; adaptive boosting; adsorption; Article; catalyst; categorical boosting; concentration (parameter); controlled study; decision tree; extra tree; extreme gradient boosting; gradient boosted decision tree; illumination; light gradient boosting machine; machine learning; open source software; outlier detection; pH; photocatalysis; photodegradation; pore volume; random forest; surface area; surface charge; waste water management; photolysis; wastewater; Anti-Bacterial Agents; Metal-Organic Frameworks; Photolysis; Tetracycline; Waste Water"
"Abdi Oskouei M., Awuah-Offei K.","A method for data-driven evaluation of operator impact on energy efficiency of digging machines","10.1007/s12053-015-9353-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955715382&doi=10.1007%2fs12053-015-9353-3&partnerID=40&md5=adb62aef3466694fa6a90e710df795b7","Material handling (including digging) is one of the most energy-intensive processes in mining. Operators’ skills and practices are known to be some of the major factors that affect energy efficiency of digging operations. Improving operators’ skills through training is an inexpensive and effective method to improve energy efficiency. The method proposed in this work uses data collected by monitoring systems on digging equipment to detect the monitored parameters that lead to differences in energy efficiency of operators (responsible parameters). After data extraction, removing the outliers, and identifying the operators with sufficient working hours, correlation analysis can be used to find parameters that are correlated with energy efficiency. Regression analysis on pairs of operators is then used to detect responsible parameters. Random sampling is used to overcome missing data issues in the analysis. This statistics-based method is simple and adequately accounts for the high variability in data collected from these monitoring systems. The proposed method was illustrated using data collected on five operators working on a 64-m3 (85 yd3) Bucyrus-Erie 1570w dragline. The case study results show that dump height and engagement/disengagement position of the bucket are the most likely parameters to cause differences between energy efficiency of these operators. On the other hand, cycle time, payload, and swing in time are least likely to influence differences in operator energy efficiency. © 2015, Springer Science+Business Media Dordrecht.","Energy efficiency; Mining and digging equipment; Operators’ performance; Operators’ skills and practice; Regression analysis","Data mining; Materials handling; Monitoring; Personnel training; Regression analysis; Correlation analysis; Data extraction; Major factors; Material handling; Monitored parameters; Monitoring system; Random sampling; Working hours; Energy efficiency"
"Abdi S., Khosravi H., Sadiq S., Gasevic D.","Complementing educational recommender systems with open learner models","10.1145/3375462.3375520","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082386741&doi=10.1145%2f3375462.3375520&partnerID=40&md5=8407815519fe3a489f27bc4d2b73b800","Educational recommender systems (ERSs) aim to adaptively recommend a broad range of personalised resources and activities to students that will most meet their learning needs. Commonly, ERSs operate as a ""black box"" and give students no insight into the rationale of their choice. Recent contributions from the learning analytics and educational data mining communities have emphasised the importance of transparent, understandable and open learner models (OLMs) that provide insight and enhance learners' understanding of interactions with learning environments. In this paper, we aim to investigate the impact of complementing ERSs with transparent and understandable OLMs that provide justification for their recommendations. We conduct a randomised control trial experiment using an ERS with two interfaces (""Non-Complemented Interface"" and ""Complemented Interface"") to determine the effect of our approach on student engagement and their perception of the effectiveness of the ERS. Overall, our results suggest that complementing an ERS with an OLM can have a positive effect on student engagement and their perception about the effectiveness of the system despite potentially making the system harder to navigate. In some cases, complementing an ERS with an OLM has the negative consequence of decreasing engagement, understandability and sense of fairness. &copy; 2020 Copyright held by the owner/author(s). © 2020 Association for Computing Machinery.","Educational Recommender Systems; Open Learner Models; User models","Computer aided instruction; Data mining; Recommender systems; Students; Black boxes; Educational data mining; Learning environments; Open learner models; Student engagement; Understandability; User models; Learning systems"
"Abdollahi B., Nasraoui O.","Using explainability for constrained matrix factorization","10.1145/3109859.3109913","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030469333&doi=10.1145%2f3109859.3109913&partnerID=40&md5=05b4f918114f38d71fa734c0b9007f5b","Accurate model-based Collaborative Filtering (CF) approaches, such as Matrix Factorization (MF), tend to be black-box machine learning models that lack interpretability and do not provide a straightforward explanation for their outputs. Yet explanations have been shown to improve the transparency of a recommender system by justifying recommendations, and this in turn can enhance the user's trust in the recommendations. Hence, one main challenge in designing a recommender system is mitigating the trade-off between an explainable technique with moderate prediction accuracy and a more accurate technique with no explainable recommendations. In this paper, we focus on factorization models and further assume the absence of any additional data source, such as item content or user attributes. We propose an explainability constrained MF technique that computes the top-n recommendation list from items that are explainable. Experimental results show that our method is effective in generating accurate and explainable recommendations. © 2017 ACM.",,"Collaborative filtering; Economic and social effects; Factorization; Learning systems; Matrix algebra; Accurate modeling; Additional datum; Black boxes; Factorization model; Interpretability; Machine learning models; Matrix factorizations; Prediction accuracy; Recommender systems"
"Abdollahi S., Lin P.-C., Chiang J.-H.","DiaDeL: An Accurate Deep Learning-Based Model With Mutational Signatures for Predicting Metastasis Stage and Cancer Types","10.1109/TCBB.2021.3115504","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131702434&doi=10.1109%2fTCBB.2021.3115504&partnerID=40&md5=d831b5ee3ed6aa14d969ab6c1397f7fe","Mutational signatures help identify cancer-associated genes that are being involved in tumorigenesis pathways. Hence, these pathways guide precision medicine approaches to find appropriate drugs and treatments. The pattern of mutations varies in different cancer types. Some mutations dysregulate protein function so that their accumulation is responsible for cancer development and might be associated with different cancer types. Therefore, mutations as a feature set can be used as an informative candidate to distinguish various cancer types. There are several options for demonstrating mutations. One might employ binary values to demonstrate mutation regions. Another potential method for extracting features is utilizing mutation interpreters. In this study, we investigate the trinucleotide mutational pattern of each cancer type. Moreover, we extract salient NMF-based mutational signatures across various cancer types. Then, we identify cancer-associated genes of a target cancer based on its salient signatures. We evaluate the cancer-associated genes using survival and gene expression analysis in different stages of cancer. Furthermore, we introduce DiaDeL, which is a deep learning-based binary classifier. The DiaDeL model uses mutational signatures as input features and distinct a cancer type from the others. Our proposed model outperforms six state-of-the-art methods with 0.824 and 0.88 for accuracy and AUC, respectively. The source code is available at https://github.com/sabdollahi/DiaDeL. © 2004-2012 IEEE.","cancer-associated genes; classification; deep learning; Mutational signatures","Deep learning; Gene expression; Binary values; Cancer development; Cancer-associated gene; Deep learning; Features sets; Learning Based Models; Mutational signature; Potential methods; Protein functions; Tumorigenesis; Diseases; carcinogenesis; genetics; human; mutation; neoplasm; pathology; software; Carcinogenesis; Deep Learning; Humans; Mutation; Neoplasms; Software"
"Abdou A., Darwish N.","Severity classification of software code smells using machine learning techniques: A comparative study","10.1002/smr.2454","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129184270&doi=10.1002%2fsmr.2454&partnerID=40&md5=d2401862857bba51e3735bd017ba13b8","Code smell is a software characteristic that indicates bad symptoms in code design which causes problems related to software quality. The severity of code smells must be measured because it will help the developers when determining the priority of refactoring efforts. Recently, several studies focused on the prediction of design patterns errors using different detection tools. Nowadays, there is a lack of empirical studies regarding how to measure severity of code smells and which learning model is best to detect the severity of code smells. To overcome such gap, this paper focuses on measuring the severity classification of code smells depending on several machine learning models such as regression models, multinominal models, and ordinal classification models. The Local Interpretable Model Agnostic Explanations (LIME) algorithm was further used to explain the machine learning model's predictions and interpretability. On the other side, we extract the prediction rules generated by the Projective Adaptive Resonance Theory (PART) algorithm in order to study the effectiveness of using software metrics to predict code smells. The results of the experiments have shown that the accuracy of severity classification model is enhanced than baseline and ranking correlation between the predicted and actual model reaches 0.92–0.97 by using Spearman's correlation measure. © 2022 John Wiley & Sons Ltd.","code smell; correlation measures; oversampling; regression models; severity classification; software refactoring","Computer software selection and evaluation; Forecasting; Lime; Machine learning; Odors; Classification models; Code smell; Correlation measures; Machine learning models; Machine learning techniques; Over sampling; Regression modelling; Severity classification; Software codes; Software refactoring; Regression analysis"
"Abdou M., Mohammed R., Hosny Z., Essam M., Zaki M., Hassan M., Eid M., Mostafa H.","End-to-end crash avoidance deep IoT-based solution","10.1109/ICM48031.2019.9021613","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082138158&doi=10.1109%2fICM48031.2019.9021613&partnerID=40&md5=94ec74f851b9109cc4c3e4feaa964bff","Fully Autonomous Driving is considered as one of the difficult problems faced the automotive applications. It is forbidden due to the presence of some restricted Laws that prevent cars from being autonomous for the fear of accidents occurrence. However, researchers try to reach autonomous driving as a new area for research for the aim of having a strong push against these restricted Laws. Crash Avoidance functionality is one of the most important features in Self-Driving Cars that is partially integrated recently. We propose an end-to-end Crash Avoidance Deep IoT solution which is decomposed into two main parts: a) Detection Deep Neural Network which aims to detect accident occurrence in front of the ego-vehicle, and b) Accident Information Spreading IoT which is responsible for informing upcoming vehicles that there is an accident, then these vehicles will be able to take the reasonable actions either changing their routes, or changing their lanes avoiding crash. Due to the lack of Crash benchmarks, we build our own benchmark, depending only on a front camera, using ROS-Gazebo Simulation environment covering various crashes situations. In General, our proposed idea is the first solution that merges Deep Learning with IoT in automotive applications. © 2019 IEEE.","Autonomous Driving; Crash Avoidance; Deep Learning; IoT","Automobiles; Autonomous vehicles; Deep learning; Deep neural networks; Internet of things; Microelectronics; Automotive applications; Autonomous driving; Crash avoidance; End to end; Important features; Information spreading; Simulation environment; Accidents"
"Abdu S.A., Yousef A.H., Salem A.","Multimodal Video Sentiment Analysis Using Deep Learning Approaches, a Survey","10.1016/j.inffus.2021.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108096174&doi=10.1016%2fj.inffus.2021.06.003&partnerID=40&md5=c3b676237a688be4068a1080fdfbefe0","Deep learning has emerged as a powerful machine learning technique to employ in multimodal sentiment analysis tasks. In the recent years, many deep learning models and various algorithms have been proposed in the field of multimodal sentiment analysis which urges the need to have survey papers that summarize the recent research trends and directions. This survey paper tackles a comprehensive overview of the latest updates in this field. We present a sophisticated categorization of thirty-five state-of-the-art models, which have recently been proposed in video sentiment analysis field, into eight categories based on the architecture used in each model. The effectiveness and efficiency of these models have been evaluated on the most two widely used datasets in the field, CMU-MOSI and CMU-MOSEI. After carrying out an intensive analysis of the results, we eventually conclude that the most powerful architecture in multimodal sentiment analysis task is the Multi-Modal Multi-Utterance based architecture, which exploits both the information from all modalities and the contextual information from the neighbouring utterances in a video in order to classify the target utterance. This architecture mainly consists of two modules whose order may vary from one model to another. The first module is the Context Extraction Module that is used to model the contextual relationship among the neighbouring utterances in the video and highlight which of the relevant contextual utterances are more important to predict the sentiment of the target one. In most recent models, this module is usually a bidirectional recurrent neural network based module. The second module is an Attention-Based Module that is responsible for fusing the three modalities (text, audio and video) and prioritizing only the important ones. Furthermore, this paper provides a brief summary of the most popular approaches that have been used to extract features from multimodal videos in addition to a comparative analysis between the most popular benchmark datasets in the field. We expect that these findings can help newcomers to have a panoramic view of the entire field and get quick experience from the provided helpful insights. This will guide them easily to the development of more effective models. © 2021 Elsevier B.V.","Audio, visual and text information fusion; Multimodal fusion; Multimodal sentiment analysis; Sentiment analysis; Sentiment classification","Classification (of information); Learning systems; Network architecture; Recurrent neural networks; Sentiment analysis; Surveys; Benchmark datasets; Bidirectional recurrent neural networks; Comparative analysis; Context extractions; Contextual information; Contextual relationships; Effectiveness and efficiencies; Machine learning techniques; Modal analysis"
"Abdukhamidov E., Juraev F., Abuhamad M., Abuhmed T.","Black-box and Target-specific Attack Against Interpretable Deep Learning Systems","10.1145/3488932.3527283","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131651353&doi=10.1145%2f3488932.3527283&partnerID=40&md5=d35f8057d205c7d7013987e5c66c1030","Deep neural network models are susceptible to malicious manipulations even in the black-box settings. Providing explanations for DNN models offers a sense of security by human involvement, which reveals whether the sample is benign or adversarial even though previous studies achieved a high attack success rate. However, interpretable deep learning systems (IDLSes) are shown to be susceptible to adversarial manipulations in white-box settings. Attacking IDLSes in black-box settings is challenging and remains an open research domain. In this work, we propose a black-box version of the white-box AdvEdge approach against IDLSes, which is query-efficient and gradient-free without obtaining any knowledge of the target DNN model and its coupled interpreter. Our approach takes advantage of transfer-based and score-based techniques using the effective microbial genetic algorithm (MGA). We achieve a high attack success rate with a small number of queries and high similarity in interpretations between adversarial and benign samples. © 2022 Owner/Author.","adversarial machine learning; genetic algorithm; interpretable machine learning; single-class attack; target-specific attack","Deep neural networks; Learning algorithms; Learning systems; Adversarial machine learning; Black boxes; Interpretable machine learning; Machine-learning; Neural network model; Sense of security; Single-class attack; Target-specific attack; White box; Genetic algorithms"
"Abdukhamidov E., Abuhamad M., Juraev F., Chan-Tin E., AbuHmed T.","AdvEdge: Optimizing Adversarial Perturbations Against Interpretable Deep Learning","10.1007/978-3-030-91434-9_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121846862&doi=10.1007%2f978-3-030-91434-9_9&partnerID=40&md5=52970cae941f57fcde76de5b4411bb53","Deep Neural Networks (DNNs) have achieved state-of-the-art performance in various applications. It is crucial to verify that the high accuracy prediction for a given task is derived from the correct problem representation and not from the misuse of artifacts in the data. Hence, interpretation models have become a key ingredient in developing deep learning models. Utilizing interpretation models enables a better understanding of how DNN models work, and offers a sense of security. However, interpretations are also vulnerable to malicious manipulation. We present AdvEdge and AdvEdge +, two attacks to mislead the target DNNs and deceive their combined interpretation models. We evaluate the proposed attacks against two DNN model architectures coupled with four representatives of different categories of interpretation models. The experimental results demonstrate our attacks’ effectiveness in deceiving the DNN models and their interpreters. © 2021, Springer Nature Switzerland AG.","Adversarial image; Deep learning; Interpretability","Accuracy prediction; Adversarial image; Deep learning; High-accuracy; Interpretability; Interpretation model; Learning models; Neural network model; Problem representation; State-of-the-art performance; Deep neural networks"
"Abdul A., Von Der Weth C., Kankanhalli M., Lim B.Y.","COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations","10.1145/3313831.3376615","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091285319&doi=10.1145%2f3313831.3376615&partnerID=40&md5=47a69231b1680816d34522240976aa08","Interpretable machine learning models trade -off accuracy for simplicity to make explanations more readable and easier to comprehend. Drawing from cognitive psychology theories in graph comprehension, we formalize readability as visual cognitive chunks to measure and moderate the cognitive load in explanation visualizations. We present Cognitive-GAM (COGAM) to generate explanations with desired cognitive load and accuracy by combining the expressive nonlinear generalized additive models (GAM) with simpler sparse linear models. We calibrated visual cognitive chunks with reading time in a user study, characterized the trade-off between cognitive load and accuracy for four datasets in simulation studies, and evaluated COGAM against baselines with users. We found that COGAM can decrease cognitive load without decreasing accuracy and/or increase accuracy without increasing cognitive load. Our framework and empirical measurement instruments for cognitive load will enable more rigorous assessment of the human interpretability of explainable AI. © 2020 ACM.","cognitive load; explainable artificial intelligence; explanations; generalized additive models; visual explanations","Computation theory; Economic and social effects; Human engineering; Cognitive loads; Cognitive psychology; Empirical measurement; Generalized additive model; Graph comprehensions; Interpretability; Machine learning models; Simulation studies; Machine learning"
"Abdul A., Vermeulen J., Wang D., Lim B.Y., Kankanhalli M.","Trends and trajectories for explainable, accountable and intelligible systems: An HCI research agenda","10.1145/3173574.3174156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044532871&doi=10.1145%2f3173574.3174156&partnerID=40&md5=31ed0a191c79b00b287822a4c312e55c","Advances in artificial intelligence, sensors and big data man-agement have far-reaching societal impacts. As these sys-tems augment our everyday lives, it becomes increasingly important for people to understand them and remain in con-trol. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explainable systems, as well as 12,412 citing papers. Using topic modeling, co-oc-currence and network analysis, we mapped the research space from diverse domains, such as algorithmic accounta-bility, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly iso-lated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible in-terfaces built-in. From our results, we propose several impli-cations and directions for future research towards this goal. © 2018 Association for Computing Machinery.","Explainable artificial intelli-gence; Explanations; Intelligibility; Interpretable machine learning","Human engineering; Speech intelligibility; Autonomous systems; Cognitive psychology; Context- awareness; Diverse domains; Explainable artificial intelli-gence; Explanations; Literature analysis; Societal impacts; Machine learning"
"Abdul Fattah H.M., Azharul Hasan K.M., Das S.","A voting classifier for the treatment of employees' mental health disorder","10.1109/ACMI53878.2021.9528102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115702767&doi=10.1109%2fACMI53878.2021.9528102&partnerID=40&md5=61fe97290ef28942fe294fe97376eeda","Mental well-being issues are progressively getting evident as genuine medical problems in the working environment. Companies must distinguish what perspectives are responsible for the most part related to mental wellbeing to play down these issues among employees. Subsequently, classification strategies are required to figure out whether a representative needs mental wellbeing treatment or not. Based on an open source survey dataset, we applied some pre- processing then used Pearson's Correlation Coefficient for feature selection. Then we applied some machine learning models to classify the dataset. Lastly, we applied a voting classifier which totals the discoveries of each classifier passed into it and predicts the final output based on the most noteworthy lion's share of voting. Machine learning models namely Gaussian Naïve Bayes, K-Nearest Neighbor, Support Vector Machine, Decision Tree and Random Forest classifiers provided 78.83%, 86.77%, 81.48%, 86.24% and 87.83% of accuracy respectively. Combining the above methods, Voting Classifier gives us a better accuracy of 90.48%. © 2021 IEEE.","Decision Tree; Gaussian Naïve Bayes; KNN; Machine Learning; Metal Health; Pearson's Correlation Coefficient; Random Forest; Support Vector Machine; Voting Classifier","Correlation methods; Decision trees; Industry 4.0; Learning systems; Medical problems; Nearest neighbor search; Personnel; Support vector machines; K-nearest neighbors; Machine learning models; Mental health; Pearson's correlation coefficients; Pre-processing; Random forest classifier; Voting classifiers; Working environment; Classification (of information)"
"Abdulkader H., Elabd E., Ead W.","Protecting Online Social Networks Profiles by Hiding Sensitive Data Attributes","10.1016/j.procs.2016.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994267509&doi=10.1016%2fj.procs.2016.04.004&partnerID=40&md5=b12db71efb03055b3249b493cd75a122","Online Social Networks (OSNs) have become a mainstream cultural phenomenon for millions of Internet users. More importantly, OSNs expose now information from multiple social spheres e.g. personal information or professional activity. We identify two stakeholders in online social networks: the OSN users and the OSN itself. On one hand, OSN users share an astonishing amount of information ranging from personal to professional. On the other hand, OSN services handle users' information and manage all users' activities in the network, being responsible for the correct functioning of its services and maintaining a profitable business model. Indirectly, this translates into ensuring that their users continue to happily use their services without becoming victims of malicious actions. We thus classify online social networks privacy and security issues into two categories of attacks on users and OSN. In this paper we propose a utility based association rule hiding algorithm for privacy preserving user profiles data against attacks from OSN users or even OSN applications. Experimental has been conducted on samples of real datasets. Experimental has been showed less attribute modification in the released user's profiles datasets.","online social networks; privacy preserving; profiles attacks; user's profiles","Data mining; Data privacy; Information management; Space division multiple access; Amount of information; On-line social networks; Online social networks (OSNs); Privacy and security; Privacy preserving; Professional activities; profiles attacks; User's profiles; Social networking (online)"
"Abdullah M., Irtaza S.A., Nida N.","Lung malignancy evaluation of the pulmonary nodules using deep learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088576919&partnerID=40&md5=57ff21ec0baa700b2b51b66d22cb04a3","Detection of pulmonary nodules is a dangerous kind of lung cancer that is responsible for majority of deaths every year. Early diagnosis and proper treatment of Pulmonary Nodules significantly improves the patient’s survival rate. In this study, we propose a multi-view convolutional network for pulmonary nodule detection. The main objective of our work is to establish a method that can automatically pre-process, localize and then segment the pulmonary nodules precisely and improve its accuracy. In our proposed method single shot multi-box detector (SSD) precisely localizes the nodules area in the form of bounding boxes and eliminates some clinical artifacts. The proposed approach was evaluated on LUNA 2016 dataset to show the robustness of our work which achieved a sensitivity and precision of 97.47 and 0.97 respectively. The results of the segmented image are also compared with the state-of-the-art methods to demonstrate the performance superiority of the proposed approach. © Pakistan Academy of Sciences.","Deep learning; LUNA16; Lung malignancy; Pulmonary nodules; Single shot detector (SSD)",
"Abdullah T.A.A., Zahid M.S.M., Ali W.","A review of interpretable ml in healthcare: Taxonomy, applications, challenges, and future directions","10.3390/sym13122439","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121535209&doi=10.3390%2fsym13122439&partnerID=40&md5=c8beba4fbe54517efeeb8d15d238234b","We have witnessed the impact of ML in disease diagnosis, image recognition and classifi-cation, and many more related fields. Healthcare is a sensitive field related to people’s lives in which decisions need to be carefully taken based on solid evidence. However, most ML models are complex, i.e., black-box, meaning they do not provide insights into how the problems are solved or why such decisions are proposed. This lack of interpretability is the main reason why some ML models are not widely used yet in real environments such as healthcare. Therefore, it would be beneficial if ML models could provide explanations allowing physicians to make data-driven decisions that lead to higher quality service. Recently, several efforts have been made in proposing interpretable machine learning models to become more convenient and applicable in real environments. This paper aims to provide a comprehensive survey and symmetry phenomena of IML models and their applications in healthcare. The fundamental characteristics, theoretical underpinnings needed to develop IML, and taxonomy for IML are presented. Several examples of how they are applied in healthcare are investigated to encourage and facilitate the use of IML models in healthcare. Fur-thermore, current limitations, challenges, and future directions that might impact applying ML in healthcare are addressed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Applications; Challenges; Healthcare; Interpretability; Machine learning; Taxonomy",
"Abdulqader Q.J., Abdulazeez M.G., Hamodat Z.","Enhancing Smart Grid Stability with the Implementation of Heuristic Algorithms","10.1109/HORA55278.2022.9799816","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133960543&doi=10.1109%2fHORA55278.2022.9799816&partnerID=40&md5=53cb216a9350fd4f6c926ce73455b7a6","The smart grid, one of the most technologically advanced systems in existence today, is responsible for balancing supply and demand (DR). Residential customers have a significant influence on the overall operation of the conventional power system due to their high levels of energy consumption. HEM is a system designed to assist consumers in monitoring, regulating, and decreasing their energy use. With the use of HEM, appliances may be designed so that their consumption is changed to match the quantity of available supply. Recent advances in artificial intelligence have facilitated the attainment of these goals (AI). Heuristic approaches include optimization techniques such as wind-driven optimization (WDO), genomics optimization (GA), and binary particle swarm optimization (BPSO) (BPSO). Simulations are used to evaluate scheduling alternatives based on parameters such as cost, peak-to-average ratio (PAR), and an equally distributed power demand pattern throughout the system. Simulation results indicate that the WDO-based HEM outperforms both the BPSO and the GA algorithms. © 2022 IEEE.","DES; EMS; GA; Smart Grid","Economics; Electric power transmission networks; Energy utilization; Genetic algorithms; Heuristic algorithms; Heuristic methods; Particle swarm optimization (PSO); Advanced systems; Binary particle swarm optimization; DES; EMS; GA; Grid stability; Heuristics algorithm; Optimisations; Residential customers; Smart grid; Smart power grids"
"AbdulRahim A.K., Folorunso O., Sharma S.K.","An Improved Dynavote E-Voting Protocol Implementation","10.4018/ijea.2011070104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001828624&doi=10.4018%2fijea.2011070104&partnerID=40&md5=64c1d21ce0ef8870717a3ddb971dea3b","Electronic voting—the use of computers or computerized voting equipment to cast and tabulate and tally ballots in an election in a trustable manner—is a pillar of e-Government. The DynaVote voting protocol system proposed by Cetinkaya and Koc (2007) is assumed secure and practicable on a network. However, the DynaVote e-Voting protocol does not completely protect the voting counter against impersonated votes, especially when the pseudo-Vote identities are known by the wrong voter or compromised by authorities. To address this problem, a prototype called improved DynaVote e-Vote protocol was designed to protect the counter from anomalies associated with counting impersonated votes (multiple votes) in the same election. This was achieved by introducing biometric fingerprint and pseudo voter identities (PVID) encryption for each voter during voter registration via online or data mining of population data containing fingerprint biometrics. Furthermore, fingerprint reader and RSA public key cryptography is used in PVID to eliminate counting impersonated votes. The performance results showed that improved DynaVote e-Vote protocol is more reliable, eligible, and accurate, and protects voter privacy against other e-Vote protocols. © 2011, IGI Global. All rights reserved.","Biometrics; DynaVote; E-Voting; E-Voting Requirements; Protocol; Pseudo Voter Identities (PVID); RSA Public Key Cryptography",
"Abdulrahman A., Richards D.","Modelling working alliance using user-aware explainable embodied conversational agents for behavior change: Framework and empirical evaluation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100824477&partnerID=40&md5=5648875e3b08e90c6ce5f7d8a63553e7","The utilisation of embodied conversational agents (ECAs) to build a human-agent working alliance holds promise to promote health behavior change and improve health outcomes. Although ECAs have been shown to build empathic relationships with users, there is no complete framework to model working alliance. In this paper, we developed a framework that is grounded on theories and findings from social science and artificial intelligence to design a cognitive architecture for a user-aware explainable ECA. An empirical evaluation with 68 undergraduate students found differences in the efficacy of explanation to change behavior intention, build trust and working alliance depending on gender, stress levels and achievement aims; confirming the imperative of incorporating shared planning and user-tailored explanation in one framework. The empirical evaluation was limited in tailoring the explanation to the user's beliefs only; however, the analyses confirmed the need for considering adequate user information such as user's goals and preferences to build a user-aware explainable agent for behavior change towards improved health outcomes. © 40th International Conference on Information Systems, ICIS 2019. All rights reserved.","Embodied conversational agent; Shared planning; Trust; Working alliance","Artificial intelligence; Human computer interaction; Information systems; Information use; Students; Cognitive architectures; Embodied conversational agent; Empirical evaluations; Health behaviors; Trust; Undergraduate students; User information; Working alliance; Health"
"Abdulrazzaq H.I., Hassan N.F.","Modified Siamese Convolutional Neural Network for Fusion Multimodal Biometrics at Feature Level","10.1109/SCCS.2019.8852593","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073757886&doi=10.1109%2fSCCS.2019.8852593&partnerID=40&md5=e220eeb7f6a236d1dad2e74e281a5abd","Feature level fusion in multimodal biometric systems is a significant topic in personal identification researches. This fusion technique faces two main problems: incompatible features of dissimilar modalities, and the curse of dimensionality. This paper presents a solution for these problems, by proposing a modified Siamese Convolutional Neural Network (Siamese-CNN) structure, which consists of two identical sub-networks with a shared set of weights. These sub-networks would be joined together and pass through a single, fully connected layer. The proposed structure is implemented as a multimodal biometric identification system for fusion face and palm veins modalities at the feature level. Each modality images pass through its corresponding sub-network. The proposed structure is responsible for feature extraction, feature fusion, and classification steps. The structure is experimented on a simulated database for 50 persons are collected from two well-known public databases; six experiments are performed to demonstrate the efficiency and consistency of the structure. The results prove that the proposed structure overcomes the problems of feature level fusion technique by increasing the consistency between the two dissimilar modalities and high identification speed. In addition to the structure is robust against imposters and achieves high accuracy, which is 99.33% $\mp$ 0.67. © 2019 IEEE.","Biometrical Identification; Deep Learning; Feature Level Fusion; Siamese CNN","Convolution; Deep learning; Neural networks; Convolutional neural network; Curse of dimensionality; Feature level fusion; Multi-modal biometrics; Multimodal biometric identifications; Multimodal biometric systems; Personal identification; Siamese CNN; Biometrics"
"Abdulsalam G., Meshoul S., Shaiba H.","Explainable Heart Disease Prediction Using Ensemble-Quantum Machine Learning Approach","10.32604/iasc.2023.032262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139973817&doi=10.32604%2fiasc.2023.032262&partnerID=40&md5=27062248a9a774f90615d1bb163ea551","Nowadays, quantum machine learning is attracting great interest in a wide range of fields due to its potential superior performance and capabilities. The massive increase in computational capacity and speed of quantum computers can lead to a quantum leap in the healthcare field. Heart disease seriously threatens human health since it is the leading cause of death worldwide. Quantum machine learning methods can propose effective solutions to predict heart disease and aid in early diagnosis. In this study, an ensemble machine learning model based on quantum machine learning classifiers is proposed to predict the risk of heart disease. The proposed model is a bagging ensemble learning model where a quantum support vector classifier was used as a base classifier. Furthermore, in order to make the model’s outcomes more explainable, the importance of every single feature in the prediction is computed and visualized using Shapley Additive exPlanations (SHAP) framework. In the experimental study, other standalone quantum classifiers, namely, Quantum Support Vector Classifier (QSVC), Quantum Neural Network (QNN), and Variational Quantum Classifier (VQC) are applied and compared with classical machine learning classifiers such as Support Vector Machine (SVM), and Artificial Neural Network (ANN). The experimental results on the Cleveland dataset reveal the superiority of QSVC compared to the others, which explains its use in the proposed bagging model. The Bagging- QSVC model outperforms all aforementioned classifiers with an accuracy of 90.16% while showing great competitiveness compared to some state-of-the-art models using the same dataset. The results of the study indicate that quantum machine learning classifiers perform better than classical machine learning classifiers in predicting heart disease. In addition, the study reveals that the bagging ensemble learning technique is effective in improving the prediction accuracy of quantum classifiers. © 2023, Tech Science Press. All rights reserved.","Ensemble learning; Explainable machine learning; Heart disease prediction; Machine learning; Quantum machine learning",
"Abe A.","Interpretable AI as curation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074596439&partnerID=40&md5=439a9ddbd1b4350095459066b125f74d","“Interpretable AI” is an artificial intelligence (AI) whose actions can be easily understood by humans. This is a theme of the workshop. For this theme, I will show several possibilities. For the “Interpretable AI,” the key factor is curation. One of the main function of curation is to show something (person’s concept etc.) to other persons. In addition curation should be conducted according to users’ cognitive level. From this viewpoint in this paper I discuss two types curations and the effectiveness of curation. In addition, from the cognitive bias, I will also discuss two types curations. © 2019 CEUR-WS. All rights reserved.",,"Cognitive bias; Cognitive levels; Curation; Key factors; Artificial intelligence"
"Abe T., Furukawa R., Iwasaki Y., Ikemura T.","Time-series trend of pandemic sars-cov-2 variants visualized using batch-learning self-organizing map for oligonucleotide compositions","10.5334/dsj-2021-029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115868664&doi=10.5334%2fdsj-2021-029&partnerID=40&md5=fdeacf4c15ab976a129bb3dbf7212f29","To confront the global threat of coronavirus disease 2019, a massive number of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) genome sequences have been decoded, with the results promptly released through the GISAID database. Based on variant types, eight clades have already been defined in GISAID, but the diversity can be far greater. Owing to the explosive increase in available sequences, it is important to develop new technologies that can easily grasp the whole picture of the big-sequence data and support efficient knowledge discovery. An ability to efficiently clarify the detailed time-series changes in genome-wide mutation patterns will enable us to promptly identify and characterize dangerous variants that rapidly increase their population frequency. Here, we collectively analyzed over 150,000 SARS-CoV-2 genomes to understand their overall features and time-dependent changes using a batch-learning self-organizing map (BLSOM) for oligonucleotide composition, which is an unsupervised machine learning method. BLSOM can separate clades defined by GISAID with high precision, and each clade is subdivided into clusters, which shows a differential increase/decrease pattern based on geographic region and time. This allowed us to identify prevalent strains in each region and to show the commonality and diversity of the prevalent strains. Comprehensive characterization of the oligonucleotide composition of SARS-CoV-2 and elucidation of time-series trends of the population frequency of variants can clarify the viral adaptation processes after invasion into the human population and the time-dependent trend of prevalent epidemic strains across various regions, such as continents. © 2021 The Author(s).","Batch-Learning Self-Organizing Map (BLSOM); COVID-19; Oligonucleotide composition; SARS-CoV-2; Time-series trend; Unsupervised explainable machine learning","Conformal mapping; Diseases; Genes; Machine learning; Oligonucleotides; Self organizing maps; Batch learning; Batch-learning self-organizing map; COVID-19; Global threats; Oligonucleotide composition; Severe acute respiratory syndrome coronavirus; Severe acute respiratory syndrome coronavirus 2; Time-series trend; Times series; Unsupervised explainable machine learning; Time series"
"Abed A.K., Qahwaji R.","The Automated Solar Activity Prediction System (ASAP) Update Based on Optimization of a Machine Learning Approach","10.1007/978-3-030-52243-8_53","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088516356&doi=10.1007%2f978-3-030-52243-8_53&partnerID=40&md5=a7f901abd87b192e9bda3312ccaef918","Quite recently, considerable attention has been paid to solar flare prediction because extreme solar eruptions could affect our daily life activities and on different technologies. Therefore, this paper presents a novel method of the development of improved second-generation of the Automated Solar Activity Prediction system (ASAP). The suggested algorithm improves the ASAP system by expanding a period of training vector and generating new machine learning rules to be more successful. Two neural networks are responsible for determining whether the sunspots group will release flare as well as determining if the flare is an M-class or X-class. Several measurement criteria are applied to determine the extent of system performance also all results are provided in this paper. Furthermore, the quadratic score (QR) is used as a metric criterion to compare between the prediction of the proposed algorithm with the Space Weather Prediction Center (SWPC) between 2012 and 2013. The results exhibit that the proposed algorithm outperforms the old ASAP system. Keywords: Solar flares, Machine Learning, Neural network, Space, Prediction, weather. © 2020, Springer Nature Switzerland AG.","Automated Solar Activity Prediction; McIntosh classifications; Neural networks; Sunspot","Intelligent computing; Neural networks; Predictive analytics; Solar energy; Solar radiation; Weather forecasting; Daily life activities; Machine learning approaches; Second generation; Solar activity; Solar eruption; Solar flare; Space weather predictions; Machine learning"
"Abedi V., Avula V., Chaudhary D., Shahjouei S., Khan A., Griessenauer C.J., Li J., Zand R.","Prediction of long-term stroke recurrence using machine learning models","10.3390/jcm10061286","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106476293&doi=10.3390%2fjcm10061286&partnerID=40&md5=7998712982eb3eab7e262ffcfa221aed","Background: The long-term risk of recurrent ischemic stroke, estimated to be between 17% and 30%, cannot be reliably assessed at an individual level. Our goal was to study whether machine-learning can be trained to predict stroke recurrence and identify key clinical variables and assess whether performance metrics can be optimized. Methods: We used patient-level data from electronic health records, six interpretable algorithms (Logistic Regression, Extreme Gradient Boosting, Gradient Boosting Machine, Random Forest, Support Vector Machine, Decision Tree), four feature selection strategies, five prediction windows, and two sampling strategies to develop 288 models for up to 5-year stroke recurrence prediction. We further identified important clinical features and different optimization strategies. Results: We included 2091 ischemic stroke patients. Model area under the receiver operating characteristic (AUROC) curve was stable for prediction windows of 1, 2, 3, 4, and 5 years, with the highest score for the 1-year (0.79) and the lowest score for the 5-year prediction window (0.69). A total of 21 (7%) models reached an AUROC above 0.73 while 110 (38%) models reached an AUROC greater than 0.7. Among the 53 features analyzed, age, body mass index, and laboratory-based features (such as high-density lipoprotein, hemoglobin A1c, and creatinine) had the highest overall importance scores. The balance between specificity and sensitivity improved through sampling strategies. Conclusion: All of the selected six algorithms could be trained to predict the long-term stroke recurrence and laboratory-based variables were highly associated with stroke recurrence. The latter could be targeted for personalized interventions. Model performance metrics could be optimized, and models can be implemented in the same healthcare system as intelligent decision support for targeted intervention. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence; Clinical decision support system; Electronic health record; Explainable machine learning; Healthcare; Interpretable machine learning; Ischemic stroke; Machine learning; Outcome prediction; Recurrent stroke","creatinine; hemoglobin A1c; high density lipoprotein; adult; age; aged; Article; body mass; brain ischemia; cerebrovascular accident; clinical feature; decision tree; electronic health record; extreme gradient boosting; feature selection; female; gradient boosting machine; human; logistic regression analysis; machine learning; major clinical study; male; performance indicator; predictive value; random forest; receiver operating characteristic; recurrent disease; sampling; scoring system; sensitivity and specificity; support vector machine"
"Abedin B.","Managing the tension between opposing effects of explainability of artificial intelligence: a contingency theory perspective","10.1108/INTR-05-2020-0300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109295744&doi=10.1108%2fINTR-05-2020-0300&partnerID=40&md5=07a53356159092378216ffe807ca8824","Purpose: Research into the interpretability and explainability of data analytics and artificial intelligence (AI) systems is on the rise. However, most recent studies either solely promote the benefits of explainability or criticize it due to its counterproductive effects. This study addresses this polarized space and aims to identify opposing effects of the explainability of AI and the tensions between them and propose how to manage this tension to optimize AI system performance and trustworthiness. Design/methodology/approach: The author systematically reviews the literature and synthesizes it using a contingency theory lens to develop a framework for managing the opposing effects of AI explainability. Findings: The author finds five opposing effects of explainability: comprehensibility, conduct, confidentiality, completeness and confidence in AI (5Cs). The author also proposes six perspectives on managing the tensions between the 5Cs: pragmatism in explanation, contextualization of the explanation, cohabitation of human agency and AI agency, metrics and standardization, regulatory and ethical principles, and other emerging solutions (i.e. AI enveloping, blockchain and AI fuzzy systems). Research limitations/implications: As in other systematic literature review studies, the results are limited by the content of the selected papers. Practical implications: The findings show how AI owners and developers can manage tensions between profitability, prediction accuracy and system performance via visibility, accountability and maintaining the “social goodness” of AI. The results guide practitioners in developing metrics and standards for AI explainability, with the context of AI operation as the focus. Originality/value: This study addresses polarized beliefs amongst scholars and practitioners about the benefits of AI explainability versus its counterproductive effects. It poses that there is no single best way to maximize AI explainability. Instead, the co-existence of enabling and constraining effects must be managed. © 2021, Babak Abedin.","Contingency theory; Explainable artificial intelligence; Interpretable analytics; Mitigating strategies; Opposing effects; Systematic literature review",
"Abedin M.A.U., Ng V., Khan L.R.","Learning cause identifiers from annotator rationales","10.5591/978-1-57735-516-8/IJCAI11-295","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873580335&doi=10.5591%2f978-1-57735-516-8%2fIJCAI11-295&partnerID=40&md5=d0856caba516a6bd139a4ce7e6ba6435","In the aviation safety research domain, cause identification refers to the task of identifying the possible causes responsible for the incident described in an aviation safety incident report. This task presents a number of challenges, including the scarcity of labeled data and the difficulties in finding the relevant portions of the text. We investigate the use of annotator rationales to overcome these challenges, proposing several new ways of utilizing rationales and showing that through judicious use of the rationales, it is possible to achieve significant improvement over a unigram SVM baseline.",,"Aviation safety; Labeled data; Safety engineering; Artificial intelligence"
"Abeliuk A., Aziz H., Berbeglia G., Gaspers S., Kalina P., Mattei N., Peters D., Stursberg P., Van Hentenryck P., Walsh T.","Interdependent scheduling games",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006160345&partnerID=40&md5=3e8ecf342333fc5fe924bd36ad2b9969","We propose a model of interdependent scheduling games in which each player controls a set of services that they schedule independently. A player is free to schedule his own services at any time; however, each of these services only begins to accrue reward for the player when all predecessor services, which may or may not be controlled by the same player, have been activated. This model, where players have interdependent services, is motivated by the problems faced in planning and coordinating large-scale infrastructures, e.g., restoring electricity and gas to residents after a natural disaster or providing medical care in a crisis when different agencies are responsible for the delivery of staff, equipment, and medicine. We undertake a game-theoretic analysis of this setting and in particular consider the issues of welfare maximization, computing best responses, Nash dynamics, and existence and computation of Nash equilibria.",,"Artificial intelligence; Disasters; Game theory; Medical problems; Scheduling; Electricity and gas; Game theoretic analysis; Large scale infrastructures; Nash dynamics; Nash equilibria; Natural disasters; Player control; Welfare maximizations; Computer games"
"Abella A., Wright J.H., Gorin A.L.","Dialog trajectory analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544303825&partnerID=40&md5=2379a4929fe42a898871566952eaed19","Spoken dialog systems are becoming increasingly common in deployed services. These systems are not perfect, and are often deployed ""open-loop"" - lacking in systematic procedures for diagnosing problems and for making improvements. In order to target improvements where they will have the biggest impact two things are needed: first, methods and tools for detailed analysis of a data feed of call logs and customer audio; second, an interactive tool for presenting an intuitive view of the results to those responsible for the application. In this paper we discuss the paradigm and an implementation through which we are able to close the loop between system execution and system evolution by providing an empirical dialog trajectory analysis represented via a stochastic finite state machine. Novel graph analysis algorithms are introduced for change detection, compression and pruning for display, based on user-interface objectives.",,"Dialog systems; Dialog trajectory analysis; Finite state machines; System execution; Acoustics; Data compression; Data mining; Database systems; Information analysis; Random processes; Servers; World Wide Web; Speech recognition"
"Abella J.R., Antunes D.A., Clementi C., Kavraki L.E.","Large-Scale Structure-Based Prediction of Stable Peptide Binding to Class I HLAs Using Random Forests","10.3389/fimmu.2020.01583","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089096469&doi=10.3389%2ffimmu.2020.01583&partnerID=40&md5=47afe4a8eb1bc3e3d92d177407de2949","Prediction of stable peptide binding to Class I HLAs is an important component for designing immunotherapies. While the best performing predictors are based on machine learning algorithms trained on peptide-HLA (pHLA) sequences, the use of structure for training predictors deserves further exploration. Given enough pHLA structures, a predictor based on the residue-residue interactions found in these structures has the potential to generalize for alleles with little or no experimental data. We have previously developed APE-Gen, a modeling approach able to produce pHLA structures in a scalable manner. In this work we use APE-Gen to model over 150,000 pHLA structures, the largest dataset of its kind, which were used to train a structure-based pan-allele model. We extract simple, homogenous features based on residue-residue distances between peptide and HLA, and build a random forest model for predicting stable pHLA binding. Our model achieves competitive AUROC values on leave-one-allele-out validation tests using significantly less data when compared to popular sequence-based methods. Additionally, our model offers an interpretation analysis that can reveal how the model composes the features to arrive at any given prediction. This interpretation analysis can be used to check if the model is in line with chemical intuition, and we showcase particular examples. Our work is a significant step toward using structure to achieve generalizable and more interpretable prediction for stable pHLA binding. © Copyright © 2020 Abella, Antunes, Clementi and Kavraki.","antigen presentation; docking; HLA-I; immunopeptidomics; machine learning; peptide binding; random forests; structural modeling","RNA directed DNA polymerase; HLA antigen class 1; peptide; protein binding; amino acid sequence; Article; binding affinity; circular dichroism; computer simulation; crystal structure; decision tree; gene frequency; genetic algorithm; Gensini score; human; learning algorithm; machine learning; molecular dynamics; peptide synthesis; protein binding; protein conformation; protein interaction; protein secondary structure; random forest; receiver operating characteristic; signal noise ratio; treatment response; algorithm; allele; binding site; chemistry; data analysis; immunology; molecular model; protein conformation; protein database; structure activity relation; Algorithms; Alleles; Amino Acid Sequence; Binding Sites; Data Analysis; Databases, Protein; Histocompatibility Antigens Class I; Humans; Models, Molecular; Peptides; Protein Binding; Protein Conformation; Structure-Activity Relationship"
"Abellán-Nebot J.V.","Project-based experience through real manufacturing activities in mechanical engineering","10.1177/0306419018787302","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050273841&doi=10.1177%2f0306419018787302&partnerID=40&md5=ee9a266adb33b91006b224176d9eb1ba","As reported by many professional bodies responsible for accrediting engineering programs, today’s engineering graduates present important limitations in the practice of engineering because current engineering curricula are still too focused on fundamental engineering science without providing sufficient integration to industrial practice. To overcome these limitations, active learning approaches have been applied in the literature with positive results in engagement, motivation and student’s performance. In this paper, we propose a project-based learning approach with real manufacturing activities in a four-year mechanical engineering course to improve the learning process. The goal of the project is to plan the manufacturing process of a real part and conduct at shop-floor levels all the activities required. The experience was evaluated considering project/exam grades, questionnaires and manufacturing quality. The results showed an increase in student’s satisfaction, improvement in the exam performance and a clear increase in student’s enrolment in the manufacturing master degree. © The Author(s) 2018.","Active learning; engineering education; experiential learning; manufacturing; mechanical engineering; project-based","Artificial intelligence; Manufacture; Mechanical engineering; Students; Surveys; Active Learning; Engineering curriculum; Engineering graduates; Experiential learning; Manufacturing activities; Manufacturing quality; Project-based; Project-based learning approach; Engineering education"
"Abera W., Tamene L., Tesfaye K., Jiménez D., Dorado H., Erkossa T., Kihara J., Ahmed J.S., Amede T., Ramirez-Villegas J.","A data-mining approach for developing site-specific fertilizer response functions across the wheat-growing environments in Ethiopia","10.1017/S0014479722000047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127059372&doi=10.1017%2fS0014479722000047&partnerID=40&md5=8dbf6c2bd859ccc68ac18d135eae7f51","The use of chemical fertilizers is among the main innovations brought by the 1960s Green Revolution. In Ethiopia, fertilizer application during the last four decades has led to significant yield gains, yet yield remains below its potential across much of the country. One of the main challenges responsible for low yield response to fertilizer application has been the use of 'blanket' recommendations, whereby no tailoring of fertilizer amount and frequency is done based on soil requirements. As a result, the amount of fertilizer applied ranges widely, and can be either sub- or supra-optimal. There is thus an increasing need for site-specific fertilizer recommendations which take into account site characteristics such as climate variables (temperature, rainfall, and solar radiation); soil factors (soil organic carbon, moisture, pH, texture, cation exchange capacity, and level of macro- and micronutrients); and topographic position indices. This article reports on a data-mining approach we developed on a large dataset of 6585 wheat (Triticum aestivum) field trials. The dataset includes detailed, site-specific biophysical variables to create nutrient response functions that can guide optimal site-specific fertilizer application. The approach used a machine-learning model (random forest) to capture the relationship between nutrients - nitrogen (N), phosphorous (P), potassium (K), and sulfur (S) - and wheat yield. The model explained about 83, 82, 47, and 69% of variances of yield for N, P, K, and S omission, respectively, with consistent performance across training and testing datasets. Expectedly, for N and P omission data, the most important explanatory variables are nutrient rate, followed by soil organic carbon and soil pH. For K and S, however, climatic variables played an important role alongside nutrient rates. The site-specific yield-fertilizer response curves derived from our model are highly variable from location to location, as they are affected by the climatic, soil, or topographic conditions of the site. Importantly, using principal component analysis, we showed that the shape of the fertilizer response curves is a result of the multiple environmental factors (including soil, topography, and climate) that are at play at a given site, rather than of a specific dominant one. The research output is expected to respond to the national policy demands for a sound method to identify the optimal fertilizer rate to increase economic returns of fertilizer investments and take fertilizer utilization research one step further. © The Author(s), 2022. Published by Cambridge University Press","Fertilizer recommendation; Fertilizer response curves; Site specific; Wheat yield",
"Abeyagunasekera S.H.P., Perera Y., Chamara K., Kaushalya U., Sumathipala P., Senaweera O.","LISA : Enhance the explainability of medical images unifying current XAI techniques","10.1109/I2CT54291.2022.9824840","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135605605&doi=10.1109%2fI2CT54291.2022.9824840&partnerID=40&md5=ae8a10cad5106b1506a268e36c483285","This work proposed a unified approach to increase the explainability of the predictions made by Convolution Neural Networks (CNNs) on medical images using currently available Explainable Artificial Intelligent (XAI) techniques. This method in-cooperates multiple techniques such as LISA aka Local Interpretable Model Agnostic Explanations (LIME), integrated gradients, Anchors and Shapley Additive Explanations (SHAP) which is Shapley values-based approach to provide explanations for the predictions provided by Blackbox models. This unified method increases the confidence in the black-box model's decision to be employed in crucial applications under the supervision of human specialists. In this work, a Chest X-ray (CXR) classification model for identifying Covid-19 patients is trained using transfer learning to illustrate the applicability of XAI techniques and the unified method (LISA) to explain model predictions. To derive predictions, an image-net based Inception V2 model is utilized as the transfer learning model. © 2022 IEEE.","Anchors; Chest X-ray; CNN; CXR; Explainable Artificial Intelligence; Integrated Gradients; LIME; LISA; Local Interpritable Model Agnostic Explanations; SHAP; Shapley Additive Explanations; Unified Explanations; XAI","Anchors; Forecasting; Image enhancement; Learning systems; Lime; Medical imaging; Chest X-ray; Convolution neural network; Explainable artificial intelligence; Integrated gradient; LISA; Local interpretable model agnostic explanation; Local interpritable model agnostic explanation; Shapley; Shapley additive explanation; Unified explanation; XAI; Additives"
"Abeyrathna K.D., Granmo O.-C., Goodwin M.","Adaptive sparse representation of continuous input for tsetlin machines based on stochastic searching on the line","10.3390/electronics10172107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113836300&doi=10.3390%2felectronics10172107&partnerID=40&md5=2a55386883ad5ecbfac1f24c50e6bf67","This paper introduces a novel approach to representing continuous inputs in Tsetlin Machines (TMs). Instead of using one Tsetlin Automaton (TA) for every unique threshold found when Booleanizing continuous input, we employ two Stochastic Searching on the Line (SSL) automata to learn discriminative lower and upper bounds. The two resulting Boolean features are adapted to the rest of the clause by equipping each clause with its own team of SSLs, which update the bounds during the learning process. Two standard TAs finally decide whether to include the resulting features as part of the clause. In this way, only four automata altogether represent one continuous feature (instead of potentially hundreds of them). We evaluate the performance of the new scheme empirically using five datasets, along with a study of interpretability. On average, TMs with SSL feature representation use 4.3 times fewer literals than the TM with static threshold-based features. Furthermore, in terms of average memory usage and F1-Score, our approach outperforms simple Multi-Layered Artificial Neural Networks, Decision Trees, Support Vector Machines, K-Nearest Neighbor, Random Forest, Gradient Boosted Trees (XGBoost), and Explainable Boosting Machines (EBMs), as well as the standard and real-value weighted TMs. Our approach further outperforms Neural Additive Models on Fraud Detection and StructureBoost on CA-58 in terms of the Area Under Curve while performing competitively on COMPAS. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Decision support system; Interpretable AI; Interpretable machine learning; Rule-based learning; Stochastic searching on the line automaton; Tsetlin Automata; Tsetlin Machine; XAI",
"Abeyrathna K.D., Granmo O.-C., Goodwin M.","Convolutional regression tsetlin machine: An interpretable approach to convolutional regression","10.1145/3468891.3468901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114683868&doi=10.1145%2f3468891.3468901&partnerID=40&md5=9d78cbf301e89dea4890fcd8b4b8854b","The Convolutional Tsetlin Machine (CTM), a variant of Tsetlin Machine (TM), represents patterns as straightforward AND-rules, to address the high computational complexity and the lack of interpretability of Convolutional Neural Networks (CNNs). CTM has shown competitive performance on MNIST, Fashion-MNIST, and Kuzushiji-MNIST pattern classification benchmarks, both in terms of accuracy and memory footprint. In this paper, we propose the Convolutional Regression Tsetlin Machine (C-RTM) that extends the CTM to support continuous output problems in image analysis. C-RTM identifies patterns in images using the convolution operation as in the CTM and then maps the identified patterns into a real-valued output as in the Regression Tsetlin Machine (RTM). The C-RTM thus unifies the two approaches. We evaluated the performance of C-RTM using 72 different artificial datasets, with and without noise in the training data. Our empirical results show the competitive performance of C-RTM compared to two standard CNNs. Additionally, the interpretability of the identified sub-patterns by C-RTM clauses is analyzed and discussed. © 2021 ACM.","Convolutional Regression Tsetlin Machine; Convolutional Tsetlin Machine; Regression Tsetlin Machine; Tsetlin Machine","Benchmarking; Convolutional neural networks; Machine learning; Artificial datasets; Competitive performance; Interpretability; Memory footprint; Sub-patterns; Training data; Convolution"
"Abeyrathna K.D., Granmo O.-C., Goodwin M.","Extending the Tsetlin Machine with Integer-Weighted Clauses for Increased Interpretability","10.1109/ACCESS.2021.3049569","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099201234&doi=10.1109%2fACCESS.2021.3049569&partnerID=40&md5=5322e5ba8ef699b8165123d956114d20","Building models that are both interpretable and accurate is an unresolved challenge for many pattern recognition problems. In general, rule-based and linear models lack accuracy, while deep learning interpretability is based on rough approximations of the underlying inference. However, recently, the rule-based Tsetlin Machines (TMs) have obtained competitive performance in terms of accuracy, memory footprint, and inference speed on diverse benchmarks (image classification, regression, natural language understanding, and game-playing). TMs construct rules using human-interpretable conjunctive clauses in propositional logic. These, in turn, are combined linearly to solve complex pattern recognition tasks. This paper addresses the accuracy-interpretability challenge in machine learning by introducing a TM with integer weighted clauses - the Integer Weighted TM (IWTM). The intent is to increase TM interpretability by reducing the number of clauses required for competitive performance. The IWTM achieves this by weighting the clauses so that a single clause can replace multiple duplicates. Since each TM clause is formed adaptively by a Tsetlin Automata (TA) team, identifying effective weights becomes a challenging online learning problem. We solve this problem by extending each team of TA with another kind of automaton: the stochastic searching on the line (SSL) automaton. We evaluate the performance of the new scheme empirically using five datasets, along with a study of interpretability. On average, IWTM uses 6.5 times fewer literals than the vanilla TM and 120 times fewer literals than a TM with real-valued weights. Furthermore, in terms of average memory usage and F1-Score, IWTM outperforms simple Multi-Layered Artificial Neural Networks, Decision Trees, Support Vector Machines, K-Nearest Neighbor, Random Forest, Gradient Boosted Trees (XGBoost), Explainable Boosting Machines (EBMs), as well as the standard and real-value weighted TMs. IWTM finally outperforms Neural Additive Models on Fraud Detection and StructureBoost on CA-58 in terms of Area Under Curve, while performing competitively on COMPAS. © 2013 IEEE.","decision support system; integer-weighted Tsetlin machine; interpretable AI; interpretable machine learning; rule-based learning; Tsetlin machine; XAI","Benchmarking; Decision trees; Deep learning; Forestry; Formal logic; Multilayer neural networks; Nearest neighbor search; Network layers; Pattern recognition; Robots; Stochastic systems; Support vector machines; Competitive performance; K-nearest neighbors; Memory footprint; Natural language understanding; Pattern recognition problems; Propositional logic; Rough approximations; Stochastic searching; Learning systems"
"Abeyrathna K.D., Pussewalage H.S.G., Ranasinghe S.N., Oleshchuk V.A., Granmo O.-C.","Intrusion Detection with Interpretable Rules Generated Using the Tsetlin Machine","10.1109/SSCI47803.2020.9308206","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099717876&doi=10.1109%2fSSCI47803.2020.9308206&partnerID=40&md5=de91dce3607b1e7dc768d4da3b73b064","The rapid deployment in information and communication technologies and internet-based services have made anomaly based network intrusion detection ever so important for safeguarding systems from novel attack vectors. To this date, various machine learning mechanisms have been considered to build intrusion detection systems. However, achieving an acceptable level of classification accuracy while preserving the interpretability of the classification has always been a challenge. In this paper, we propose an efficient anomaly based intrusion detection mechanism based on the Tsetlin Machine (TM). We have evaluated the proposed mechanism over the Knowledge Discovery and Data Mining 1999 (KDD'99) dataset and the experimental results demonstrate that the proposed TM based approach is capable of achieving superior classification performance in comparison to several simple Multi-Layered Artificial Neural Networks, Support Vector Machines, Decision Trees, Random Forest, and K-Nearest Neighbor machine learning algorithms while preserving the interpretability. © 2020 IEEE.",,"Classification (of information); Data mining; Decision trees; Intelligent computing; Learning algorithms; Learning systems; Multilayer neural networks; Nearest neighbor search; Network layers; Support vector machines; Anomaly-based intrusion detection; Classification accuracy; Classification performance; Information and Communication Technologies; Intrusion Detection Systems; Knowledge discovery and data minings; Machine learning mechanism; Network intrusion detection; Intrusion detection"
"Abeyrathna K.D., Granmo O.-C., Zhang X., Goodwin M.","Adaptive Continuous Feature Binarization for Tsetlin Machines Applied to Forecasting Dengue Incidences in the Philippines","10.1109/SSCI47803.2020.9308291","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099695122&doi=10.1109%2fSSCI47803.2020.9308291&partnerID=40&md5=618b70a1549f71914dac5a6144a553d7","The Tsetlin Machine (TM) is a recent interpretable machine learning algorithm that requires relatively modest computational power, yet attains competitive accuracy in several benchmarks. TMs are inherently binary; however, many machine learning problems are continuous. While binarization of continuous data through brute-force thresholding has yielded promising accuracy, such an approach is computationally expensive and hinders extrapolation. In this paper, we address these limitations by standardizing features to support scale shifts in the transition from training data to real-world operation, typical for e.g. forecasting. For scalability, we employ sampling to reduce the number of binarization thresholds, relying on stratification to minimize loss of accuracy. We evaluate the approach empirically using two artificial datasets before we apply the resulting TM to forecast dengue outbreaks in the Philippines using the spatiotemporal properties of the data. Our results show that the loss of accuracy due to threshold sampling is insignificant. Furthermore, the dengue outbreak forecasts made by the TM are more accurate than those obtained by Support Vector Machines (SVMs), Decision Trees (DTs), and several multi-layered Artificial Neural Networks (ANNs), both in terms of forecasting precision and Fl-score. © 2020 IEEE.",,"Decision trees; Forecasting; Intelligent computing; Learning systems; Multilayer neural networks; Network layers; Support vector machines; Artificial datasets; Binarization threshold; Decision trees (DTs); Forecasting precision; Machine learning problem; Real world operations; Spatio-temporal properties; Support vector machine (SVMs); Learning algorithms"
"Abeyrathna K.D., Granmo O.-C., Goodwin M.","Integer weighted regression tsetlin machines","10.1007/978-3-030-55789-8_59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091322684&doi=10.1007%2f978-3-030-55789-8_59&partnerID=40&md5=b9c52ceca64216e3d19c6015e879ac47","The Regression Tsetlin Machine (RTM) addresses the lack of interpretability impeding state-of-the-art nonlinear regression models. It does this by using conjunctive clauses in propositional logic to capture the underlying non-linear frequent patterns in the data. These, in turn, are combined into a continuous output through summation, akin to a linear regression function, however, with non-linear components and binary weights. However, the resolution of the RTM output is proportional to the number of clauses employed. This means that computation cost increases with resolution. To address this problem, we here introduce integer weighted RTM clauses. Our integer weighted clause is a compact representation of multiple clauses that capture the same sub-pattern—w repeating clauses are turned into one, with an integer weight w. This reduces computation cost w times, and increases interpretability through a sparser representation. We introduce a novel learning scheme, based on so-called stochastic searching on the line. We evaluate the potential of the integer weighted RTM empirically using two artificial datasets. The results show that the integer weighted RTM is able to acquire on par or better accuracy using significantly less computational resources compared to regular RTM and an RTM with real-valued weights. © Springer Nature Switzerland AG 2020.","Interpretable machine learning; Regression tsetlin machines; Stochastic searching on the line; Tsetlin machines; Weighted tsetlin machines","Formal logic; Regression analysis; Stochastic systems; Artificial datasets; Compact representation; Computational resources; Nonlinear regression models; Propositional logic; Regression function; Stochastic searching; Weighted regression; Intelligent systems"
"Abid F.B., Sallem M., Braham A.","Robust Interpretable Deep Learning for Intelligent Fault Diagnosis of Induction Motors","10.1109/TIM.2019.2932162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085090909&doi=10.1109%2fTIM.2019.2932162&partnerID=40&md5=24de833ceb94f27c0e30f5e36c3b3978","In modern manufacturing processes, motivations for automatic fault diagnosis (FD) are increasingly growing as a result of the great trends toward achieving zero breakdowns. Induction motors (IMs) represent a critical part in most of the applications. Due to its high potential of automatic feature extraction, the deep learning (DL)-based FD of IM has recently been introduced and has essentially emphasized on the diagnosis using the vibration analysis. However, this approach has not received considerable attention when using the current analysis, although it represents a cost-effective alternative. Moreover, the already implemented DL architectures are still suffering from lack of physical interpretability. In this article, a new DL architecture called deep-SincNet is implemented for a multi-FD task. The proposed end-to-end scheme automatically learns the fault features from the raw motor current and accordingly finalizes the FD process. A high accuracy for several separated and combined faults, a more physical interpretability, a high robustness against noisy environments, and a significant gain in implementation cost prove the competitive performance of the proposed approach. © 1963-2012 IEEE.","Bearing fault; broken rotor bar; combined faults; condition monitoring; convolutional neural network (CNN); current analysis; deep learning (DL); induction motor (IM)","Cost benefit analysis; Cost effectiveness; Electric fault currents; Failure analysis; Fault detection; Finite difference method; Induction motors; Vibration analysis; Automatic fault diagnosis; Automatic feature extraction; Competitive performance; Current analysis; Implementation cost; Intelligent fault diagnosis; Manufacturing process; Noisy environment; Deep learning"
"Abidi Syed Sibte Raza, Ong Jason","Data mining strategy for inductive data clustering: A synergy between self-organizing neural networks and K-means clustering techniques",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034430678&partnerID=40&md5=83d76ce7d84a7cee22639bb691b08aca","Self organizing neural networks have a natural propensity to cluster well-defined data into visually distinct clusters, which can then be easily interpretable by data analysts. However, there are situations when the clustering output of the self-organizing network does not render distinct clusters. In this paper, we present a technique to automate the data mining task of data clustering, i.e. to automate cluster identification/demarcation by drawing upon a synergy between the self-organizing neural networks and statistical data clustering techniques. The implied hybrid of diverse data clustering techniques provides an improved strategy to (a) discover hidden similarities between data items; (b) group similar data items into distinct and well-defined clusters - i.e. with explicit boundaries between different clusters and defined cluster membership characteristics; and (c) visualize the emergent data clusters in a 2D and 3D manner. Our proposed solution is implemented in terms of a Data Clustering Workbench (DCW) - an all-encompassing (exploratory) data mining application.",,"Automation; Data mining; Mathematical models; Data clustering workbench (DCW); Self-organizing neural networks; Neural networks"
"Abidoye R.B., Chan A.P.C.","Valuers’ receptiveness to the application of artificial intelligence in property valuation","10.1080/14445921.2017.1299453","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022203667&doi=10.1080%2f14445921.2017.1299453&partnerID=40&md5=a2cb64b5a6f62528aa8ed623a527c213","Studies have shown that the level of valuation inaccuracy in Nigeria is higher than the acceptable international standard. This may be linked to the preference for traditional valuation approaches. This study investigates the readiness of Nigerian valuers to adopt the artificial intelligence (AI) property valuation techniques that have proven to be reliable and accurate in property valuation. A crosssectional study was conducted via a web-based questionnaire survey to registered estate surveyors and valuers practicing in Nigeria. The collected data were analyzed and presented with descriptive statistics in percentiles and mean score, in addition to the chi-square analysis. The results show that more than half of the respondents are aware of the AI valuation techniques. However, the techniques are not used in practice. The low adoption of the AI techniques is attributed to professional bodies responsible for regulation of real estate practice and tertiary educational institutions in Nigeria, who were not proactive enough to promote their know-how and application. It was found that active collaboration between local professional bodies and similar international organizations on member training and development may improve the usage of the AI techniques. The study highlights the need for a paradigm shift in the Nigerian property valuation practice. © 2017 Pacific Rim Real Estate Society.","Artificial intelligence; Nigeria; Property valuation; Valuation techniques; Valuers",
"Abioye E.A., Hensel O., Esau T.J., Elijah O., Abidin M.S.Z., Ayobami A.S., Yerima O., Nasirahmadi A.","Precision Irrigation Management Using Machine Learning and Digital Farming Solutions","10.3390/agriengineering4010006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125667422&doi=10.3390%2fagriengineering4010006&partnerID=40&md5=9155393a4206b2a91faa5ce33c72829b","Freshwater is essential for irrigation and the supply of nutrients for plant growth, in order to compensate for the inadequacies of rainfall. Agricultural activities utilize around 70% of the available freshwater. This underscores the importance of responsible management, using smart agricultural water technologies. The focus of this paper is to investigate research regarding the integration of different machine learning models that can provide optimal irrigation decision management. This article reviews the research trend and applicability of machine learning techniques, as well as the deployment of developed machine learning models for use by farmers toward sustainable irrigation management. It further discusses how digital farming solutions, such as mobile and web frameworks, can enable the management of smart irrigation processes, with the aim of reducing the stress faced by farmers and researchers due to the opportunity for remote monitoring and control. The challenges, as well as the future direction of research, are also discussed. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","digitalization; machine learning; mobile app; precision irrigation; smart agriculture; water; web app",
"Abir W.H., Khanam F.R., Alam K.N., Hadjouni M., Elmannai H., Bourouis S., Dey R., Khan M.M.","Detecting Deepfake Images Using Deep Learning Techniques and Explainable AI Methods","10.32604/iasc.2023.029653","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134333733&doi=10.32604%2fiasc.2023.029653&partnerID=40&md5=1a2cd8182099d2fd5ad86c873bbc2573","Nowadays, deepfake is wreaking havoc on society. Deepfake content is created with the help of artificial intelligence and machine learning to replace one person’s likeness with another person in pictures or recorded videos. Although visual media manipulations are not new, the introduction of deepfakes has marked a breakthrough in creating fake media and information. These manipulated pictures and videos will undoubtedly have an enormous societal impact. Deepfake uses the latest technology like Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) to construct automated methods for creating fake content that is becoming increasingly difficult to detect with the human eye. Therefore, automated solutions employed by DL can be an efficient approach for detecting deepfake. Though the “black-box” nature of the DL system allows for robust predictions, they cannot be completely trustworthy. Explainability is the first step toward achieving transparency, but the existing incapacity of DL to explain its own decisions to human users limits the efficacy of these systems. Though Explainable Artificial Intelligence (XAI) can solve this problem by inter-preting the predictions of these systems. This work proposes to provide a comprehensive study of deepfake detection using the DL method and analyze the result of the most effective algorithm with Local Interpretable Model-Agnostic Explanations (LIME) to assure its validity and reliability. This study identifies real and deepfake images using different Convolutional Neural Network (CNN) models to get the best accuracy. It also explains which part of the image caused the model to make a specific classification using the LIME algorithm. To apply the CNN model, the dataset is taken from Kaggle, which includes 70 k real images from the Flickr dataset collected by Nvidia and 70 k fake faces generated by StyleGAN of 256 px in size. For experimental results, Jupyter notebook, TensorFlow, Num-Py, and Pandas were used as software, InceptionResnetV2, DenseNet201, Incep-tionV3, and ResNet152V2 were used as CNN models. All these models’ performances were good enough, such as InceptionV3 gained 99.68% accuracy, ResNet152V2 got an accuracy of 99.19%, and DenseNet201 performed with 99.81% accuracy. However, InceptionResNetV2 achieved the highest accuracy of 99.87%, which was verified later with the LIME algorithm for XAI, where the proposed method performed the best. The obtained results and dependability demonstrate its preference for detecting deepfake images effectively. © 2023, Tech Science Press. All rights reserved.","convolutional neural network (CNN); deep learning; Deepfake; explainable artificial intelligence (XAI); local interpretable model-agnostic explanations (LIME)",
"Abir W.H., Uddin M.F., Khanam F.R., Tazin T., Khan M.M., Masud M., Aljahdali S.","Explainable AI in Diagnosing and Anticipating Leukemia Using Transfer Learning Method","10.1155/2022/5140148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129923800&doi=10.1155%2f2022%2f5140148&partnerID=40&md5=ed7528ff80cd750bcc5d7c7575b6e2b7","White blood cells (WBCs) are blood cells that fight infections and diseases as a part of the immune system. They are also known as ""defender cells.""But the imbalance in the number of WBCs in the blood can be hazardous. Leukemia is the most common blood cancer caused by an overabundance of WBCs in the immune system. Acute lymphocytic leukemia (ALL) usually occurs when the bone marrow creates many immature WBCs that destroy healthy cells. People of all ages, including children and adolescents, can be affected by ALL. The rapid proliferation of atypical lymphocyte cells can cause a reduction in new blood cells and increase the chances of death in patients. Therefore, early and precise cancer detection can help with better therapy and a higher survival probability in the case of leukemia. However, diagnosing ALL is time-consuming and complicated, and manual analysis is expensive, with subjective and error-prone outcomes. Thus, detecting normal and malignant cells reliably and accurately is crucial. For this reason, automatic detection using computer-aided diagnostic models can help doctors effectively detect early leukemia. The entire approach may be automated using image processing techniques, reducing physicians' workload and increasing diagnosis accuracy. The impact of deep learning (DL) on medical research has recently proven quite beneficial, offering new avenues and possibilities in the healthcare domain for diagnostic techniques. However, to make that happen soon in DL, the entire community must overcome the explainability limit. Because of the black box operation's shortcomings in artificial intelligence (AI) models' decisions, there is a lack of liability and trust in the outcomes. But explainable artificial intelligence (XAI) can solve this problem by interpreting the predictions of AI systems. This study emphasizes leukemia, specifically ALL. The proposed strategy recognizes acute lymphoblastic leukemia as an automated procedure that applies different transfer learning models to classify ALL. Hence, using local interpretable model-agnostic explanations (LIME) to assure validity and reliability, this method also explains the cause of a specific classification. The proposed method achieved 98.38% accuracy with the InceptionV3 model. Experimental results were found between different transfer learning methods, including ResNet101V2, VGG19, and InceptionResNetV2, later verified with the LIME algorithm for XAI, where the proposed method performed the best. The obtained results and their reliability demonstrate that it can be preferred in identifying ALL, which will assist medical examiners. © 2022 Wahidul Hasan Abir et al.",,"Blood; Bone; Cells; Cytology; Deep learning; Diagnosis; Disease control; Image processing; Immune system; Lime; Blood cancer; Blood cells; Bone marrow; Cell-be; Cell/B.E; Cell/BE; Children and adolescents; Lymphocytic leukemia; Transfer learning methods; White blood cells; Diseases; adolescent; artificial intelligence; child; human; image processing; leukemia; machine learning; procedures; reproducibility; Adolescent; Artificial Intelligence; Child; Humans; Image Processing, Computer-Assisted; Leukemia; Machine Learning; Reproducibility of Results"
"Abisha A., Bharathi N.","Feature Extraction from Plant Leaves and Classification of Plant Health Using Machine Learning","10.1007/978-981-19-0840-8_67","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134328024&doi=10.1007%2f978-981-19-0840-8_67&partnerID=40&md5=94b699ee21942c8a64cb087d61cfb62c","Feature extraction (FE) is an important method that contributes dimensionality reduction especially when extracting the features from images. It is the method of converting the input images into a set of features understandable by the system or software. While processing images, it is desirable to extract features that are oriented toward discernment between two or more classes. In this proposed method, two classes are considered as infected and healthy, and data is distributed into six cases. Features are extracted from the plant images using feature descriptors Hu moments, Haralick texture and color histogram. After feature extraction, the images are classified as infected or healthy based on the feature descriptors. The extracted data are trained with the various classifier models such as LR, LDA, KNN, CART and RF, and validation is performed. 10 k cross-validation is used, and the three best algorithms are chosen to classify and predict the model. Among these models, RF, CART and KNN have given better performance, respectively. Finally, accuracy score above 95% and r2 score above 85% is found for various cases, and the results are visualized for all six cases. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Classification; Feature extraction; Machine learning; Plant health","Classification (of information); Extraction; Image classification; Plants (botany); Textures; Dimensionality reduction; Feature descriptors; Features extraction; Hu moments; Input image; Machine-learning; Plant classification; Plant healths; Plant leaves; Sets of features; Feature extraction"
"Abitha N., Sarada G., Manikandan G., Sairam N.","A cryptographic approach for achieving privacy in data mining","10.1109/ICCPCT.2015.7159300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945956400&doi=10.1109%2fICCPCT.2015.7159300&partnerID=40&md5=7c3e3b45420eb7598970444f0f078c7e","Data governs the present world. Data mining has become a vital tool among many industries and organizations for effective decision making. Data mining involves extracting patterns from large data sets to form an understandable structure for future use. In recent years, sharing of data between organizations has taken place for their development. Even though sharing of data has an advantage, data which contains sensitive attributes pose a threat of exposing private information. Thus privacy of the sensitive attributes is of a greater concern. This work aims at preserving privacy of sensitive attributes by using two modified cryptographic techniques namely, Rail Fence and Vigenere Cipher algorithms. The unique feature of this approach is that the encryption matrix is formed from the original data itself. The modified data is generated based on this key matrix. From our experimental results it is evident that the original data cannot be inferred from the modified data. © 2015 IEEE.","Data Perturbation; Data Privacy; Rail-Fence; Security; Vigenere Cipher","Computer circuits; Cryptography; Data privacy; Data Sharing; Decision making; Fences; Matrix algebra; Cryptographic techniques; Data perturbation; Large datasets; Private information; Security; Sensitive attribute; Unique features; Vigenere ciphers; Data mining"
"Abkarian H., Chen Y., Mahmassani H.S.","Understanding Ridesplitting Behavior with Interpretable Machine Learning Models Using Chicago Transportation Network Company Data","10.1177/03611981211036363","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125555583&doi=10.1177%2f03611981211036363&partnerID=40&md5=0e11b4deac4eb9e9d9f2538ec5ee5f4d","As congestion levels increase in cities, it is important to analyze people’s choices of different services provided by transportation network companies (TNCs). Using machine learning techniques in conjunction with large TNC data, this paper focuses on uncovering complex relationships underlying ridesplitting market share. A real-world dataset provided by TNCs in Chicago is used in analyzing ridesourcing trips from November 2018 to December 2019 to understand trends in the city. Aggregated origin–destination trip-level characteristics, such as mean cost, mean time, and travel time reliability, are extracted and combined with origin–destination community-level characteristics. Three tree-based algorithms are then utilized to model the market share of ridesplitting trips. The most significant factors are extracted as well as their marginal effect on ridesplitting behavior, using partial dependency plots for interpretation of the machine learning model results. The results suggest that, overall, community-level factors are as or more important than trip-level characteristics. Additionally, the percentage of White people highly affects ridesplitting market share as well as the percentage of bachelor’s degree holders and households with two people residing in them. Travel time reliability and cost variability are also deemed more important than travel time and cost savings. Finally, the potential impact of taxes, crimes, cultural differences, and comfort is discussed in driving the market share, and suggestions are presented for future research and data collection attempts. © National Academy of Sciences: Transportation Research Board 2021.",,"Commerce; Machine learning; Traffic congestion; Travel time; Chicago; Congestion level; Different services; Level characteristic; Machine learning models; Market share; Network company; Origin destination; Transportation network; Travel time reliability; Competition"
"Aboamer M.A., Sikkandar M.Y., Gupta S., Vives L., Joshi K., Omarov B., Singh S.K.","An Investigation in Analyzing the Food Quality Well-Being for Lung Cancer Using Blockchain through CNN","10.1155/2022/5845870","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130400017&doi=10.1155%2f2022%2f5845870&partnerID=40&md5=fe60467e41c37d30cd51af3198fd3b73","Deep learning (DL) is a new approach that provides exceptional speed in healthcare activities with greater accuracy. In this regard, ""convolutional neural network""or CNN and blockchain are two important parts that together fasten the disease detection procedures securely. CNN can detect and predict diseases like lung cancer and help determine food quality, and blockchain is responsible for data. This research is going to analyze the extension of blockchain with the help of CNN for lung cancer prediction and making food safer. CNN algorithm has been trained with a huge number of images by altering the filters, features, epoch values, padding value, kernel size, and resolution. Subsequently, the CNN accuracy has been measured to understand how these factors affect the accuracy. A linear regression analysis has been carried out in IBM SPSS where the independent variables selected are image dataset augmentation, epochs, features, pixel size (90 × 90 to 512 × 512), kernel size (0-7), filters (10-40), and padding. The dependent variable is the accuracy of CNN. Findings suggested that a larger number of epochs improve the CNN accuracy; however, when more than 12 epochs are considered, the accuracy may decrease. A greater pixel/resolution also improves the accuracy of cancer and food image detection. When images are provided with excellent features and filters, the CNN accuracy improves. The main objective of this research is to comprehend how the independent variables affect the accuracy (dependent), but the reading may not be fully exact, and thus, the researcher has conceded out a minor task, which delivered evidence supportive of the analysis and against the analysis. As a result, it can be determined that image augmentation and a large number of images develop the CNN accuracy in lung cancer prediction and food safety determination when features and filters are applied correctly. A total of 10-12 epochs are desirable for CNN to receive 99% accuracy with 1 padding. © 2022 Mohamed Abdelkader Aboamer et al.",,"Biological organs; Convolutional neural networks; Deep learning; Diseases; Forecasting; Image enhancement; Pixels; Regression analysis; And filters; Block-chain; Cancer prediction; Convolutional neural network; Food quality; Independent variables; Kernel size; Lung Cancer; New approaches; Well being; Blockchain"
"Abokersh M.H., Vallès M., Cabeza L.F., Boer D.","A framework for the optimal integration of solar assisted district heating in different urban sized communities: A robust machine learning approach incorporating global sensitivity analysis","10.1016/j.apenergy.2020.114903","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083065890&doi=10.1016%2fj.apenergy.2020.114903&partnerID=40&md5=80bd0c587c277ac40c1de75507059e0b","A promising pathway towards sustainable transaction to clean energy production lies in the adoption of solar assisted district heating systems (SDHS). However, SDHS technical barriers during their design and operation phases, combined with their economic limitation, promote a high variation in quantifying SDHS benefits over their lifetime. This study proposes a complete multi-objective optimization framework using a robust machine learning approach to inherent sustainability principles in the design of SDHS. Moreover, the framework investigates the uncertainty in the context of SDHS design, in which the Global Sensitivity Analysis (GSA) is combined with the heuristics optimization approach. The framework application is illustrated through a case study for the optimal integration of SHDS at different urban community sizes (10, 25, 50, and 100 buildings) located in Madrid. The results reveal a substantial improvement in economic and environmental benefits for deploying SDHS, especially with including the seasonal storage tank (SST) construction properties in the optimization problem, and it can achieve a payback period up to 13.7 years. In addition, the solar fraction of the optimized SDHS never falls below 82.1% for the investigated community sizes with an efficiency above 69.5% for the SST. Finally, the GSA indicates the SST investment cost and its relevant construction materials, are primarily responsible for the variability in the optimal system feasibility. The proposed framework can provide a good starting point to solve the enormous computational expenses drawbacks associated with the heuristics optimization approach. Furthermore, it can function as a decision support tool to fulfill the European Union energy targets regarding clean energy production. © 2020 The Authors","Artificial Neural Network; Bayesian optimization approach; Global sensitivity analysis; Life cycle assessment; Multi-objective optimization; Solar assist district heating system","Building materials; Decision support systems; District heating; Heating equipment; Investments; Machine learning; Multiobjective optimization; Sustainable development; Uncertainty analysis; Decision support tools; Design and operations; Economic and environmental benefits; Global sensitivity analysis; Machine learning approaches; Optimization approach; Solar assisted district heating; Sustainability principles; Sensitivity analysis; design method; European Union; global perspective; heating; machine learning; power generation; sensitivity analysis; solar power; sustainability; urban area; Madrid [Spain]; Spain"
"Abonyi J.","Supervised fuzzy clustering based initialization of fuzzy partitions for decision tree induction","10.1007/978-3-642-11282-9_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651290772&doi=10.1007%2f978-3-642-11282-9_4&partnerID=40&md5=bf8bdd8c4f3b7bca634107ee89e35fbe","The generation of highly precise and interpretable models is the most frequent task of data mining. The interpretability of models and the nature of learning algorithms require effectively discretized (partitioned) features. Fuzzy partitioning of continuous features can increase the flexibility of the classifier since it has the ability to model fine knowledge details. Decision tree and rule induction methods often relay on a priori discretized (partitioned) continuous features. However, advantages of the supervised and fuzzy discretization of attributes have not yet been shown. This paper includes such study and proposes supervised clustering algorithm for providing informative input information for decision tree induction algorithms. © Springer-Verlag Berlin Heidelberg 2010.",,"Decision tree induction; Discretizations; Fuzzy partition; Fuzzy partitioning; Interpretability; Rule Induction Methods; Supervised clustering; Supervised fuzzy clustering; Data mining; Decision trees; Learning algorithms; Plant extracts; Trees (mathematics); Clustering algorithms"
"Abonyi J.","A novel bitmap-based algorithm for frequent itemsets mining","10.1007/978-3-642-15220-7_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049290532&doi=10.1007%2f978-3-642-15220-7_14&partnerID=40&md5=2ca6a9f186404bf44f751fadbfd9da4a","Mining frequent itemsets in databases is an important and widely studied problem in data mining research. The problem of mining frequent itemsets is usually solved by constructing candidates of itemsets, and identifying those itemsets that meet the requirement of frequent itemsets. This paper proposes a novel algorithm based on BitTable (or bitmap) representation of the data. Data - related to frequent itemsets - are stored in spare matrices. Simple matrix and vector multiplications are used to calculate the support of the potential n+1 itemsets. The main benefit of this approach is that only bitmaps of the frequent itemsets are generated. The concept is simple and easily interpretable and it supports a compact and effective implementation (in MATLAB). An application example related to the BMS-WebView-1 benchmark data is presented to illustrate the applicability of the proposed algorithm. © 2010 Springer-Verlag Berlin Heidelberg.","BitTable.; frequent itemsets",
"Abonyi J.","Generating (Fuzzy) frequent itemsets by a bitmap-based algorithm-The word's most compact frequent itemset miner",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883120253&partnerID=40&md5=909d1115609abdc8efdecbf9902728b8","Mining frequent itemsets in databases is an important and widely studied problem in data mining research. The problem of mining frequent itemsets is usually solved by constructing candidates of itemsets, and identifying those itemsets that meet the requirement of frequent itemsets. This paper proposes a novel algorithm based on BitTable (or bitmap) representation of the data. Data-related to frequent itemsets-are stored in spare matrices. Simple matrix and vector multiplications are used to calculate the support of the potential n+1 itemsets. The main benefit of this approach is that only bitmaps of the frequent itemsets are generated. The concept is simple and easily interpretable and it supports a compact and effective implementation (in MATLAB). An application example related to the BMS-WebView-1 benchmark data is presented to illustrate the applicability of the proposed algorithm.","BitTable; Frequent itemsets","Application examples; Benchmark data; Bittable; Frequent itemset; Item sets; Mining frequent itemsets; Novel algorithm; Vector multiplication; Algorithms; Artificial intelligence; Benchmarking; Data mining; Information science; Research; Matrix algebra"
"Abouelseoud G., Shoukry A.","A neural-based architecture for bridging the gap between symbolic and non-symbolic knowledge modeling: A robot obstacle avoidance case study","10.1109/JEC-ECC.2012.6186963","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860798951&doi=10.1109%2fJEC-ECC.2012.6186963&partnerID=40&md5=8605d9f3b5a7760bb0116a8b4320d4ef","During the last decade many research efforts have been directed towards studying the relative merits of the symbolic (rooted in logic, easily understandable) and non-symbolic (numeric, difficult to understand) Artificial Intelligence (AI). Specifically, efforts have been directed towards discovering techniques to translate between knowledge available in one format to another; such as between Fuzzy Rule-based Systems (FRS) and Artificial Neural Networks (ANNs); combining both formats in a single hybrid system; such as Adaptive Neuro-Fuzzy Systems (ANFIS); or even equating both of them by introducing a new fuzzy logic operator [1]. The present paper proposes a new framework; based on a modification of the work given in [1]; that has several advantages over pure FRS, pure ANN systems and existing hybrid approaches. It is capable of producing meaningful plausible rules whether prior expert's knowledge is available or not. The theoretical foundation of this framework, as well as its application to a robot obstacle avoidance case study are discussed. Its suitability for the solution of general optimization problems is highlighted in [14]. © 2012 IEEE.","""So Long As None of the Conditions is Violated"" (SLANCV); Adaptive Neuro-Fuzzy Inference Systems (ANFIS); Artificial Neural Networks (ANNs); Fuzzy Logic (FL); Objective Function","Adaptive neuro fuzzy system; Adaptive neuro-fuzzy inference system; Fuzzy logic operators; Fuzzy rule-based systems; Hybrid approach; Knowledge modeling; Objective functions; Optimization problems; Research efforts; Robot obstacle avoidance; So Long As None of the Conditions is Violated (SLANCV); Theoretical foundations; Fuzzy logic; Fuzzy systems; Hybrid systems; Neural networks; Research"
"Abraham A., Le B., Kosti I., Straub P., Velez-Edwards D.R., Davis L.K., Newton J.M., Muglia L.J., Rokas A., Bejan C.A., Sirota M., Capra J.A.","Dense phenotyping from electronic health records enables machine learning-based prediction of preterm birth","10.1186/s12916-022-02522-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138854947&doi=10.1186%2fs12916-022-02522-x&partnerID=40&md5=dbc696127d8f62ca44696c4c57b5b5d5","Background: Identifying pregnancies at risk for preterm birth, one of the leading causes of worldwide infant mortality, has the potential to improve prenatal care. However, we lack broadly applicable methods to accurately predict preterm birth risk. The dense longitudinal information present in electronic health records (EHRs) is enabling scalable and cost-efficient risk modeling of many diseases, but EHR resources have been largely untapped in the study of pregnancy. Methods: Here, we apply machine learning to diverse data from EHRs with 35,282 deliveries to predict singleton preterm birth. Results: We find that machine learning models based on billing codes alone can predict preterm birth risk at various gestational ages (e.g., ROC-AUC = 0.75, PR-AUC = 0.40 at 28 weeks of gestation) and outperform comparable models trained using known risk factors (e.g., ROC-AUC = 0.65, PR-AUC = 0.25 at 28 weeks). Examining the patterns learned by the model reveals it stratifies deliveries into interpretable groups, including high-risk preterm birth subtypes enriched for distinct comorbidities. Our machine learning approach also predicts preterm birth subtypes (spontaneous vs. indicated), mode of delivery, and recurrent preterm birth. Finally, we demonstrate the portability of our approach by showing that the prediction models maintain their accuracy on a large, independent cohort (5978 deliveries) from a different healthcare system. Conclusions: By leveraging rich phenotypic and genetic features derived from EHRs, we suggest that machine learning algorithms have great potential to improve medical care during pregnancy. However, further work is needed before these models can be applied in clinical settings. © 2022, The Author(s).","Artificial intelligence; Electronic health records; Machine learning; Preterm birth","algorithm; electronic health record; female; gestational age; human; machine learning; newborn; pregnancy; prematurity; Algorithms; Electronic Health Records; Female; Gestational Age; Humans; Infant, Newborn; Machine Learning; Pregnancy; Premature Birth"
"Abramov M.V., Tulupyev A.L.","Soft estimates of user protection from social engineering attacks: Fuzzy combination of user vulnerabilities and malefactor competencies in the attacking impact success prediction","10.1007/978-3-030-34518-1_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076199567&doi=10.1007%2f978-3-030-34518-1_4&partnerID=40&md5=ff1e631bae96917e30949ca79079c2c8","The material is devoted to solving an individual task of the research aimed at automating the construction of estimates of protection of users and, indirectly, critical documents from social engineering attacks. This issue is closely related to soft social computing. Also the estimates of protection of users are the basis of building a expert system, which can substitute for a social engineering specialist. The material describes the models of the following: the malefactor, the user, the relationships between them, the critical document, the information system, all of which represent the basic entities necessary for simulating social engineering attacks and building appropriate attack trees. Here we propose an approach to estimating the probability of a successful social engineering attacking impact by a malefactor on the user, based on the use of triangular norms. The conclusion is reached that in conditions of a lack of information for soft estimates at this stage, t-norms may be applicable, since, firstly, they provide the expected properties of a combination of the malefactor’s competency and user vulnerability models, secondly, they are not computationally difficult and, thirdly, in a number of cases allow for an understandable interpretation within the proposed area. © Springer Nature Switzerland AG 2019.","Artificial intelligence; Attack graph; Behavioral factors; Expert system; Information security; Knowledge engineering; Network security analysis; Social engineering attacks; Soft estimates; Soft social computing","Artificial intelligence; Expert systems; Knowledge engineering; Security of data; Attack graph; Behavioral factors; Network security analysis; Social computing; Social engineering; Soft estimates; Network security"
"Abramovitch Daniel","Some crisp thoughts on fuzzy logic",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028562598&partnerID=40&md5=4d0160f527a310ff9316f9f21ae21bfb","In the past year I have been inundated with articles on fuzzy logic as well as encouraged to use it for control systems. After reading some articles on fuzzy logic control, listening to a seminar by Zadeh, and attending a one day course on Intelligent Control, I started forming an opinion about how fuzzy logic control works. I believe that there are some fundamental pieces of information not provided in most fuzzy logic control papers. When one realizes what those pieces of information are, one gets a different opinion about how and when fuzzy logic control works and when it is more practical than conventional control. I will first state some opinions on fuzzy logic and try to justify them. Once this is done, I will return to some of the articles written by proponents of fuzzy logic and use the previous understanding to shed some light on what is really responsible for the improved system performance.",,"Artificial intelligence; Control systems; Control theory; Robustness (control systems); Fuzzy logic control; Intelligent control; Fuzzy sets"
"Abrantes R., Mestre P., Cunha A.","Exploring Dataset Manipulation via Machine Learning for Botnet Traffic","10.1016/j.procs.2021.11.082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122873951&doi=10.1016%2fj.procs.2021.11.082&partnerID=40&md5=c03513021a3b08bb615f9e763772b768","Botnets are responsible for some of the major malicious traffic on the Internet: DDoS attacks, Mail SPAM, brute force attacks, portscans, and others. Its dangerousness is due to the coordinated amount of infected hosts focusing on a single target. More contributions are in need, considering that (A) ML has been used for cyberattacks identification with better accuracy than standard NIDS equipments, (B) Botnet attacks are one of the most dangerous threats on the Internet. (C) the difficulties in getting representative datasets on some Botnets, and (D) Botnet traffic can be misunderstood by its infrastructure protocol. In this paper, we focus on the identification of Botnet traffic, preventing the communication from the Botmaster to the infected hosts and consequently the Botnet cyberattacks. CICFlowMeter and Machine Learning algorithms were used to analyse Botnet2014 public dataset on four different scenarios: all Botnet traffic on a single class, each class per Botnet traffic and the influence of the IPs address fields Botnet traffic detection. The results shows that Random Forest (RF) and Decision Tree (CART) archived similar accuracies on Botnet traffic classification. Important to say that CART obtained similar results with 10-20% of machine time. The metrics shown that the analysis per specific Botnet has higher accuracy than Any Botnet Traffic analysis. Also, the analysis with the IP addresses and L4 Ports scenario has higher accuracy but lower F1-Score that the equivalent without IP addresses or L4 Ports. At last, Feature Importance results confirms the literature, that Botnet traffic is not a single uniform protocol, but a collection of very different ways of communications between the botmaster and the infected hosts. © 2021 Elsevier B.V. All rights reserved.","Botnet Traffic; Botnet2014; CICFlowMeter; Decision Tree Classifier; Machine Learning; Random Forest Classifier","Classification (of information); Decision trees; Denial-of-service attack; Internet protocols; Learning algorithms; Machine learning; Network security; Telecommunication traffic; Botmaster; Botnet traffic; Botnet2014; Botnets; Cicflowmeter; Cyber-attacks; Decision tree classifiers; High-accuracy; Machine-learning; Random forest classifier; Botnet"
"Abrar M., Sim A.T.H., Abbas S.","Associative classification using automata with structure based merging","10.14569/ijacsa.2019.0100788","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070106065&doi=10.14569%2fijacsa.2019.0100788&partnerID=40&md5=e11a5b493cce6561a2c85f22a86c4fa6","Associative Classification, a combination of two important and different fields (classification and association rule mining), aims at building accurate and interpretable classifiers by means of association rules. The process used to generate association rules is exponential by nature; thus in AC, researchers focused on the reduction of redundant rules via rules pruning and rules ranking techniques. These techniques take an important part in improving the efficiency; however, pruning may negatively affect the accuracy by pruning interesting rules. Further, these techniques are time consuming in term of processing and also require domain specific knowledge to decide upon the selection of the best ranking and pruning strategy. In order to overcome these limitations, in this research, an automata based solution is proposed to improve the classifier's accuracy while replacing ranking and pruning. A new merging concept is introduced which used structure based similarity to merge the association rules. The merging not only help to reduce the classifier size but also minimize the loss of information by avoiding the pruning. The extensive experiments showed that the proposed algorithm is efficient than AC, Naive Bayesian, and Rule and Tree based classifiers in term of accuracy, space, and speed. The merging takes the advantages of the repetition in the rules set and keep the classifier as small as possible. © 2018 The Science and Information (SAI) Organization Limited.","Associative classification; Automata; Classification; Ranking and pruning; Rules merging","Association rules; Automata theory; Classification (of information); Data mining; Domain Knowledge; Trees (mathematics); % reductions; Associative classification; Automaton; Exponentials; Field classification; Ranking and pruning; Redundant rules; Rule merging; Rule mining; Structure-based; Merging"
"Abrate C., Bonchi F.","Counterfactual Graphs for Explainable Classification of Brain Networks","10.1145/3447548.3467154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114951238&doi=10.1145%2f3447548.3467154&partnerID=40&md5=4326bd82bbeedc2275f8572a15e997ed","Training graph classifiers able to distinguish between healthy brains and dysfunctional ones, can help identifying substructures associated to specific cognitive phenotypes. However, the mere predictive power of the graph classifier is of limited interest to the neuroscientists, which have plenty of tools for the diagnosis of specific mental disorders. What matters is the interpretation of the model, as it can provide novel insights and new hypotheses. In this paper we propose counterfactual graphs as a way to produce local post-hoc explanations of any black-box graph classifier. Given a graph and a black-box, a counterfactual is a graph which, while having high structural similarity with the original graph, is classified by the black-box in a different class. We propose and empirically compare several strategies for counterfactual graph search. Our experiments against a white-box classifier with known optimal counterfactual, show that our methods, although heuristic, can produce counterfactuals very close to the optimal one. Finally, we show how to use counterfactual graphs to build global explanations correctly capturing the behaviour of different black-box classifiers and providing interesting insights for the neuroscientists. © 2021 ACM.","brain networks; explainability; graph classification","Heuristic methods; Optimization; Black boxes; Brain networks; Counterfactuals; Different class; Graph search; Mental disorders; Predictive power; Structural similarity; Data mining"
"Abri F., Gutiérrez L.F., Kulkarni C.T., Namin A.S., Jones K.S.","Toward explainable users: Using NLP to enable AI to understand users’ perceptions of cyber attacks","10.1109/COMPSAC51774.2021.00254","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115862366&doi=10.1109%2fCOMPSAC51774.2021.00254&partnerID=40&md5=c1bd1de1c3b4e051b73f645fa9c969d3","To understand how end-users conceptualize consequences of cyber security attacks, we performed a card sorting study, a well-known technique in Cognitive Sciences, where participants were free to group the given consequences of chosen cyber attacks into as many categories as they wished using rationales they see fit. The results of the open card sorting study showed a large amount of inter-participant variation making the research team wonder how the consequences of security attacks were comprehended by the participants. As an exploration of whether it is possible to explain user’s mental model and behavior through Artificial Intelligence (AI) techniques, the research team compared the card sorting data with the outputs of a number of Natural Language Processing (NLP) techniques with the goal of understanding how participants perceived and interpreted the consequences of cyber attacks written in natural languages. The results of the NLP-based exploration methods revealed an interesting observation implying that participants had mostly employed checking individual keywords in each sentence to group cyber attack consequences together and less considered the semantics behind the description of consequences of cyber attacks. The results reported in this paper are seemingly useful and important for cyber attacks comprehension from user’s perspectives. To the best of our knowledge, this paper is the first introducing the use of AI techniques in explaining and modeling users’ behavior and their perceptions about a context. The novel idea introduced here is about explaining users using AI. © 2021 IEEE.","Artificial intelligence; Explainable users; Mental model; Natural language processing; Perception; Security attacks comprehension","Application programs; Artificial intelligence; Crime; Natural language processing systems; Network security; Semantics; Sorting; Cognitive science; Cyber security; Exploration methods; Large amounts; NAtural language processing; Natural languages; Research teams; Security attacks; Computer crime"
"Abro A.G., Mohamad-Saleh J.","Intelligent scout-bee based Artificial Bee Colony optimization algorithm","10.1109/ICCSCE.2012.6487175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875987625&doi=10.1109%2fICCSCE.2012.6487175&partnerID=40&md5=9d207217eb1568bbf4a5e47d480a8697","Artificial Bee Colony (ABC) optimization algorithm has captured much attention of researchers from various fields, in recent times. Moreover, various comparative studies clearly states dominant convergence of ABC algorithm over numerous other bio-inspired optimization algorithms. ABC optimization algorithm, its variants and hybrids have unique ability to induct new possible-solutions to replace the existing-but-poor possible-solutions. Scout-bee is responsible to induct the new possible-solutions into the population. This research work has proposed a novel scheme for enhancement of scout-bee stage of ABC optimization algorithm. The scheme capitalizes on so-far the best-found possible-solution. The proposed scheme has been compared with the existing scheme on various high-dimensional benchmark functions. The results analysis proved the need of replacing the existing scheme with the proposed scheme for performance enhancement of ABC optimization algorithm, its variants and hybrids. © 2012 IEEE.","ABC variant; computational intelligence; metaheuristic algorithms; swarm intelligence","Artificial intelligence; Biomimetics; Control systems; Engineering research; Swarm intelligence; ABC variant; Artificial bee colonies (ABC); Artificial bee colony optimization algorithms; Benchmark functions; Bio-inspired optimizations; Meta heuristic algorithm; Optimization algorithms; Performance enhancements; Optimization"
"Abry P., Spilka J., Leonarduzzi R., Chudáček V., Pustelnik N., Doret M.","Sparse learning for Intrapartum fetal heart rate analysis","10.1088/2057-1976/aabc64","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047266507&doi=10.1088%2f2057-1976%2faabc64&partnerID=40&md5=60b4be622bddedf210bf46e3c9088f20","Fetal Heart Rate (FHR) monitoring is used during delivery for fetal well-being assessment. Classically based on the visual evaluation of FIGO criteria, FHR characterization remains a challenging task that continuously receives intensive research efforts. Intrapartum FHR analysis is further complicated by the two different stages of labor (dilation and active pushing). Research works aimed at devising automated acidosis prediction procedures are either based on designing new advanced signal processing analyses or on efficiently combining a large number of features proposed in the literature. Such multi-feature procedures either rely on a prior feature selection step or end up with decision rules involving long lists of features. This many-feature outcome rule does not permit to easily interpret the decision and is hence not well suited for clinical practice. Machine-learning-based decision-rule assessment is often impaired by the use of different, proprietary and small databases, preventing meaningful comparisons of results reported in the literature. Here, sparse learning is promoted as a way to perform jointly feature selection and acidosis prediction, hence producing an optimal decision rule based on as few features as possible. Making use of a set of 20 features (gathering 'FIGO-like' features, classical spectral features and recently proposed scale-free features), applied to two large-size (respectively ≃1800 and ≃500 subjects), well-documented databases, collected independently in French and Czech hospitals, the benefits of sparse learning are quantified in terms of: (i) accounting for class imbalance (few acidotic subjects), (ii) producing simple and interpretable decision rules, (iii) evidences for differences between the temporal dynamics of active pushing and dilation stages, and (iv) of validity/generalizability of decision rules learned on one database and applied to the other one. © 2018 IOP Publishing Ltd.","acidosis prediction; intrapartum fetal heart rate; labor stages; learning generalization; scale-free; sparse learning","acidosis; Article; data base; deceleration; fetus heart rate; fetus monitoring; human; labor stage; learning; prediction; signal processing; sparse learning; support vector machine"
"Abualsaud H., Liu S., Lu D.B., Situ K., Rangesh A., Trivedi M.M.","LaneAF: Robust Multi-Lane Detection with Affinity Fields","10.1109/LRA.2021.3098066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111066836&doi=10.1109%2fLRA.2021.3098066&partnerID=40&md5=cc4a70e28a3ad6751454228b8444f66a","This study presents an approach to lane detection involving the prediction of binary segmentation masks and per-pixel affinity fields. These affinity fields, along with the binary masks, can then be used to cluster lane pixels horizontally and vertically into corresponding lane instances in a post-processing step. This clustering is achieved through a simple row-by-row decoding process with little overhead; such an approach allows LaneAF to detect a variable number of lanes without assuming a fixed or maximum number of lanes. Moreover, this form of clustering is more interpretable in comparison to previous visual clustering approaches, and can be analyzed to identify and correct sources of error. Qualitative and quantitative results obtained on popular lane detection datasets demonstrate the model's ability to detect and cluster lanes effectively and robustly. Our proposed approach sets a new state-of-the-art on the challenging CULane dataset and the recently introduced Unsupervised LLAMAS dataset. © 2016 IEEE.","deep learning for visual perception; Object detection; segmentation and categorization","Agricultural robots; Binary segmentation masks; Decoding process; Pixel affinities; Post processing; Quantitative result; State of the art; Variable number; Visual clustering; Pixels"
"Abubacker N.F., Azman A., Doraisamy S., Murad M.A.A.","Breast cancer detection by using associative classifier with rule refinement method based on relevance feedback","10.1007/s00521-022-07336-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132776337&doi=10.1007%2fs00521-022-07336-9&partnerID=40&md5=8f1fbacc2ab41fa8fa74fb48182e5aee","Computer-aided diagnosis system that uses classification process for an automated detection of breast cancer could provide a second opinion that improves diagnosis. Several researchers have proposed the use of associative classifier that generates strong associations between features and reveals hidden relationship that can be missed by other classification algorithms. However, the effectiveness of an associative classifier depends largely on the generalized rules based on training data. Often, the number of training data is limited, which may further produce the classification rules that are stagnant and cannot adapt to a changing distribution of test images, as such it may not produce complete and accurate rules for classification for future cases. This paper aims to address this issue by refining rules that are static using dynamic rule refinement technique. This technique helps to adapt to the changes in the new evidences that can be used for classification to further enhance the performance of the associative classifier. A method named the rule refinement based on incremental modification is proposed that dynamically refines the rules after the suggested term is validated by the experts. Once the initial classification is performed using the generalized rules for each test example, the results are validated using the experts feedback. Based on the validated classification result either correct or incorrect, the rules that are responsible for classification are refined in three phases. These refined rules are used for classification of future test examples that leads to improved prediction accuracy in comparison with the classifier that uses generalized static rules. The performance of the proposed method evaluated on the digital database for screening mammography dataset is promising and has achieved an overall classification accuracy of 96% in the biased setting and an accuracy of 95.24% in the unbiased setting as compared to the accuracy 90.48% of baseline classifier with static rules. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Association rule mining; Breast cancer; Computer aided diagnosis; Image processing; Machine learning; Mammogram classification","Classification (of information); Computer aided instruction; Diseases; Image classification; Machine learning; Medical imaging; Associative classifiers; Breast Cancer; Breast cancer detection; Images processing; Machine-learning; Mammogram classifications; Performance; Rule refinement; Test examples; Training data; Computer aided diagnosis"
"Abubakar S., Shariff A.B.M., Zaini K.M., Fadilah S.I., Ahmed M.A.","A Representation of 3GPP 5G-V2X Sidelink Enhancements in Releases 14, 15, 16, and 17","10.18280/ts.390216","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131580386&doi=10.18280%2fts.390216&partnerID=40&md5=0a79a78e57e2b992c3b87281af964a60","Vehicle-to-Vehicle (V2V) communication is considered the enabler of road safety, traffic efficiency, and compatibility for drivers, passengers, and vulnerable road users. The evolution of 5G V2X enhances the physical layer of LTE V2X in terms of resource allocation, frame structure, and many more. The amendments in the physical layer are to enable the exchange of safety and advanced driving services within a geographical area. The introduction of 25 use cases in the 3GPP Release 15 standard with different quality of service (QoS) requirements and high demand for data rates makes it possible for the enhancements. Delivering the services of the established use cases becomes a challenge in 5G-V2V communication. Release 14 and release 15 mark the evolution of LTE V2X, while Release 16 highlights 3GPP advancements for 5G V2X NR services, and Release 17 focuses on future enhancements with the applications of Machine Learning (ML) and Artificial Intelligence (AI). This study examines sidelink communication, resource allocation for LTE V2V and 5G-V2V, and NR V2X enablers such as Network Slicing (NS) and Machine Learning (ML) with recent capacity studies on V2V employing the two 5G enablers. In 5G-V2V Sidelink communication, Physical Sidelink Shared Channel (PSSCH) is responsible for broadcasting various messages using release 14 and release 16 waveforms. The capacity of PSSCH to broadcast the 5G services, such as Cooperative Awareness Messages (CAM), Decentralized Environmental Messages (DENM), Light Detection and Ranging (LIDAR) contents for environmental perception, and sensor DATA applications for driving intention, becomes a challenge. This study considered this challenge by providing a mathematical model of the capacity of V2V and V2I links in the 5G V2X network for mode selection using the Poisson point process and Poisson line process within a circular region. © 2022 Lavoisier. All rights reserved.","3GPP; LTEV2X; machine learning; network slicing; sidelink capacity; sidelink communication","Automobile drivers; Machine learning; Motor transportation; Network layers; Optical radar; Quality of service; Resource allocation; Roads and streets; Vehicle to Everything; Vehicle to vehicle communications; 3GPP; LTEV2X; Machine-learning; Network slicing; Physical layers; Shared channel; Sidelink capacity; Sidelink communication; V2V communications; Vehicle to vehicles; 5G mobile communication systems"
"Abufouda M.","Postmortem analysis of decayed online social communities: Cascade pattern analysis and prediction","10.1155/2018/3873601","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062701496&doi=10.1155%2f2018%2f3873601&partnerID=40&md5=fbd43e66ce32da2bfea084259fa652f4","Recently, many online social networks, such as MySpace, Orkut, and Friendster, have faced inactivity decay of their members, which contributed to the collapse of these networks. The reasons, mechanics, and prevention mechanisms of such inactivity decay are not fully understood. In this work, we analyze decayed and alive subwebsites from the Stack Exchange platform. The analysis mainly focuses on the inactivity cascades that occur among the members of these communities. We provide measures to understand the decay process and statistical analysis to extract the patterns that accompany the inactivity decay. Additionally, we predict cascade size and cascade virality using machine learning. The results of this work include a statistically significant difference of the decay patterns between the decayed and the alive subwebsites. These patterns are mainly cascade size, cascade virality, cascade duration, and cascade similarity. Additionally, the contributed prediction framework showed satisfactorily prediction results compared to a baseline predictor. Supported by empirical evidence, the main findings of this work are (1) there are significantly different decay patterns in the alive and the decayed subwebsites of the Stack Exchange; (2) the cascade's node degrees contribute more to the decay process than the cascade's virality, which indicates that the expert members of the Stack Exchange subwebsites were mainly responsible for the activity or inactivity of the Stack Exchange subwebsites; (3) the Statistics subwebsite is going through decay dynamics that may lead to it becoming fully-decayed; (4) the decay process is not governed by only one network measure, it is better described using multiple measures; (5) decayed subwebsites were originally less resilient to inactivity decay, unlike the alive subwebsites; and (6) network's structure in the early stages of its evolution dictates the activity/inactivity characteristics of the network. Copyright © 2018 Mohammed Abufouda.",,"Forecasting; Learning systems; Social networking (online); Decay dynamics; Decay patterns; Network measures; On-line social networks; Online social communities; Pattern analysis; Postmortem analysis; Statistically significant difference; Decay (organic)"
"Abu-Ghazaleh N.B., Wilsey P.A.","Shared control - Supporting control parallelism using a SIMD-like architecture","10.1007/bfb0057970","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882582148&doi=10.1007%2fbfb0057970&partnerID=40&md5=a65474264fca85a974d12501dcdf75a5","SIMD machines are considered special purpose architectures chiefly because of their inability to support control parallelism. This restriction exists because there is a single control unit that is shared at the thread level; concurrent control threads must time-share the control unit. We present an alternative model for building centralized control architectures that better supports control parallelism. This model, called shared control, shares the control unit(s) at the instruction level - in each cycle the control signals for the supported instructions are broadcast to the PEs. In turn, a PE receive its control by synchronizing with the control unit responsible for its current instruction. There are a number of architectural issues that must be resolved. This paper identifies some of these issues and suggests solutions to them. An integrated shared-control/SIMD architecture design (SharC) is presented and used to demonstrate the performance relative to a SIMD architecture.",,"Artificial intelligence; Computer science; Computers; Architecture designs; Centralized control architecture; Concurrent control; Control signal; Instruction-level; Shared control; SIMD architecture; SIMD machines; Concurrency control"
"Abuhasel K.A.","Machine learning approach to handle data-driven model for simulation and forecasting of the cone crusher output in the stone crushing plant","10.1111/coin.12338","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084843819&doi=10.1111%2fcoin.12338&partnerID=40&md5=447213dc469d3b9440f51e11dc8e67ee","Grinding and crushing of stones and other particles are associated with various significant applications. Different sectors have continuously evolved in this area. In the crushing industry, plants function under strict conditions, many of which involve grinding materials. Therefore, various factors are responsible for how the crushers perform. This research investigated the ability of the adaptive neuro fuzzy inference system (ANFIS) to simulate the effects of throw, eccentric speed, closed side setting, and the size of the particle on crusher output. The developed simulation model was adjusted and authenticated alongside the experimental data of the investigated parameters. The model's performance was computed by the use of several prediction criteria skills. The results of the study indicated that the developed ANFIS model could simulate the Cone crusher output and give a dependable forecast of the cumulative weight fraction. The researchers resolved that the model fostered was a suitable instrument for the onsite cone crusher assessment. © 2020 Wiley Periodicals LLC.","cone crushers; data-driven modeling; forecasting; production; simulation","Crushed stone plants; Crushers; Crushing; Forecasting; Fuzzy neural networks; Fuzzy systems; Grinding (machining); Machine learning; Adaptive neuro-fuzzy inference system; ANFIS model; Crushing industry; Cumulative weight; Data-driven model; Machine learning approaches; Simulation model; Stone-crushing; Fuzzy inference"
"Abuhmed T., El-Sappagh S., Alonso J.M.","Robust hybrid deep learning models for Alzheimer's progression detection","10.1016/j.knosys.2020.106688","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099654736&doi=10.1016%2fj.knosys.2020.106688&partnerID=40&md5=36cb65e0c966fffa759423cdd5a2c579","The prevalence of Alzheimer's disease (AD) in the growing elderly population makes accurately predicting AD progression crucial. Due to AD's complex etiology and pathogenesis, an effective and medically practical solution is a challenging task. In this paper, we developed and evaluated two novel hybrid deep learning architectures for AD progression detection. These models are based on the fusion of multiple deep bidirectional long short-term memory (BiLSTM) models. The first architecture is an interpretable multitask regression model that predicts seven crucial cognitive scores for the patient 2.5 years after their last observations. The predicted scores are used to build an interpretable clinical decision support system based on a glass-box model. This architecture aims to explore the role of multitasking models in producing more stable, robust, and accurate results. The second architecture is a hybrid model where the deep features extracted from the BiLSTM model are used to train multiple machine learning classifiers. The two architectures were comprehensively evaluated using different time series modalities of 1371 subjects participated in the study of the Alzheimer's disease neuroimaging initiative (ADNI). The extensive, real-world experimental results over ADNI data help establish the effectiveness and practicality of the proposed deep learning models. © 2020 Elsevier B.V.","Alzheimer's disease; Alzheimer's progression; Cognitive scores regression; Computer-aided diagnosis; Information fusion; Multimodal multitask learning","Decision support systems; Learning systems; Neurodegenerative diseases; Neuroimaging; Regression analysis; Alzheimer's disease; Clinical decision support systems; Elderly populations; Learning architectures; Learning models; Multiple machine; Practical solutions; Regression model; Deep learning"
"Abukmeil M., Ferrari S., Genovese A., Piuri V., Scotti F.","Grad 2 VAE: An Explainable Variational Autoencoder Model Based on Online Attentions Preserving Curvatures of Representations","10.1007/978-3-031-06427-2_56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130881412&doi=10.1007%2f978-3-031-06427-2_56&partnerID=40&md5=984d71dc3dee242acd65209e09092745","Unsupervised learning (UL) is a class of machine learning (ML) that learns data, reduces dimensionality, and visualizes decisions without labels. Among UL models, a variational autoencoder (VAE) is considered a UL model that is regulated by variational inference to approximate the posterior distribution of large datasets. In this paper, we propose a novel explainable artificial intelligence (XAI) method to visually explain the VAE behavior based on the second-order derivative of the latent space concerning the encoding layers, which reflects the amount of acceleration required from encoding to decoding space. Our model is termed as Grad 2 VAE and it is able to capture the local curvatures of the representations to build online attention that visually explains the model’s behavior. Besides the VAE explanation, we employ our method for anomaly detection, where our model outperforms the recent UL deep models when generalizing it for large-scale anomaly data. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Anomaly detection; Unsupervised learning; VAE; XAI","Anomaly detection; E-learning; Encoding (symbols); Large dataset; Signal encoding; Anomaly detection; Auto encoders; Behavior-based; Large datasets; Learn+; Model-based OPC; Posterior distributions; Variational autoencoder; Variational inference; XAI; Unsupervised learning"
"Abukmeil M., Genovese A., Piuri V., Rundo F., Scotti F.","Towards explainable semantic segmentation for autonomous driving systems by multi-scale variational attention","10.1109/ICAS49788.2021.9551172","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117468856&doi=10.1109%2fICAS49788.2021.9551172&partnerID=40&md5=679cd4b63e4ab1c73cfdde41cbe1fbfe","Explainable autonomous driving systems (EADS) are emerging recently as a combination of explainable artificial intelligence (XAI) and vehicular automation (VA). EADS explains events, ambient environments, and engine operations of an autonomous driving vehicular, and it also delivers explainable results in an orderly manner. Explainable semantic segmentation (ESS) plays an essential role in building EADS, where it offers visual attention that helps the drivers to be aware of the ambient objects irrespective if they are roads, pedestrians, animals, or other objects. In this paper, we propose the first ESS model for EADS based on the variational autoencoder (VAE), and it uses the multiscale second-order derivatives between the latent space and the encoder layers to capture the curvatures of the neurons' responses. Our model is termed as Mgrad2 VAE and is bench-marked on the SYNTHIA and A2D2 datasets, where it outperforms the recent models in terms of image segmentation metrics. © 2021 IEEE.","Autonomous Driving System; ESS; VAE; XAI","Autonomous vehicles; Behavioral research; Image segmentation; Semantics; Ambient environment; Auto encoders; Autonomous driving; Autonomous driving system; Driving systems; Explainable semantic segmentation; Multi-scales; Semantic segmentation; Variational autoencoder; XAI; Semantic Segmentation"
"Abukmeil M., Ferrari S., Genovese A., Piuri V., Scotti F.","A Survey of Unsupervised Generative Models for Exploratory Data Analysis and Representation Learning","10.1145/3450963","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110303007&doi=10.1145%2f3450963&partnerID=40&md5=d645e4bb7f05c548a28cc554d22b7d92","For more than a century, the methods for data representation and the exploration of the intrinsic structures of data have developed remarkably and consist of supervised and unsupervised methods. However, recent years have witnessed the flourishing of big data, where typical dataset dimensions are high and the data can come in messy, incomplete, unlabeled, or corrupted forms. Consequently, discovering the hidden structure buried inside such data becomes highly challenging. From this perspective, exploratory data analysis plays a substantial role in learning the hidden structures that encompass the significant features of the data in an ordered manner by extracting patterns and testing hypotheses to identify anomalies. Unsupervised generative learning models are a class of machine learning models characterized by their potential to reduce the dimensionality, discover the exploratory factors, and learn representations without any predefined labels; moreover, such models can generate the data from the reduced factors' domain. The beginner researchers can find in this survey the recent unsupervised generative learning models for the purpose of data exploration and learning representations; specifically, this article covers three families of methods based on their usage in the era of big data: blind source separation, manifold learning, and neural networks, from shallow to deep architectures. © 2021 ACM.","Blind source separation; explainable machine learning; exploratory data analysis; manifold learning; neural networks; representation learning; unsupervised deep learning","Blind source separation; Data handling; Information analysis; Large dataset; Surveys; Data representations; Deep architectures; Exploratory data analysis; Hidden structures; Intrinsic structures; Machine learning models; Testing hypothesis; Unsupervised method; Learning systems"
"Abukmeil M., Ferrari S., Genovese A., Piuri V., Scotti F.","On Approximating the Non-negative Rank: Applications to Unsupervised Image Reduction","10.1109/CIVEMSA48639.2020.9132972","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089889679&doi=10.1109%2fCIVEMSA48639.2020.9132972&partnerID=40&md5=8865cd7e25435e32dbf46eaf603d37a5","Unsupervised Learning (UL) methods are a class of machine learning which aims to disentangle the representations and reduce the dimensionality among the data without any predefined labels. Among all UL methods, the Non-negative Matrix Factorization (NMF) factorizes the data into two subspaces of non-negative components. Moreover, the NMF enforces the non-negativity, sparsity, and part-based analysis, thus the representations can be interpreted and explained for the Explainable Artificial Intelligence (XAI) applications. However, one of the main issues when using the NMF is to impose the factorization rank r to identify the dimensionality of the subspaces, where the rank is usually unknown in advance and known as the non-negative rank that is used as a prior to carrying out the factorization. Accordingly, we propose a novel method for the non-negative rank r approximation to help solving this problem, and we generalize our method among different image scales. Where, the results on different image data sets confirm the validity of our approach. © 2020 IEEE.","Explainable Artificial Intelligence (XAI); Image Reduction; Non-negative Matrix Factorization; Non-negative Rank; Unsupervised Learning","Factorization; Intelligent computing; Image datasets; Image reduction; Image scale; Non negatives; Non-negativity; Nonnegative matrix factorization; Part based; Rank-R approximation; Matrix algebra"
"Abulaish M., Fazil M.","A machine learning approach for socialbot targets detection on Twitter","10.3233/JIFS-200682","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102410993&doi=10.3233%2fJIFS-200682&partnerID=40&md5=0e629273171d82dfb6c7ef17900f73ec","In online social networks (OSNs), socialbots are responsible for various malicious activities, and they are mainly programmed to imitate human-behavior to bypass the existing detection systems. The socialbots are generally successful in their malicious intent due to the existence of OSN users who follow them and thereby increase their reputation in the network. Analysis of the socialbot networks and their users is vital to comprehend the socialbot problem from target users' perspective. In this paper, we present a machine learning-based approach for characterizing and detecting socialbot targets, i.e., users who are susceptible to be trapped by the socialbots. We model OSN users based on their identity and behavior information, representing the static and dynamic components of their personality. The proposed approach classifies socialbot targets into three categories viz. active, reactive, and inactive users. We evaluate the proposed approach using three classifiers over a dataset collected from a live socialbot injection experiment conducted on Twitter. We also present a comparative evaluation of the proposed approach with a state-of-The-Art method and show that it performs significantly better. On feature ablation analysis, we found that network structure and user intention and personality related dynamic features are most discriminative, whereas static features show the least impact on the classification. Additionally, following rate, multimedia ratio, and follower rate are most relevant to segregate different categories of the socialbot targets. We also perform a detailed topical and behavioral analysis of socialbot targets and found active users to be suspicious. Further, joy and agreeableness are the most dominating personality traits among the three categories of the users. © 2021-IOS Press. All rights reserved.","Machine learning; social network analysis; social network security; socialbots; user profiling","Classification (of information); Machine learning; Online systems; Social networking (online); Turing machines; Behavioral analysis; Comparative evaluations; Machine learning approaches; Malicious activities; Network structures; Online social networks (OSNs); Personality traits; State-of-the-art methods; Behavioral research"
"Abungu C.Y.","Democratic Culture and the Development of Artificial Intelligence in the USA and China","10.1093/cjcl/cxaa032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112228024&doi=10.1093%2fcjcl%2fcxaa032&partnerID=40&md5=43073871b2e5f69af3a3a79b9626670a","Advancement in artificial intelligence has gradually become a pressing concern for the world's leading nations, especially the USA and China. In this article, the author confronts an argument made and alluded to in certain quarters: that when considered next to China's, the US democratic culture leaves it disadvantaged in the development of artificial intelligence insofar as it stands in the way of decisive and coordinated action. The author deploys a variety of case study situations to analyse the claim and eventually finds that the apparent disadvantage is, at worst, inexistent and, at best, negligible. Instead, it is argued that the differences in democratic culture between the two countries may, in fact, be responsible for the still-leading innovation within artificial intelligence development in the USA. Through demonstrating the faults of this disadvantage thesis, the final aim of this article is to call on US law- and policy-makers to retain faith in their democratic culture. © 2021 The Author(s) (2021). Published by Oxford University Press. All rights reserved. For permissions, please email: journals.permissions@oup.com.",,
"AbuSalim S., Zakaria N., Islam M.R., Kumar G., Mokhtar N., Abdulkadir S.J.","Analysis of Deep Learning Techniques for Dental Informatics: A Systematic Literature Review","10.3390/healthcare10101892","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140640771&doi=10.3390%2fhealthcare10101892&partnerID=40&md5=5b0778d10a67b94b3e44dc5af71c2233","Within the ever-growing healthcare industry, dental informatics is a burgeoning field of study. One of the major obstacles to the health care system’s transformation is obtaining knowledge and insightful data from complex, high-dimensional, and diverse sources. Modern biomedical research, for instance, has seen an increase in the use of complex, heterogeneous, poorly documented, and generally unstructured electronic health records, imaging, sensor data, and text. There were still certain restrictions even after many current techniques were used to extract more robust and useful elements from the data for analysis. New effective paradigms for building end-to-end learning models from complex data are provided by the most recent deep learning technology breakthroughs. Therefore, the current study aims to examine the most recent research on the use of deep learning techniques for dental informatics problems and recommend creating comprehensive and meaningful interpretable structures that might benefit the healthcare industry. We also draw attention to some drawbacks and the need for better technique development and provide new perspectives about this exciting new development in the field. © 2022 by the authors.","deep learning; dental diagnosis; dental informatics; dental practice; health informatics",
"Acar E., Gürdeniz G., Rasmussen M.A., Rago D., Dragsted L.O., Bro R.","Coupled matrix factorization with sparse factors to identify potential biomarkers in metabolomics","10.1109/ICDMW.2012.17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873126892&doi=10.1109%2fICDMW.2012.17&partnerID=40&md5=dd2d8a03014540bd290c1685521c83fd","Metabolomics focuses on the detection of chemical substances in biological fluids such as urine and blood using a number of analytical techniques including Nuclear Magnetic Resonance (NMR) spectroscopy and Liquid Chromatography-Mass Spectroscopy (LC-MS). Among the major challenges in analysis of metabolomics data are (i) joint analysis of data from multiple platforms and (ii) capturing easily interpretable underlying patterns, which could be further utilized for biomarker discovery. In order to address these challenges, we formulate joint analysis of data from multiple platforms as a coupled matrix factorization problem with sparsity constraints on the factor matrices. We develop an all-at-once optimization algorithm, called CMF-SPOPT (Coupled Matrix Factorization with SParse OPTimization), which is a gradientbased optimization approach solving for all factor matrices simultaneously. Using numerical experiments on simulated data, we demonstrate that CMF-SPOPT can capture the underlying sparse patterns in data. Furthermore, on a real data set of blood samples collected from a group of rats, we use the proposed approach to jointly analyze metabolomic data sets and identify potential biomarkers for apple intake. © 2012 IEEE.","Coupled matrix factorization; Gradientbased optimization; Metabolomics; Missing data; Sparsity","Coupled matrix; Gradient-based optimization; Metabolomics; Missing data; Sparsity; Algorithms; Blood; Chemical detection; Data mining; Liquid chromatography; Mass spectrometry; Matrix algebra; Nuclear magnetic resonance spectroscopy; Optimization; Factor analysis"
"Accardo A., Restivo L., Ajčević M., Miladinović A., Iscra K., Silveri G., Merlo M., Sinagra G.","Toward a diagnostic CART model for Ischemic heart disease and idiopathic dilated cardiomyopathy based on heart rate total variability","10.1007/s11517-022-02618-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133655834&doi=10.1007%2fs11517-022-02618-9&partnerID=40&md5=e003de7d8e461d3fbfe577c3ea2fe2a5","Diagnosis of etiology in early-stage ischemic heart disease (IHD) and dilated cardiomyopathy (DCM) patients may be challenging. We aimed at investigating, by means of classification and regression tree (CART) modeling, the predictive power of heart rate variability (HRV) features together with clinical parameters to support the diagnosis in the early stage of IHD and DCM. The study included 263 IHD and 181 DCM patients, as well as 689 healthy subjects. A 24 h Holter monitoring was used and linear and non-linear HRV parameters were extracted considering both normal and ectopic beats (heart rate total variability signal). We used a CART algorithm to produce classification models based on HRV together with relevant clinical (age, sex, and left ventricular ejection fraction, LVEF) features. Among HRV parameters, MeanRR, SDNN, pNN50, LF, LF/HF, LFn, FD, Beta exp were selected by the CART algorithm and included in the produced models. The model based on pNN50, FD, sex, age, and LVEF features presented the highest accuracy (73.3%). The proposed approach based on HRV parameters, age, sex, and LVEF features highlighted the possibility to produce clinically interpretable models capable to differentiate IHD, DCM, and healthy subjects with accuracy which is clinically relevant in first steps of the IHD and DCM diagnostic process. Graphical abstract: [Figure not available: see fulltext.]. © 2022, The Author(s).","Computer-aided diagnosis; Dilated cardiomyopathy; Heart rate variability; Interpretable machine learning; Ischemic heart disease","Cardiology; Computer aided diagnosis; Computer aided instruction; Diseases; Finite difference method; Heart; Learning algorithms; Classification and regression tree models; Classification trees; Dilated cardiomyopathy; Healthy subjects; Heart rate variability; Heart-rate; Interpretable machine learning; Ischemic heart disease; Machine-learning; Total variabilities; Machine learning; aged; area under the curve; Article; cardiovascular risk factor; classification and regression tree; clinical feature; congestive cardiomyopathy; controlled study; coronary angiography; cross validation; decision tree; diagnostic accuracy; diagnostic test accuracy study; disease classification; electrocardiogram; extrasystole; female; heart left ventricle ejection fraction; heart left ventricle volume; heart rate variability; heart rhythm; heart ventricle extrasystole; Holter monitoring; human; ischemic heart disease; machine learning; major clinical study; male; recursive partitioning; regression analysis; RR interval; sympathetic nerve"
"Aceto G., Bovenzi G., Ciuonzo D., Montieri A., Persico V., Pescapé A.","Characterization and Prediction of Mobile-App Traffic Using Markov Modeling","10.1109/TNSM.2021.3051381","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099587597&doi=10.1109%2fTNSM.2021.3051381&partnerID=40&md5=b1364792b3d9416f220a4888e98561f1","Modeling network traffic is an endeavor actively carried on since early digital communications, supporting a number of practical applications, that range from network planning and provisioning to security. Accordingly, many theoretical and empirical approaches have been proposed in this long-standing research, most notably, Machine Learning (ML) ones. Indeed, recent interest from network equipment vendors is sparking around the evaluation of solid information-theoretical modeling approaches complementary to ML ones, especially applied to new network traffic profiles stemming from the massive diffusion of mobile apps. To cater to these needs, we analyze mobile-app traffic available in the public dataset MIRAGE-2019 adopting two related modeling approaches based on the well-known methodological toolset of Markov models (namely, Markov Chains and Hidden Markov Models). We propose a novel heuristic to reconstruct application-layer messages in the common case of encrypted traffic. We discuss and experimentally evaluate the suitability of the provided modeling approaches for different tasks: characterization of network traffic (at different granularities, such as application, application category, and application version), and prediction of network traffic at both packet and message level. We also compare the results with several ML approaches, showing performance comparable to a state-of-the-art ML predictor (Random Forest Regressor). Also, with this work we provide a viable and theoretically sound traffic-analysis toolset to help improving ML evaluation (and possibly its design), and a sensible and interpretable baseline. © 2021 IEEE.","Android apps; encrypted traffic; Markov models; mobile apps; traffic characterization; traffic modeling; traffic prediction","Decision trees; Digital communication systems; Information theory; Markov chains; Different granularities; Digital communications; Empirical approach; Encrypted traffic; Network equipment; Network planning and provisioning; Theoretical modeling; Traffic analysis; Hidden Markov models"
"Acharya B., Bacca S.","Gaussian process error modeling for chiral effective-field-theory calculations of np↔dγ at low energies","10.1016/j.physletb.2022.137011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125901420&doi=10.1016%2fj.physletb.2022.137011&partnerID=40&md5=42d8c68e2e8565e0aeb7d7fb160cbbd9","We calculate the energy-dependent cross section of the np↔dγ process in chiral effective field theory and apply state-of-the-art tools for quantification of theory uncertainty. We focus on the low-energy regime, where the magnetic dipole and the electric dipole transitions cross over, including the range relevant for big-bang nucleosynthesis. Working with the leading one- and two-body electromagnetic currents, we study the order-by-order convergence of this observable in the chiral expansion of the nuclear potential. We find that the Gaussian process error model describes the observed convergence very well, allowing us to present Bayesian credible intervals for the truncation error with correlations between the cross sections at different energies taken into account. We obtain a 1σ estimate of about 0.2% for the uncertainty from the truncation of the nuclear potential. This is an important step towards calculations with statistically interpretable uncertainties for astrophysical reactions involving light nuclei. © 2022","Bayesian analysis; Big Bang nucleosynthesis; Gaussian process; Machine learning; Uncertainty quantification",
"Acharya D., Guda R.K.S., Raovenkatajammalamadaka K.","Enhanced EfficientNet Network for Classifying Laparoscopy Videos using Transfer Learning Technique","10.1109/IJCNN55064.2022.9891989","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140708358&doi=10.1109%2fIJCNN55064.2022.9891989&partnerID=40&md5=53e29e63162b0813441548437ff70670","Recent days have seen a lot of interest in surgical data science (SDS) methods and imaging technologies. As a result of these developments, surgeons may execute less invasive procedures. Using pathology and no pathology situations to classify laparoscopic video pictures of surgical activities, in this research work authors conducted their investigation using a transfer learning technique named enhanced ENet (eENet) network based on enhanced EfficientNet network. Two base versions of the EfficientNet model named ENetB0 and ENetB7 along with the two proposed versions of the EfficientNet network as enhanced EfficientNetB0 (eENetB0) and enhanced EfficientnetB7 (eENetB7) are implemented in the proposed framework using publicly available GLENDA [1] dataset. The proposed eENetB0 and eENetB7 models have classified the features extracted using the transfer learning technique into binary classification. For 70-30 and 10-fold Cross-Validation (10-fold CV), the data splitting eENetB0 model has achieved maximum classification accuracy as 88.43% and 97.59%, and the eENetB7 model has achieved 97.72% and 98.78% accuracy. We also compared the performance of our proposed enhanced version of EfficientNet (eENetB0 and eENetB7) with the base version of the models (ENetB0 and ENetB7) it shows that among these four models eENetB7 performed well. For GUI-based visualization purposes, we also created a platform named IAS.ai that detects the surgical video clips having blood and dry scenarios and uses explainable AI for unboxing the deep learning model's performance. IAS.ai is a real-time application of our approach. For further validation, we compared our framework's performance with other leading approaches cited in the literature [2]-[4]. We can see how well the proposed eENet model does compare to existing models, as well as the current best practices. © 2022 IEEE.",,"Deep learning; Learning algorithms; Learning systems; Pathology; Transfer learning; 10-fold cross-validation; Binary classification; Classifieds; Data splitting; Imaging technology; Learning techniques; Network-based; Performance; Science methods; Transfer learning; Laparoscopy"
"Acharya S., Reza M.","Real-time emotion engagement tracking of students using human biometric emotion intensities","10.1016/B978-0-323-85209-8.00008-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131283274&doi=10.1016%2fB978-0-323-85209-8.00008-0&partnerID=40&md5=f676560a3df9aec6d47b8bc343d309bc","This chapter spouts the usefulness of using Human Emotion Intensities to track a Student’s emotional engagement in a class being part of Behavioral Biometrics. The behavioral biometrics deals with the patterns of human activities. It includes physical and emotional biometrics, which signifies a trait to recognize the behavior. In a classroom, student distraction contributes to being one of the factors of behavioral biometrics. It varies in magnitude and duration. Student engagement is frequently viewed as one of the signs of addressing low performance, apathy and disengagement, and high failure rates. The emotions are intertwined with our facial expressions regulated by 43 facial muscles, which themselves are managed by two facial nerves. The dominant features, which show an absent-minded face present in a class using FAU (Facial Action Unit), are examined. This covers multiple aspects of tracking, ranging from the student entering the classroom when the first frame is captured, and then respective structures are differentiated till he/she is present in the class. In our approach, initially, the facial landmarks were recognized by annotating the distracted face dataset. Then, the same is trained with the responsible pixel values and labels to help the machine identify the associated features of a distracted/undistracted face. After processing the dataset, we concatenated two models to train the distracted dataset, one is Custom-Deep Emotion Network using Convolutional Neural Network, and the other one is Reversed-NN to maintain the symmetricity. Thus, in the output layer, we have the concatenation of both the outputs to recognize whether a face is distracted/undistracted. The pipeline supports deploying the model in real-time cameras to trace the behavioral biometrics of students. The experiments performed on the database strengthened the “proposed characteristics” that can be used for biometrics intentions. © 2022 Elsevier Inc. All rights reserved.","Behavioral biometrics; Biometric; Computer vision; Deep emotion-CNN; FAU (Facial Action Units); Machine learning; Reversed-NN (neural network)",
"Achilleos K.G., Leandrou S., Prentzas N., Kyriacou P.A., Kakas A.C., Pattichis C.S.","Extracting Explainable Assessments of Alzheimer's disease via Machine Learning on brain MRI imaging data","10.1109/BIBE50027.2020.00175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099560689&doi=10.1109%2fBIBE50027.2020.00175&partnerID=40&md5=46da1c27beb04f3618f2837e461b29ca","A plethora of machine learning and deep learning methods are used for the assessment of Alzheimer's Disease (AD) from brain structural changes as seen in Magnetic Resonance Imaging (MRI) with highly satisfactory results. However, these models are black-box and lack an explicit declarative knowledge representation and thus there is a difficulty in generating the underlying explanatory imaging structures. The objective of this study was to investigate the usefulness of rule extraction in the assessment of AD using decision trees (DT) and random forests (RF) algorithms and integrating the extracted rules within an argumentation-based reasoning framework in order to make the results easy to interpret and explain. The DT and RF algorithms were applied on brain MRI images acquired from normal controls (NC) and AD subjects. The KNIME analytics platform was used to compute the DT and the R project was used for the RF. The argumentation model implemented in the Gorgias framework achieved an average accuracy of 91%, exhibiting improved results compared to the models of DT and RF. The overall performance of all models in this study is in agreement with other studies. In addition, the explanations given by our approach for the various possible predictions provide a more useful and complete assessment of the state of the patient/case at hand. This study demonstrated the usefulness of rule extraction in the assessment of AD based on MRI features and the positive results of the use of the argumentation based symbolic reasoning for composing and interpreting the ML results. © 2020 IEEE.","Alzheimer's disease; Argumentation; decision trees; Explainable AI; quantitative MRI; random forests","Bioinformatics; Decision trees; Deep learning; Extraction; Knowledge representation; Learning systems; Neurodegenerative diseases; Alzheimer's disease; Argumentation model; Based reasonings; Brain mri images; Declarative knowledge; Imaging structure; Learning methods; Symbolic reasoning; Magnetic resonance imaging"
"Achituve I., Kraus S., Goldberger J.","Interpretable Online Banking Fraud Detection Based on Hierarchical Attention Mechanism","10.1109/MLSP.2019.8918896","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077723448&doi=10.1109%2fMLSP.2019.8918896&partnerID=40&md5=cdd7df7b6d96091aa0d573f4bb30b06a","Online banking activities are constantly growing and are likely to become even more common as digital banking platforms evolve. One side effect of this trend is the rise in attempted fraud. However, there is very little work in the literature on online banking fraud detection. We propose an attention based architecture for classifying online banking transactions as either fraudulent or genuine. The proposed method allows transparency to its decision by identifying the most important transactions in the sequence and the most informative features in each transaction. Experiments conducted on a large dataset of real online banking data demonstrate the effectiveness of the method in terms of both classification accuracy and interpretability of the results. © 2019 IEEE.","attention; deep learning; fraud detection; interpretability; Online banking","Classification (of information); Crime; Deep learning; Large dataset; Machine learning; attention; Attention mechanisms; Classification accuracy; Fraud detection; Interpretability; On-line banking; Side effect; Signal processing"
"Achour Y., Pourghasemi H.R.","How do machine learning techniques help in increasing accuracy of landslide susceptibility maps?","10.1016/j.gsf.2019.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074322878&doi=10.1016%2fj.gsf.2019.10.001&partnerID=40&md5=d13434fcd18a2a981ea0277888a81639","Landslides are abundant in mountainous regions. They are responsible for substantial damages and losses in those areas. The A1 Highway, which is an important road in Algeria, was sometimes constructed in mountainous and/or semi-mountainous areas. Previous studies of landslide susceptibility mapping conducted near this road using statistical and expert methods have yielded ordinary results. In this research, we are interested in how do machine learning techniques help in increasing accuracy of landslide susceptibility maps in the vicinity of the A1 Highway corridor. To do this, an important section at Ain Bouziane (NE, Algeria) is chosen as a case study to evaluate the landslide susceptibility using three different machine learning methods, namely, random forest (RF), support vector machine (SVM), and boosted regression tree (BRT). First, an inventory map and nine input factors were prepared for landslide susceptibility mapping (LSM) analyses. The three models were constructed to find the most susceptible areas to this phenomenon. The results were assessed by calculating the receiver operating characteristic (ROC) curve, the standard error (Std. error), and the confidence interval (CI) at 95%. The RF model reached the highest predictive accuracy (AUC ​= ​97.2%) comparatively to the other models. The outcomes of this research proved that the obtained machine learning models had the ability to predict future landslide locations in this important road section. In addition, their application gives an improvement of the accuracy of LSMs near the road corridor. The machine learning models may become an important prediction tool that will identify landslide alleviation actions. © 2019 China University of Geosciences (Beijing) and Peking University","Algeria; Boosted regression tree; Random forest; Spatial modelling; Support vector machine; Validation measures","landslide; machine learning; mapping method; mountain region; prediction; regression analysis; slope stability; support vector machine; Algeria"
"Aciole Barbosa D., Menegidio F.B., Alencar V.C., Gonçalves R.S., Silva J.D.F.S., Vilas Boas R.O., Faustino de Maria Y.N.L., Jabes D.L., Costa de Oliveira R., Nunes L.R.","ParaDB: A manually curated database containing genomic annotation for the human pathogenic fungi Paracoccidioides spp.","10.1371/journal.pntd.0007576","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070659973&doi=10.1371%2fjournal.pntd.0007576&partnerID=40&md5=aa3ad9c60fcefa7e3a644852039709be","Background: The genus Paracoccidioides consists of thermodymorphic fungi responsible for Paracoccidioidomycosis (PCM), a systemic mycosis that has been registered to affect ~10 million people in Latin America. Biogeographical data subdivided the genus Paracoccidioides in five divergent subgroups, which have been recently classified as different species. Genomic sequencing of five Paracoccidioides isolates, representing each of these subgroups/species provided an important framework for the development of post-genomic studies with these fungi. However, functional annotations of these genomes have not been submitted to manual curation and, as a result, ~60–90% of the Paracoccidioides protein-coding genes (depending on isolate/annotation) are currently described as responsible for hypothetical proteins, without any further functional/structural description. Principal findings: The present work reviews the functional assignment of Paracoccidioides genes, reducing the number of hypothetical proteins to ~25–28%. These results were compiled in a relational database called ParaDB, dedicated to the main representatives of Paracoccidioides spp. ParaDB can be accessed through a friendly graphical interface, which offers search tools based on keywords or protein/DNA sequences. All data contained in ParaDB can be partially or completely downloaded through spreadsheet, multi-fasta and GFF3-formatted files, which can be subsequently used in a variety of downstream functional analyses. Moreover, the entire ParaDB environment has been configured in a Docker service, which has been submitted to the GitHub repository, ensuring long-term data availability to researchers. This service can be downloaded and used to perform fully functional local installations of the database in alternative computing ecosystems, allowing users to conduct their data mining and analyses in a personal and stable working environment. Conclusions: These new annotations greatly reduce the number of genes identified solely as hypothetical proteins and are integrated into a dedicated database, providing resources to assist researchers in this field to conduct post-genomic studies with this group of human pathogenic fungi. © 2019 Aciole Barbosa et al.",,"fungal protein; RNA 18S; RNA 28S; RNA 5S; small nuclear RNA; untranslated RNA; fungal protein; Article; computer model; data analysis; data base; data clustering; data mining; fungal gene; fungal genome; gene expression; gene ontology; gene sequence; genetic association study; genomic annotation; genomics; manually curated database; molecular genetics; nonhuman; Paracoccidioides; Paracoccidioides lutzii; phylogenetic tree; protein synthesis; sequence alignment; sequence homology; spliceosome; amino acid sequence; ecosystem; fungal genome; genetic database; genetics; human; isolation and purification; microbiology; molecular computer; nucleotide sequence; Paracoccidioides; research; South American blastomycosis; South and Central America; Amino Acid Sequence; Base Sequence; Computers, Molecular; Databases, Genetic; Ecosystem; Fungal Proteins; Genome, Fungal; Humans; Latin America; Molecular Sequence Annotation; Paracoccidioides; Paracoccidioidomycosis; Research"
"Ackerman S., Raz O., Zalmanovici M.","FreaAI: Automated Extraction of Data Slices to Test Machine Learning Models","10.1007/978-3-030-62144-5_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097426498&doi=10.1007%2f978-3-030-62144-5_6&partnerID=40&md5=557f5a008d7143613d62ab31b5e58fd3","Although machine learning (ML) solutions are prevalent, in order for them to be truly ‘business-grade’ and reliable, their performance must be shown to be robust for many different data subsets observations with similar feature values (which we call ‘slices’) which they are expected to encounter in deployment. However, ML solutions are often evaluated only based on aggregate performance (e.g., overall accuracy) and not on the variability on various slices. For example, a text classifier deployed on bank terms may have very high accuracy (e.g., but might perform poorly for the data slice of terms that include short descriptions and originate from commercial accounts. Yet a business requirement may be for the classifier to perform well regardless of the text characteristics. In previous work[1] we demonstrated the effectiveness of using feature-based analysis to highlight such gaps in performance assessment. Here we demonstrate a novel technique, called IBM FreaAI, which automatically extracts explainable feature slices for which the ML solution’s performance is statistically significantly worse than the average. We demonstrate results of evaluating ML classifier models on seven open datasets. © 2020, Springer Nature Switzerland AG.",,"Classification (of information); Aggregate performance; Automated extraction; Business requirement; Classifier models; Novel techniques; Overall accuracies; Performance assessment; Text classifiers; Machine learning"
"Acosta J., Nebot À., Villar P., Fuertes J.M.","Estimating the maintenance cost in electric networks using an hybrid soft computing methodology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870229370&partnerID=40&md5=58d720510f11b71a10fb682a5510658e","The main goal of this research is the development of a genetic fuzzy system (GFS) to solve the problem of estimating the maintenance cost of medium voltage lines in Spanish towns. The hybrid system is composed by the Fuzzy Inductive Reasoning (FIR) methodology and a genetic algorithm (GA) that is the responsible of determining in an automatic way the fuzzification parameters involved in the fuzzy system, i.e. the number of fuzzy sets (classes) per variable and the membership functions. The results obtained are compared with some of the most popular classical statistical modeling methods, neural networks and other hybrid evolutionary data analysis techniques.","Electric distribution networks; Fuzzy inductive reasoning; Genetic algorithms; Genetic fuzzy systems; Machine learning","Data analysis techniques; Fuzzifications; Fuzzy inductive reasoning; Genetic fuzzy systems; Maintenance cost; Medium voltage; Soft computing methodologies; Statistical modeling; Circuit theory; Electric power distribution; Fuzzy sets; Fuzzy systems; Genetic algorithms; Hybrid systems; Learning systems; Soft computing; Maintenance"
"Acosta-Vargas P., Salvador-Acosta B., Salvador-Ullauri L., Jadán-Guerrero J.","Accessibility challenges of e-commerce websites","10.7717/PEERJ-CS.891","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125879855&doi=10.7717%2fPEERJ-CS.891&partnerID=40&md5=f0445f2b087e9b0d176e338a4d510474","Today, there are many e-commerce websites, but not all of them are accessible. Accessibility is a crucial element that can make a difference and determine the success or failure of a digital business. The study was applied to 50 e-commerce sites in the top rankings according to the classification proposed by ecommerceDB. In evaluating the web accessibility of e-commerce sites, we applied an automatic review method based on a modification of Website Accessibility Conformance Evaluation Methodology (WCAG-EM) 1.0. To evaluate accessibility, we used Web Accessibility Evaluation Tool (WAVE) with the extension for Google Chrome, which helps verify password-protected, locally stored, or highly dynamic pages. The study found that the correlation between the ranking of e-commerce websites and accessibility barriers is 0.329, indicating that the correlation is low positive according to Spearman’s Rho. According to the WAVE analysis, the research results reveal that the top 10 most accessible websites are Sainsbury’s Supermarkets, Walmart, Target Corporation, Macy’s, IKEA, H&M Hennes, Chewy, Kroger, QVC, and Nike. The most significant number of accessibility barriers relate to contrast errors that must be corrected for e-commerce websites to reach an acceptable level of accessibility. The most neglected accessibility principle is perceivable, representing 83.1%, followed by operable with 13.7%, in third place is robust with 1.7% and finally understandable with 1.5%. Future work suggests constructing a software tool that includes artificial intelligence algorithms that help the software identify accessibility barriers. © 2022 Acosta-Vargas et al.","Accessibility; E-commerce; Wcag; Web; Websites","Electronic commerce; Accessibility barriers; Accessibility evaluation; Digital business; E-commerce sites; E-commerce websites; Evaluation methodologies; Wcag; Web; Web accessibility; Website accessibility; Websites"
"Acton A.L., Goswami T., Mckenna D.S.","Utility of near infrared spectroscopy for the screening of the growth restricted fetus","10.1111/cga.12015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883370216&doi=10.1111%2fcga.12015&partnerID=40&md5=e1fdadf15d6ad4cec9fe07cbabc1fd37","Abstract: A myriad of factors have been linked to increased risk for intrauterine growth restriction and the associated complications; the majority of which are based on observational statistics of demographics, socioeconomics and patient history. Unfortunately, there is a paucity of factors available that can appropriately address the underlying anatomy and physiology responsible for intrauterine growth restriction. To this point, it becomes necessary to use data acquisition modalities capable of addressing both the etiology and pathology in an effort to improve clinical management strategies. Near-infrared spectroscopy, although not traditionally used in standard, clinical screening has proven valuable for risk assessment in a number of recent investigational studies. Simulations based on the current literature are presented to assess near infrared spectroscopy utility regarding the ability to distinguish between the normal fetus and the growth restricted fetus. Findings are presented for all simulated data as well as the equipment-specific data derived from the NIRO-100 system (Hamamatsu Photonics, Hamamatsu, Japan). Results suggest an overall sensitivity and specificity on the order of 62% and 58%, respectively, and NIRO-100 sensitivity and specificity on the order of 85% and 92%, respectively. © 2013 The Authors Congenital Anomalies © 2013 Japanese Teratology Society.","Data mining; Intrauterine growth restriction; Near-infrared; Preterm; Small for gestational age","article; data mining; fetus; health care utilization; human; intrauterine growth retardation; near infrared spectroscopy; neonatology; prenatal screening; priority journal; risk assessment; sensitivity and specificity; simulation; socioeconomics; data mining; intrauterine growth restriction; near-infrared; preterm; small for gestational age; Female; Fetal Growth Retardation; Gestational Age; Humans; Oxygen Consumption; Placenta; Pregnancy; Reproducibility of Results; Sensitivity and Specificity; Spectroscopy, Near-Infrared"
"Acuña-Escobar D., Intriago-Pazmiño M., Ibarra-Fiallo J.","Weather Recognition Using Self-supervised Deep Learning","10.1007/978-3-030-99170-8_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128476456&doi=10.1007%2f978-3-030-99170-8_12&partnerID=40&md5=a596791bb0537a5621d341e39a94b201","The automatic recognition of weather in images has many important applications in different fields, such as: land and air traffic control, autonomous vehicles, road safety warnings, crop control, improvement of images taken in outdoor areas, among others. Despite the great applicability, this field of study has not yet been explored in detail, primarily due to the great challenge and difficulty involved in extracting deterministic features for each type of weather. Several works have focused their efforts on designing binary classifiers that allow determining just two classes. A difficulty lies especially in the fact that the target classes are not completely exclusive in an image. Different classes can share the same features. Another difficulty that previous work has faced is the need for a large number of labeled images to model the various weather states. In this work, we propose an approach called self-supervised deep learning applied to weather recognition in order to reduce the requirement of the huge amount of labeled images. Our architecture, a ResNet-50 implementation, is responsible for obtaining the representations of each unlabeled image with a self-supervised approach for both pre-training and fine-tuning steps. It has been used transfer learning for sharing the architecture between these steps. Our results reached an average accuracy of 0.8833. Based on this result, it can be concluded that self-supervised learning is a convenient solution to obtain high performance in the weather recognition task from digital images. © 2022, Springer Nature Switzerland AG.","Fine tuning; Residual learning; Self-supervised deep learning; Transfer learning; Weather recognition","Air traffic control; Deep learning; Motor transportation; Air traffics; Automatic recognition; Autonomous Vehicles; Fine tuning; Labeled images; Residual learning; Road safety; Self-supervised deep learning; Transfer learning; Weather recognition; Image enhancement"
"Adabor E.S.","A statistical analysis of antigenic similarity among influenza A (H3N2) viruses","10.1016/j.heliyon.2021.e08384","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119194861&doi=10.1016%2fj.heliyon.2021.e08384&partnerID=40&md5=af7c1a52f42d67dba0892a994f57a779","An accurate assessment of antigenic similarity between influenza viruses is important for vaccine strain recommendations and influenza surveillance. Due to the mechanisms that result in frequent changes in the antigenicities of strains, it is desirable to obtain an antigenic similarity measure that accounts for specific changes in strains that are of epidemiological importance in influenza. Empirically grounded statistical models best achieve this. In this study, an interpretable machine-learning model was developed using distinguishing features of antigenic variants to analyze antigenic similarity. The features comprised of cluster information, amino acid sequences located in known antigenic and receptor-binding sites of influenza A (H3N2). In order to assess validity of parameters, accuracy and relevance of model to vaccine effectiveness, the model was applied to influenza A (H3N2) viruses due to their abundant genetic data and epidemiological relevance to influenza surveillance. An application of the model revealed that all model parameters were statistically significant to determining antigenic similarity between strains. Furthermore, upon evaluating the model for predicting antigenic similarity between strains, it achieved 95% area under Receiver Operating Characteristic curve (AUC), 94% accuracy, 76% precision, 97% specificity, 68% sensitivity and a diagnostic odds ratio (DOR) of 83.19. Above all, the model was found to be strongly related to influenza vaccine effectiveness to indicate the correlation between vaccine effectiveness and antigenic similarity between vaccine and circulating strains in an epidemic. The study predicts probabilities of antigenic similarity and estimates changes in strains that lead to antigenic variants. A successful application of the methods presented in this study would complement the global efforts in influenza surveillance. © 2021 The Author","Antigenic similarity; Influenza virus; Machine learning; Statistical model; Vaccine effectiveness",
"Adadi A., Lahmer M., Nasiri S.","Artificial Intelligence and COVID-19: A Systematic umbrella review and roads ahead","10.1016/j.jksuci.2021.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111143925&doi=10.1016%2fj.jksuci.2021.07.010&partnerID=40&md5=77f64c6d6503c08884e7f230c67f0337","Artificial Intelligence (AI) has played a substantial role in the response to the challenges posed by the current pandemic. The growing interest in using AI to handle Covid-19 issues has accelerated the pace of AI research and resulted in an exponential increase in articles and review studies within a very short period of time. Hence, it is becoming challenging to explore the large corpus of academic publications dedicated to the global health crisis. Even with the presence of systematic review studies, given their number and diversity, identifying trends and research avenues beyond the pandemic should be an arduous task. We conclude therefore that after the one-year mark of the declaration of Covid-19 as a pandemic, the accumulated scientific contribution lacks two fundamental aspects: Knowledge synthesis and Future projections. In contribution to fill this void, this paper is a (i) synthesis study and (ii) foresight exercise. The synthesis study aims to provide the scholars a consolidation of findings and a knowledge synthesis through a systematic review of the reviews (umbrella review) studying AI applications against Covid-19. Following the PRISMA guidelines, we systematically searched PubMed, Scopus, and other preprint sources from 1st December 2019 to 1st June 2021 for eligible reviews. The literature search and screening process resulted in 45 included reviews. Our findings reveal patterns, relationships, and trends in the AI research community response to the pandemic. We found that in the space of few months, the research objectives of the literature have developed rapidly from identifying potential AI applications to evaluating current uses of intelligent systems. Only few reviews have adopted the meta-analysis as a study design. Moreover, a clear dominance of the medical theme and the DNN methods has been observed in the reported AI applications. Based on its constructive systematic umbrella review, this work conducts a foresight exercise that tries to envision the post-Covid-19 research landscape of the AI field. We see seven key themes of research that may be an outcome of the present crisis and which advocate a more sustainable and responsible form of intelligent systems. We set accordingly a post-pandemic research agenda articulated around these seven drivers. The results of this study can be useful for the AI research community to obtain a holistic view of the current literature and to help prioritize research needs as we are heading toward the new normal. © 2021 The Authors","Artificial Intelligence; Covid-19; Deep learning; Foresight analysis; Machine learning; Robotic; Umbrella review",
"Adadi A., Berrada M.","Explainable AI for Healthcare: From Black Box to Interpretable Models","10.1007/978-981-15-0947-6_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085188186&doi=10.1007%2f978-981-15-0947-6_31&partnerID=40&md5=a247ae41188e55948d9f59236f5eff75","As artificial intelligence penetrates deeper into work and personal life, it raises questions about trust and transparency. These questions are of greater consequence in healthcare where decisions are literally a matter of life and death. In this paper, we reflect on recent investigations about the interpretability and explainability of artificial intelligence methods and discuss their impact on medicine and healthcare. © Springer Nature Singapore Pte Ltd. 2020.","Explainable AI; Healthcare; Machine learning","Artificial intelligence; Health care; Artificial intelligence methods; Black boxes; Interpretability; Life and death; Personal lives; Embedded systems"
"Adadi A., Berrada M.","Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)","10.1109/ACCESS.2018.2870052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053352477&doi=10.1109%2fACCESS.2018.2870052&partnerID=40&md5=cb385461376b3fd0420c138ded6d133f","At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories. © 2018 IEEE.","black-box models; Explainable artificial intelligence; interpretable machine learning","Artificial intelligence; Bioinformatics; Biological systems; Learning systems; Surveys; Transparency; Biological system modeling; Black-box model; Conferences; Market researches; Prediction algorithms; Learning algorithms"
"Adak A., Pradhan B., Shukla N., Alamri A.","Unboxing Deep Learning Model of Food Delivery Service Reviews Using Explainable Artificial Intelligence (XAI) Technique","10.3390/foods11142019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136199561&doi=10.3390%2ffoods11142019&partnerID=40&md5=fc89e7dc947a679260b4095e207012e4","The demand for food delivery services (FDSs) during the COVID-19 crisis has been fuelled by consumers who prefer to order meals online and have it delivered to their door than to wait at a restaurant. Since many restaurants moved online and joined FDSs such as Uber Eats, Menulog, and Deliveroo, customer reviews on internet platforms have become a valuable source of information about a company’s performance. FDS organisations strive to collect customer complaints and effectively utilise the information to identify improvements needed to enhance customer satisfaction. However, only a few customer opinions are addressed because of the large amount of customer feedback data and lack of customer service consultants. Organisations can use artificial intelligence (AI) instead of relying on customer service experts and find solutions on their own to save money as opposed to reading each review. Based on the literature, deep learning (DL) methods have shown remarkable results in obtaining better accuracy when working with large datasets in other domains, but lack explainability in their model. Rapid research on explainable AI (XAI) to explain predictions made by opaque models looks promising but remains to be explored in the FDS domain. This study conducted a sentiment analysis by comparing simple and hybrid DL techniques (LSTM, Bi-LSTM, Bi-GRU-LSTM-CNN) in the FDS domain and explained the predictions using SHapley Additive exPlanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME). The DL models were trained and tested on the customer review dataset extracted from the ProductReview website. Results showed that the LSTM, Bi-LSTM and Bi-GRU-LSTM-CNN models achieved an accuracy of 96.07%, 95.85% and 96.33%, respectively. The model should exhibit fewer false negatives because FDS organisations aim to identify and address each and every customer complaint. The LSTM model was chosen over the other two DL models, Bi-LSTM and Bi-GRU-LSTM-CNN, due to its lower rate of false negatives. XAI techniques, such as SHAP and LIME, revealed the feature contribution of the words used towards positive and negative sentiments, which were used to validate the model. © 2022 by the authors.","deep learning; explainable AI; food delivery service; LIME; sentiment analysis; SHapley",
"Adak A., Pradhan B., Shukla N.","Sentiment Analysis of Customer Reviews of Food Delivery Services Using Deep Learning and Explainable Artificial Intelligence: Systematic Review","10.3390/foods11101500","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130924665&doi=10.3390%2ffoods11101500&partnerID=40&md5=dc64b385a1517f8bb85ab6bca9beb9ca","During the COVID-19 crisis, customers’ preference in having food delivered to their doorstep instead of waiting in a restaurant has propelled the growth of food delivery services (FDSs). With all restaurants going online and bringing FDSs onboard, such as UberEATS, Menulog or Deliveroo, customer reviews on online platforms have become an important source of information about the company’s performance. FDS organisations aim to gather complaints from customer feedback and effectively use the data to determine the areas for improvement to enhance customer satisfaction. This work aimed to review machine learning (ML) and deep learning (DL) models and explainable artificial intelligence (XAI) methods to predict customer sentiments in the FDS domain. A literature review revealed the wide usage of lexicon-based and ML techniques for predicting sentiments through customer reviews in FDS. However, limited studies applying DL techniques were found due to the lack of the model interpretability and explainability of the decisions made. The key findings of this systematic review are as follows: 77% of the models are non-interpretable in nature, and organisations can argue for the explainability and trust in the system. DL models in other domains perform well in terms of accuracy but lack explainability, which can be achieved with XAI implementation. Future research should focus on implementing DL models for sentiment analysis in the FDS domain and incorporating XAI techniques to bring out the explainability of the models. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","deep learning; explainable artificial intelli-gence; food delivery services; lime; sentiment analysis; shapley",
"Adak C., Chaudhuri B.B., Lin C.-T., Blumenstein M.","Why Not? Tell us the Reason for Writer Dissimilarity","10.1109/IJCNN48605.2020.9207245","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093825644&doi=10.1109%2fIJCNN48605.2020.9207245&partnerID=40&md5=ea3e31fc1b29c1884988e7fa0f9b9340","Writer verification has drawn significant attention over the past few decades due to its extensive applications in forensics and biometrics. In traditional writer verification, handwriting similarity/dissimilarity analysis is mostly performed by extracting two feature vectors from two respective handwritten samples, followed by comparing them in relation to their similarity. In the state-of-the-art writer verification approaches, a distance metric is usually employed in terms of the similarity between two handwritten samples. If the distance between two handwritten samples is greater than a given threshold, then the samples are assumed to be written by two different writers, otherwise, they are considered to be due to the same writer. In this paper, for the very first time, we propose a model that generates English sentences to explain reasons for writer dissimilarity/similarity. First, our proposed model obtains features from handwritten images by employing a convolutional neural network, verifies the writer using a Siamese architecture, and generates English words using a recurrent neural network. Finally, these two networks are merged using an affine transformation to produce an explanatory sentence in support of writer similarity/dissimilarity. We evaluated our model on a handwritten numeral database of 100 writers and obtained promising results. © 2020 IEEE.","Index Terms - Deep Neural Network, Explainable Artificial Intelligence, Handwriting Understanding, Writer Verification.","Convolutional neural networks; Affine transformations; Distance metrics; English sentences; Feature vectors; Handwritten images; Handwritten numeral; State of the art; Writer verification; Recurrent neural networks"
"Adams Gerald Michael, Meyer Charles N., Meyer Robert A.","MACHINE INTELLIGENCE FOR DOD COMMUNICATIONS SYSTEM CONTROL.",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023590545&partnerID=40&md5=87e05f7cd0356b78871cd0c16426928c","The use of machine intelligence techniques to provide real-time expert assistance to the human operators responsible for the management and control of the defense communications system is investigated. An evolutionary approach to the development of intelligent assistants, in which modulator hardware and software systems are developed and tested in an incremental progression of closely coupled projects, is described.",,"ARTIFICIAL INTELLIGENCE - Expert Systems; DEFENSE COMMUNICATIONS; EXPERT ASSISTANCE; HUMAN OPERATORS; INTELLIGENT ASSISTANTS; MACHINE INTELLIGENCE; MILITARY COMMUNICATIONS"
"Adams J., Agyenkwa-Mawuli K., Agyapong O., Wilson M.D., Kwofie S.K.","EBOLApred: A machine learning-based web application for predicting cell entry inhibitors of the Ebola virus","10.1016/j.compbiolchem.2022.107766","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137274982&doi=10.1016%2fj.compbiolchem.2022.107766&partnerID=40&md5=a50378db1518cff6c7aa4f5ca0894aca","Ebola virus disease (EVD) is a highly virulent and often lethal illness that affects humans through contact with the body fluid of infected persons. Glycoprotein and matrix protein VP40 play essential roles in the virus life cycle within the host. Whilst glycoprotein mediates the entry and fusion of the virus with the host cell membrane, VP40 is also responsible for viral particle assembly and budding. This study aimed at developing machine learning models to predict small molecules as possible anti-Ebola virus compounds capable of inhibiting the activities of GP and VP40 using Ebola virus (EBOV) cell entry inhibitors from the PubChem database as training data. Predictive models were developed using five algorithms comprising random forest (RF), support vector machine (SVM), naïve Bayes (NB), k-nearest neighbor (kNN), and logistic regression (LR). The models were evaluated using a 10-fold cross-validation technique and the algorithm with the best performance was the random forest model with an accuracy of 89 %, an F1 score of 0.9, and a receiver operating characteristic curve (ROC curve) showing the area under the curve (AUC) score of 0.95. LR and SVM models also showed plausible performances with overall accuracy values of 0.84 and 0.86, respectively. The models, RF, LR, and SVM were deployed as a web server known as EBOLApred accessible via http://197.255.126.13:8000/. © 2022 Elsevier Ltd","Ebola virus protein; Inhibitors; Logistic regression; Machine learning; Random forest; Support vector machine","Body fluids; Cytology; Diseases; Glycoproteins; Learning systems; Life cycle; Logistic regression; Nearest neighbor search; Proteins; Random forests; Support vector machines; Viruses; Ebola virus; Ebola virus protein; Inhibitor; Logistics regressions; Machine-learning; Performance; Random forests; Regression vectors; Support vectors machine; Virus proteins; Decision trees"
"Adams J., Hagras H.","A type-2 fuzzy logic approach to explainable ai for regulatory compliance, fair customer outcomes and market stability in the global financial sector","10.1109/FUZZ48607.2020.9177542","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090496841&doi=10.1109%2fFUZZ48607.2020.9177542&partnerID=40&md5=07f8d19cfad294088da2ee294b6593c1","The field of Artificial Intelligence (AI) is enjoying unprecedented success and is dramatically transforming the landscape of the financial services industry. However, there is a strong need to develop an accountability and explainability framework for AI in financial services, based on a risk-based assessment of appropriate explainability levels and techniques by use case and domain.This paper proposes a risk management framework for the implementation of AI in banking with consideration of explainability and outlines the implementation requirements to enable AI to achieve positive outcomes for financial institutions and the customers, markets and societies they serve. The work presents the evaluation of three algorithmic approaches (Neural Networks, Logistic Regression and Type 2 Fuzzy Logic with evolutionary optimisation) for nine banking use cases. We review the emerging regulatory and industry guidance on ethical and safe adoption of AI from key markets worldwide and compare leading AI explainability techniques.We will show that the Type-2 Fuzzy Logic models deliver very good performance which is comparable to or lagging marginally behind the Neural Network models in terms of accuracy, but outperform all models for explainability, thus they are recommended as a suitable machine learning approach for use cases in financial services from an explainability perspective. This research is important for several reasons: (i) there is limited knowledge and understanding of the potential for Type-2 Fuzzy Logic as a highly adaptable, high performing, explainable AI technique; (ii) there is limited cross discipline understanding between financial services and AI expertise and this work aims to bridge that gap; (iii) regulatory thinking is evolving with limited guidance worldwide and this work aims to support that thinking; (iv) it is important that banks retain customer trust and maintain market stability as adoption of AI increases. © 2020 IEEE.","Accountability and Explainability; Neural Networks; Regulatory Compliance; Type-2 Fuzzy Logic","Air navigation; Banking; Commerce; Computer circuits; Fintech; Fuzzy inference; Fuzzy systems; Logistic regression; Neural networks; Optimization; Regulatory compliance; Risk assessment; Risk management; Sales; Service industry; Algorithmic approach; Evolutionary optimisation; Financial institution; Financial services industries; Machine learning approaches; Neural network model; Risk management framework; Risk-based assessments; Fuzzy logic"
"Adams J.N., van Zelst S.J., Quack L., Hausmann K., van der Aalst W.M.P., Rose T.","A Framework for Explainable Concept Drift Detection in Process Mining","10.1007/978-3-030-85469-0_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115178805&doi=10.1007%2f978-3-030-85469-0_25&partnerID=40&md5=889ba105fbb7f19776ca44fe0568e0f3","Rapidly changing business environments expose companies to high levels of uncertainty. This uncertainty manifests itself in significant changes that tend to occur over the lifetime of a process and possibly affect its performance. It is important to understand the root causes of such changes since this allows us to react to change or anticipate future changes. Research in process mining has so far only focused on detecting, locating and characterizing significant changes in aprocess and not on finding root causes of such changes. In this paper, we aim to close this gap. We propose a framework that adds an explainability level onto concept drift detection in process mining and provides insights into the cause-effect relationships behind significant changes. We define different perspectives of a process, detect concept drifts in these perspectives and plug the perspectives into a causality check that determines whether these concept drifts can be causal to each other. We showcase the effectiveness of our framework by evaluating it on both synthetic and real event data. Our experiments show that our approach unravels cause-effect relationships and provides novel insights into executed processes. © 2021, Springer Nature Switzerland AG.","Cause-effect analysis; Concept drift; Process mining","Enterprise resource management; Cause-effect relationships; Changing business environment; Concept drifts; Finding roots; In-process; Root cause; Data mining"
"Adams S.M., Harralson A.F., Feroze H., Nguyen T., Eum S., Cornelio C.","Genome wide epistasis study of on-statin cardiovascular events with iterative feature reduction and selection","10.3390/jpm10040212","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096119346&doi=10.3390%2fjpm10040212&partnerID=40&md5=c271573f266fa138aa398cd720bc81eb","Predicting risk for major adverse cardiovascular events (MACE) is an evidence-based practice that incorporates lifestyle, history, and other risk factors. Statins reduce risk for MACE by decreasing lipids, but it is difficult to stratify risk following initiation of a statin. Genetic risk determinants for on-statin MACE are low-effect size and impossible to generalize. Our objective was to determine high-level epistatic risk factors for on-statin MACE with GWAS-scale data. Controlled-access data for 5890 subjects taking a statin collected from Vanderbilt University Medical Center’s BioVU were obtained from dbGaP. We used Random Forest Iterative Feature Reduction and Selection (RF-IFRS) to select highly informative genetic and environmental features from a GWAS-scale dataset of patients taking statin medications. Variant-pairs were distilled into overlapping networks and assembled into individual decision trees to provide an interpretable set of variants and associated risk. 1718 cases who suffered MACE and 4172 controls were obtained from dbGaP. Pathway analysis showed that variants in genes related to vasculogenesis (FDR = 0.024), angiogenesis (FDR = 0.019), and carotid artery disease (FDR = 0.034) were related to risk for on-statin MACE. We identified six gene-variant networks that predicted odds of on-statin MACE. The most elevated risk was found in a small subset of patients carrying variants in COL4A2, TMEM178B, SZT2, and TBXAS1 (OR = 4.53, p < 0.001). The RF-IFRS method is a viable method for interpreting complex “black-box” findings from machine-learning. In this study, it identified epistatic networks that could be applied to risk estimation for on-statin MACE. Further study will seek to replicate these findings in other populations. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Cardiovascular disease; Epistasis; Pharmacogenomics; Random forest; Statin","hydroxymethylglutaryl coenzyme A reductase inhibitor; simvastatin; angiogenesis; Article; cardiovascular disease; carotid artery disease; data processing; decision tree; demography; electronic medical record; epistasis; female; gene; gene frequency; gene linkage disequilibrium; genetic risk; genetic screening; genetic variability; genome-wide association study; genotype; human; iterative reconstruction; pharmacogenomics; phenotype; risk factor; software"
"Adão T., Pádua L., Pinho T.M., Hruška J., Sousa A., Sousa J.J., Morais R., Peres E.","Multi-purpose chestnut clusters detection using deep learning: A preliminary approach","10.5194/isprs-archives-XLII-3-W8-1-2019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074279645&doi=10.5194%2fisprs-archives-XLII-3-W8-1-2019&partnerID=40&md5=4ee1aae7fbc5ac3b731daee62020d0f2","In the early 1980′s, the European chestnut tree (Castanea sativa, Mill.) assumed an important role in the Portuguese economy. Currently, the Trás-os-Montes region (Northeast of Portugal) concentrates the highest chestnuts production in Portugal, representing the major source of income in the region (€50M-€60M). The recognition of the quality of the Portuguese chestnut varieties has increasing the international demand for both industry and consumer-grade segments. As result, chestnut cultivation intensification has been witnessed, in such a way that widely disseminated monoculture practices are currently increasing environmental disaster risks. Depending on the dynamics of the location of interest, monocultures may lead to desertification and soil degradation even if it encompasses multiple causes and a whole range of consequences or impacts. In Trás-os-Montes, despite the strong increase in the cultivation area, phytosanitary problems, such as the chestnut ink disease (Phytophthora cinnamomi) and the chestnut blight (Cryphonectria parasitica), along with other threats, e.g. chestnut gall wasp (Dryocosmus kuriphilus) and nutritional deficiencies, are responsible for a significant decline of chestnut trees, with a real impact on production. The intensification of inappropriate agricultural practices also favours the onset of phytosanitary problems. Moreover, chestnut trees management and monitoring generally rely on in-field time-consuming and laborious observation campaigns. To mitigate the associated risks, it is crucial to establish an effective management and monitoring process to ensure crop cultivation sustainability, preventing at the same time risks of desertification and land degradation. Therefore, this study presents an automatic method that allows to perform chestnut clusters identification, a key-enabling task towards the achievement of important goals such as production estimation and multi-temporal crop evaluation. The proposed methodology consists in the use of Convolutional Neural Networks (CNNs) to classify and segment the chestnut fruits, considering a small dataset acquired based on digital terrestrial camera. © 2019 International Society for Photogrammetry and Remote Sensing.","Chestnut; Chestnut Detection; Chestnut Tree; CNN; Convolutional Neural Networks; Deep Learning; DL; Rough Segmentation; Tiling Segmentation; Xception","Classification (of information); Climatology; Convolution; Crops; Cultivation; Deep learning; Deep neural networks; Disaster prevention; Disasters; Forestry; Neural networks; Plants (botany); Sustainable development; Chestnut; Chestnut Tree; Convolutional neural network; Rough segmentation; Xception; Fruits"
"Adão T., Pinho T.M., Ferreira A., Sousa A., Pádua L., Sousa J., Sousa J.J., Peres E., Morais R.","Digital ampelographer: A CNN based preliminary approach","10.1007/978-3-030-30241-2_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072898802&doi=10.1007%2f978-3-030-30241-2_23&partnerID=40&md5=f176a8379b4dd45c6186f402e573bc6f","Authenticity, traceability and certification are key to assure both quality and confidence to wine consumers and an added commercial value to farmers and winemakers. Grapevine variety stands out as one of the most relevant factors to be considered in wine identification within the whole wine sector value chain. Ampelography is the science responsible for grapevine varieties identification based on (i) in-situ visual inspection of grapevine mature leaves and (ii) on the ampelographer experience. Laboratorial analysis is a costly and time-consuming alternative. Both the lack of experienced professionals and context-induced error can severely hinder official regulatory authorities’ role and therefore bring about a significant impact in the value chain. The purpose of this paper is to assess deep learning potential to classify grapevine varieties through the ampelometric analysis of leaves. Three convolutional neural networks architectures performance are evaluated using a dataset composed of six different grapevine varieties leaves. This preliminary approach identified Xception architecture as very promising to classify grapevine varieties and therefore support a future autonomous tool that assists the wine sector stakeholders, particularly the official regulatory authorities. © Springer Nature Switzerland AG 2019.","Ampelography; CNN; Douro Demarcated Region; Grapevine identification; Precision viticulture; ResNet; VGG; Wine Certification; Xception","Deep learning; Network architecture; Neural networks; Ampelography; Douro Demarcated Region; Grapevine identifications; Precision viticulture; ResNet; Xception; Wine"
"Adapa K., Pillai M., Foster M., Charguia N., Mazur L.","Using Explainable Supervised Machine Learning to Predict Burnout in Healthcare Professionals","10.3233/SHTI220396","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131107515&doi=10.3233%2fSHTI220396&partnerID=40&md5=038d1fb2034d654c30d49f6cdb5e7792","Burnout in healthcare professionals (HCPs) is a multi-factorial problem. There are limited studies utilizing machine learning approaches to predict HCPs' burnout during the COVID-19 pandemic. A survey consisting of demographic characteristics and work system factors was administered to 450 HCPs during the pandemic (participation rate: 59.3%). The highest performing machine learning model had an area under the receiver operating curve of 0.81. The eight key features that best predicted burnout are excessive workload, inadequate staffing, administrative burden, professional relationships, organizational culture, values and expectations, intrinsic motivation, and work-life integration. These findings provide evidence for resource allocation and implementation of interventions to reduce HCPs' burnout and improve the quality of care. © 2022 European Federation for Medical Informatics (EFMI) and IOS Press.","burnout; healthcare professionals; supervised machine learning","Health care; Medical informatics; Burnout; Demographic characteristics; Health care professionals; Key feature; Machine learning approaches; Machine learning models; Participation rate; Receiver operating curves; Supervised machine learning; Work system; Supervised learning; burnout; diagnosis; health care delivery; health care personnel; human; pandemic; prevention and control; supervised machine learning; Burnout, Professional; Burnout, Psychological; COVID-19; Delivery of Health Care; Health Personnel; Humans; Pandemics; Supervised Machine Learning"
"Adday G.H., Subramaniam S.K., Zukarnain Z.A., Samian N.","Fault Tolerance Structures in Wireless Sensor Networks (WSNs): Survey, Classification, and Future Directions","10.3390/s22166041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136617665&doi=10.3390%2fs22166041&partnerID=40&md5=86abf34ea476d94be17665cf30b05657","The Industrial Revolution 4.0 (IR 4.0) has drastically impacted how the world operates. The Internet of Things (IoT), encompassed significantly by the Wireless Sensor Networks (WSNs), is an important subsection component of the IR 4.0. WSNs are a good demonstration of an ambient intelligence vision, in which the environment becomes intelligent and aware of its surroundings. WSN has unique features which create its own distinct network attributes and is deployed widely for critical real-time applications that require stringent prerequisites when dealing with faults to ensure the avoidance and tolerance management of catastrophic outcomes. Thus, the respective underlying Fault Tolerance (FT) structure is a critical requirement that needs to be considered when designing any algorithm in WSNs. Moreover, with the exponential evolution of IoT systems, substantial enhancements of current FT mechanisms will ensure that the system constantly provides high network reliability and integrity. Fault tolerance structures contain three fundamental stages: error detection, error diagnosis, and error recovery. The emergence of analytics and the depth of harnessing it has led to the development of new fault-tolerant structures and strategies based on artificial intelligence and cloud-based. This survey provides an elaborate classification and analysis of fault tolerance structures and their essential components and categorizes errors from several perspectives. Subsequently, an extensive analysis of existing fault tolerance techniques based on eight constraints is presented. Many prior studies have provided classifications for fault tolerance systems. However, this research has enhanced these reviews by proposing an extensively enhanced categorization that depends on the new and additional metrics which include the number of sensor nodes engaged, the overall fault-tolerant approach performance, and the placement of the principal algorithm responsible for eliminating network errors. A new taxonomy of comparison that also extensively reviews previous surveys and state-of-the-art scientific articles based on different factors is discussed and provides the basis for the proposed open issues. © 2022 by the authors.","error detection; error diagnosis; error recovery; Fault Tolerance (FT); Wireless Sensor Networks (WSNs)","Artificial intelligence; Computer system recovery; Error detection; Fault detection; Internet of things; Sensor nodes; Surveys; Error diagnosis; Error-recovery; Exponentials; Fault tolerance; Industrial revolutions; Real-time application; Stringents; Tolerance management; Unique features; Wireless sensor network; Fault tolerance"
"Adde A., Darveau M., Barker N., Cumming S.","Predicting spatiotemporal abundance of breeding waterfowl across Canada: A Bayesian hierarchical modelling approach","10.1111/ddi.13129","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089378072&doi=10.1111%2fddi.13129&partnerID=40&md5=361fa112a33928cd3b1d572e01f4499e","Aim: Our aim was to develop predictive statistical models for mapping the abundance of 18 waterfowl species at a pan-Canadian level. We refined the previous generation of national waterfowl models by (a) developing new, more interpretable statistical models that (b) explicitly account for spatiotemporal variations in waterfowl abundance, while (c) testing for associations with an updated suite of habitat covariates. Location: All of Canada, excluding the Northern Arctic ecozone. Methods: Our response variables were annual species counts on 2,227 aerial-survey segments over a period of 25 years (1990–2015). Combining machine-learning and hierarchical regression modelling, we devised an innovative covariate selection strategy to select for each species the best subset of a panel of 232 candidate habitat covariates. With the selected covariates, we implemented hierarchical generalized linear models in a Bayesian framework, using the integrated nested Laplace approximation and stochastic partial differential equation approaches. Results: On average, our models explained 47% of the observed variance for spatiotemporal predictions and 74% for temporally averaged spatial predictions. The 18 species models included 94 significant waterfowl-habitat associations involving 42 distinct habitat covariates, with an average of 5.3 covariates per model. Covariates for forest attributes were the most represented in our models. The proportional biomass of Populus tremuloides was the most frequently selected covariate (10/94 associations in 10/18 species). Model predictions generated spatial and spatiotemporal maps of species abundances over almost all of Canada. Main conclusions: We showed that it is possible to efficiently combine machine-learning, variable selection and hierarchical Bayesian methods that exploit high-dimensional covariate spaces. Our approach yielded powerful and easily interpretable species distribution models with very few covariates, while accounting for residual autocorrelation. Possible applications of the resulting models and maps include the development of biodiversity indicators, the evaluation and execution of conservation planning strategies, and ecosystem services monitoring. © 2020 The Authors. Diversity and Distributions published by John Wiley & Sons Ltd","Bayesian hierarchical models; breeding waterfowl; Canada; covariate selection; habitat; INLA-SPDE; spatiotemporal; species distribution modelling","abundance; aerial survey; Bayesian analysis; biodiversity; biomass; conservation planning; ecological modeling; ecosystem service; ecozone; habitat type; spatiotemporal analysis; strategic approach; waterfowl; Arctic; Canada; Anatidae; Populus tremuloides"
"Adderley R.","Exploring the differences between the cross industry process for data mining and the national intelligence model using a self organising map case study","10.1007/978-1-4471-4866-1_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903551859&doi=10.1007%2f978-1-4471-4866-1_7&partnerID=40&md5=1279869386861c0938b5838140a31001","All Police Analysts in the UK, and many Forces in Europe and the USA, use the National Intelligence Model as a means to provide relevant, timely and actionable intelligence. In order to produce the required documentation analysts have to mine a variety of in-house data systems but do not receive any formal data mining training. The Cross Industry Standard Process for Data Mining is a database agnostic data mining methodology which is logical and easy to follow. By using a self-organising map to suggest offenders who may be responsible for sets of house burglary, this study explores the difference between both processes and suggests that they could be used to complement each other in real Police work. © Springer-Verlag London 2013.",,
"Adderley R., Musgrove P.B.","Data mining case study: Modeling the behavior of offenders who commit serious sexual assaults",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035789623&partnerID=40&md5=730ad640bdda10baa0b8dc93f7efd4df","This paper looks at the use of a Self Organizing Map (SOM), to link of records of crimes of serious sexual attacks. Once linked a profile can be derived of the offender(s) responsible. The data was drawn from the major crimes database at the National Crime Faculty of the National Police Staff College Bramshill UK. The data was encoded from text by a small team of specialists working to a well-defined protocol. The encoded data was analyzed using SOMs. Two exercises were conducted. These resulted in the linking of several offences in to clusters each of which were sufficiently similar to have possibly been committed by the same offender(s). A number of clusters were used to form profiles of offenders. Some of these profiles were confirmed by independent analysts as either belonging to known offenders or appeared sufficiently interesting to warrant further investigation. The prototype was developed over 10 weeks. This contrasts with an in-house study using a conventional approach, which took 2 years to reach similar results. As a consequence of this study the NCF intends to pursue an in-depth follow up study.","Crime Pattern Analysis; Data Mining; Knowledge Discovery; Offender Behavior; Self Organizing Map","Data mining; Database systems; Knowledge engineering; Self organizing maps; Crime pattern analysis; Behavioral research"
"Adebayo J., Gilmer J., Goodfellow I., Kim B.","Local explanation methods for deep neural networks lack sensitivity to parameter values",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083953956&partnerID=40&md5=0551bfbf03f56bf8c5989ad6dfc5adc2","Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN’s output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat surprisingly, we find that DNNs with randomly-initialized weights produce explanations that are both visually and quantitatively similar to those produced by DNNs with learned weights. Our conjecture is that this phenomenon occurs because these explanations are dominated by the lower level features of a DNN, and that a DNN’s architecture provides a strong prior which significantly affects the representations learned at these lower layers. © 6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings. All rights reserved.",,"Machine learning; Machine learning models; Sensitivity to parameters; Single input; Deep neural networks"
"Adel H., Palizban S.M.M., Sharifi S.S., Ilchi Ghazaan M., Habibnejad Korayem A.","Predicting mechanical properties of carbon nanotube-reinforced cementitious nanocomposites using interpretable ensemble learning models","10.1016/j.conbuildmat.2022.129209","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138454386&doi=10.1016%2fj.conbuildmat.2022.129209&partnerID=40&md5=792f4d3088b56c72933b8e3f169423ce","Compressive and flexural strength are important characteristics that indicate the efficiency of utilizing carbon nanotube (CNT) in cementitious nanocomposites. Preparing numerous test samples and conducting mechanical tests at different ages is costly and time-consuming, so a predictive model appears essential. Due to the complexity of recent building materials, experimental and statistical models do not function well for such complex materials, so machine learning techniques were adopted in this study. From the results of experiments performed in the literature, separate comprehensive data sets for compressive and flexural strength were collected, and water to cement ratio (W/C), CNT type, CNT content, CNT length, CNT diameter, sand to cement ratio for mortars (S/C), surfactant type, dispersion method, curing days (age) and compressive or flexural strengths of the control sample (C0 or F0) considered as input variables. Four ensemble learning-based algorithms, including random forest, AdaBoost, Gradient Boost, and XGBoost, were developed alongside a standalone model, the decision tree. To assess the sensitivity of input variables, four methodologies, including the Gini importance, permutation importance, F score, and mean of relative Shapely additive explanation (SHAP) values, were obtained, and compressive or flexural strengths of the control sample and CNT content identified overall as the most influential variables. The results revealed that XGBoost provides more reliable and accurate results for compressive and flexural characteristics and better maps the relationships between input variables. In addition, SHAP analysis was performed to explain the contribution of each input according to its value on the output of XGBoost. © 2022 Elsevier Ltd","Carbon nanotube (CNT); Cementitious nanocomposite; Ensemble learning; Machine learning; Sensitivity analysis; SHAP","Adaptive boosting; Bending strength; Carbon nanotubes; Cements; Compressive strength; Decision trees; Learning systems; Machine learning; Nanocomposites; Carbon nanotube; Cementitious; Cementitious nanocomposite; Compressive and flexural strengths; Control samples; Ensemble learning; Input variables; Learning models; Machine-learning; Shapely additive explanation; Sensitivity analysis"
"Adel T., Ghahramani Z., Wcller A.","Discovering interpretable representations for both deep generative and discriminative models",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057259441&partnerID=40&md5=aa05129c23cb1e46f9ff43975025b699","Interprctability of representations in both deep generative and discriminative models is highly desirable. Current methods jointly optimize an objective combining accuracy and interpretabil- ity. However, this may reduce accuracy, and is not applicable to already trained models. Wc propose two interprctability frameworks. First, we provide an interpretable lens for an existing model. We use a generative model which takes as input the representation in an existing (generative or discriminative) model, weakly supervised by limited side information. Applying a flexible and invertible transformation to the input leads to an interpretable representation with no loss in accuracy. We extend the approach using an active learning strategy to choose the most useful side information to obtain, allowing a human to guide what ""interpretable"" means. Our second framework relies on joint optimization for a representation which is both maximally infor-mative about the side information and maximally compressive about the non-interpretable data factors. This leads to a novel perspective on the relationship between compression and regulariza- tion. We also propose a new interprctability evaluation metric based on our framework. Empiri-cally, we achieve state-of-the-art results on three datasets using the two proposed algorithms. © Copyright 2018 by the Authors. All rights reserved.",,"Artificial intelligence; Active learning strategies; Discriminative models; Evaluation metrics; Generative model; Interpretable representation; Joint optimization; Side information; State of the art; Learning systems"
"Adela R., Reddy P.N.C., Ghosh T.S., Aggarwal S., Yadav A.K., Das B., Banerjee S.K.","Serum protein signature of coronary artery disease in type 2 diabetes mellitus","10.1186/s12967-018-1755-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060381300&doi=10.1186%2fs12967-018-1755-5&partnerID=40&md5=f4bb5edf37a3393988a81a19b9fa8f46","Background: Coronary artery disease (CAD) is the leading cause of morbidity and mortality in patients with type 2 diabetes mellitus (T2DM). The purpose of the present study was to discriminate the Indian CAD patients with or without T2DM by using multiple pathophysiological biomarkers. Methods: Using sensitive multiplex protein assays, we assessed 46 protein markers including cytokines/chemokines, metabolic hormones, adipokines and apolipoproteins for evaluating different pathophysiological conditions of control, T2DM, CAD and T2DM with CAD patients (T2DM-CAD). Network analysis was performed to create protein-protein interaction networks by using significantly (p < 0.05) altered protein markers in each disease using STRING 10.5 database. We used two supervised analysis methods i.e., between class analysis (BCA) and principal component analysis (PCA) to reveals distinct biomarkers profiles. Further, random forest classification (RF) was used to classify the diseases by the panel of markers. Results: Our two supervised analysis methods BCA and PCA revealed a distinct biomarker profiles and high degree of variability in the marker profiles for T2DM-CAD and CAD. Thereafter, the present study identified multiple potential biomarkers to differentiate T2DM, CAD, and T2DM-CAD patients based on their relative abundance in serum. RF classified T2DM based on the abundance patterns of nine markers i.e., IL-1β, GM-CSF, glucagon, PAI-I, rantes, IP-10, resistin, GIP and Apo-B; CAD by 14 markers i.e., resistin, PDGF-BB, PAI-1, lipocalin-2, leptin, IL-13, eotaxin, GM-CSF, Apo-E, ghrelin, adipsin, GIP, Apo-CII and IP-10; and T2DM -CAD by 12 markers i.e., insulin, resistin, PAI-1, adiponectin, lipocalin-2, GM-CSF, adipsin, leptin, Apo-AII, rantes, IL-6 and ghrelin with respect to the control subjects. Using network analysis, we have identified several cellular network proteins like PTPN1, AKT1, INSR, LEPR, IRS1, IRS2, IL1R2, IL6R, PCSK9 and MYD88, which are responsible for regulating inflammation, insulin resistance, and atherosclerosis. Conclusion: We have identified three distinct sets of serum markers for diabetes, CAD and diabetes associated with CAD in Indian patients using nonparametric-based machine learning approach. These multiple marker classifiers may be useful for monitoring progression from a healthy person to T2DM and T2DM to T2DM-CAD. However, these findings need to be further confirmed in the future studies with large number of samples. © 2019 The Author(s).","Adipokines; Apolipoproteins; Coronary artery diseases; Cytokines/chemokines; Metabolic hormones and biomarkers; Type 2 diabetes mellitus","adenosine kinase; adipocytokine; adiponectin; apolipoprotein; apolipoprotein A2; apolipoprotein B; apolipoprotein C2; apolipoprotein E; biological marker; complement factor D; eotaxin; gamma interferon inducible protein 10; gastric inhibitory polypeptide; ghrelin; glucagon; insulin receptor substrate 1; insulin receptor substrate 2; interleukin 1 receptor type II; interleukin 13; interleukin 1beta; interleukin 6; interleukin 6 receptor; leptin; neutrophil gelatinase associated lipocalin; plasminogen activating factor 1; platelet derived growth factor BB; protein tyrosine phosphatase 1B; RANTES; resistin; unclassified drug; unindexed drug; biological marker; plasma protein; adult; aged; Article; atherosclerosis; controlled study; coronary artery disease; correlation analysis; disease classification; female; hormone metabolism; human; inflammation; insulin resistance; machine learning; major clinical study; male; non insulin dependent diabetes mellitus; protein analysis; protein blood level; protein protein interaction; algorithm; area under the curve; blood; case control study; complication; coronary artery disease; metabolism; middle aged; non insulin dependent diabetes mellitus; principal component analysis; signal transduction; Adult; Aged; Algorithms; Area Under Curve; Biomarkers; Blood Proteins; Case-Control Studies; Coronary Artery Disease; Diabetes Mellitus, Type 2; Female; Humans; Machine Learning; Male; Middle Aged; Principal Component Analysis; Signal Transduction"
"Adelsberger R., Valko Y., Straumann D., Tröster G.","Automated romberg testing in patients with benign paroxysmal positional vertigo and healthy subjects","10.1109/TBME.2014.2354053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919898037&doi=10.1109%2fTBME.2014.2354053&partnerID=40&md5=014c82388030f995af9dabe5bb64d06e","Objective: Benign paroxysmal positional vertigo (BPPV) is the most common cause of dizziness. The underlying pathomechanism responsible for the recurrent vertigo attacks has been elucidated in detail and highly effective treatment strategies (liberation maneuvers) have been developed. However, many BPPV patients complain about problems of balance especially following liberation maneuvers.Aim: To objectively demonstrate differences in balance performance in BPPV patients compared to healthy subjects both prior and after BPPV liberation maneuvers.Methods: Seven patients with BPPV of the posterior semicircular canal and nine healthy subjects participated. To assess balance while standing, we analyzed the location and temporal stability of the center of pressure recorded by pressure-sensitive electronic soles during Romberg testing (on stable ground and on foam) and tandem stand. To assess regularity of gait, we analyzed the step frequency during walking of 50 m. All tests were performed prior and after liberation maneuvers in both groups.Results: Healthy subjects and patients differ significantly in their balance performance and use different stabilization strategies both prior and after liberation maneuvers. Both Romberg tests indicated poorer balance in BPPV patients (mean COP shifted towards toes), especially in posttreatment tests, while tandem stand appeared unaltered. We did not observe differences in escorted (by an experimenter) walking regularities between patients and healthy subjects and between pre- and post-maneuver testing.Conclusion and significance: Our findings confirm the typical clinical observation of a further posttreatment deterioration of already impaired postural performance in BPPV patients. While the etiology and the time course of this peculiar problem warrants further studies, the treating physician should be familiar with this transient side effect of therapeutic maneuvers to provide adequate counseling of patients. Finally, we successfully demonstrated the pressure-sensitive electronic soles as a new and potentially useful tool for both clinical and research purposes. © 2014 IEEE.","Balance; bipedal performance; BPPV; center of pressure; COP; embedded; sensors","Balancing; Sensors; bipedal performance; BPPV; Center of pressure; COP; embedded; Biomedical engineering; adult; Article; automated romberg testing; automation; benign paroxysmal positional vertigo; classifier; clinical article; controlled study; female; gait; human; male; middle aged; pressure; semicircular canal; standing; task performance; vestibular test; algorithm; artificial intelligence; automated pattern recognition; benign paroxysmal positional vertigo; comparative study; computer assisted diagnosis; device failure analysis; devices; equipment design; procedures; reference value; reproducibility; sensitivity and specificity; vestibular test; Adult; Algorithms; Artificial Intelligence; Benign Paroxysmal Positional Vertigo; Diagnosis, Computer-Assisted; Equipment Design; Equipment Failure Analysis; Humans; Male; Middle Aged; Pattern Recognition, Automated; Reference Values; Reproducibility of Results; Sensitivity and Specificity; Vestibular Function Tests"
"Adhane G., Dehshibi M.M., Masip D.","On the Use of Uncertainty in Classifying Aedes Albopictus Mosquitoes","10.1109/JSTSP.2021.3122886","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118559827&doi=10.1109%2fJSTSP.2021.3122886&partnerID=40&md5=5bb17ffb477f1ea01e110ada82251f63","The re-emergence of mosquito-borne diseases (MBDs), which kill hundreds of thousands of people each year, has been attributed to increased human population, migration, and environmental changes. Convolutional neural networks (CNNs) have been used by several studies to recognise mosquitoes in images provided by projects such as Mosquito Alert to assist entomologists in identifying, monitoring, and managing MBD. Nonetheless, utilising CNNs to automatically label input samples could involve incorrect predictions, which may mislead future epidemiological studies. Furthermore, CNNs require large numbers of manually annotated data. In order to address the mentioned issues, this paper proposes using the Monte Carlo Dropout method to estimate the uncertainty scores in order to rank the classified samples to reduce the need for human supervision in recognising Aedes albopictus mosquitoes. The estimated uncertainty was also used in an active learning framework, where just a portion of the data from large training sets was manually labelled. The experimental results show that the proposed classification method with rejection outperforms the competing methods by improving overall performance and reducing entomologist annotation workload. We also provide explainable visualisations of the different regions that contribute to a set of samples' uncertainty assessment. © 2007-2012 IEEE.","convolutional neural network; deep learning explainability; mosquito classification; Uncertainty","Computer vision; Convolution; Deep learning; Neural networks; Uncertainty analysis; Convolutional neural network; Deep learning; Deep learning explainability; MonteCarlo methods; Mosquito classification; Mosquito-borne disease; Predictive models; Uncertainty; Monte Carlo methods"
"Adhane G., Dehshibi M.M., Masip D.","A Deep Convolutional Neural Network for Classification of Aedes Albopictus Mosquitoes","10.1109/ACCESS.2021.3079700","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105866330&doi=10.1109%2fACCESS.2021.3079700&partnerID=40&md5=884ea0acde698cdda1aa8dfe1524333f","Monitoring the spread of disease-carrying mosquitoes is a first and necessary step to control severe diseases such as dengue, chikungunya, Zika or yellow fever. Previous citizen science projects have been able to obtain large image datasets with linked geo-tracking information. As the number of international collaborators grows, the manual annotation by expert entomologists of the large amount of data gathered by these users becomes too time demanding and unscalable, posing a strong need for automated classification of mosquito species from images. We introduce the application of two Deep Convolutional Neural Networks in a comparative study to automate this classification task. We use the transfer learning principle to train two state-of-the-art architectures on the data provided by the Mosquito Alert project, obtaining testing accuracy of 94%. In addition, we applied explainable models based on the Grad-CAM algorithm to visualise the most discriminant regions of the classified images, which coincide with the white band stripes located at the legs, abdomen, and thorax of mosquitoes of the Aedes albopictus species. The model allows us to further analyse the classification errors. Visual Grad-CAM models show that they are linked to poor acquisition conditions and strong image occlusions. © 2013 IEEE.","Aedes albopictus mosquito; alert project; Asian tiger mosquito; class activation map; convolutional neural network; explainable deep learning","Convolution; Deep neural networks; Disease control; Large dataset; Transfer learning; Automated classification; Classification errors; Classification tasks; Comparative studies; Manual annotation; Mosquito species; Spread of disease; Testing accuracy; Convolutional neural networks"
"Adhikari A., Adhikari N., Patra K.C.","Genetic Programming: A Complementary Approach for Discharge Modelling in Smooth and Rough Compound Channels","10.1007/s40030-019-00367-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071618586&doi=10.1007%2fs40030-019-00367-x&partnerID=40&md5=0bcf9f304b7aeed5043b7923213cae7d","Use of genetic programming (GP) in the field of river engineering is rare. During flood when the water overflows beyond its main course known as floodplain encounters various obstacles through rough materials and vegetation. Again the flow behaviour becomes more complex in a compound channel section due to shear at different regions. Discharge results from the experimental channels for varying roughness surfaces, along with data from a compound river section, are used in the GP. Model equations are derived for prediction of discharge in the compound channel using five hydraulic parameters. Derived models are tested and compared with other soft computing techniques. Few performance parameters are used to evaluate all the approaches taken for analysis. From the sensitivity analysis, the effects of parameters responsible for the flow behaviour are inferred. GP is found to give the most potential results with the highest level of accuracy. This work aims to benefit the researchers studying machine learning approaches for application in stream flow analysis. © 2019, The Institution of Engineers (India).","ANFIS; FIS; GP","Banks (bodies of water); Floods; Genetic algorithms; Global positioning system; Hydraulic machinery; Rivers; Sensitivity analysis; Shear flow; Soft computing; Stream flow; ANFIS; Discharge modelling; Experimental channels; Hydraulic parameters; Machine learning approaches; Performance parameters; Roughness surfaces; Softcomputing techniques; Genetic programming"
"Adhikari A., Tax D.M.J., Satta R., Faeth M.","LEAFAGE: Example-based and Feature importance-based Explanations for Black-box ML models","10.1109/FUZZ-IEEE.2019.8858846","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073788465&doi=10.1109%2fFUZZ-IEEE.2019.8858846&partnerID=40&md5=643bef68f599273880b31946a1ca9973","Explainable Artificial Intelligence (XAI) is an emergent research field which tries to cope with the lack of transparency of AI systems, by providing human understandable explanations for the underlying Machine Learning models. This work presents a new explanation extraction method called LEAFAGE. Explanations are provided both in terms of feature importance and of similar classification examples. The latter is a well known strategy for problem solving and justification in social science. LEAFAGE leverages on the fact that the reasoning behind a single decision/prediction for a single data point is generally simpler to understand than the complete model; it produces explanations by generating simpler yet locally accurate approximations of the original model. LEAFAGE performs overall better than the current state of the art in terms of fidelity of the model approximation, in particular when Machine Learning models with non-linear decision boundaries are analysed. LEAFAGE was also tested in terms of usefulness for the user, an aspect still largely overlooked in the scientific literature. Results show interesting and partly counter-intuitive findings, such as the fact that providing no explanation is sometimes better than providing certain kinds of explanation. © 2019 IEEE.","empirical study; example-based reasoning; eXplainable AI","Fuzzy systems; Machine learning; Empirical studies; Example based; Extraction method; Machine learning models; Model approximations; Scientific literature; Single decision; State of the art; Problem solving"
"Adhikari B., Era S.I., Mohamed M.D., Mia M.J.","Depression Level Prediction for Students Using Machine Learning in the Context of Bangladesh","10.1109/ICCCNT51525.2021.9579475","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126251851&doi=10.1109%2fICCCNT51525.2021.9579475&partnerID=40&md5=29b02799933bfdc8ff31ec218faf2c75","Depression represents a psychological illness or disorder which is very common nowadays. It's serious and has a growing number of problems that affect one's way of life and interferes with the functioning of the environment. Moreover, it has many detrimental impacts on society as well as the country, which lead to deteriorating the society. So, it's a matter of fact to taking proper steps to forecast the level of depression. Very few works have been done in this context. Our aim is to forecast the level of depression which helps us to get rid of this problem at the early stage. We have selected various factors which are responsible for these problems. These factors are determined with the help of a psychologist. We have employed eight machine learning classifiers for experiments and various performance evaluation matrices to evaluate our work. In the end, we have found that SMO performs better than any other classifiers, which is promising for our work. © 2021 IEEE.","Depression; Evaluation matrices; Machine learning; Psychologist; Students","Machine learning; Bangladesh; Depression; Evaluation matrices; Machine-learning; Performance evaluation matrix; Psychologist; Forecasting"
"Adhikari P., Jawad B., Rao P., Podgornik R., Ching W.-Y.","Delta Variant with P681R Critical Mutation Revealed by Ultra-Large Atomic-Scale Ab Initio Simulation: Implications for the Fundamentals of Biomolecular Interactions","10.3390/v14030465","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125668641&doi=10.3390%2fv14030465&partnerID=40&md5=780f2914d650cd7f278ea822e7495dd3","The SARS-CoV-2 Delta variant is emerging as a globally dominant strain. Its rapid spread and high infection rate are attributed to a mutation in the spike protein of SARS-CoV-2 allowing for the virus to invade human cells much faster and with an increased efficiency. In particular, an especially dangerous mutation P681R close to the furin cleavage site has been identified as responsible for increasing the infection rate. Together with the earlier reported mutation D614G in the same domain, it offers an excellent instance to investigate the nature of mutations and how they affect the interatomic interactions in the spike protein. Here, using ultra large-scale ab initio computational modeling, we study the P681R and D614G mutations in the SD2-FP domain, including the effect of double mutation, and compare the results with the wild type. We have recently developed a method of calculating the amino-acid–amino-acid bond pairs (AABP) to quantitatively characterize the details of the interatomic interactions, enabling us to explain the nature of mutation at the atomic resolution. Our most significant finding is that the mutations reduce the AABP value, implying a reduced bonding cohesion between interacting residues and increasing the flexibility of these amino acids to cause the damage. The possibility of using this unique mutation quantifiers in a machine learning protocol could lead to the prediction of emerging mutations. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Amino-acid–amino-acid bond pair; Delta variant; Interatomic interaction; Machine learning; SARS-CoV-2; Spike-protein","virus spike protein; ab initio calculation; amino acid sequence; Article; atom; chemical binding; chemical structure; gene mutation; genetic variability; hydrogen bond; hydrophobicity; molecular interaction; molecular model; mutagenesis; protein domain; protein interaction; protein structure; SARS-CoV-2 Delta; computer simulation; genetics; human; mutation; Computer Simulation; COVID-19; Humans; Mutation; SARS-CoV-2"
"Adhikary A., Majumder K., Chatterjee S., Shaw R.N., Ghosh A.","Machine Learning Based Approaches in the Detection of Parkinson’s Disease – A Comparative Study","10.1007/978-981-19-1677-9_68","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128983701&doi=10.1007%2f978-981-19-1677-9_68&partnerID=40&md5=640a791b4e87649ecd2bbf4e632b43ce","Parkinson’s disease (PD) is one of the enfeeble diseases that is incurable. It occurs when dopamine is not properly produced in substantia nigra of our central nervous system. Dopamine is responsible for our movement and activities like speaking, walking and writing. In the recent papers, researchers have found that speech, Freezing of Gait (FOG), writing problems are the most common symptoms for PD. However, most of the people who suffer from Parkinson have speech and FoG disorder. With the increase of severity, some other symptoms also arise gradually. Some works have been done by the researchers in early prediction and detection of PD using Machine Learning (ML) and Deep Learning (DL) classifiers. In this paper a detailed survey of the different works in this area has been carried out. We identified certain drawbacks of the existing works. Based on this, we tried to explore the future scope of works which can enhance the performance of the existing works in early detection of PD. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Deep learning; Machine learning; Parkinson’s Disease (PD)","Amines; Deep learning; Central nervous systems; Comparatives studies; Deep learning; Dopamine; Early prediction; Freezing of gaits; Learning-based approach; Machine-learning; Parkinson’s disease; Substantia nigra; Neurophysiology"
"Adib Q.A.R., Tasmi S.T., Bhuiyan S.I., Raihan M.S., Shams A.B.","Prediction Model for Mortality Analysis of Pregnant Women Affected with COVID-19","10.1109/ICCIT54785.2021.9689824","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125011855&doi=10.1109%2fICCIT54785.2021.9689824&partnerID=40&md5=29fefcf6bf054cd26780bc4b57f1cd52","COVID-19 pandemic is an ongoing global pandemic which has caused unprecedented disruptions in the public health sector and global economy. The virus, SARS-CoV-2 is responsible for the rapid transmission of coronavirus disease. Due to its contagious nature, the virus can easily infect an unprotected and exposed individual from mild to severe symptoms. The study of the virus's effects on pregnant mothers and neonatal is now a concerning issue globally among civilians and public health workers considering how the virus will affect the mother and the neonate's health. This paper aims to develop a predictive model to estimate the possibility of death for a COVID-diagnosed mother based on documented symptoms: dyspnea, cough, rhinorrhea, arthralgia, and the diagnosis of pneumonia. The machine learning models that have been used in our study are support vector machine, decision tree, random forest, gradient boosting, and artificial neural network. The models have provided impressive results and can accurately predict the mortality of pregnant mother's with a given input. The precision rate for 3 models(ANN, Gradient Boost, Random Forest) is 100% The highest accuracy score(Gradient Boosting, ANN) is 95%, highest recall(Support Vector Machine) is 92.75% and highest f1 score(Gradient Boosting, ANN) is 94.66%. Due to the accuracy of the model, pregnant mother can expect immediate medical treatment based on their possibility of death due to the virus. The model can be utilized by health workers globally to list down emergency patients, which can ultimately reduce the death rate of COVID-19 diagnosed pregnant mothers. © 2021 IEEE.","Artificial Neural Network; COVID-19; Decision Tree; Gradient Boosting; Machine Learning; Mortality Analysis; Pregnant Women; Random Forest; SMOTE; Support Vector Machine","Adaptive boosting; Diagnosis; Diseases; Neural networks; Public health; Support vector machines; Viruses; COVID-19; Gradient boosting; Machine-learning; Mortality analyse; Prediction modelling; Pregnant woman; Random forests; SMOTE; Support vectors machine; Workers'; Decision trees"
"Adibi J., Shen W.-M., Noorbakhsh E.","Self-similarity for data mining and predictive modeling a case study for network data","10.1007/3-540-47887-6_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945263132&doi=10.1007%2f3-540-47887-6_20&partnerID=40&md5=1faaad5b69946b8722420f187e4efcda","Recently there are a handful study and research on observing selfsimilarity and fractals in natural structures and scientific database such as traffic data from networks. However, there are few works on employing such information for predictive modeling, data mining and knowledge discovery. In this paper we study, analyze our experiments and observation of self-similar structure embedded in Network data for prediction through Self Similar Layered Hidden Markov Model (SSLHMM). SSLHMM is a novel alternative of Hidden Markov Models (HMM) which proven to be useful in a variety of real world applications. SSLHMM leverage HMM power and extend such capability to self-similar structures and exploit this property to reduce the complexity of predictive modeling process. We show that SSLHMM approach can captures self-similar information and provides more accurate and interpretable model comparing to conventional techniques. © Springer-Verlag Berlin Heidelberg 2002.",,"Hidden Markov models; Predictive analytics; Trellis codes; Conventional techniques; Data mining and knowledge discovery; Layered hidden markov models; Natural structures; Network data; Predictive modeling; Scientific database; Self-similarities; Data mining"
"Adibi J., Shen W.-M., Noorbakhsh E.","Self-similarity for data mining and predictive modeling a case study for network data","10.1117/12.460227","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036031928&doi=10.1117%2f12.460227&partnerID=40&md5=c0400ee0d1aeb6b3e87e882c433f36c8","Recently there are a handful study and research on observing self-similarity and fractals in natural structures and scientific database such as traffic data from networks. However, there are few works on employing such information for predictive modeling, data mining and knowledge discovery. In this paper we study, analyze our experiments and observation of self-similar structure embedded in Network data for prediction through Self Similar Layered Hidden Markov Model (SSLHMM). SSLHMM is a novel alternative of Hidden Markov Models (HMM) which proven to be useful in a variety of real world applications. SSLHMM leverage HMM power and extend such capability to self-similar structures and exploit this property to reduce the complexity of predictive modeling process. We show that SSLHMM approach can captures self-similar information and provides more accurate and interpretable model comparing to conventional techniques and one-step, flat method for model constructions such as HMM. © 2002 SPIE · 0277-786X/02/$15.00.",,"Database systems; Embedded systems; Fractals; Knowledge engineering; Markov processes; Mathematical models; Predictive modeling processes; Data mining"
"Adil S.M., Charalambous L.T., Rajkumar S., Seas A., Warman P.I., Murphy K.R., Rahimpour S., Parente B., Dharmapurikar R., Dunn T.W., Lad S.P.","Machine Learning to Predict Successful Opioid Dose Reduction or Stabilization After Spinal Cord Stimulation","10.1227/neu.0000000000001969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134430291&doi=10.1227%2fneu.0000000000001969&partnerID=40&md5=3ac965117e57054a49b608a12c3cb39f","BACKGROUND: Spinal cord stimulation (SCS) effectively reduces opioid usage in some patients, but preoperatively, there is no objective measure to predict who will most benefit. OBJECTIVE: To predict successful reduction or stabilization of opioid usage after SCS using machine learning models we developed and to assess if deep learning provides a significant benefit over logistic regression (LR). METHODS: We used the IBM MarketScan national databases to identify patients undergoing SCS from 2010 to 2015. Our models predict surgical success as defined by opioid dose stability or reduction 1 year after SCS. We incorporated 30 predictors, primarily regarding medication patterns and comorbidities. Two machine learning algorithms were applied: LR with recursive feature elimination and deep neural networks (DNNs). To compare model performances, we used nested 5-fold cross-validation to calculate area under the receiver operating characteristic curve (AUROC). RESULTS: The final cohort included 7022 patients, of whom 66.9% had successful surgery. Our 5-variable LR performed comparably with the full 30-variable version (AUROC difference <0.01). The DNN and 5-variable LR models demonstrated similar AUROCs of 0.740 (95% CI, 0.727-0.753) and 0.737 (95% CI, 0.728-0.746) ( P = .25), respectively. The simplified model can be accessed at SurgicalML.com . CONCLUSION: We present the first machine learning-based models for predicting reduction or stabilization of opioid usage after SCS. The DNN and 5-variable LR models demonstrated comparable performances, with the latter revealing significant associations with patients' pre-SCS pharmacologic patterns. This simplified, interpretable LR model may augment patient and surgeon decision making regarding SCS. Copyright © Congress of Neurological Surgeons 2022. All rights reserved.",,"narcotic analgesic agent; human; machine learning; spinal cord stimulation; statistical model; Analgesics, Opioid; Drug Tapering; Humans; Logistic Models; Machine Learning; Spinal Cord Stimulation"
"Adilina S., Farid D.M., Shatabda S.","Effective DNA binding protein prediction by using key features via Chou's general PseAAC","10.1016/j.jtbi.2018.10.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054864927&doi=10.1016%2fj.jtbi.2018.10.027&partnerID=40&md5=faed7d7c80b726251d724b3cf230a809","DNA-binding proteins (DBPs) are responsible for several cellular functions, starting from our immunity system to the transport of oxygen. In the recent studies, scientists have used supervised machine learning based methods that use information from the protein sequence only to classify the DBPs. Most of the methods work effectively on the train sets but performance of most of them degrades in the independent test set. It shows a room for improving the prediction method by reducing over-fitting. In this paper, we have extracted several features solely using the protein sequence and carried out two different types of feature selection on them. Our results have proven comparable on training set and significantly improved on the independent test set. On the independent test set our accuracy was 82.26% which is 1.62% improved compared to the previous best state-of-the-art methods. Performance in terms of sensitivity and area under receiver operating characteristic curve for the independent test set was also higher and they were 0.95 and 0.823 respectively. © 2018 Elsevier Ltd","Classification algorithm; DNA binding proteins; Feature selection; Handling overfitting; Independent test set; Sequence based features","DNA binding protein; DNA binding protein; algorithm; DNA; immune system; machine learning; oxygen; prediction; protein; testing method; algorithm; amino acid composition; Article; information processing; mathematical computing; mathematical model; performance; priority journal; protein analysis; pseudo amino acid composition; random forest; sensitivity analysis; sequence analysis; amino acid sequence; biology; chemistry; classification; procedures; receiver operating characteristic; reproducibility; support vector machine; Algorithms; Amino Acid Sequence; Computational Biology; DNA-Binding Proteins; Reproducibility of Results; ROC Curve; Support Vector Machine"
"Adilkhanova I., Ngarambe J., Yun G.Y.","Recent advances in black box and white-box models for urban heat island prediction: Implications of fusing the two methods","10.1016/j.rser.2022.112520","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131069005&doi=10.1016%2fj.rser.2022.112520&partnerID=40&md5=771d6a48931c03a2f059d45f63061328","The urban heat island (UHI) phenomenon is a serious concern for urban planners and policymakers, requiring effective and efficient mitigation policies. To develop such policies, accurate and pre-emptive estimations of current and future UHI manifestations are vital elements that help determine efficient policies and mitigation techniques. There are two fundamental approaches for modelling overheating in an urban environment: white-box and black-box based methods. The first one is characterized by the easily interpretable working process, while the unclear working procedure defines the second one. The present study comprehensively reviews the commonly used white-box and black-box based approaches applied for UHI predictions, analyses the existing literature adopting these tools for UHI prediction, and discusses the effectiveness of fusing both methods at the design and operation stages of the urban area for effective prediction and mitigation of UHI effect. The literature analysis showed that the transparent working process and high prediction accuracy of the physical-based white-box models make them a popular and reliable tool for UHI evaluation. Nevertheless, some white-box based simulation tools are too complex and require a high level of expertise to operate, leading to potential inaccuracies in the obtained outcomes. Black-box models, in turn, despite their opaque working process, are more straightforward in use and require less computation time. The fusion of these two methods is a novel approach that may benefit both UHI prediction and mitigation at the design and operation stages, respectively. © 2022 Elsevier Ltd","Black-box models; Machine learning; Physical-based modelling; The fusion of white box and black box models; UHI; White-box models","Atmospheric temperature; Forecasting; Urban planning; Black box modelling; Black boxes; Machine-learning; Physical based modeling; The fusion of white box and black box model; Urban heat island; White box; White-box models; Working process; Machine learning"
"Adiloglu K., Alpaslan F.N.","A machine learning approach to two-voice counterpoint composition","10.1016/j.knosys.2006.04.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947213260&doi=10.1016%2fj.knosys.2006.04.018&partnerID=40&md5=37c21d01cfa4da254cce9088037365b5","Algorithmic composition of musical pieces is one of the most popular areas of computer aided music research. Various attempts have been made successfully in the area of music composition. Artificial intelligence methods have been extensively applied in this area. Representation of musical pieces in a computer-understandable form plays an important role in computer aided music research. This paper presents a neural network-based knowledge representation schema for representing notes, melodies, and time in first species counterpoint pieces. A musical note is composed of pitch and duration in this representation schema. The proposed representation technique was tested using the back-propagation algorithm to generate two-voice counterpoint pieces. © 2006 Elsevier B.V. All rights reserved.","Algorithmic composition; Artificial neural networks; Counterpoint; Duration; Pitch","Artificial intelligence; Computer music; Musical instruments; Neural networks; Algorithmic composition; Counterpoints; Pitch; Learning systems"
"Aditya C.R., Pande M.B.S.","Devising an interpretable calibrated scale to quantitatively assess the dementia stage of subjects with alzheimer's disease: A machine learning approach","10.1016/j.imu.2016.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014029360&doi=10.1016%2fj.imu.2016.12.004&partnerID=40&md5=3d5d08d1b7cbe113bc2c1d8e4b0e7439","Background: Machine learning and data mining techniques have been successfully applied on MRI images for detecting Alzheimer's disease (AD). But only a few studies have explored the possibility of AD detection from non-image data. These studies have applied traditional data visualization and classification algorithms. There is a need for new sophisticated learning algorithms, for detecting and quantifying the severity of AD by exploring the complex interactions between the features in AD subjects. Method: In this work, a supervised learning model to effectively capture the complex feature interactions, in the sample space of AD data, is presented for knowledge discovery. The discovered knowledge is further used to quantify the similarity of a test subject to the demented class. Results: Evaluation of the proposed model, on OASIS database of Alzheimer's subjects, validates the well established risk factors and identifiers for AD: Age, Socio-Economic Status, MMSE Score and Whole Brain Volume. The Test subjects are affiliated to either non-demented (ND) or AD class, with non-overlapping and measurable similarity indices: Female ND (CDR=0) [0.48–2.90], Female AD (CDR=0.5) [90.16–774.51], Female AD (CDR=1) [1633.90–7182.23], Female AD (CDR=2) [55258.51–66382.44], Male ND (CDR=0)[0.69–3.66], Male AD (CDR=0.5) [99.18–647.51] and Male AD (CDR=1) [3880.16–6519.40]. Conclusion: The outcome of the work clearly demonstrates that, supervised learning model can be used effectively to quantify the severity of AD on a standard measurable scale. This scale of distance can be used as a supplement for clinical dementia rating. © 2017","Affiliation analysis; Alzheimer's disease; Dementia; Knowledge discovery; Multifactor dimensionality reduction; Similarity measure",
"Aditya Shastry K., Sanjay H.A., Lakshmi M., Preetham N.","Deep Learning for Medical Informatics and Public Health","10.1007/978-3-030-95419-2_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127859251&doi=10.1007%2f978-3-030-95419-2_13&partnerID=40&md5=79816f5b9e3aa91a22260c11b94d8972","The technology & healthcare intersect to constitute medical informatics (MI). MI enhances the outcomes of patients & healthcare through the skills of medical & computer sciences. The fusion of both these disciplines enables the related personnel to improve the patient care along with the research & clinical settings. Public health (PH) represents the science of safeguarding & improving community health. This outcome is attained by encouraging lifestyles that are healthy in nature, investigating new illnesses, preventing injuries and contagious illnesses. In general, PH focuses on safeguarding the health of entire communities. These communities can be small (local communities) or large (whole country/a global region). In this regard, Deep learning (DL) represents an interesting area of research. DL application has been observed in several domains due to the fast growth of both data & computational power. In recent years, the application of DL in the domain of MI has increased due to the probable advantages of DL applications in healthcare. DL can aid medical experts in diagnosing several illnesses, detecting sites of cancer, determining the impacts of medicines on each patient, comprehending the association among phenotypes & genotypes, discover novel phenotypes, & forecasting the outbreaks of contagious illnesses with higher precision. When compared to classical techniques, DL does not need data which is specific to a domain. DL is likely to transform the lives of humans. Regardless of these benefits, DL faces certain drawbacks related to data (higher number of features, dissimilar data, reliance on time, unsupervised data, etc.) & model (dependability, understandable, likelihood, scalable, safety) for real world applications. This chapter emphasizes on DL techniques applied in MI & PH, recent case studies related to the application of DL in MI & PH, and certain critical research questions. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Agriculture; Analysis; Big-data analytics; Prediction","Big data; Clinical research; Data Analytics; Deep learning; Diagnosis; Diseases; Health care; Medical computing; Medical informatics; Analyse; Clinical settings; Community health; Community OR; General publics; Global regions; Local community; Medical computer science; Medical informatics; Patient care; Public health"
"Aditya Shastry K., Sanjay H.A., Sajini M.C.","Decision Tree Based Crop Yield Prediction Using Agro-climatic Parameters","10.1007/978-981-16-1338-8_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120521050&doi=10.1007%2f978-981-16-1338-8_8&partnerID=40&md5=87df15b1c04517b24dd9c2f44711a887","The extraction of embedded knowledge from data is the fundamental task of data mining (DM). This extracted knowledge should be understandable to the end user. Earlier, statistical methods were utilized for the purpose of knowledge extraction. Later, semi-automated DM methods were developed through technological advancements. As data increased, these semi-automated DM techniques became ineffective. Hence, currently for synthesizing knowledge from data, fully automated DM techniques are being utilized. The early yield predictions of important crops such as soybean benefit the agriculture stakeholders by increasing their profits. This work deals with the prediction of soybean yield as high yield or low yield using ID3 algorithm. The result of ID3 was compared with naïve Bayes (NB) classifier. Results demonstrated that the ID3 algorithm performed with improvements of 7% when compared to the NB classifier. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Crop yield prediction; Data mining; Decision tree","Automation; Barium compounds; Crops; Decision trees; Extraction; Forecasting; Automated data mining; Climatic parameters; Crop yield; Crop yield prediction; Data-mining techniques; End-users; ID3 algorithm; Naive Bayes classifiers; Tree-based; Yield prediction; Data mining"
"Adje E.A., Houndji V.R., Dossou M.","Features analysis of internet traffic classification using interpretable machine learning models","10.11591/ijai.v11.i3.pp1175-1183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133548599&doi=10.11591%2fijai.v11.i3.pp1175-1183&partnerID=40&md5=7e7540a7f8e560dbed3b0661641f8112","Internet traffic classification is a fundamental task for network services and management. There are good machine learning models to identify the class of traffic. However, finding the most discriminating features to have efficient models remains essential. In this paper, we use interpretable machine learning algorithms such as decision tree, random forest and eXtreme gradient boosting (XGBoost) to find the most discriminating features for internet traffic classification. The dataset used contains 377,526 traffics. Each traffic is described by 248 features. From these features, we propose a 12-feature model with an accuracy of up to 99.76%. We tested it on another dataset with 19626 flows and obtained 98.40% of accuracy. This shows the efficiency and stability of our model. Also, we identify a set of 14 important features for internet traffic classification, including two that are crucial: port number (server) and minimum segment size (client to server). © 2022, Institute of Advanced Engineering and Science. All rights reserved.","Classification algorithm; Internet traffic; Machine learning; Traffic classification; Traffic internet discriminators",
"Adjed F., Mziou-Sallami M., Pelliccia F., Rezzoug M., Schott L., Bohn C., Jaafra Y.","Coupling algebraic topology theory, formal methods and safety requirements toward a new coverage metric for artificial intelligence models","10.1007/s00521-022-07363-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131051910&doi=10.1007%2fs00521-022-07363-6&partnerID=40&md5=ed7385d858490b772a3da50e2ddb68d0","Safety requirements are among the main barriers to the industrialization of machine learning based on deep learning architectures. In this work, a new metric of data coverage is presented by exploring the algebraic topology theory and the abstract interpretation process. The algebraic topology connects the cloud points of the dataset, and the abstract interpretation evaluates the robustness of the model. Thus, the coverage metric evaluates simultaneously the dataset and the robustness and highlights safe and unsafe areas. We also propose the first complete process to evaluate, in terms of data completeness, the machine learning models by providing a workflow based on the proposed metric and a set of safety requirements applied on autonomous driving. The obtained results provide an interpretable coverage evaluation and a promising line of research in the industrialization of artificial intelligence models. It is important to mention that the proposed metric is not dependent on the specific data. In other terms, it can be applied on 1 to n-dimensional data. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Abstract interpretation for artificial intelligence; Coverage metric; Deep reinforcement learning; Topological data analysis","Algebra; Deep learning; Formal methods; Model checking; Reinforcement learning; Topology; Abstract interpretation for artificial intelligence; Abstract interpretations; Algebraic topology; Coverage metrics; Industrialisation; Intelligence models; Learning architectures; Safety requirements; Topological data analysis; Topology theory; Abstracting"
"Adjei R.A., Wang W.Z., Liu Y.Z.","Aerodynamic design optimization of an axial flow compressor stator using parameterized free-form deformation","10.1115/1.4044692","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074579517&doi=10.1115%2f1.4044692&partnerID=40&md5=bc2fe48aff99d86bfa2470093df173b8","This paper describes an aerodynamic design optimization of a highly loaded compressor stator blade using parameterized free-form deformation (FFD). The optimization methodology presented utilizes a B-spline-based FFD control volume to map the blade from the object space to the parametric space via transformation operations in order to perturb the blade surface. Coupled with a multi-objective genetic algorithm (MOGA) and a Gaussian process-based response surface method (RSM), a fully automated iterative loop was used to run the optimization on a fitted correlation function. A weighted average reduction of 6.1% and 36.9% in total pressure loss and exit whirl angle was achieved, showing a better compromise of objective functions with smoother blade shape than other results obtained in the open literature. Data mining of the Pareto set of optimums revealed four groups of data interactions of which some design variables were found to have skewed scatter relationship with objective functions and can be redefined for further improvement of performance. Analysis of the flow field showed that the thinning of the blade at midspan and reduction in camber distribution were responsible for the elimination of the focal-type separation vortex by redirecting the secondary flow in an axially forward direction toward the midspan and near the hub endwall downstream. Furthermore, the reduction in exit whirl angle especially at the shroud was due to the mild bow shape which generated radial forces on the flow field thereby reducing the flow diffusion rate at the suction surface corner. This effect substantially delayed or eliminated the formation of corner separation at design and off-design operating conditions. Parameterized FFD was found to have superior benefits of smooth surface generation with low number of design variables while maintaining a good compromise between objective functions when coupled with a genetic algorithm. © 2019 by ASME","Data mining; Free-form deformation; Manufacturing constraints; Secondary flow losses; Surrogate-based global optimization","Aerodynamics; Data mining; Deformation; Flow fields; Genetic algorithms; Global optimization; Iterative methods; Parameterization; Secondary flow; Stators; Aerodynamic design optimization; Free-form deformation; Manufacturing constraint; Multi-objective genetic algorithm; Off-design operating conditions; Optimization methodology; Response surface method; Secondary flow loss; Axial-flow compressors"
"Adla A., Nachet B., Ould-Mahraz A.","A multi-agent view of distributed collaborative decision support systems","10.3233/978-1-61499-073-4-253","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879218477&doi=10.3233%2f978-1-61499-073-4-253&partnerID=40&md5=61684304722e6d365d57694948ea5965","In this paper, we propose a Multi-agent view of distributed collaborative decision support systems in which a facilitator and group decision makers are supported by intelligent agents. It considers group participants as multiple agents concerned with the quality of the collective decision. We define a facilitator agent as that agent responsible for the overall decision making process. This includes managing the complex negotiation processes that are required among those participants collaborating on decision making. The agents in the system autonomously plan and pursue their actions and sub-goals to cooperate, coordinate and negotiate with others, and to respond flexibly and intelligently to dynamic and unpredictable situations. Specifically, agents were used to collect information and generate alternatives that would allow the user to focus on solutions that were found to be significant. The decision making process, applied to the fault diagnosis in an oil plant, relies on a cycle that includes recognition of the causes of a defect (diagnosis), plan actions to solve the incidences and, execution of the selected actions. © 2012 The authors and IOS Press.","Collaborative decision making; Distributed decision support systems; Facilitation; Multi-agent systems","Artificial intelligence; Autonomous agents; Decision making; Decision support systems; Fault detection; Intelligent agents; Collaborative decision making; Collaborative decisions; Collective decision; Complex negotiation; Decision making process; Distributed decision support systems; Facilitation; Facilitator agents; Multi agent systems"
"Adlam B., Pennington J.","Understanding double descent requires a fine-grained bias-variance decomposition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099889036&partnerID=40&md5=7f5ac9bae7428aac96de38f91c463686","Classical learning theory suggests that the optimal generalization performance of a machine learning model should occur at an intermediate model complexity, with simpler models exhibiting high bias and more complex models exhibiting high variance of the predictive function. However, such a simple trade-off does not adequately describe deep learning models that simultaneously attain low bias and variance in the heavily overparameterized regime. A primary obstacle in explaining this behavior is that deep learning algorithms typically involve multiple sources of randomness whose individual contributions are not visible in the total variance. To enable fine-grained analysis, we describe an interpretable, symmetric decomposition of the variance into terms associated with the randomness from sampling, initialization, and the labels. Moreover, we compute the high-dimensional asymptotic behavior of this decomposition for random feature kernel regression, and analyze the strikingly rich phenomenology that arises. We find that the bias decreases monotonically with the network width, but the variance terms exhibit non-monotonic behavior and can diverge at the interpolation boundary, even in the absence of label noise. The divergence is caused by the interaction between sampling and initialization and can therefore be eliminated by marginalizing over samples (i.e. bagging) or over the initial parameters (i.e. ensemble learning). © 2020 Neural information processing systems foundation. All rights reserved.",,"Complex networks; Deep learning; Economic and social effects; Learning systems; Random processes; Turing machines; Asymptotic behaviors; Bias variance decomposition; Fine-grained analysis; Generalization performance; Intermediate model; Machine learning models; Monotonic behavior; Predictive function; Learning algorithms"
"Adlassnig K.-P.","Medical informatics in research, teaching, and patient care [MEDIZINISCHE INFORMATIK IN FORSCHUNG, LEHRE UN PATIENTENVERSORGUNG]",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028890155&partnerID=40&md5=496516a3433992808f1c3a3b36dcd741","The field of medical informatics in its current understanding is defined and criteria distinguishing this field from similar areas are provided. Special consideration is given to its position at a School of Medicine - in particular to the University of Vienna Medical School with the Vienna General Hospital as its teaching hospital. Demands for medical informatics and electronic data processing (EDP) in this extended field of activity come from four different sources: (1) research in medical informatics, (2) teaching of medical informatics as well as EDP training, (3) EDP service for research and teaching, and (4) EDP hospital operations to assist patient care. (Purely administrative EDP demands are not considered here.) It is shown that the different demands can be fulfilled by the usually available institutions involved in medical informatics and EDP at a School of Medicine. At many places these institutions are as follows: (1) a department or division of medical informatics with a possibly attached computer center dedicated to provide assistance in the area of research and teaching, (2) the computer center of the respective university the School of Medicine belongs to (3) the computer center of the hospital-owned institution responsible for all EDP activities connected to patient care, and (4) external software companies and EDP training centers. To succeed in the development of an exhaustive, school-wide system of medical informatics and EDP that considers the different demands in research, teaching, and EDP hospital operations equally, close and well-suited coordination between the institutions involved is necessary.","EDP hospital operations; EPD services; medical informatics; research; teaching","article; computer analysis; human; information processing; medical education; medical information; medical research; Artificial Intelligence; Austria; Education, Medical; English Abstract; Expert Systems; Hospital Information Systems; Human; Medical Informatics Applications; Medical Informatics Computing; Research; Software Design"
"Adler P., Falk C., Friedler S.A., Nix T., Rybeck G., Scheidegger C., Smith B., Venkatasubramanian S.","Auditing black-box models for indirect influence","10.1007/s10115-017-1116-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032187305&doi=10.1007%2fs10115-017-1116-3&partnerID=40&md5=5dedc1b9bfb8d45facd684650071d00c","Data-trained predictive models see widespread use, but for the most part they are used as black boxes which output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior and in particular how different features influence the model prediction. This is important when interpreting the behavior of complex models or asserting that certain problematic attributes (such as race or gender) are not unduly influencing decisions. In this paper, we present a technique for auditing black-box models, which lets us study the extent to which existing models take advantage of particular features in the data set, without knowing how the models work. Our work focuses on the problem of indirect influence: how some features might indirectly influence outcomes via other, related features. As a result, we can find attribute influences even in cases where, upon further direct examination of the model, the attribute is not referred to by the model at all. Our approach does not require the black-box model to be retrained. This is important if, for example, the model is only accessible via an API, and contrasts our work with other methods that investigate feature influence such as feature selection. We present experimental evidence for the effectiveness of our procedure using a variety of publicly available data sets and models. We also validate our procedure using techniques from interpretable learning and feature selection, as well as against other black-box auditing procedures. To further demonstrate the effectiveness of this technique, we use it to audit a black-box recidivism prediction algorithm. © 2017, Springer-Verlag London Ltd.","Algorithmic accountability; ANOVA; Black-box auditing; Deep learning; Discrimination-aware data mining; Feature influence; Interpretable machine learning","Analysis of variance (ANOVA); Data mining; Deep learning; Feature extraction; Forecasting; Genetic algorithms; Algorithmic accountability; Black boxes; Direct examinations; Experimental evidence; Feature influence; Modeling behavior; Prediction algorithms; Predictive models; Predictive analytics"
"Adler P., Falk C., Friedler S.A., Rybeck G., Scheidegger C., Smith B., Venkatasubramanian S.","Auditing black-box models for indirect influence","10.1109/ICDM.2016.158","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014529607&doi=10.1109%2fICDM.2016.158&partnerID=40&md5=16d83433f768ee47986bf33c3a930150","Data-Trained predictive models see widespread use, but for the most part they are used as black boxes which output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior, and in particular how different features influence the model prediction. This is important when interpreting the behavior of complex models, or asserting that certain problematic attributes (like race or gender) are not unduly influencing decisions. In this paper, we present a technique for auditing black-box models, which lets us study the extent to which existing models take advantage of particular features in the dataset, without knowing how the models work. Our work focuses on the problem of indirect influence: how some features might indirectly influence outcomes via other, related features. As a result, we can find attribute influences even in cases where, upon further direct examination of the model, the attribute is not referred to by the model at all. Our approach does not require the black-box model to be retrained. This is important if (for example) the model is only accessible via an API, and contrasts our work with other methods that investigate feature influence like feature selection. We present experimental evidence for the effectiveness of our procedure using a variety of publicly available datasets and models. We also validate our procedure using techniques from interpretable learning and feature selection, as well as against other black-box auditing procedures. © 2016 IEEE.",,"Data mining; Black boxes; Black-box model; Complex model; Direct examinations; Experimental evidence; Model prediction; Modeling behavior; Predictive models; Feature extraction"
"Adlung L., Cohen Y., Mor U., Elinav E.","Machine learning in clinical decision making","10.1016/j.medj.2021.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113773331&doi=10.1016%2fj.medj.2021.04.006&partnerID=40&md5=96e058558560599a6a627147b48301e7","Machine learning is increasingly integrated into clinical practice, with applications ranging from pre-clinical data processing, bedside diagnosis assistance, patient stratification, treatment decision making, and early warning as part of primary and secondary prevention. However, a multitude of technological, medical, and ethical considerations are critical in machine-learning utilization, including the necessity for careful validation of machine-learning-based technologies in real-life contexts, unbiased evaluation of benefits and risks, and avoidance of technological over-dependence and associated loss of clinical, ethical, and social-related decision-making capacities. Other challenges include the need for careful benchmarking and external validations, dissemination of end-user knowledge from computational experts to field users, and responsible code and data sharing, enabling transparent assessment of pipelines. In this review, we highlight key promises and achievements in integration of machine-learning platforms into clinical medicine while highlighting limitations, pitfalls, and challenges toward enhanced integration of learning systems into the medical realm. © 2021 Elsevier Inc.","artificial intelligence; computer-aided detection and diagnosis; personalized and precision medicine; recommendation systems","clinical decision making; human; machine learning; Clinical Decision-Making; Humans; Machine Learning"
"Admass W.S.","Developing knowledge-based system for the diagnosis and treatment of mango pests using data mining techniques","10.1007/s41870-022-00870-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125842182&doi=10.1007%2fs41870-022-00870-8&partnerID=40&md5=52d3f2defcdc444d5729ae642d9e4046","Detecting the pests of mango plants at an early stage requires an expert to identify the pests, describe the methods of treatment and protection. Expert systems help a great deal in identifying those diseases and pests and describing methods of treatment to be carried out. To empower the expert’s knowledge rule-based reasoning knowledge-based system is designed for the diagnosis and treatment of mango pests. In this study, the applicability of data mining techniques are demonstrated for the development of the rule-based knowledge-based system and the designed knowledge-based system helps to fill the knowledge gaps of human experts in the diagnosis and treatment of mango pests. Knowledge is acquired from a domain expert and document analysis. The acquired knowledge is modeled using hybrid knowledge modeling techniques and the modeled knowledge is represented in machine understandable format using production rules. The researcher used tools used for both knowledge modeling and knowledge representation. CommonKADS are used as knowledge modeling and prolog programming languages are used for rule representation. The knowledge extracted from boosting the J48 algorithm and the expert knowledge is integrated using integration at the decision phase approach. After the integration is done, a rule-based knowledge-based system prototype is implemented. The prototype knowledge-based system is evaluated on both system performance testing and user acceptance testing methods. Based on these evaluation techniques the overall performance of the designed model result achieves 90% accuracy. Finally, this study concludes that the integration of expert knowledge and data mining results in the development of a knowledge-based system that achieve better performance concerning the performance in the identification, recommending first-line treatment, and prevention of mango infection. The finding of this study can be used as supportive tools for agricultural extension workers, farmers, and farmworkers to help in the diagnosis and treatment of mango pests. © 2022, The Author(s), under exclusive licence to Bharati Vidyapeeth's Institute of Computer Applications and Management.","Data mining; Knowledge-based system; Mango; Mango pests; Rule-based reasoning",
"Adnan M.M.J., Hemmje M.L., Kaufmann M.A.","Social media mining to study social user group by visualizing tweet clusters using Word2Vec, PCA and k-means",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106020283&partnerID=40&md5=5253f3b409609579bcaf6215853defa1","In social media mining, unsupervised machine learning has not been studied extensively. Based on a specific use case in political polarization detection, we contribute an experimental method to find clusters in social media data and to explain the semantics of cluster membership using frequent word mentions. 100 dimensional word2vec word embedding vectors are generated from the raw text data. Based on this, three clustering approaches were tested: K-means, K-Means with Principal Components Analysis (PCA), and Deep Embedded Clustering (DEC). For K-Means, the optimal number of clusters was estimated visually. The performance of the clustering approaches was compared using the Silhouette score. Using PCA to reduce the dimensionality from 100 to only 2 most significant principal components had a significant performance impact, and combined with K-means, provided the best results. To explain the clusters visually, word counts of the most frequent words were plotted for three clusters. It became evident that the most frequent words per cluster explained the semantics of cluster membership in a consistent way. We conclude the following: our method demonstrates it is possible to find and explain meaningful clusters of users in social media data. Dimensionality reduction of text based on Word2Vec and PCA led to semantically coherent, clearly distinct clusters. Based on our understanding of the underlying use case, the clusters generated by the proposed method clearly reflect social phenomena that can be verified by a qualitative interpretation of the cluster visualizations. Thus, the proposed method can provide a basis for studying social behavior using social media data. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)","Cluster visualization; Computational social science; Explainable clustering; Interpretability; Social media mining","Birds; Data Science; Dimensionality reduction; Information retrieval; Semantics; Social networking (online); Cluster memberships; Cluster visualization; Experimental methods; Polarization detection; Principal Components; Principal components analysis; Social media minings; Unsupervised machine learning; K-means clustering"
"Adnan N., Zand M., Huang T.H.M., Ruan J.","Construction and Evaluation of Robust Interpretation Models for Breast Cancer Metastasis Prediction","10.1109/TCBB.2021.3120673","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118231351&doi=10.1109%2fTCBB.2021.3120673&partnerID=40&md5=962365c8de3344a8d01f8c76f3396b01","Interpretability of machine learning (ML) models represents the extent to which a model's decision-making process can be understood by model developers and/or end users. Transcriptomics-based cancer prognosis models, for example, while achieving good accuracy, are usually hard to interpret, due to the high-dimensional feature space and the complexity of models. As interpretability is critical for the transparency and fairness of ML models, several algorithms have been proposed to improve the interpretability of arbitrary classifiers. However, evaluation of these algorithms often requires substantial domain knowledge. Here, we propose a breast cancer metastasis prediction model using a very small number of biologically interpretable features, and a simple yet novel model interpretation approach that can provide personalized interpretations. In addition, we contributed, to the best of our knowledge, the first method to quantitatively compare different interpretation algorithms. Experimental results show that our model not only achieved competitive prediction accuracy, but also higher inter-classifier interpretation consistency than state-of-the-art interpretation methods. Importantly, our interpretation results can improve the generalizability of the prediction models. Overall, this work provides several novel ideas to construct and evaluate interpretable ML models that can be valuable to both the cancer machine learning community and related application domains. © 2004-2012 IEEE.","Cancer metastasis; feature engineering; interpretable machine learning; performance evaluation","Artificial intelligence; Bioinformatics; Biological systems; Decision making; Diseases; Forecasting; Learning systems; Perturbation techniques; Biological system modeling; Breast Cancer; Cancer metastasis; Computational modelling; Feature engineerings; Interpretable machine learning; Performances evaluation; Perturbation method; Prediction algorithms; Predictive models; Pathology; algorithm; breast tumor; female; genetics; human; machine learning; melanoma; skin tumor; Algorithms; Breast Neoplasms; Female; Humans; Machine Learning; Melanoma; Skin Neoplasms"
"Adomavicius G., Wang Y.","Improving Reliability Estimation for Individual Numeric Predictions: A Machine Learning Approach","10.1287/ijoc.2020.1019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129929029&doi=10.1287%2fijoc.2020.1019&partnerID=40&md5=5686bc3517a7e7fabe5b6ef430b2b4e9","Numerical predictive modeling is widely used in different application domains. Although many modeling techniques have been proposed, and a number of different aggregate accuracy metrics exist for evaluating the overall performance of predictive models, other important aspects, such as the reliability (or confidence and uncertainty) of individual predictions, have been underexplored. We propose to use estimated absolute prediction error as the indicator of individual prediction reliability, which has the benefits of being intuitive and providing highly interpretable information to decision makers, as well as allowing for more precise evaluation of reliability estimation quality. As importantly, the proposed reliability indicator allows the reframing of reliability estimation itself as a canonical numeric prediction problem, which makes the proposed approach general-purpose (i.e., it can work in conjunction with any outcome prediction model), alleviates the need for distributional assumptions, and enables the use of advanced, state-of-the-art machine learning techniques to learn individual prediction reliability patterns directly from data. Extensive experimental results on multiple real-world data sets show that the proposed machine learning-based approach can significantly improve individual prediction reliability estimation as compared with a number of baselines from prior work, especially in more complex predictive scenarios. Copyright: © 2021 INFORMS.","machine learning; numeric prediction; reliability of individual predictions","Decision making; Machine learning; Reliability; Uncertainty analysis; Applications domains; Individual prediction; Machine learning approaches; Machine-learning; Modelling techniques; Numeric prediction; Performance; Predictive models; Reliability estimation; Reliability of individual prediction; Forecasting"
"Adrian A.B., Corchado J.C., Comeron J.M.","Predictive models of recombination rate variation across the drosophila melanogaster genome","10.1093/gbe/evw181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030620251&doi=10.1093%2fgbe%2fevw181&partnerID=40&md5=0fee2fb4bd745f141698dd53dc351f73","In all eukaryotic species examined, meiotic recombination, and crossovers in particular, occur non-randomly along chromosomes. The cause for this non-random distribution remains poorly understood but some specific DNA sequence motifs have been shown to be enriched near crossover hotspots in a number of species. We present analyses using machine learning algorithms to investigate whether DNA motif distribution across the genome can be used to predict crossover variation in Drosophila melanogaster, a species without hotspots. Our study exposes a combinatorial non-linear influence of motif presence able to account for a significant fraction of the genome-wide variation in crossover rates at all genomic scales investigated, from 20% at 5-kb to almost 70% at 2,500-kb scale. The models are particularly predictive for regions with the highest and lowest crossover rates and remain highly informative after removing sub-telomeric and -centromeric regions known to have strongly reduced crossover rates. Transcriptional activity during early meiosis and differences in motif use between autosomes and the X chromosome add to the predictive power of the models. Moreover, we show that population-specific differences in crossover rates can be partly explained by differences in motif presence. Our results suggest that crossover distribution in Drosophila is influenced by both meiosis-specific chromatin dynamics and very local constitutive open chromatin associated with DNA motifs that prevent nucleosome stabilization. These findings provide new information on the genetic factors influencing variation in recombination rates and a baseline to study epigenetic mechanisms responsible for plastic recombination as response to different biotic and abiotic conditions and stresses. © The Author 2016.","DNA motif analysis; Double strand break; Machine-learning algorithms; Recombination","chromatin; animal; biological model; chromatin; crossing over; Drosophila melanogaster; genetic polymorphism; genetics; insect chromosome; insect genome; machine learning; nucleotide motif; Animals; Chromatin; Chromosomes, Insect; Crossing Over, Genetic; Drosophila melanogaster; Genome, Insect; Machine Learning; Models, Genetic; Nucleotide Motifs; Polymorphism, Genetic"
"Adrian B., Dengel A.","Linked Open Data Perspectives: Incorporating Linked Open Data into Information Extraction on the Web [Linked Open Data Perspectives: Integration von Linked Open Data in Informationsextraktion im World Wide Web]","10.1524/itit.2011.0633","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117597646&doi=10.1524%2fitit.2011.0633&partnerID=40&md5=f9a2752c99ea180de0e95eea276bf2ac","Currently, the World Wide Web can be divided into two separate fields. The traditional Web of Documents consisting of hyperlinked web documents and the emerging Web of Data consisting of linked open data. We present ontology-based information extraction as core technology for bridging the gap between both fields. Based on this, we list three basic applications that integrate web data to web documents. Our SCOOBIE system can extract information of a linked open dataset mentioned as textual phrases in web documents. SCOOBIE returns machine interpretable metadata summarizing the content of a web document from the perspective of a linked open dataset. Based on SCOOBIE we present EPIPHANY, a system that returns extracted metadata back to the originating web document in form of semantic annotations. This allows users to request the Web of Data for more information about annotated subjects inside the web document. STERNTALER is a system that analyses extracted metadata from search results of a search engine. It generates semantic filters filled with facets of things that were extracted from web documents inside search results. This allows users filtering those web documents that contain information about specific subjects and facets. © by Oldenbourg Wissenschaftsverlag, Kaiserslautern, Germany 2011.","information extraction; semantic annotations; semantic filtering; semantic indexing; semantic Web","Data mining; Information filtering; Information retrieval; Information retrieval systems; Metadata; Ontology; Search engines; Semantic Web; World Wide Web; Basic application; Core technology; Linked open datum; Ontology-based information extraction; Semantic annotations; Semantic filtering; Semantic indexing; Semantic-Web; Web document; Web of datum; Linked data"
"Adriano B., Xia J., Yokoya N., Miura H., Matsuoka M., Koshimura S.","Damage Characterization in Urban Environments from Multitemporal Remote Sensing Datasets Built from Previous Events","10.1109/IGARSS39084.2020.9323415","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101965349&doi=10.1109%2fIGARSS39084.2020.9323415&partnerID=40&md5=e94aa2cd33960c1a5581b83b4d43befb","Disasters such as earthquakes, hurricanes, and flooding are responsible for large-scale infrastructure damages and loss of human lives. Immediately after disaster strikes, one of the most critical and difficult tasks is accurately assessing the extent and severity of the disaster. This task is especially challenging in areas isolated by the disaster; in such cases, remote sensing information provides the best alternative to tackle this problem. This paper presents a damage mapping framework using remote sensing imagery acquired from previous disasters. The proposed deep learning-based framework is trained to learn features related to building damage using imagery from previous disasters that were collected from different regions around the world. Then, it is tested to recognize damage from a different urban environment. © 2020 IEEE.","Damage Mapping; Deep Learning; Multitemporal; Tsunami-induced damage","Deep learning; Disasters; Geology; Urban planning; Building damage; Damage characterization; Damage mapping; Large scale infrastructures; Multi-temporal remote sensing; Remote sensing imagery; Remote sensing information; Urban environments; Remote sensing"
"Aechtner J., Cabrera L., Katwal D., Onghena P., Valenzuela D.P., Wilbik A.","Comparing User Perception of Explanations Developed with XAI Methods","10.1109/FUZZ-IEEE55066.2022.9882743","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138821769&doi=10.1109%2fFUZZ-IEEE55066.2022.9882743&partnerID=40&md5=a184bdf930cd911a8175c612d2680cf3","Artificial Intelligence (AI) has gained notable momentum, culminating in the rise of intelligent machines that deliver unprecedented levels of performance in many application sectors across the field. In recent years, the sophistication of these systems has increased to an extent where almost no human intervention is required for their deployment. A crucial feature for the practical deployment of AI-powered systems in critical decision-making processes is the ability to understand how these systems derive their decisions. Accordingly, the AI community is confronted with the barrier of explaining the reasoning behind machine-made decisions. Paradigms underlying this problem fall within the field of eXplainable AI (XAI). Research in this field has introduced various methods to shed light into black box models such as deep neural networks. While local explanation methods explain the reasoning behind an output for a single decision, global explanations aim to describe the general behaviour of a model, i.e. for all decisions. This paper investigates users' perceptions of local and global explanations generated with popular XAI methods-LIME, SHAP, and PDP-by conducting a survey to find which of the explanations are preferred by different users. Meanwhile, two hypotheses are tested: first, explanations increase users' trust in a system, and second, AI novices prefer local over global explanations. The results show that explanations from PDP achieved the best user evaluation among the considered XAI methods. © 2022 IEEE.",,"Decision making; Lime; Black box modelling; Community IS; Decision-making process; Human intervention; Intelligence communities; Intelligent machine; Performance; Single decision; User evaluations; User perceptions; Deep neural networks"
"Aellen F.M., Göktepe-Kavis P., Apostolopoulos S., Tzovara A.","Convolutional neural networks for decoding electroencephalography responses and visualizing trial by trial changes in discriminant features","10.1016/j.jneumeth.2021.109367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116931667&doi=10.1016%2fj.jneumeth.2021.109367&partnerID=40&md5=13a501286fc5567fa720c90700853408","Background: Deep learning has revolutionized the field of computer vision, where convolutional neural networks (CNNs) extract complex patterns of information from large datasets. The use of deep networks in neuroscience is mainly focused to neuroimaging or brain computer interface -BCI- applications. In electroencephalography (EEG) research, multivariate pattern analysis (MVPA) mainly relies on linear algorithms, which require a homogeneous dataset and assume that discriminant features appear at consistent latencies and electrodes across trials. However, neural responses may shift in time or space during an experiment, resulting in under-estimation of discriminant features. Here, we aimed at using CNNs to classify EEG responses to external stimuli, by taking advantage of time- and space- unlocked neural activity, and at examining how discriminant features change over the course of an experiment, on a trial by trial basis. New method: We present a novel pipeline, consisting of data augmentation, CNN training, and feature visualization techniques, fine-tuned for MVPA on EEG data. Results: Our pipeline provides high classification performance and generalizes to new datasets. Additionally, we show that the features identified by the CNN for classification are electrophysiologically interpretable and can be reconstructed at the single-trial level to study trial-by-trial evolution of class-specific discriminant activity. Comparison with existing techniques: The developed pipeline was compared to commonly used MVPA algorithms like logistic regression and support vector machines, as well as to shallow and deep convolutional neural networks. Our approach yielded significantly higher classification performance than existing MVPA techniques (p = 0.006) and comparable results to other CNNs for EEG data. Conclusion: In summary, we present a novel deep learning pipeline for MVPA of EEG data, that can extract trial-by-trial discriminative activity in a data-driven way. © 2021 The Authors","Classification; Convolutional neural networks; Deep learning; Electroencephalography; Feature extraction; Multivariate pattern analysis","article; controlled study; convolutional neural network; deep learning; electroencephalography; feature extraction; human; intermethod comparison; nerve potential; pipeline; support vector machine; algorithm; brain computer interface; Algorithms; Brain-Computer Interfaces; Electroencephalography; Neural Networks, Computer"
"Aevermann B.D., Shannon C.P., Novotny M., Ben-Othman R., Cai B., Zhang Y., Ye J.C., Kobor M.S., Gladish N., Lee A.H.-Y., Blimkie T.M., Hancock R.E., Llibre A., Duffy D., Koff W.C., Sadarangani M., Tebbutt S.J., Kollmann T.R., Scheuermann R.H.","Machine Learning-Based Single Cell and Integrative Analysis Reveals That Baseline mDC Predisposition Correlates With Hepatitis B Vaccine Antibody Response","10.3389/fimmu.2021.690470","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119047554&doi=10.3389%2ffimmu.2021.690470&partnerID=40&md5=11d4e0c92e67a09107104c666ad6e056","Vaccination to prevent infectious disease is one of the most successful public health interventions ever developed. And yet, variability in individual vaccine effectiveness suggests that a better mechanistic understanding of vaccine-induced immune responses could improve vaccine design and efficacy. We have previously shown that protective antibody levels could be elicited in a subset of recipients with only a single dose of the hepatitis B virus (HBV) vaccine and that a wide range of antibody levels were elicited after three doses. The immune mechanisms responsible for this vaccine response variability is unclear. Using single cell RNA sequencing of sorted innate immune cell subsets, we identified two distinct myeloid dendritic cell subsets (NDRG1-expressing mDC2 and CDKN1C-expressing mDC4), the ratio of which at baseline (pre-vaccination) correlated with the immune response to a single dose of HBV vaccine. Our results suggest that the participants in our vaccine study were in one of two different dendritic cell dispositional states at baseline – an NDRG2-mDC2 state in which the vaccine elicited an antibody response after a single immunization or a CDKN1C-mDC4 state in which the vaccine required two or three doses for induction of antibody responses. To explore this correlation further, genes expressed in these mDC subsets were used for feature selection prior to the construction of predictive models using supervised canonical correlation machine learning. The resulting models showed an improved correlation with serum antibody titers in response to full vaccination. Taken together, these results suggest that the propensity of circulating dendritic cells toward either activation or suppression, their “dispositional endotype” at pre-vaccination baseline, could dictate response to vaccination. Copyright © 2021 Aevermann, Shannon, Novotny, Ben-Othman, Cai, Zhang, Ye, Kobor, Gladish, Lee, Blimkie, Hancock, Llibre, Duffy, Koff, Sadarangani, Tebbutt, Kollmann and Scheuermann.","baseline correlates; canonical correlation analysis; dendritic cells; endotypes; machine learning; single cell RNA sequencing; vaccines","hepatitis B vaccine; major histocompatibility antigen class 2; recombinant hepatitis B vaccine; RNA 16S; hepatitis B antibody; hepatitis B vaccine; vaccine; antibody blood level; antibody titer; Article; blood sampling; clinical article; cohort analysis; dendritic cell; DNA methylation; flow cytometry; gene cluster; gene expression; hepatitis B; human; human cell; in vitro study; innate immunity; machine learning; mass spectrometry; myeloid dendritic cell; natural killer cell; observational study; peripheral blood mononuclear cell; prospective study; protein expression; quality control; real time polymerase chain reaction; RNA extraction; single cell RNA seq; T lymphocyte activation; vaccination; whole blood stimulation; adult; aged; correlation analysis; female; gene expression profiling; hepatitis B; high throughput sequencing; host pathogen interaction; immunology; male; metabolism; middle aged; procedures; single cell analysis; Adult; Aged; Canonical Correlation Analysis; Dendritic Cells; Female; Gene Expression Profiling; Hepatitis B; Hepatitis B Antibodies; Hepatitis B Vaccines; High-Throughput Nucleotide Sequencing; Host-Pathogen Interactions; Humans; Machine Learning; Male; Middle Aged; Single-Cell Analysis; Vaccination; Vaccine Efficacy"
"Afanasyev V., Chernyshenko V., Kuzmin V., Voronin V., Mkrttchian V.","Advanced information technology for development of electric power market","10.1007/s00170-021-07324-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107195699&doi=10.1007%2fs00170-021-07324-8&partnerID=40&md5=dc449c8d2aa66cb0d3ec76b45675590a","Features of the electric power market functioning, in comparison with common properties of modern markets, are considered. It is shown that continuous reforms, which take place in many countries, are far from reaching effective solution. Sectoral peculiarities, connected with both technological complexity of the field and economic specificity of the market participants, form very special model of the market. The main way to increase effectivity of the market, to make it more competitive and, in the same time, socially responsible, is to use widely and deeply the modern information technologies, including technologies of artificial intelligence. A technological scheme (especially targeted on features of the electric power market) has been proposed in the article. First, the technologies merge methods of collection, storage, processing, and presentation of information. The collection of market data should be automated, particularly, by the so-called intellectual avatars, which operate in the market as virtual agents. A reliable distributed storage of data, based on blockchain technology, is chosen. The big data technologies of data structuring, storage, and processing are involved. Datamining methods include effective presentation of results in forms of graphs and diagrams. The second set of technologies includes core mathematical models and “digital twins.” They are aimed for data interpolation and extrapolation, in particular, for prediction of further system dynamics. The used neural networks have option of self-learning and self-development. At last, the third set is a set of user interfaces, which provides market actors by complete and adopted information and is organized in accordance with ideas of ergonomics. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Economic competition; Electric power market; ICT; Technologies of artificial intelligence","Artificial intelligence; Digital twin; Ergonomics; Power markets; User interfaces; Data interpolation; Distributed storage; Effective solution; Electric power markets; Market participants; Modern information technologies; Technological complexity; Technological scheme; Digital storage"
"Afchar D., Melchiorre A.B., Schedl M., Hennequin R., Epure E.V., Moussallam M.","Explainability in music recommender systems","10.1002/aaai.12056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134434789&doi=10.1002%2faaai.12056&partnerID=40&md5=1e3fd59e360e5780db986c549f086619","The most common way to listen to recorded music nowadays is via streaming platforms, which provide access to tens of millions of tracks. To assist users in effectively browsing these large catalogs, the integration of music recommender systems (MRSs) has become essential. Current real-world MRSs are often quite complex and optimized for recommendation accuracy. They combine several building blocks based on collaborative filtering and content-based recommendation. This complexity can hinder the ability to explain recommendations to end users, which is particularly important for recommendations perceived as unexpected or inappropriate. While pure recommendation performance often correlates with user satisfaction, explainability has a positive impact on other factors such as trust and forgiveness, which are ultimately essential to maintain user loyalty. In this article, we discuss how explainability can be addressed in the context of MRSs. We provide perspectives on how explainability could improve music recommendation algorithms and enhance user experience. First, we review common dimensions and goals of recommenders explainability and in general of eXplainable Artificial Intelligence (XAI), and elaborate on the extent to which these apply-or need to be adapted-to the specific characteristics of music consumption and recommendation. Then, we show how explainability components can be integrated within a MRS and in what form explanations can be provided. Since the evaluation of explanation quality is decoupled from pure accuracy-based evaluation criteria, we also discuss requirements and strategies for evaluating explanations of music recommendations. Finally, we describe the current challenges for introducing explainability within a large-scale industrial MRS and provide research perspectives. © 2022 The Authors.",,"Industrial research; Music; Quality control; Recommender systems; 'current; Block based; Building blockes; Content-based recommendation; End-users; Music recommendation; Music recommender systems; Real-world; Recommendation accuracy; Recommendation performance; Collaborative filtering"
"Afchar D., Hennequin R.","Making Neural Networks Interpretable with Attribution: Application to Implicit Signals Prediction","10.1145/3383313.3412253","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092706698&doi=10.1145%2f3383313.3412253&partnerID=40&md5=ad221d25e0ef48ddbc2577c33032ba5e","Explaining recommendations enables users to understand whether recommended items are relevant to their needs and has been shown to increase their trust in the system. More generally, if designing explainable machine learning models is key to check the sanity and robustness of a decision process and improve their efficiency, it however remains a challenge for complex architectures, especially deep neural networks that are often deemed ""black-box"". In this paper, we propose a novel formulation of interpretable deep neural networks for the attribution task. Differently to popular post-hoc methods, our approach is interpretable by design. Using masked weights, hidden features can be deeply attributed, split into several input-restricted sub-networks and trained as a boosted mixture of experts. Experimental results on synthetic data and real-world recommendation tasks demonstrate that our method enables to build models achieving close predictive performances to their non-interpretable counterparts, while providing informative attribution interpretations. © 2020 ACM.","Implicit Recommender System; Interpretable machine learning","Deep neural networks; Recommender systems; Black boxes; Complex architectures; Decision process; Machine learning models; Mixture of experts; Predictive performance; Sub-network; Synthetic data; Neural networks"
"Afiaz A., Arusha A.R., Ananna N., Kabir E., Biswas R.K.","A national assessment of elective cesarean sections in Bangladesh and the need for health literacy and accessibility","10.1038/s41598-021-96337-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113267009&doi=10.1038%2fs41598-021-96337-0&partnerID=40&md5=34d6ee3ea934afab0705705f2ddf3b7c","There has been a gradual rise in the number of cesarean sections (CSs) in Bangladesh. The present study identified the cohort of women, who were more likely to opt for an elective CS based on their sociodemographic characteristics, pre-delivery care history, and media exposure, using the Bangladesh Multiple Indicator Cluster Survey-2019. The survey stratification adjusted logistic regression model and interpretable machine learning method of building classification trees were utilized to analyze a sample of 9202 women, alongside district-wise heat maps. One-in-five births (20%) were elective CSs in the 2 years prior to the survey. Women residing in affluent households with educated house-heads, who accessed antenatal care prior to delivery (AOR 4.12; 95% CI 3.06, 5.54) with regular access to media (AOR 1.31; 95% CI 1.10, 1.56) and who owned a mobile phone (AOR 1.25; 95% CI 1.04, 1.50) were more likely to opt for elective CSs, which suggests that health access and health literacy were crucial factors in women’s mode of delivery. Spatial analyses revealed that women living in larger cities had more elective CS deliveries, pointing towards the availability of better health and access to multiple safe delivery options in peripheral areas. © 2021, The Author(s).",,"adult; Bangladesh; cesarean section; elective surgery; female; health care delivery; health literacy; human; statistical model; Adult; Bangladesh; Cesarean Section; Elective Surgical Procedures; Female; Health Literacy; Health Services Accessibility; Humans; Linear Models"
"Afifi S., GholamHosseini H., Sinha R.","A low-cost FPGA-based SVM classifier for melanoma detection","10.1109/IECBES.2016.7843526","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015662902&doi=10.1109%2fIECBES.2016.7843526&partnerID=40&md5=0ea6ff7f0975a713ebda9bcc41e105d8","Support Vector Machines (SVMs) are common machine learning tools with accurate classification. Hardware implementation of SVM classifiers for real-Time applications can improve their computing performance and reduce power consumption. This study aims to develop a real-Time embedded classifier to be implemented on a low-cost handheld device dedicated for early detection of melanoma. Melanoma is the most dangerous form of skin cancer, which is responsible for the majority of skin cancer related deaths. Therefore, the proposed device would be very beneficial in the primary care. In this paper, a hardware design is proposed to implement a linear binary SVM classifier in an FPGA targeting online melanoma classification. A recent hybrid Zynq platform is used for the implementation of the proposed system designed using the latest High Level Synthesis design methodology. The implemented system demonstrates high performance, low hardware resources utilization and low power consumption that meet vital embedded systems constraints. © 2016 IEEE.","CAD; Embedded Systems; FPGA; Melanoma; SVM","Biomedical engineering; Computer aided design; Diseases; Electric power utilization; Embedded systems; Field programmable gate arrays (FPGA); Green computing; Hand held computers; Hardware; High level synthesis; Integrated circuit design; Oncology; Support vector machines; Computing performance; Embedded classifiers; Hardware implementations; Hardware resources; Low-power consumption; Melanoma; Real-time application; Support vector machine (SVMs); Dermatology"
"Afnan M.A.M., Ali F., Worthington H., Netke T., Singh P., Kajamuhan C.","Triage nurse prediction as a covariate in a machine learning prediction algorithm for hospital admission from the emergency department","10.1016/j.ijmedinf.2021.104528","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111583436&doi=10.1016%2fj.ijmedinf.2021.104528&partnerID=40&md5=a12ce3233bd92a94f2327e6b9c97e5d4",[No abstract available],"Admission Prediction; Bed Management; Crowding; Emergency Department Crowding; Hospital Admission Planning; Interpretability; Interpretable Machine Learning; Triage Nurses","algorithm; comorbidity; consultation; emergency health service; emergency ward; hospital admission; human; length of stay; Letter; logistic regression analysis; machine learning; meta analysis (topic); prediction; hospital; hospital emergency service; hospitalization; machine learning; Emergency Service, Hospital; Hospitalization; Hospitals; Humans; Machine Learning; Triage"
"Afnan M.A.M., Rudin C., Conitzer V., Savulescu J., Mishra A., Liu Y., Afnan M.","Ethical Implementation of Artificial Intelligence to Select Embryos in in Vitro Fertilization","10.1145/3461702.3462589","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112411784&doi=10.1145%2f3461702.3462589&partnerID=40&md5=5144fb80c994288f7ef24a084a9459bc","AI has the potential to revolutionize many areas of healthcare. Radiology, dermatology, and ophthalmology are some of the areas most likely to be impacted in the near future, and they have received significant attention from the broader research community. But AI techniques are now also starting to be used in in vitro fertilization (IVF), in particular for selecting which embryos to transfer to the woman. The contribution of AI to IVF is potentially significant, but must be done carefully and transparently, as the ethical issues are significant, in part because this field involves creating new people. We first give a brief introduction to IVF and review the use of AI for embryo selection. We discuss concerns with the interpretation of the reported results from scientific and practical perspectives. We then consider the broader ethical issues involved. We discuss in detail the problems that result from the use of black-box methods in this context and advocate strongly for the use of interpretable models. Importantly, there have been no published trials of clinical effectiveness, a problem in both the AI and IVF communities, and we therefore argue that clinical implementation at this point would be premature. Finally, we discuss ways for the broader AI community to become involved to ensure scientifically sound and ethically responsible development of AI in IVF. © 2021 ACM.","AI; artificial intelligence; black-box; embryo selection; ethics; in vitro fertilization; interpretable; IVF; machine learning; randomised controlled trials; RCT","Philosophical aspects; AI techniques; Black box method; Clinical effectiveness; Ethical issues; In-vitro; Most likely; Research communities; Artificial intelligence"
"Afonso J., Saraiva M.J.M., Ferreira J.P.S., Cardoso H., Ribeiro T., Andrade P., Parente M., Jorge R.N., Saraiva M.M., Macedo G.","Development of a Convolutional Neural Network for Detection of Erosions and Ulcers With Distinct Bleeding Potential in Capsule Endoscopy","10.1016/j.tige.2021.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110387515&doi=10.1016%2fj.tige.2021.06.003&partnerID=40&md5=94996df5634ea509dfe61c6baab35820","Background and aims: The use of capsule endoscopy (CE) is paramount for the detection of small bowel ulcers and erosions. These lesions are responsible for a significant part of obscure gastrointestinal bleeding cases. The interpretation of CE exams is time-consuming and susceptible to errors. This study aims to develop a convolutional neural network (CNN) model for identification and differentiation of ulcers and erosion with distinct hemorrhagic potential in CE images. Methods: A CNN based on CE images was developed. This database included images of normal small intestinal mucosa, mucosal erosions, and ulcers with distinct bleeding potential. The hemorrhagic risk was assessed by the Saurin's classification. For CNN development, 23,720 images were ultimately extracted (18,045 normal mucosa, 1765 mucosal erosions, 1300 images of ulcers with uncertain bleeding potential—P1 ulcers; and 2610 ulcers with high bleeding potential—P2 ulcers. Two image datasets were created for CNN training and validation. Results: Overall, the network had a sensitivity of 86.6% and a specificity of 95.9% for detection of ulcers and erosions. Mucosal erosions were detected with a sensitivity and specificity of 73.1% and 96.1%, respectively. P1 ulcers were identified with a sensitivity of 71.5%, and a specificity of 97.8%. P2 ulcers were detected with a sensitivity and specificity of 91.4% and 98.8%, respectively. Conclusion: Our algorithm is the first deep learning-based model to accurately detect and distinguish enteric mucosal breaks with different hemorrhagic risk. CNN-assisted CE reading may improve the diagnostic of these lesions and overall CE efficiency. © 2021 Elsevier Inc.","Artificial intelligence; Capsule endoscopy; Convolutional neural network; Gastrointestinal bleeding; Ulcers","Article; capsule endoscopy; convolutional neural network; diagnostic accuracy; diagnostic value; erosion; gastrointestinal hemorrhage; human; intestine ulcer; major clinical study; predictive value; retrospective study; risk assessment; sensitivity and specificity; small intestine mucosa; validation process"
"Afrabandpey H., Peltola T., Piironen J., Vehtari A., Kaski S.","A decision-theoretic approach for model interpretability in Bayesian framework","10.1007/s10994-020-05901-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090308542&doi=10.1007%2fs10994-020-05901-8&partnerID=40&md5=b9c4333dfc138c216ba1c3c6f8f9ca89","A salient approach to interpretable machine learning is to restrict modeling to simple models. In the Bayesian framework, this can be pursued by restricting the model structure and prior to favor interpretable models. Fundamentally, however, interpretability is about users’ preferences, not the data generation mechanism; it is more natural to formulate interpretability as a utility function. In this work, we propose an interpretability utility, which explicates the trade-off between explanation fidelity and interpretability in the Bayesian framework. The method consists of two steps. First, a reference model, possibly a black-box Bayesian predictive model which does not compromise accuracy, is fitted to the training data. Second, a proxy model from an interpretable model family that best mimics the predictive behaviour of the reference model is found by optimizing the interpretability utility function. The approach is model agnostic—neither the interpretable model nor the reference model are restricted to a certain class of models—and the optimization problem can be solved using standard tools. Through experiments on real-word data sets, using decision trees as interpretable models and Bayesian additive regression models as reference models, we show that for the same level of interpretability, our approach generates more accurate models than the alternative of restricting the prior. We also propose a systematic way to measure stability of interpretabile models constructed by different interpretability approaches and show that our proposed approach generates more stable models. © 2020, The Author(s).","Bayesian predictive models; Interpretable machine learning","Decision trees; Economic and social effects; Machine learning; Regression analysis; Trees (mathematics); Additive regression; Bayesian frameworks; Decision theoretic approach; Optimization problems; Predictive modeling; Reference modeling; Reference models; Utility functions; Predictive analytics"
"Afsar M.M., Saqib S., Ghadi Y.Y., Alsuhibany S.A., Jalal A., Park J.","Body Worn Sensors for Health Gaming and e-Learning in Virtual Reality","10.32604/cmc.2022.028618","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135072572&doi=10.32604%2fcmc.2022.028618&partnerID=40&md5=7382a2940ee25bc7c9c1db429029ca5c","Virtual reality is an emerging field in the whole world. The problem faced by people today is that they are more indulged in indoor technology rather than outdoor activities. Hence, the proposed system introduces a fitness solution connecting virtual reality with a gaming interface so that an individual can play first-person games. The system proposed in this paper is an efficient and cost-effective solution that can entertain people along with playing outdoor games such as badminton and cricket while sitting in the room. To track the human movement, sensors Micro Processor Unit (MPU6050) are used that are connected with Bluetooth modules and Arduino responsible for sending the sensor data to the game. Further, the sensor data is sent to a machine learning model, which detects the game played by the user. The detected game will be operated on human gestures. A publicly available dataset named IM-Sporting Behaviors is initially used, which utilizes triaxial accelerometers attached to the subject’s wrist, knee, and below neck regions to capture important aspects of human motion. The main objective is that the person is enjoying while playing the game and simultaneously is engaged in some kind of sporting activity. The proposed system uses artificial neural networks classifier giving an accuracy of 88.9%. The proposed system should apply to many systems such as construction, education, offices and the educational sector. Extensive experimentation proved the validity of the proposed system. © 2022 Tech Science Press. All rights reserved.","Artificial neural networks; bluetooth connection; inertial sensors; machine learning; virtual reality exergaming","Bluetooth; Cost effectiveness; E-learning; Machine learning; Virtual reality; Wearable sensors; Bluetooth connections; Body-worn sensors; Cost-effective solutions; E - learning; First person; Inertial sensor; Machine-learning; Outdoor activities; Sensors data; Virtual reality exergaming; Neural networks"
"Afsar M.M.","Intelligent multi-purpose healthcare bot facilitating shared decision making",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077021999&partnerID=40&md5=9c3964072bd0032435db3116dc9ff092","Patient decision AIDS (PtDAs) have been promoted to facilitate personalized information retrieval and decision support; nonetheless, although promoted for more than 20 years, they have generally failed to gain a foothold in the general delivery of healthcare. Intelligent interactive agent technologies could address the design features necessary to facilitate support and shared-decision making. In this thesis, we develop and build a PtDA for Prostate cancer using intelligent agent technology. The proposed system, called ALAN, has a multi-layered architecture with three layers. While the first layer (User-Interface) is responsible to effectively interact with users (patients and physicians), the bottom layer (Data) handles requests regarding storing and retrieving the data. Unlike most existing bots, our core objective is to enable ALAN with learning abilities, which can evolve in the course of time and improve its behaviour with minimum distraction of the user. To this end, reinforcement learning and deep learning algorithms are employed in the main layer, i.e., Analytical Decision Making. This research is expected to have impact on delivery of personalized healthcare. Montreal. Canada. © 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","Chatbots; Multi-agent systems; Patient decision AIDS; Reinforcement learning; Shared-decision making","Autonomous agents; Decision making; Decision support systems; Deep learning; Diseases; Health care; Intelligent agents; Learning algorithms; Machine learning; Reinforcement learning; User interfaces; Analytical decisions; Chatbots; Decision aids; Intelligent agent technology; Interactive agents; Personalized healthcare; Personalized information retrieval; Shared decision makings; Multi agent systems"
"Afsari B., Kuo A., Zhang Y., Li L., Lahouel K., Danilova L., Favorov A., Rosenquist T., Grollman A.P., Kinzler K.W., Cope L., Vogelstein B., Tomasetti C.","Supervised mutational signatures for obesity and other tissue-specific etiological factors in cancer","10.7554/eLife.61082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100223299&doi=10.7554%2feLife.61082&partnerID=40&md5=b805699eef52bfbf7d46354937a08061","Determining the etiologic basis of the mutations that are responsible for cancer is one of the fundamental challenges in modern cancer research. Different mutational processes induce different types of DNA mutations, providing “mutational signatures” that have led to key insights into cancer etiology. The most widely used signatures for assessing genomic data are based on unsupervised patterns that are then retrospectively correlated with certain features of cancer. We show here that supervised machine-learning techniques can identify signatures, called SuperSigs, that are more predictive than those currently available. Surprisingly, we found that aging yields different SuperSigs in different tissues, and the same is true for environmental exposures. We were able to discover SuperSigs associated with obesity, the most important lifestyle factor contributing to cancer in Western populations. © 2021, eLife Sciences Publications Ltd. All rights reserved.",,"aristolochic acid; asbestos; acute myeloid leukemia; aging; alcohol consumption; Article; bladder cancer; body mass; breast cancer; cancer susceptibility; DNA repair; endometrium cancer; environmental exposure; environmental factor; esophagus cancer; gene frequency; gene mutation; human; infectious hepatitis; kidney cancer; lifestyle; lung adenocarcinoma; machine learning; malignant neoplasm; measurement accuracy; mismatch repair; mutational signature; obesity; ovary carcinoma; pancreas cancer; receiver operating characteristic; risk factor; smoking; somatic mutation; supervised machine learning; ultraviolet radiation; uterine cervix cancer; whole genome sequencing; genetics; machine learning; mutation; neoplasm; obesity; Humans; Machine Learning; Mutation; Neoplasms; Obesity"
"Afshar M., Usefi H.","Optimizing feature selection methods by removing irrelevant features using sparse least squares","10.1016/j.eswa.2022.116928","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127647940&doi=10.1016%2fj.eswa.2022.116928&partnerID=40&md5=e697ee08fba538fbe72dfe4026a36bcd","Feature (variable) selection is recognized as an integral part of model construction in machine learning. One can use feature selection to remove redundant and irrelevant features. This in turn can help overcome the curse of dimensionality, reduce overfitting, and come up with interpretable models. In this paper, we propose Sparse Least Squares method (SLS) based on singular value decomposition and least squares to remove irrelevant features. We show that augmenting well-known feature selection methods with SLS significantly reduces the running time while improving or maintaining the prediction accuracy of the model. © 2022 Elsevier Ltd","Feature selection; Irrelevant features; Least squares; Rank-1 update; Singular value decomposition; Supervised learning","Least squares approximations; Singular value decomposition; Supervised learning; Feature selection methods; Feature variable; Features selection; Integral part; Irrelevant feature; Least Square; Least-squares- methods; Model construction; Rank-1 update; Variables selections; Feature extraction"
"Afshar M., Joyce C., Dligach D., Sharma B., Kania R., Xie M., Swope K., Salisbury-Afshar E., Karnik N.S.","Subtypes in patients with opioid misuse: A prognostic enrichment strategy using electronic health record data in hospitalized patients","10.1371/journal.pone.0219717","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069736756&doi=10.1371%2fjournal.pone.0219717&partnerID=40&md5=c3f09bd802984b2b1acc85a5f7448fd7","Background Approaches are needed to better delineate the continuum of opioid misuse that occurs in hospitalized patients. A prognostic enrichment strategy with latent class analysis (LCA) may facilitate treatment strategies in subtypes of opioid misuse. We aim to identify subtypes of patients with opioid misuse and examine the distinctions between the subtypes by examining patient characteristics, topic models from clinical notes, and clinical outcomes. Methods This was an observational study of inpatient hospitalizations at a tertiary care center between 2007 and 2017. Patients with opioid misuse were identified using an operational definition applied to all inpatient encounters. LCA with eight class-defining variables from the electronic health record (EHR) was applied to identify subtypes in the cohort of patients with opioid misuse. Comparisons between subtypes were made using the following approaches: (1) descriptive statistics on patient characteristics and healthcare utilization using EHR data and census-level data; (2) topic models with natural language processing (NLP) from clinical notes; (3) association with hospital outcomes. Findings The analysis cohort was 6,224 (2.7% of all hospitalizations) patient encounters with opioid misuse with a data corpus of 422,147 clinical notes. LCA identified four subtypes with differing patient characteristics, topics from the clinical notes, and hospital outcomes. Class 1 was categorized by high hospital utilization with known opioid-related conditions (36.5%); Class 2 included patients with illicit use, low socioeconomic status, and psychoses (12.8%); Class 3 contained patients with alcohol use disorders with complications (39.2%); and class 4 consisted of those with low hospital utilization and incidental opioid misuse (11.5%). The following hospital outcomes were the highest for each subtype when compared against the other subtypes: readmission for class 1 (13.9% vs. 10.5%, p<0.01); discharge against medical advice for class 2 (12.3% vs. 5.3%, p<0.01); and in-hospital death for classes 3 and 4 (3.2% vs. 1.9%, p<0.01). Conclusions A 4-class latent model was the most parsimonious model that defined clinically interpretable and relevant subtypes for opioid misuse. Distinct subtypes were delineated after examining multiple domains of EHR data and applying methods in artificial intelligence. The approach with LCA and readily available class-defining substance use variables from the EHR may be applied as a prognostic enrichment strategy for targeted interventions. © 2019 Afshar et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"opiate; narcotic analgesic agent; adult; alcoholism; Article; cohort analysis; disease classification; drug misuse; electronic health record; female; health care utilization; hospital discharge; hospital patient; hospital readmission; hospital utilization; human; intermethod comparison; major clinical study; male; methodology; middle aged; observational study; prognosis; social status; tertiary care center; young adult; alcoholism; classification; hospitalization; latent class analysis; machine learning; natural language processing; opiate addiction; personalized medicine; prescription drug misuse; prognosis; theoretical model; treatment outcome; Adult; Alcoholism; Analgesics, Opioid; Electronic Health Records; Female; Hospitalization; Humans; Inpatients; Latent Class Analysis; Machine Learning; Male; Middle Aged; Models, Theoretical; Natural Language Processing; Opioid-Related Disorders; Patient Discharge; Precision Medicine; Prescription Drug Misuse; Prognosis; Tertiary Care Centers; Treatment Outcome; Young Adult"
"Afshar P., Naderkhani F., Oikonomou A., Rafiee M.J., Mohammadi A., Plataniotis K.N.","MIXCAPS: A capsule network-based mixture of experts for lung nodule malignancy prediction","10.1016/j.patcog.2021.107942","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103613944&doi=10.1016%2fj.patcog.2021.107942&partnerID=40&md5=903aa69db749bf47f539cd2469ff2e17","Lung cancer is among the most common and deadliest cancers with a low 5-year survival rate. Timely diagnosis of lung cancer is, therefore, of paramount importance as it can save countless lives. In this regard, Computed Tomography (CT) scan is widely used for early detection of lung cancer, where human judgment is currently considered as the gold standard approach. Recently, there has been a surge of interest on development of automatic solutions via radiomics, as human-centered diagnosis is subject to inter-observer variability and is highly burdensome. Hand-crafted radiomics, serving as a radiologist assistant, requires fine annotations and pre-defined features. Deep learning radiomics solutions, however, have the promise of extracting the most useful features on their own in an end-to-end fashion without having access to the annotated boundaries. Among different deep learning models, Capsule Networks are proposed to overcome shortcomings of the Convolutional Neural Networks (CNNs) such as their inability to recognize detailed spatial relations. Capsule networks have so far shown satisfying performance in medical imaging problems. Capitalizing on their success, in this study, we propose a novel capsule network-based mixture of experts, referred to as the MIXCAPS. The proposed MIXCAPS architecture takes advantage of not only the capsule network's capabilities to handle small datasets, but also automatically splitting dataset through a convolutional gating network. MIXCAPS enables capsule network experts to specialize on different subsets of the data. Our results show that MIXCAPS outperforms a single capsule network, a single CNN, a mixture of CNNs, and an ensemble of capsule networks, with an average accuracy of 90.7%, average sensitivity of 89.5%, average specificity of 93.4% and average area under the curve of 0.956. Our experiments also show that there is a relation between the gate outputs and a couple of hand-crafted features, illustrating explainable nature of the proposed MIXCAPS. To further evaluate generalization capabilities of the proposed MIXCAPS architecture, additional experiments on a brain tumor dataset are performed showing potentials of MIXCAPS for detection of tumors related to other organs. © 2021 Elsevier Ltd","Capsule network; Mixture of experts; Tumor type classification","Biological organs; Computerized tomography; Convolution; Convolutional neural networks; Deep learning; Diagnosis; Diseases; Medical imaging; Network architecture; Tumors; Capsule network; Computed tomography scan; Convolutional neural network; Diagnosis of lung cancer; Lung Cancer; Lung nodule; Mixture of experts; Network-based; Survival rate; Tumor type classification; Mixtures"
"Aftab K., Fatima H.S., Aziz N., Baig E., Khurram M., Mubarak F., Enam S.A.","Machine Learning and Sampling Techniques to Enhance Radiological Diagnosis of Cerebral Tuberculosis","10.1109/ICEET53442.2021.9659603","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124645577&doi=10.1109%2fICEET53442.2021.9659603&partnerID=40&md5=20c17cd0bb3a7d1eb8eba7f9e0bf064a","Cerebral tuberculosis (TB) is one of the neurological manifestations of tuberculosis infections responsible for devastating sequelae and mortality. It is a challenge to diagnose as it mimics other infectious and neoplastic pathologies of the brain. There is a need for a rapid and accurate diagnostic approaches, in order to prevent the dismal outcomes arising as a result of delayed or incorrect diagnosis. This paper aims to develop a classifier to diagnose cerebral TB using various radiological features present on Magnetic Resonance Imaging (MRI) of the brain with the help of Machine Learning (ML). Cases of TB and non-TB conditions (including meningiomas, gliomas, fungal and bacterial brain infection) presenting to Aga Khan University Hospital, Karachi, Pakistan, were included and divided into training and test datasets. Features were selected using correlation, and besides age and gender, included multiple radiological features recorded from MRI of the brain. After the application of Synthetic Minority Over-sampling Technique (SMOTE), SMOTE-Tomek Links, Edited Nearest Neighbor (ENN) SMOTE-ENN, and Adaptive Synthetic (ADASYN) techniques for balancing the datasets, classifier accuracy was tested using two models: logistic regression and random forest. Highest accuracy (90.9%) was achieved using logistic regression along with SMOTE+TOMEK with 95.4% Area under the Curve while obtaining a F1 score of 92.8% © 2021 IEEE.","ADASYN; Brain imaging; Cerebral tuberculosis; Logistic Regression; Random Forest; SMOTE-ENN; SMOTE. SMOTE-TOMEK","Balancing; Brain; Brain mapping; Classification (of information); Logistic regression; Machine learning; Magnetic resonance imaging; Random forests; Adaptive synthetic; Brain imaging; Cerebral tuberculosis; Logistics regressions; Nearest-neighbour; Random forests; Synthetic minority over-sampling technique-edited near neighbor; Synthetic minority over-sampling technique-TOMEK; Synthetic minority over-sampling technique.; Synthetic minority over-sampling techniques; Decision trees"
"Aftab S., Alanazi S., Ahmad M., Khan M.A., Fatima A., Elmitwally N.S.","Cloud-Based Diabetes Decision Support System Using Machine Learning Fusion","10.32604/cmc.2021.016814","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103655589&doi=10.32604%2fcmc.2021.016814&partnerID=40&md5=cf68307a394a8c89135147b7f6f24751","Diabetes mellitus, generally known as diabetes, is one of the most common diseases worldwide. It is a metabolic disease characterized by insulin deciency, or glucose (blood sugar) levels that exceed 200 mg/dL (11.1 ml/L) for prolonged periods, and may lead to death if left uncontrolled by medication or insulin injections. Diabetes is categorized into two main types type 1 and type 2 both of which feature glucose levels above normal, dened as 140 mg/dL. Diabetes is triggered by malfunction of the pancreas, which releases insulin, a natural hormone responsible for controlling glucose levels in blood cells. Diagnosis and comprehensive analysis of this potentially fatal disease necessitate application of techniques with minimal rates of error. The primary purpose of this research study is to assess the potential role of machine learning in predicting a person's risk of developing diabetes. Historically, research has supported the use of various machine algorithms, such as naive Bayes, decision trees, and articial neural networks, for early diagnosis of diabetes. However, to achieve maximum accuracy and minimal error in diagnostic predictions, there remains an immense need for further research and innovation to improve the machine-learning tools and techniques available to healthcare professionals. Therefore, in this paper, we propose a novel cloud-based machine-learning fusion technique involving synthesis of three machine algorithms and use of fuzzy systems for collective generation of highly accurate nal decisions regarding early diagnosis of diabetes. © 2021 Tech Science Press. All rights reserved.","articial neural network; decision trees; diabetes prediction; Machine learning fusion; naive Bayes","Blood; Decision support systems; Decision trees; Diagnosis; Glucose; Insulin; Risk assessment; Articial neural networks; Comprehensive analysis; Fusion techniques; Health care professionals; Insulin injections; Machine algorithm; Maximum accuracies; Metabolic disease; Machine learning"
"Afzaal M., Nouri J., Zia A., Papapetrou P., Fors U., Wu Y., Li X., Weegar R.","Explainable AI for Data-Driven Feedback and Intelligent Action Recommendations to Support Students Self-Regulation","10.3389/frai.2021.723447","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120401479&doi=10.3389%2ffrai.2021.723447&partnerID=40&md5=9bb89f74ef3835f4155ee8b2f7122800","Formative feedback has long been recognised as an effective tool for student learning, and researchers have investigated the subject for decades. However, the actual implementation of formative feedback practices is associated with significant challenges because it is highly time-consuming for teachers to analyse students’ behaviours and to formulate and deliver effective feedback and action recommendations to support students’ regulation of learning. This paper proposes a novel approach that employs learning analytics techniques combined with explainable machine learning to provide automatic and intelligent feedback and action recommendations that support student’s self-regulation in a data-driven manner, aiming to improve their performance in courses. Prior studies within the field of learning analytics have predicted students’ performance and have used the prediction status as feedback without explaining the reasons behind the prediction. Our proposed method, which has been developed based on LMS data from a university course, extends this approach by explaining the root causes of the predictions and by automatically providing data-driven intelligent recommendations for action. Based on the proposed explainable machine learning-based approach, a dashboard that provides data-driven feedback and intelligent course action recommendations to students is developed, tested and evaluated. Based on such an evaluation, we identify and discuss the utility and limitations of the developed dashboard. According to the findings of the conducted evaluation, the dashboard improved students’ learning outcomes, assisted them in self-regulation and had a positive effect on their motivation. Copyright © 2021 Afzaal, Nouri, Zia, Papapetrou, Fors, Wu, Li and Weegar.","AI; automatic data-driven feedback; dashboard; explainable machine learning-based approach; learning analytics; recommender system; self-regulated learning",
"Afzaal M., Nouri J., Zia A., Papapetrou P., Fors U., Wu Y., Li X., Weegar R.","Generation of Automatic Data-Driven Feedback to Students Using Explainable Machine Learning","10.1007/978-3-030-78270-2_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126779857&doi=10.1007%2f978-3-030-78270-2_6&partnerID=40&md5=50d36b59d9cbd771c8f6f8b50176b7d7","This paper proposes a novel approach that employs learning analytics techniques combined with explainable machine learning to provide automatic and intelligent actionable feedback that supports students self-regulation of learning in a data-driven manner. Prior studies within the field of learning analytics predict students’ performance and use the prediction status as feedback without explaining the reasons behind the prediction. Our proposed method, which has been developed based on LMS data from a university course, extends this approach by explaining the root causes of the predictions and automatically provides data-driven recommendations for action. The underlying predictive model effectiveness of the proposed approach is evaluated, with the results demonstrating 90 per cent accuracy. © 2021, Springer Nature Switzerland AG.","Dashboard; Explainable machine learning; Feedback provision; Learning analytics; Recommendations generation","Machine learning; Predictive analytics; Students; Analytic technique; Dashboard; Data driven; Explainable machine learning; Feedback provision; Feedback to students; Learning analytic; Machine-learning; Recommendation generation; Self regulation; Forecasting"
"Afzal B., Zhang X., Srivastava A.K.","Enhanced Hybrid Model to Predict the Surface Roughness of Honed Cylinder Bore","10.1115/1.4052280","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126806370&doi=10.1115%2f1.4052280&partnerID=40&md5=8cfc570df27e35f87158c0f40b677a97","Cylinder bore honing is a finishing process that generates a crosshatch pattern with alternate valleys and plateaus responsible for enhancing lubrication and preventing gas and oil leakage in the engine cylinder bore. The required functional surface in the cylinder bore is generated by a sequential honing process and is characterized by Rk roughness parameters (Rk, Rvk, Rpk, Mr1, Mr2). Predicting the desired surface roughness relies primarily on two techniques: (i) analytical models (AM) and (ii) machine learning (ML) models. Both of these techniques offer certain advantages and limitations. AM’s are interpretable as they indicate distinct mapping relation between input variables and honed surface texture. However, AM’s are usually based on simplified assumptions to ensure the traceability of multiple variables. Consequently, their prediction accuracy is adversely impacted when these assumptions are not satisfied. However, ML models accurately predict the surface texture but their prediction mechanism is challenging to interpret. Furthermore, the ML models’ performance relies heavily on the representativeness of data employed in developing them. Thus, either prediction accuracy or model interpretability suffers when AM and ML models are implemented independently. This study proposes a hybrid model framework to incorporate the benefits of AM and ML simultaneously. In the hybrid model, an artificial neural network (ANN) compensates the AM by correcting its error. This retains the physical understanding built into the model while simultaneously enhancing the prediction accuracy. The proposed approach resulted in a hybrid model that significantly improved the prediction accuracy of the AM and additionally provided superior performance compared to independent ANN. Copyright © 2021 by ASME.","Analytical model; Artificial neural network; Hybrid model; Sequential honing process; Surface roughness","Analytical models; Engine cylinders; Forecasting; Honing; Surface roughness; Textures; Analytical model(s); Crosshatch pattern; Cylinder bore; Finishing process; Hybrid model; Lubrication /; Machine learning models; Prediction accuracy; Sequential honing process; Surface textures; Neural networks"
"Aga S., Jayasena N., Ignatowski M.","Co-ML: A case for collaborative ML acceleration using near-data processing","10.1145/3357526.3357532","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075871097&doi=10.1145%2f3357526.3357532&partnerID=40&md5=5da12eb0e01cf13ed3d68474723cf831","The growing importance of Machine Learning (ML) has led to a proliferation of accelerator designs that target ML workloads. The majority of these designs focus on accelerating compute-intensive regions of ML workloads such as general matrix multiplications (GEMMs) and convolutions. While this is a legitimate approach, we observe in this work that ML workloads also comprise data-intensive computations that manifest low compute-to-byte ratios and can often contribute considerably to the total execution time. Further, we also observe that, the presence of such computations opens up an exciting opportunity for near-data processing (NDP) architectures as they often provision for higher memory bandwidth that can benefit such computations. Based on the above observations, in this work we make a case for a more collaborative approach to ML acceleration, termed Co-ML, in which memory plays an active role and is responsible for NDP-amenable computations while the compute-intensive computations are executed on the host accelerator as before. We demonstrate how even a relatively simple NDP design can increase performance of data-intensive computations in ML by up to 20×. Further, for a suite of ML workloads we demonstrate that Co-ML can deliver speedups as high as 20% with average speedups of 14%. Finally, we show that with increasing efforts to build better accelerators for compute-intensive computations, these benefits will likely increase. © 2019 Association for Computing Machinery.","GPU; HBM; Machine learning; Near-data processing","Acceleration; Graphics processing unit; Learning systems; Machine learning; Accelerator design; Collaborative approach; Data-intensive computation; MAtrix multiplication; Memory bandwidths; Data handling"
"Agafonov A., Yumaganov A., Myasnikov V.","Adaptive Traffic Signal Control Based on Maximum Weighted Traffic Flow","10.1109/ITNT55410.2022.9848651","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137777254&doi=10.1109%2fITNT55410.2022.9848651&partnerID=40&md5=0ada6169f8fc7fe34587d8c0eec54857","The paper proposes a method for adaptive traffic signal control. The proposed method includes two stages, each of which is responsible for a separate algorithm. At the first step, the predicted 'flow' of vehicles through the intersection along the given lane is estimated at the allowed traffic light signal. At the second step, a 'weighted' flow estimate is formed, which takes into account the waiting time for vehicles at the intersection. The next phase of the traffic signal is determined by the criterion of maximizing the weighted traffic flow through the intersection. An experimental study of the proposed algorithm was conducted both on synthetic and real-world traffic scenarios, including an isolated intersection, a highway, and a city area. Based on experimental results, we can conclude that the proposed algorithm outperforms baseline classical and reinforcement learning methods for traffic signal control in terms of average waiting time and average travel time. © 2022 IEEE.","adaptive control; intelligent transportation systems; machine learning; traffic flow; traffic signal control","Adaptive control systems; Intelligent systems; Learning systems; Reinforcement learning; Street traffic control; Traffic signals; Adaptive Control; Adaptive traffic signal control; Intelligent transportation systems; Light signal; Machine-learning; Separate algorithm; Traffic flow; Traffic light; Traffic signal control; Waiting time; Travel time"
"Agapie A., Caragea D.","Genetic algorithms, schemata construction and statistics","10.1007/3-540-62868-1_93","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894479817&doi=10.1007%2f3-540-62868-1_93&partnerID=40&md5=d96fc87b8b99e268efa2b4346d2ff667","The paper performs a comparison between two types of binary Genetic Algorithms (GA): Statistical GA vs. Messy GA. They have the same challenge - solving “deceptive” problems - and, up to a point, they are designed on the same paradigm: improving the GA by directing the search using some statistical derived schemata. The Statistical GA that we propose does not use a larger population, but only a real-valued string with the average numbers of “ones” produced on each position of the chromosome during the GA’s evolution. Assuming the stagnation of the GA in sub-optimal points (which is usually the case of deceptive problems), we extract - by imposing a threshold on the real-valued string - the schema responsible for stagnation, derive its complementary schema and resume the GAs evolution imposing that schema to all the new chromosomes. © Springer-Verlag Berlin Heidelberg 1997.",,"Artificial intelligence; Chromosomes; Computation theory; Population statistics; Average numbers; Binary genetic algorithm; Deceptive problem; Gas evolution; Optimal points; Genetic algorithms"
"Agapito G., Pastrello C., Jurisica I.","Comprehensive pathway enrichment analysis workflows: COVID-19 case study","10.1093/bib/bbaa377","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105156996&doi=10.1093%2fbib%2fbbaa377&partnerID=40&md5=d52562b510be4d546250cbc57a59f95c","The coronavirus disease 2019 (COVID-19) outbreak due to the novel coronavirus named severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been classified as a pandemic disease by the World Health Organization on the 12th March 2020. This world-wide crisis created an urgent need to identify effective countermeasures against SARS-CoV-2. In silico methods, artificial intelligence and bioinformatics analysis pipelines provide effective and useful infrastructure for comprehensive interrogation and interpretation of available data, helping to find biomarkers, explainable models and eventually cures. One class of such tools, pathway enrichment analysis (PEA) methods, helps researchers to find possible key targets present in biological pathways of host cells that are targeted by SARS-CoV-2. Since many software tools are available, it is not easy for non-computational users to choose the best one for their needs. In this paper, we highlight how to choose the most suitable PEA method based on the type of COVID-19 data to analyze. We aim to provide a comprehensive overview of PEA techniques and the tools that implement them. © 2020 The Author(s) 2020. Published by Oxford University Press. All rights reserved.","biological pathways; COVID-19; network analysis; pathway enrichment analysis; statistical analysis",
"Agarwal A., Rathore P., Jain V., Rai B.","In-silico model for predicting the corrosion inhibition efficiency of steel inhibitors",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070080338&partnerID=40&md5=1b49ef7fba8707058ef8514d73f7a40a","Quantitative Structure-Activity Relationships (QSAR) based models have been widely used for predicting corrosion inhibition performance of metals. However, one of the major limitations in these studies is that the authors have restricted themselves to use only a single class of molecules having similar molecular structure. In this study, a computational end-to-end framework was developed to investigate the properties of organic corrosion inhibitors which are responsible for inhibition of steel in acidic solution. The framework consists of modules like data preprocessing, descriptor selection and model building. A robust predictive model for multiple class of corrosion inhibitors was developed using advanced machine learning algorithm such as gradient boosting machine (GBM), random forest, support vector machines (SVM) etc. The descriptors were selected using novel integrated ensemble technique. The model based on GBM algorithm was able to predict the corrosion inhibition efficiency of inhibitors with significantly higher accuracy. © 2019 by NACE International","Corrosion; Inhibitor; Machine learning; QSAR; Steel","Adaptive boosting; Computational chemistry; Corrosion; Corrosion inhibitors; Decision trees; Efficiency; Forecasting; Learning systems; Machine learning; Molecular graphics; Steel; Support vector machines; Corrosion inhibition efficiency; Corrosion inhibition performance; Ensemble techniques; Inhibitor; Organic corrosion inhibitors; Predictive modeling; QSAR; Quantitative structure-activity relationships; Steel corrosion"
"Agarwal A., Baechle C., Behara R., Zhu X.","A Natural Language Processing Framework for Assessing Hospital Readmissions for Patients with COPD","10.1109/JBHI.2017.2684121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032799305&doi=10.1109%2fJBHI.2017.2684121&partnerID=40&md5=2ed4830b0662997da119c8b6b817fad6","With the passage of recent federal legislation, many medical institutions are now responsible for reaching target hospital readmission rates. Chronic diseases account for many hospital readmissions and chronic obstructive pulmonary disease has been recently added to the list of diseases for which the United States government penalizes hospitals incurring excessive readmissions. Though there have been efforts to statistically predict those most in danger of readmission, a few have focused primarily on unstructured clinical notes. We have proposed a framework, which uses natural language processing to analyze clinical notes and predict readmission. Many algorithms within the field of data mining and machine learning exist, so a framework for component selection is created to select the best components. Naïve Bayes using Chi-Squared feature selection offers an AUC of 0.690 while maintaining fast computational times. © 2013 IEEE.","Data mining; decision support systems; feature extraction; medical information systems; natural language processing","Data mining; Decision support systems; Feature extraction; Hospitals; Machine learning; Medical computing; Medical information systems; Pulmonary diseases; Chronic disease; Chronic obstructive pulmonary disease; Clinical notes; Component selection; Computational time; Federal legislations; Medical institutions; NAtural language processing; Natural language processing systems; Article; artificial neural network; chronic obstructive lung disease; classification algorithm; classifier; disease exacerbation; dyspnea; electronic health record; false positive result; hospital readmission; human; k nearest neighbor; machine learning; major clinical study; methodology; natural language processing; oxygen tension; receiver operating characteristic; spirometry; support vector machine; algorithm; area under the curve; chronic obstructive lung disease; clinical decision support system; hospital readmission; medical informatics; procedures; statistics and numerical data; Algorithms; Area Under Curve; Decision Support Systems, Clinical; Humans; Medical Informatics; Natural Language Processing; Patient Readmission; Pulmonary Disease, Chronic Obstructive"
"Agarwal A., Nanavati N.","Association rule mining using hybrid GA-PSO for multi-objective optimisation","10.1109/ICCIC.2016.7919571","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019999039&doi=10.1109%2fICCIC.2016.7919571&partnerID=40&md5=d8e3415916781c7dce11b8ecf4d6135e","Association Rule Mining (ARM), a Data Mining process, extracts hidden strong relationships among a large set of the correlated data. With the burgeoning advancement and application of Association Rule Mining in diverse fields ranging from the web usage mining to medical diagnosis and business intelligence to geographical information systems, the decision-making in ARM involves a multi-objective perspective to obtain an interesting and accurate rule set. By considering the Pareto optimality, an optimal trade-off is established between the conflicting and incommensurate performance parameters-comprehensibility, interestingness and confidence of the mined rules. Both, Genetic Algorithm (GA) and Particle Swarm Optimisation (PSO), being population-based stochastic search method, have found their strong base in mining association rules. We propose an association rule mining scheme using our proposed multi-objective hybridisation of GA-PSO algorithm. The primary advantage of the proposed algorithm is that the hybridisation of multiple objective-GA with multi objective-PSO balances the exploration and exploitation tasks, resulting in valuable extraction of accurate and interpretable mined rules. Evaluating this hybrid model on Bakery dataset shows that with generation of comprehensible, interesting and reliable association rules, the model also converges four times faster than mono-objective hybridisation. © 2016 IEEE.","Association Rule Mining; Genetic Algorithm; Hybrid NSGA-II-MOPSO; Hybridisation; Multi-Objective Optimization; Pareto optimality; Particle Swarm Optimisation","Artificial intelligence; Association rules; Decision making; Diagnosis; Economic and social effects; Genetic algorithms; Medical information systems; Multiobjective optimization; Optimization; Pareto principle; Particle swarm optimization (PSO); Stochastic systems; Association rule minings (ARM); Exploration and exploitation; Hybridisation; NSGA-II; Pareto-optimality; Particle swarm optimisation; Performance parameters; Stochastic search methods; Data mining"
"Agarwal A.S., Iyer S., Patel A.V., Saxena A.","An Approach to Identifying Pre-Existing Chronic Conditions for Corona Virus Positive Patients Using Regression","10.4018/978-1-6684-4225-8.ch007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136457538&doi=10.4018%2f978-1-6684-4225-8.ch007&partnerID=40&md5=dc0e815d8445a9874bbb7ede1337c85b","COVID-19 has various symptoms, and they are cold, cough, mild or high fever, and breathing problems for severe cases. In addition, it is in talk that diabetes, chronic obstructive pulmonary disease (COPD), cardiovascular, asthma, and many diseases invite the virus. Machine learning and data analysis are considered best for predicting approaches for finding various aspects of the effect of the virus. In this chapter, the authors deal with symptoms that have been recorded in the dataset and try to find which pre-defined symptoms are considered effective or responsible in positive case of infection by coronavirus. The dataset has been taken from Kaggle. As the dataset is categorical in nature, the authors use correlation and logistic regression analysis to find the symptoms that prevail in the patient and have caused the infection in them. This is also about dimensionality reduction and feature selection where they are reducing the available features based on regression. © 2022, IGI Global.",,
"Agarwal C., Gupta S., Najjar M., Weaver T.E., Zhou X.J., Schonfeld D., Prasad B.","Deep Learning Analyses of Brain MRI to Identify Sustained Attention Deficit in Treated Obstructive Sleep Apnea: A Pilot Study","10.1007/s41782-021-00190-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123096958&doi=10.1007%2fs41782-021-00190-0&partnerID=40&md5=18c42154dee85ef17cab82499ac2c894","Purpose: Persistent sustained attention deficit (SAD) after continuous positive airway pressure (CPAP) treatment is a source of quality of life and occupational impairment in obstructive sleep apnea (OSA). However, persistent SAD is difficult to predict in patients initiated on CPAP treatment. We performed secondary analyses of brain magnetic resonance (MR) images in treated OSA participants, using deep learning, to predict SAD. Methods: 26 middle-aged men with CPAP use of more than 6 h daily and MR imaging were included. SAD was defined by psychomotor vigilance task lapses of more than 2. 17 participants had SAD and 9 were without SAD. A Convolutional Neural Network (CNN) model was used for classifying the MR images into + SAD and − SAD categories. Results: The CNN model achieved an accuracy of 97.02 ± 0.80% in classifying MR images into + SAD and − SAD categories. Assuming a threshold of 90% probability for the MR image being correctly classified, the model provided a participant-level accuracy of 99.11 ± 0.55% and a stable image level accuracy of 97.45 ± 0.63%. Conclusion: Deep learning methods, such as the proposed CNN model, can accurately predict persistent SAD based on MR images. Further replication of these findings will allow early initiation of adjunctive pharmacologic treatment in high-risk patients, along with CPAP, to improve quality of life and occupational fitness. Future augmentation of this approach with explainable artificial intelligence methods may elucidate the neuroanatomical areas underlying persistent SAD to provide mechanistic insights and novel therapeutic targets. © 2021, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.","Alertness; Daytime sleepiness; MRI; OSA; Sleep apnea","adult; Article; artificial intelligence; attention deficit hyperactivity disorder; clinical article; continuous positive airway pressure; controlled study; convolutional neural network; deep learning; diagnostic test accuracy study; functional neuroimaging; high risk patient; human; middle aged; nuclear magnetic resonance imaging; pilot study; psychomotor vigilance task; quality of life; risk assessment; signal processing; sleep disordered breathing"
"Agarwal C., Khobahi S., Schonfeld D., Soltanalian M.","CoroNet: A deep network architecture for enhanced identification of COVID-19 from chest x-ray images","10.1117/12.2580738","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103694608&doi=10.1117%2f12.2580738&partnerID=40&md5=3b9c38900bf2e2b28d2ece6c547e625c","In this paper, we consider the problem of Coronavirus disease (COVID-19) diagnosis from chest X-ray images in a multi-label classification scenario, where the ultimate goal is to distinguish among Healthy, non-COVID Pneumonia, and COVID-19 infection cases from the chest X-ray manifestations. Particularly, we establish the use of a rapid, non-invasive and cost-effective X-ray-based method as a key diagnosis and screening tool for COVID-19 at early and intermediate stages of the disease. To this end, we propose CoroNet, a deep learning framework that is built upon a two-stage learning methodology: 1) an AutoEncoder to extract the infected regions in the chest X-ray manifestation of COVID-19 and other Pneumonia-like diseases and 2) a deep convolutional neural network for the multi-label classification. We utilize this tailored deep architecture to extract the relevant features specific to each class to perform the task of automatic diagnosis and classification. The unsupervised part of the proposed framework helps with proper identification of the disease given the scarcity of quality datasets on COVID-19, and at the same time, facilitates exploiting the large X-ray datasets that are readily available for Healthy and non-COVID Pneumonia cases. Our numerical investigations demonstrate that the proposed framework outperforms the state-of-the-art methods for COVID-19 identification while employing approximately ten times fewer training parameters as compared to other existing methodologies. Furthermore, we make use of attribution maps, an explainable artificial intelligence tool, to interpret the diagnosis offered by the network. We have made the codes of our proposed CoroNet framework publicly available to the research community. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","AutoEncoder; Computed Tomography (CT); COVID-19; Deep Learning; Explainable Artificial Intelligence (XAI); Transfer Learning; X-ray Imaging","Classification (of information); Convolutional neural networks; Cost effectiveness; Deep learning; Deep neural networks; Image enhancement; Large dataset; Learning systems; Medical imaging; Network architecture; Numerical methods; Artificial intelligence tools; Automatic diagnosis; Learning frameworks; Multi label classification; Numerical investigations; Research communities; State-of-the-art methods; Training parameters; Computer aided diagnosis"
"Agarwal C., Khobahi S., Bose A., Soltanalian M., Schonfeld D.","DEEP-URL: A Model-Aware Approach to Blind Deconvolution Based on Deep Unfolded Richardson-Lucy Network","10.1109/ICIP40778.2020.9190825","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098660074&doi=10.1109%2fICIP40778.2020.9190825&partnerID=40&md5=832708aeb782e5eb5e87e51924786f00","The lack of interpretability in current deep learning models causes serious concerns as they are extensively used for various life-critical applications. Hence, it is of paramount importance to develop interpretable deep learning models. In this paper, we consider the problem of blind deconvolution and propose a novel model-aware deep architecture that allows for the recovery of both the blur kernel and the sharp image from the blurred image. In particular, we propose the Deep Unfolded Richardson-Lucy (Deep-URL) framework - an interpretable deep-learning architecture that can be seen as an amalgamation of classical estimation technique and deep neural network, and consequently leads to improved performance. Our numerical investigations demonstrate significant improvement compared to state-of-the-art algorithms. © 2020 IEEE.","Blind deconvolution; deep unfolding; machine learning; model-aware deep learning; non-convex optimization","Convolution; Deep neural networks; Image processing; Learning systems; Metals; Network architecture; Blind deconvolution; Deep architectures; Estimation techniques; Interpretability; Learning architectures; Life-critical applications; Numerical investigations; State-of-the-art algorithms; Deep learning"
"Agarwal D., Covarrubias-Zambrano O., Bossmann S.H., Natarajan B.","Early Detection of Pancreatic Cancers Using Liquid Biopsies and Hierarchical Decision Structure","10.1109/JTEHM.2022.3186836","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133787120&doi=10.1109%2fJTEHM.2022.3186836&partnerID=40&md5=0eef5a1cb25532216d5de9e259242624","Objective: Pancreatic cancer (PC) is a silent killer, because its detection is difficult and to date no effective treatment has been developed. In the US, the current 5-year survival rate of 11%. Therefore, PC has to be detected as early as possible. Methods and procedures: In this work, we have combined the use of ultrasensitive nanobiosensors for protease/arginase detection with information fusion based hierarchical decision structure to detect PC at the localized stage by means of a simple Liquid Biopsy. The problem of early-stage detection of pancreatic cancer is modelled as a multi-class classification problem. We propose a Hard Hierarchical Decision Structure (HDS) along with appropriate feature engineering steps to improve the performance of conventional multi-class classification approaches. Further, a Soft Hierarchical Decision Structure (SDS) is developed to additionally provide confidences of predicted labels in the form of class probability values. These frameworks overcome the limitations of existing research studies that employ simple biostatistical tools and do not effectively exploit the information provided by ultrasensitive protease/arginase analyses. Results: The experimental results demonstrate that an overall mean classification accuracy of around 92% is obtained using the proposed approach, as opposed to 75% with conventional multi-class classification approaches. This illustrates that the proposed HDS framework outperforms traditional classification techniques for early-stage PC detection. Conclusion: Although this study is only based on 31 pancreatic cancer patients and a healthy control group of 48 human subjects, it has enabled combining Liquid Biopsies and Machine Learning methodologies to reach the goal of earliest PC detection. The provision of both decision labels (via HDS) as well as class probabilities (via SDS) helps clinicians identify instances where statistical model-based predictions lack confidence. This further aids in determining if more tests are required for better diagnosis. Such a strategy makes the output of our decision model more interpretable and can assist with the diagnostic procedure. Clinical impact: With further validation, the proposed framework can be employed as a decision support tool for the clinicians to help in detection of pancreatic cancer at early stages. © 2013 IEEE.","early cancer detection; hierarchical decision structure; information fusion; liquid biopsy; Pancreatic cancer (PC)","Artificial intelligence; Biological systems; Biopsy; Classification (of information); Decision support systems; Diseases; Learning systems; Liquids; Biological system modeling; Cancer; Early cancer detection; Hierarchical decision structure; Hierarchical decisions; Liquid biopsy; Magnetic liquids; Nanobiosciences; Pancreatic cancer; Pancreatic cancers; Information fusion; arginase; gelatinase B; interstitial collagenase; proteinase; stromelysin; tumor marker; arginase; peptide hydrolase; Article; Bayesian learning; binary classification; calculation; classifier; clinical decision support system; decision tree; early cancer diagnosis; hard hierarchical decision structure; hierarchical clustering; human; k nearest neighbor; liquid biopsy; logistic regression analysis; machine learning; multiclass classification; pancreas cancer; random forest; soft hierarchical decision structure; support vector machine; validation study; liquid biopsy; pancreas tumor; Arginase; Humans; Liquid Biopsy; Pancreatic Neoplasms; Peptide Hydrolases"
"Agarwal D., Babel N., Baker R.S.","Contextual derivation of stable BKT parameters for analyzing content efficacy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084016789&partnerID=40&md5=012ae89970577a6f149f12e3ca93c723","One of the key benefits that Bayesian Knowledge Tracing (BKT) offers compared to many competing student modelling paradigms is that its parameters are meaningful and interpretable. These parameters have been used to answer basic research questions and identify content in need of iterative improvement (due to, for instance, low learning or high slip rates). However, a core challenge to the interpretation of BKT parameters is that several combinations of BKT parameters can often fit the same data comparably well. Even if, as some have argued, BKT is not truly non-identifiable, in practice highly different parameters with comparable goodness are often found using modern BKT fitting packages. These parameter sets can have highly divergent values for guess and slip. Several approaches have been proposed but none of those have yet led to fully stable and trustworthy parameter estimates. In this work, we propose a new iterative method based on contextual guess and slip estimation that converges to stable estimates for skill-level guess and slip parameters. This method alternates between calculating contextual estimates of guess and slip and estimating skill-level parameters, iterating until convergence. Thus, it produces a more stable set of parameters that can be more confidently used in analyzing content efficacy. © 2018 International Educational Data Mining Society. All rights reserved.","Bayesian Knowledge Tracing; Brute Force BKT Model; Content efficacy; Contextual guess slip; EDM","Data mining; Iterative methods; Bayesian knowledge tracings; Brute force; Content efficacy; Contextual guess; Iterative improvements; Parameter estimate; Research questions; Student modelling; Parameter estimation"
"Agarwal N., Das S.","Interpretable Machine Learning Tools: A Survey","10.1109/SSCI47803.2020.9308260","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099680630&doi=10.1109%2fSSCI47803.2020.9308260&partnerID=40&md5=bc3806755281463f2285394c9add2fa1","In recent years machine learning (ML) systems have been deployed extensively in various domains. But most MLbased frameworks lack transparency. To believe in ML models, an individual needs to understand the reasons behind the ML predictions. In this paper, we provide a survey of open-source software tools that help explore and understand the behavior of the ML models. Also, these tools include a variety of interpretable machine learning methods that assist people with understanding the connection between input and output variables through interpretation, validate the decision of a predictive model to enable lucidity, accountability, and fairness in the algorithmic decision making policies. Furthermore, we provide the state-of-the-art of interpretable machine learning (IML) tools, along with a comparison and a brief discussion of the implementation of those IML tools in various programming languages. © 2020 IEEE.","interpretable machine learning; interpretable machine learning tools survey; machine learning; open-source tools","Behavioral research; Decision making; Intelligent computing; Object oriented programming; Open source software; Open systems; Predictive analytics; Surveys; Input and outputs; Machine learning methods; Predictive modeling; State of the art; Machine learning"
"Agarwal P., Tamer M., Budman H.","Explainability: Relevance based dynamic deep learning algorithm for fault detection and diagnosis in chemical processes","10.1016/j.compchemeng.2021.107467","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112487870&doi=10.1016%2fj.compchemeng.2021.107467&partnerID=40&md5=40570dbe24cd3d722d00ede8a89adda6","The focus of this work is on Statistical Process Control (SPC) of a manufacturing process based on available measurements. Two important applications of SPC in industrial settings are fault detection and diagnosis (FDD). In this work, a deep learning (DL) based methodology is proposed for FDD. We investigate the application of an explainability concept (explainable artificial intelligence (XAI)) to enhance the FDD accuracy of a deep neural network model trained with a dataset of relatively small number of samples. The explainability is quantified by a novel relevance measure of input variables that is calculated from a Layerwise Relevance Propagation (LRP) algorithm. It is shown that the relevances can be used to discard redundant input feature vectors/ variables iteratively thus resulting in reduced over-fitting of noisy data, increasing distinguishability between output classes and superior FDD test accuracy. The efficacy of the proposed method is demonstrated on the benchmark Tennessee Eastman Process. © 2021 Elsevier Ltd","Autoencoders; Deep learning; Explainability; Fault detection and diagnosis; Tennessee eastman process","Chemical detection; Deep neural networks; Iterative methods; Learning algorithms; Statistical process control; Autoencoders; Chemical process; Deep learning; Explainability; Fault detection and diagnosis; Industrial settings; Manufacturing process; Process-based; Statistical process-control; Tennessee Eastman process; Fault detection"
"Agarwal P., Swarup D., Prasannakumar S., Dechu S., Gupta M.","Unsupervised Contextual State Representation for Improved Business Process Models","10.1007/978-3-030-66498-5_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101306773&doi=10.1007%2f978-3-030-66498-5_11&partnerID=40&md5=03fb0f760d4b36f5042437ef8dec4032","Predictive Business Process Monitoring tasks such as next activity prediction, next timestamp prediction, etc. are becoming crucial as new technologies are enabling intelligent automation of business processes. Recent works try to address this problem by using deep learning models that encode limited attribute information of past activities for a case independently w.r.t the other cases in execution. However, the predictions for a case can also depend on contextual information such as inter-case dependencies and domain-specific attributes, which is not considered in previous works. We propose a novel method of encoding the contextual state information i.e., encoding the state of on-going cases and multi-attribute domain-specific information along with intra-case information in an unsupervised manner. We train two widely used deep learning models i.e., LSTM and Transformer using the proposed representation, and compare their performance to show the improved results over the state-of-the-art models. We also investigate the influence of past activities and other on-going cases on prediction using self-attention, making the framework to provide interpretable predictions for a decision making business user. © 2020, Springer Nature Switzerland AG.","Contextual representation; Inter-case; Interpretability","Decision making; Deep learning; Encoding (symbols); Enterprise resource management; Forecasting; Learning systems; Process monitoring; Signal encoding; Activity predictions; Attribute information; Business process model; Business process monitoring; Contextual information; Domain-specific information; Intelligent automation; State representation; Long short-term memory"
"Agarwal P., Verma R., Mallik A.","Ontology based disease diagnosis system with probabilistic inference","10.1109/IICIP.2016.7975383","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027470135&doi=10.1109%2fIICIP.2016.7975383&partnerID=40&md5=b22f90179fcc0ea8f1905237a003889d","Disease diagnosis system integrates fragmented and constantly evolving information about diseases from various sources, such as the web, and builds up an ontology model and inference system, which could be used by doctors and researchers in the medical domain. It can also be used to make the information about rare diseases accessible for experts world over, which may not, otherwise, be easily available. This paper presents an ontology-based system that creates a knowledge graph of the extracted information to make it machine understandable. Instead of using deterministic logic, it generates a Bayesian network out of it that can be utilized to infer the most probable diseases given the patient's symptoms, age, gender, lifestyle and personal and parental medical history as input. It further recommends relevant diagnostic tests and pathology labs where the tests could be undertaken. It has a user-friendly interface through which it can take SPARQL query as an input and show the result to the user. © 2016 IEEE.","Bayesian Network; Disease diagnosis; Ontology; Probabilistic Inference; Semantic Web","Bayesian networks; Data mining; Diagnosis; Ontology; Semantic Web; Diagnostic tests; Disease diagnosis; Inference systems; Knowledge graphs; Medical domains; Ontology-based systems; Probabilistic inference; User friendly interface; Medical information systems"
"Agarwal R., Melnick L., Frosst N., Zhang X., Lengerich B., Caruana R., Hinton G.E.","Neural Additive Models: Interpretable Machine Learning with Neural Nets",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131621495&partnerID=40&md5=9de5a047da951ff7b7ab9fe0c2208f96","Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19. Source code is available at neural-additive-models.github.io. © 2021 Neural information processing systems foundation. All rights reserved.",,"Additives; Classification (of information); Complex networks; Decision trees; Forestry; Random forests; Additive models; Black boxes; Combination of neural-network; Decisions makings; Generalized additive model; Input features; Learn+; Linear combinations; Machine-learning; Performance; Deep neural networks"
"Agarwal R., Mittal M.","Optimal ordering policy with inventory classification using data mining techniques","10.4018/978-1-5225-3232-3.ch017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045712258&doi=10.4018%2f978-1-5225-3232-3.ch017&partnerID=40&md5=1b688868f53930828f72410dff87bcea","Data mining is a technique to identify valid novel, potentially useful, and understandable correlations and patterns in existing data. Data mining techniques, such as clustering, association rule mining, classification, and sequential pattern mining, have attracted a great deal of attention in the information industry and in society as a whole in recent years. Some research studies have also extended the usage of this concept in inventory management. Yet, not many research studies have considered the application of data mining approach on determining both optimal order quantity and loss profit of frequent items. This helps inventory manager to determine optimum order quantity of frequent items together with the most profitable item for optimal inventory control. In this chapter, two different cases for determining ordering policy and inventory classification based on loss rule are presented. An example is illustrated to validate the results. © 2018, IGI Global.",,
"Agarwal S., Joshi K.","Looking beyond Adsorption Energies to Understand Interactions at Surface using Machine Learning","10.1002/slct.202202414","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140252506&doi=10.1002%2fslct.202202414&partnerID=40&md5=4a47d979d0ddec9ecc1db66a196224ed","Identifying factors that influence interactions at the surface is still an active area of research. In this work, the importance of analyzing bond length activations (BLact) along with adsorption energies (Ea) while interpreting Density Functional Theory (DFT) results is emphasized. Investigating adsorption of different small molecules, such as O2, N2, CO, and CO2, on commonly studied facets ((100), (110), and (111)) of seven fcc transition metal surfaces (M=Ag, Au, Cu, Ir, Rh, Pt, and Pd) demonstrates the missing linear correlation between Ea and BLact. Further, tree based Machine Learning (ML) models reinforce the missing linear correlation between the two parameters and also highlight the importance of analyzing both to develop a better understanding of adsorption at surfaces. The best performing Random Forest models have a mean absolute error (MAE) of 0.19 eV for Ea prediction, and even lower MAE of 0.012 Å for BLact prediction. While often d-band center is correlated with Ea, our observations show that infact the d-band center has a better correlation with BLact. These observations emphasizes the role of BLact in gaining a fuller picture for catalysis. The fact that the factors responsible for BLact is a lesser-explored subject adds to the novelty of the findings. © 2022 Wiley-VCH GmbH.","Adsorption energy; Bondlength activation; Catalysis; DFT; Machine Learning",
"Agarwal S., Sukritin, Sharma A., Mishra A.","Next Word Prediction Using Hindi Language","10.1007/978-981-16-7952-0_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130400969&doi=10.1007%2f978-981-16-7952-0_10&partnerID=40&md5=aeb75c7b37a6b03570b785fe1da44113","Natural language generation is a process that concerns on generating human understandable language. This study provides method to guess next word from previous sequence of words that are in Hindi language. This process reduces the keystrokes of a user by predicting next word. In this two machine learning technology, BERT (Bidirectional Encoder Representations from Transformers) Model and ML (Masked Language) model is used to predict next word from previous words. Next word prediction is a technology that takes input and simplify typing process as of suggest user the next word according to the understanding of previous words. It makes typing less time consuming and error free. In current times, everyone is writing data digitally and to make typing efficient in Hindi language we require such systems. There are various systems related to English Language. Similar systems are required for Hindi language as Hindi is a more frequent language in India. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","BERT model; Fine tuning; Machine learning; Masked language model; Natural language processing; Next word prediction; Prediction",
"Agarwal S.","Data mining: Data mining concepts and techniques","10.1109/ICMIRA.2013.45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910003761&doi=10.1109%2fICMIRA.2013.45&partnerID=40&md5=7ebb4e7c4427bb1df5853d606cc7db9c","Data mining is a field of intersection of computer science and statistics used to discover patterns in the information bank. The main aim of the data mining process is to extract the useful information from the dossier of data and mold it into an understandable structure for future use. There are different process and techniques used to carry out data mining successfully. © 2013 IEEE.","database; knowledge; patterns; phase; techniques; tuple","Database systems; knowledge; patterns; phase; techniques; tuple; Data mining"
"Agarwal V., Mishra P., Sharan R., Sharma N., Patel R.","IoT and AI Based Advance LPG System (ALS)","10.1007/978-981-16-6369-7_44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122094769&doi=10.1007%2f978-981-16-6369-7_44&partnerID=40&md5=b8b83dea52e42747c497aef27db3a51a","The liquefied petroleum gas (LPG) plays a very crucial role in our day to-day life, whether it is for cooking, heating appliances, industrial use or used by vehicles. The use of LPG in a safe and responsible manner is a necessity of life whereas it is also a necessity to advance the LPG system, but inconsolably until now, no major revolutionary steps are taken to implement the safety and advancement in the field of LPG system. We all observed that the booking and handling gas stove is a bit more laborious task; to first book the cylinder by handling the calls and data and then delivering; these are very cost-consuming and laborious work. To contribute to the motion of safe, advanced, and more responsibility of LPG, we came with an innovative product to satisfy the concern regarding the safe use of LPG gas as well as way more advanced and comfortable transaction between customer and distributor in case of domestic usage of LPG. Our whole idea revolves around the Internet of things and artificial intelligence. The need of this project is to save the time and life of the consumer while operating the gas system, as we are introducing automatic gas booking system, gas leakage detection system, and smart gas stove. Hence, these features are justifying the name of this product “The Advance LPG System (ALS).” © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","ALS; Application programming interface management; Artificial intelligence; Automatic booking system; Consumer Web application; Data analysis; Embedded systems; Gas cylinder; Gas leakage detection; Interactive Web distributor portal; Internet of things; Json data packet; LPG; PNG; Raspberry Pi; Recording gas consumption; Sensors and instrumentation; Smart gas stove",
"Agatonovic-Kustrin S., Morton D.","Data Mining in Drug Discovery and Design","10.1016/B978-0-12-801559-9.00009-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967648025&doi=10.1016%2fB978-0-12-801559-9.00009-0&partnerID=40&md5=f0a80f2872ef3375dc2b8fcf23faeb65","One of the main challenges in drug discovery is to design a new biologically active compound on the basis of previously synthesized molecules and from established quantitative structure-activity relationships. This knowledge is then used to propose new drugs with enhanced biological activity and a better selectivity profile for a specific therapeutic target. Improvements in computer speed and capacity have led to the generation and collection of an enormous amount of data. Thus, the great challenge is to discover useful and understandable patterns from these huge in silico libraries. Therefore, data mining has become a very important research direction; developing data mining tools for drug discovery is the first step set since classical statistical methods are insufficient. © 2016 Elsevier Inc. All rights reserved.","Artificial neural networks; Data mining; Linear regression; Molecular descriptor; Partial least squares; Quantitative structure-activity relationships; Virtual screening","Bioactivity; Least squares approximations; Linear regression; Molecular graphics; Neural networks; Synthesis (chemical); Biologically active compounds; Data-mining tools; Molecular descriptors; Partial least square (PLS); Quantitative structure activity relationship; Synthesized molecules; Therapeutic targets; Virtual Screening; Data mining"
"Agbozo R.S.K., Zheng P., Peng T., Tang R.","Towards Cognitive Intelligence-Enabled Manufacturing","10.1007/978-3-031-16411-8_50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138808613&doi=10.1007%2f978-3-031-16411-8_50&partnerID=40&md5=b9fd9bd5a4d2c688f94fff64b6c7be1c","Cognitive intelligence-enabled manufacturing (CoIM) uses machines to utilize technologies that mimic human cognitive abilities to solve complex problems in manufacturing. With the support of a cognitive intelligence-enabled manufacturing system (CoIMS) architecture, information flow is organized and coordinated appropriately, starting from the machine sensory system, central system to the motor system. Machine perceptive abilities monitor, sense and capture equipment performance, aggregate data, and help gain valuable insights into the production process. It uses the industrial internet of things, data analytics, artificial intelligence and related techniques and cognitive computing and related technologies to address production issues in an autonomous manner. As such, CoIMS solves complex production problems. It also transforms manufacturing by improving product quality, productivity, and safety, reducing costs and downtimes, identifying knowledge gaps, and enhancing customer experience. Even so, a CoIMS is not responsible for making the final decision. Instead, it supplements information on the fly for engineers to take necessary actions. © 2022, IFIP International Federation for Information Processing.","Artificial intelligence; Cognitive intelligence; Manufacturing; Self-X cognition; Smart decision making","Artificial intelligence; Data Analytics; Industry 4.0; Cognitive intelligence; Complex problems; Decisions makings; Human cognitive abilities; Information flows; Manufacturing; Self-X cognition; Sensory system; Smart decision making; Systems architecture; Decision making"
"Aggarwal C.C.","An intuitive framework for understanding changes in evolving data streams",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036211966&partnerID=40&md5=d35c57cfc2ca552cccbb0de8f96b0928","Many organizations today store large streams of transactional data in real time. This data can often show important changes in trends over time. In many commercial applications, it may be valuable to provide the user with an understanding of the nature of changes occuring over time in the data stream. In this poster, we discuss the process of analysis of the significant changes and trends in data streams in a way which is understandable, intuitive and friendly to a user.",,"Algorithms; Data mining; Real time systems; Telecommunication networks; Data densities; Data streams; Database systems"
"Aggarwal G., Jain S.","Analysis of Genes Responsible for the Development of Cancer using Machine Learning","10.1109/ICISC44355.2019.9036398","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083023690&doi=10.1109%2fICISC44355.2019.9036398&partnerID=40&md5=3a7916f91211880b6dd6dbce1b33f9c2","The average adult human's body is made up of approximately 37 trillion cells. Healthy cells in our bodies divide and replace themselves in a controlled fashion throughout our lives. But when healthy cells divide uncontrollably, they form new, abnormal cells, thus making a lesion in an affected body part. This lesion can be cancerous or non-cancerous. At what stage one got to know about the cancerous tumour is crucial as that would decide the basis of treatment. In this paper, we mentioned the basics of cancer and used different algorithms of data mining to detect breast cancer. In our proposed research work, WEKA software is applied with ten cross validation to calculate and accumulate result. © 2019 IEEE.","Data Mining; Machine Learning; Naïve Bayes; Random Tree; WEKA Tool","Cytology; Data mining; Diseases; Machine learning; Body parts; Breast Cancer; Cross validation; Cells"
"Aggarwal M.","On learning of choice models with interactive attributes","10.1109/TKDE.2016.2563434","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990966933&doi=10.1109%2fTKDE.2016.2563434&partnerID=40&md5=0c87f787ce813d3e5345a1e7cb042136","Introducing recent advances in the machine learning techniques to state-of-the-art discrete choice models, we develop an approach to infer the unique and complex decision making process of a decision-maker (DM), which is characterized by the DM's priorities and attitudinal character, along with the attributes interaction, to name a few. On the basis of exemplary preference information in the form of pairwise comparisons of alternatives, our method seeks to induce a DM's preference model in terms of the parameters of recent discrete choice models. To this end, we reduce our learning function to a constrained non-linear optimization problem. Our learning approach is a simple one that takes into consideration the interaction among the attributes along with the priorities and the unique attitudinal character of a DM. The experimental results on standard benchmark datasets suggest that our approach is not only intuitively appealing and easily interpretable but also competitive to state-of-the-art methods. © 1989-2012 IEEE.","attitudinal character; attributes interaction; choice modelling; multi-attribute decision making; Preference learning","Artificial intelligence; Constrained optimization; Learning systems; Nonlinear programming; Optimization; attitudinal character; attributes interaction; Choice modelling; Multi attribute decision making; Preference learning; Decision making"
"Aggarwal R., Podder P., Khamparia A.","ECG Classification and Analysis for Heart Disease Prediction Using XAI-Driven Machine Learning Algorithms","10.1007/978-981-19-1476-8_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128177823&doi=10.1007%2f978-981-19-1476-8_7&partnerID=40&md5=2854068cce925ccc7f0bd72c2975cf85","In the biomedical science and research field, the electrocardiogram provides better results due to advancements in technologies. The electrocardiogram is the electrical activity of the heart. It generates in form like electrical indication at a greater extent number of people are suffering from heart disease. Envision of heart sicknesses in clinical focuses is huge to recognize if the individual has heart sickness or not. Information mining is utilized to recover covered-up data in clinical directions that help to foresee unique sickness. Coronary illness is one of the most widely recognized infections that lead to death in this world. This proposed work used the classifying method of machine learning based on electrocardiogram features and analyzing the ECG report. The heartbeat classification is varying person to person because it is a combination of impulse waveforms. The waveform is described by specific features. These features are the inputs of ML algorithms. The proposed approach is implemented by MIT MLlib and on a Python Jupyter notebook. ECG is the best way to diagnose heart-related issues. Precise identification of obsessive heart occasions is a significant piece of (ECG) assessment and resulting proper patient treatment. This chapter proposes a framework for ECG examination and heartbeat arrangement and normal and abnormal ECG. Explainable Artificial Intelligence (XAI) in terms of machine learning is implemented in this chapter. This proposed work used ANN for counting the neurons as per the diseases that extracting in this chapter. The main challenge in ECG signals by their classification is to handle the irregularities that are very important for detecting patient status. In this research, five machine learning algorithms are used to envision patients suffering from a particular disease and, based on their characteristics, recognize between the normal ECG and abnormal ECG. This chapter divided the dataset for accurate results; the cross-validation method and artificial neural network are used. This proposed work for the disease RBBB and CAS has provided better results using logistic regression and support vector machine classifier and achieved an accuracy of 95%. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Artificial neural network; Coronary artery disease; Electrocardiogram; Heartbeats classification; Logistic regression; Machine learning libraries; Sinus bradycardia; Sinus tachycardia; Support vector machine",
"Aghaei A., Ebrahimi Moghaddam M., Malek H.","Interpretable ensemble deep learning model for early detection of Alzheimer's disease using local interpretable model-agnostic explanations","10.1002/ima.22762","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131573044&doi=10.1002%2fima.22762&partnerID=40&md5=5746c3824ebacbc15429aa06e3da667a","Early diagnosis of Alzheimer's is crucial to slow the progression of the disease. In this regard, there are many attempts to detect this disease at the early stages using AI techniques such as deep learning. We have proposed an explainable method to solve the early-stage detection of Alzheimer's using transfer learning as a well-known approach when there is not enough data. The employed transfer learning method is a combination of fine-tuned ResNet-50 and Inception-V3 with Soft-max and SVM classifiers using averaging. Moreover, local interpretable model-agnostic explanations (LIME) are used to show the explainability of the proposed method. The AUC, accuracy, specificity, and sensitivity on structural MRI data of 100 MCI patients were 0.94, 87%, 92%, and 82%, respectively. Also, the LIME results were subjectively evaluated. The results showed the proposed method outperformed some related works. In addition, LIME technique make model more reliable to identify the parts involved in the patient's brain. © 2022 Wiley Periodicals LLC.","Alzheimer's disease; ensemble deep learning; Inception-V3; ResNet-50; structural MRI; transfer learning","Deep learning; Diagnosis; Lime; AI techniques; Alzheimers disease; Early diagnosis; Ensemble deep learning; Inception-v3; Learning models; Resnet-50; Structural MRI; Transfer learning; Transfer learning methods; Neurodegenerative diseases"
"Aghaei S., Azizi M.J., Vayanos P.","Learning optimal and fair decision trees for non-discriminative decision-making",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090176132&partnerID=40&md5=f598454dc235d10f2b1eaf428b584f6e","In recent years, automated data-driven decision-making systems have enjoyed a tremendous success in a variety of fields (e.g., to make product recommendations, or to guide the production of entertainment). More recently, these algorithms are increasingly being used to assist socially sensitive decision-making (e.g., to decide who to admit into a degree program or to prioritize individuals for public housing). Yet, these automated tools may result in discriminative decision-making in the sense that they may treat individuals unfairly or unequally based on membership to a category or a minority, resulting in disparate treatment or disparate impact and violating both moral and ethical standards. This may happen when the training dataset is itself biased (e.g., if individuals belonging to a particular group have historically been discriminated upon). However, it may also happen when the training dataset is unbiased, if the errors made by the system affect individuals belonging to a category or minority differently (e.g., if misclassification rates for Blacks are higher than for Whites). In this paper, we unify the definitions of unfairness across classification and regression. We propose a versatile mixed-integer optimization framework for learning optimal and fair decision trees and variants thereof to prevent disparate treatment and/or disparate impact as appropriate. This translates to a flexible schema for designing fair and interpretable policies suitable for socially sensitive decision-making. We conduct extensive computational studies that show that our framework improves the state-of-the-art in the field (which typically relies on heuristics) to yield non-discriminative decisions at lower cost to overall accuracy. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Decision trees; Entertainment industry; Forestry; Housing; Integer programming; Computational studies; Ethical standards; Misclassification rates; Mixed integer optimization; Overall accuracies; Product recommendation; State of the art; Training dataset; Decision making"
"Aghaeipoor F., Javidi M.M., Fernandez A.","IFC-BD: An Interpretable Fuzzy Classifier for Boosting Explainable Artificial Intelligence in Big Data","10.1109/TFUZZ.2021.3049911","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099593252&doi=10.1109%2fTFUZZ.2021.3049911&partnerID=40&md5=021d3c130e029073633863bb10663bfe","In current Data Science applications, the course of action has derived to adapt the system behavior for the human cognition, resulting in the emerging area of explainable artificial intelligence. Among different classification paradigms, those based on fuzzy rules are suitable solutions to stress the interpretability of the global systems. However, in case of addressing Big Data analytics, they may comprise an excessive number of rules and/or linguistic labels that not only may cause losing the system performance but also may affect the system semantic as well as the system interpretability. In this article, we propose IFC-BD, an interpretable fuzzy classifier for Big Data, aiming at boosting the horizons of explainability by learning a compact yet accurate fuzzy model. IFC-BD is developed in a cell-based distributed framework through the three working stages of initial rule learning, rule generalization, and heuristic rule selection. This whole procedure allows reaching from a high number of specific rules to less number of more general and confident rules. Additionally, in order to resolve possible rules conflict, a new estimated rule weight is proposed specifically for big data problems. IFC-BD was evaluated in comparison to the state-of-the-art approaches of the fuzzy classification paradigm, considering interpretability, accuracy, and running time. The findings of the experiments revealed that the proposed algorithm was able to improve the explainability of fuzzy rule-based classifiers as well as their predictive performance. © 1993-2012 IEEE.","Apache spark framework; Big Data; explainable artificial intelligence (XAI); fuzzy rule-based classification systems (FRBCSs); interpretability; scalability","Advanced Analytics; Behavioral research; Big data; Classification (of information); Data Analytics; Fuzzy inference; Fuzzy rules; Semantics; Distributed framework; Fuzzy classification; Fuzzy rule-based classifier; Linguistic labels; Predictive performance; Science applications; State-of-the-art approach; Suitable solutions; Artificial intelligence"
"Aghaeipoor F., Javidi M.M., Triguero I., Fernandez A.","Chi-BD-DRF: Design of scalable fuzzy classifiers for big data via a dynamic rule filtering approach","10.1109/FUZZ48607.2020.9177626","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090499331&doi=10.1109%2fFUZZ48607.2020.9177626&partnerID=40&md5=1bdc4fedafb82299dff4504a8f8aba44","Big data classification problems are known to be no longer addressable by sequential algorithms. Therefore, it is necessary to design and develop novel solutions to provide accurate yet interpretable models in a tolerable elapsed time. In this area, Fuzzy Rule-Based Classification Systems are very advantageous due to their intrinsic interpretable and accurate capabilities. However, when these systems are applied in Big Data scenarios, the size of the rule set can become too large to be useful, whereas many of the generated rules could be associated with the non-dense areas or outliers. The presence of such rules in the rule base not only increases the running time and computation overheads but also affects on the interpretability of the fuzzy system. In this contribution, we propose a novel approach to obtain compact and accurate fuzzy models for Big data problems in a linearly scalable complex time. To do so, a dynamic filtering approach is applied to remove low supporting rules. Moreover, an efficient computation of the rules' weights is presented to improve the accuracy of the predictions. This model is developed for Big Data analytics by using Apache Spark framework. This allows taking advantage of the built-in resources and directives for a transparent distributed computing, as well as the machine learning pipeline to ease the complete processing. Experimental results, using different Big Data problems, confirmed the goodness of the proposed algorithm with respect to the baseline fuzzy classifier. © 2020 IEEE.","Big Data; Dynamic Rule Filtering; Fuzzy Rule-Based Classification Systems; Rule Weights; Spark","Big data; Data Analytics; Fuzzy inference; Fuzzy sets; Predictive analytics; Computation overheads; Data classification problems; Dynamic filtering; Efficient computation; Fuzzy classifiers; Fuzzy rule based classification systems; Interpretability; Sequential algorithm; Classification (of information)"
"Agham N.D., Chaskar U.M.","An advanced LAN model based on optimized feature algorithm: Towards hypertension interpretability","10.1016/j.bspc.2021.102760","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106938648&doi=10.1016%2fj.bspc.2021.102760&partnerID=40&md5=b648b10c2b977a2480ebb370c5a04ebe","Background: The existing cuffless BP measuring methods still show the inadequacy of the model that utilizes long term dependencies in BP dynamics and come up with the outcome of recurrent calibration. Objective: This study aims to develop a learning-based predictive model for continuous BP measurement with long-term evaluation of the cardiovascular parameter. Methods: Towards hypertension interpretability, the proposed model assists the current as well as the previous status of cardiovascular events based on long short-term memory networks (LSTM) and flexible autoregressive integrated moving average (ARIMA). It is often complicated and challenging to continuously estimate BP from random cardiac events. This paper proposes a new SGFA (Sequential Genetic Feature Algorithm) to tackle the feature optimization problem. Results: The experiment was performed on a MIMIC database containing 2780 datafiles of PPG-BP. The proposed model provides the best performance with root mean square error (RMSE) and mean absolute error (MAE) of 1.17 and 1.04 for SBP, whereas 1.06 and 1.02 for DBP. The proposed method has also been evaluated on hypertensive and hypotensive patients. In hypertension condition, estimated SBP and DBP present a good correlation with the true measurements. MAE and RMSE of the estimated SBP are 0.96 and 1.21, whereas, for DBP, it shows 0.42 and 0.57 respectively. Extensive experimentation results confirm that the proposed method delivers a remarkable performance of BP prediction. Conclusion: The proposed work shows a remarkable BP estimation performance compared to the previous inventive methods and signifies insight of various cardiovascular events responsible for BP variation and hypertension interpretability. © 2021 Elsevier Ltd","AutoRegressive integrated moving average; Blood pressure; Genetic algorithm; Long short-term memory","Blood pressure; Brain; Genetic algorithms; Mean square error; Autoregressive integrated moving average; Cardiovascular event; Cuffless; Interpretability; LAN model; Mean absolute error; Model-based OPC; Performance; Root mean square errors; Short term memory; Long short-term memory; Article; blood pressure measurement; cardiovascular parameters; correlation analysis; data base; evaluation study; experimentation; genetic algorithm; human; hypertension; learning; long short term memory network; machine learning; predictive value; priority journal; systolic blood pressure"
"Aghamohammadi A., Izadi M., Heydarnoori A.","Generating summaries for methods of event-driven programs: An Android case study","10.1016/j.jss.2020.110800","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089832552&doi=10.1016%2fj.jss.2020.110800&partnerID=40&md5=4ec764683d60204b4ae45c885249678b","The lack of proper documentation makes program comprehension a cumbersome process for developers. Source code summarization is one of the existing solutions to this problem. Many approaches have been proposed to summarize source code in recent years. A prevalent weakness of these solutions is that they do not pay much attention to interactions among elements of software. An element is simply a callable code snippet such as a method or even a clickable button. As a result, these approaches cannot be applied to event-driven programs, such as Android applications, because they have specific features such as numerous interactions between their elements. To tackle this problem, we propose a novel approach based on deep neural networks and dynamic call graphs to generate summaries for methods of event-driven programs. First, we collect a set of comment/code pairs from Github and train a deep neural network on the set. Afterward, by exploiting a dynamic call graph, the Pagerank algorithm, and the pre-trained deep neural network, we generate summaries. An empirical evaluation with 14 real-world Android applications and 42 participants indicates 32.3% BLEU4 which is a definite improvement compared to the existing state-of-the-art techniques. We also assessed the informativeness and naturalness of our generated summaries from developers’ perspectives and showed they are sufficiently understandable and informative. © 2020 Elsevier Inc.","Deep learning; Event-driven programs; Neural machine translation; Source code summarization","Android (operating system); Application programs; Deep neural networks; Graph algorithms; Program documentation; XML; Android applications; Empirical evaluations; Event-driven; Informative ness; PageRank algorithm; Program comprehension; Source codes; State-of-the-art techniques; Neural networks"
"Aghamohammadi M., Madan M., Hong J.K., Watson I.","Predicting Heart Attack Through Explainable Artificial Intelligence","10.1007/978-3-030-22741-8_45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067645835&doi=10.1007%2f978-3-030-22741-8_45&partnerID=40&md5=a37a4f26762ab5218e7ba5d7884b8484","A novel classification technique based on combined Genetic Algorithm (GA) and Adaptive Neural Fuzzy Inference System (ANFIS) for diagnosis of heart Attack is reported. Exploiting the combined advantages of neural networks, fuzzy logic and GA, the performance of the proposed system is investigated by evaluation functions such as sensitivity, specificity, precision, accuracy and Root Mean Squared Error (RMSE). Also, the efficiency of the algorithm is evaluated by employing 9-fold cross validation. To address the explainability of the predictions, explainable graphs are provided. The results show that the performance of the proposed algorithm is quite satisfactory. Furthermore, the importance of various symptoms in diagnosis of heart attack is investigated through defining and employing an importance evaluation function. It is shown that some symptoms have key roles in effective prediction of heart Attack. © Springer Nature Switzerland AG 2019.","ANFIS; Explainable artificial intelligence; GA; Heart attack","Computer aided diagnosis; Forecasting; Function evaluation; Fuzzy inference; Fuzzy logic; Fuzzy neural networks; Gallium; Genetic algorithms; Heart; Inference engines; Mean square error; Adaptive neural fuzzy inference system (ANFIS); ANFIS; Classification technique; Cross validation; Evaluation function; Heart attack; Neural networks , fuzzy logic; Root mean squared errors; Artificial heart"
"Aghazadeh A., Ocal O., Ramchandran K.","CRISPRLAND: Interpretable large-scale inference of DNA repair landscape based on a spectral approach","10.1093/BIOINFORMATICS/BTAA505","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087917786&doi=10.1093%2fBIOINFORMATICS%2fBTAA505&partnerID=40&md5=6bbe08bdabaa1503519d463c71679c45","We propose a new spectral framework for reliable training, scalable inference and interpretable explanation of the DNA repair outcome following a Cas9 cutting. Our framework, dubbed CRISPRLAND, relies on an unexploited observation about the nature of the repair process: the landscape of the DNA repair is highly sparse in the (Walsh-Hadamard) spectral domain. This observation enables our framework to address key shortcomings that limit the interpretability and scaling of current deep-learning-based DNA repair models. In particular, CRISPRLAND reduces the time to compute the full DNA repair landscape from a striking 5230 years to 1 week and the sampling complexity from 1012 to 3 million guide RNAs with only a small loss in accuracy (R2R2 ∼ 0.9). Our proposed framework is based on a divide-and-conquer strategy that uses a fast peeling algorithm to learn the DNA repair models. CRISPRLAND captures lower-degree features around the cut site, which enrich for short insertions and deletions as well as higher-degree microhomology patterns that enrich for longer deletions. © The Author(s) 2020. Published by Oxford University Press.",,"algorithm; DNA repair; software; Algorithms; DNA Repair; Software"
"Agiollo A., Ciatto G., Omicini A.","Shallow2Deep: Restraining Neural Networks Opacity Through Neural Architecture Search","10.1007/978-3-030-82017-6_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113325735&doi=10.1007%2f978-3-030-82017-6_5&partnerID=40&md5=5774eb7e3ab27b920c7e15b540cdac09","Recently, the Deep Learning (DL) research community has focused on developing efficient and highly performing Neural Networks (NN). Meanwhile, the eXplainable AI (XAI) research community has focused on making Machine Learning (ML) and Deep Learning methods interpretable and transparent, seeking explainability. This work is a preliminary study on the applicability of Neural Architecture Search (NAS) (a sub-field of DL looking for automatic design of NN structures) in XAI. We propose Shallow2Deep, an evolutionary NAS algorithm that exploits local variability to restrain opacity of DL-systems through NN architectures simplification. Shallow2Deep effectively reduces NN complexity – therefore their opacity – while reaching state-of-the-art performances. Unlike its competitors, Shallow2Deep promotes variability of localised structures in NN, helping to reduce NN opacity. The proposed work analyses the role of local variability in NN architectures design, presenting experimental results that show how this feature is actually desirable. © 2021, Springer Nature Switzerland AG.","Evolutionary algorithm; Interpretability; Neural Architecture Search; Opacity","Deep learning; Evolutionary algorithms; Learning systems; Multi agent systems; Network architecture; Opacity; Automatic design; Learning methods; Neural architectures; Neural network (nn); Research communities; State-of-the-art performance; Sub fields; Work analysis; Neural networks"
"Agius R., Brieghel C., Andersen M.A., Pearson A.T., Ledergerber B., Cozzi-Lepri A., Louzoun Y., Andersen C.L., Bergstedt J., von Stemann J.H., Jørgensen M., Tang M.-H.E., Fontes M., Bahlo J., Herling C.D., Hallek M., Lundgren J., MacPherson C.R., Larsen J., Niemann C.U.","Machine learning can identify newly diagnosed patients with CLL at high risk of infection","10.1038/s41467-019-14225-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078064654&doi=10.1038%2fs41467-019-14225-8&partnerID=40&md5=43ef28c8add9899af87936330294c10a","Infections have become the major cause of morbidity and mortality among patients with chronic lymphocytic leukemia (CLL) due to immune dysfunction and cytotoxic CLL treatment. Yet, predictive models for infection are missing. In this work, we develop the CLL Treatment-Infection Model (CLL-TIM) that identifies patients at risk of infection or CLL treatment within 2 years of diagnosis as validated on both internal and external cohorts. CLL-TIM is an ensemble algorithm composed of 28 machine learning algorithms based on data from 4,149 patients with CLL. The model is capable of dealing with heterogeneous data, including the high rates of missing data to be expected in the real-world setting, with a precision of 72% and a recall of 75%. To address concerns regarding the use of complex machine learning algorithms in the clinic, for each patient with CLL, CLL-TIM provides explainable predictions through uncertainty estimates and personalized risk factors. © 2020, The Author(s).",,"algorithm; cohort analysis; identification method; immune response; machine learning; morbidity; mortality; risk factor; algorithm; Article; blood culture; chronic lymphatic leukemia; cohort analysis; comparative study; ecosystem resilience; follow up; high risk patient; human; incidence; infection risk; machine learning; measurement accuracy; measurement precision; morbidity; mortality; nonhuman; prediction; prognosis; risk factor; aged; benchmarking; chronic lymphatic leukemia; complication; diagnosis; etiology; factual database; female; Kaplan Meier method; male; middle aged; risk factor; antineoplastic agent; Aged; Algorithms; Antineoplastic Agents; Benchmarking; Cohort Studies; Databases, Factual; Female; Humans; Infections; Kaplan-Meier Estimate; Leukemia, Lymphocytic, Chronic, B-Cell; Machine Learning; Male; Middle Aged; Risk Factors"
"Aglin G., Nijssen S., Schaus P.","PyDL8.5: A library for learning optimal decision trees",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097346694&partnerID=40&md5=8e325eb8798f074e525ac033b2de6fcb","Decision Trees (DTs) are widely used Machine Learning (ML) models with a broad range of applications. The interest in these models has increased even further in the context of Explainable AI (XAI), as decision trees of limited depth are very interpretable models. However, traditional algorithms for learning DTs are heuristic in nature; they may produce trees that are of suboptimal quality under depth constraints. We introduce PyDL8.5, a Python library to infer depth-constrained Optimal Decision Trees (ODTs). PyDL8.5 provides an interface for DL8.5, an efficient algorithm for inferring depth-constrained ODTs. The library provides an easy-to-use scikit-learn compatible interface. It cannot only be used for classification tasks, but also for regression, clustering, and other tasks. We introduce an interface that allows users to easily implement these other learning tasks. We provide a number of examples of how to use this library. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.",,"Decision trees; Heuristic algorithms; Optimization; Random forests; Classification tasks; Decision trees (DTs); Depth constraints; Learning tasks; Optimal decisions; Artificial intelligence"
"Agogino A.K., Lee R., Giannakopoulou D.","Machine learning explainability and transferability for path navigation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099798797&partnerID=40&md5=a543839d8bda9973110e0fdbe59eb1d7","Deep neural networks are powerful tools for machine perception. Unfortunately their decisions are difficult to explain due to the complexity and size of the networks. Previously we have alleviated this issue by using the representational portion of a deep neural network and combining it with a nearest neighbor (KNN) classifier. Through inspection of the decisions made by the KNN, we can directly see the training data responsible for the decisions, allowing us to determine the quality of the overall decision and the quality of the representational layer of the deep NN. While the technique worked well, it requires tens of thousands of latent vectors to be stored for classification. In addition, it lacks the ability to show how parts of an image influence the classification decision. Here we address these issues by 1) Using a radial basis function network (RBFN) in place of the KNN allowing far fewer images to be used in deployment and 2) Using an autoencoder network for explainability. In addition to these techniques, we examine the effects of transfer learning to determine that results are robust. All results are tested on a domain where an unmanned aerial vehicle (UAV) navigates a forest trail through a single camera. © 2021, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.",,"Air navigation; Antennas; Machine Perception; Nearest neighbor search; Radial basis function networks; Unmanned aerial vehicles (UAV); Auto encoders; Classification decision; Latent vectors; Machine perception; Nearest-neighbour; Path navigation; Single cameras; Training data; Deep neural networks"
"Agostinelli F., Mavalankar M., Khandelwal V., Tang H., Wu D., Berry B., Srivastava B., Sheth A., Irvin M.","Designing Children's New Learning Partner: Collaborative Artificial Intelligence for Learning to Solve the Rubik's Cube","10.1145/3459990.3465175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110242466&doi=10.1145%2f3459990.3465175&partnerID=40&md5=5dedfd0a79654c915e0ca872e9765e51","Developing the problem solving skills of children is a challenging problem that is crucial for the future of our society. Given that artificial intelligence (AI) has been used to solve problems across a wide variety of domains, AI offers unique opportunities to develop problem solving skills using a multitude of tasks that pique the curiosity of children. To make this a reality, it is necessary to address the uninterpretable ""black-box""that AI often appears to be. Towards this goal, we design a collaborative artificial intelligence algorithm that uses a human-in-the-loop approach to allow students to discover their own personalized solutions to problems. This collaborative algorithm builds on state-of-the-art AI algorithms and leverages additional interpretable structures, namely knowledge graphs and decision trees, to create a fully interpretable process that is able to explain solutions in their entirety. We describe this algorithm when applied to solving the Rubik's cube as well as our planned user-interface and assessment methods. © 2021 Owner/Author.","deep learning; explainability; reinforcement learning; Rubik's cube","Decision trees; Graph algorithms; Knowledge representation; Trees (mathematics); User interfaces; AI algorithms; Artificial intelligence algorithms; Black boxes; Human-in-the-loop; Knowledge graphs; On state; Problem solving skills; Rubik's cubes; Problem solving"
"Agrawal A., Chauhan A., Shetty M.K., P G.M., Gupta M.D., Gupta A.","ECG-iCOVIDNet: Interpretable AI model to identify changes in the ECG signals of post-COVID subjects","10.1016/j.compbiomed.2022.105540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129709369&doi=10.1016%2fj.compbiomed.2022.105540&partnerID=40&md5=47e6e4d534432b966c102dc41ff84c51","Objective: Studies showed that many COVID-19 survivors develop sub-clinical to clinical heart damage, even if subjects did not have underlying heart disease before COVID. Since Electrocardiogram (ECG) is a reliable technique for cardiovascular disease diagnosis, this study analyzes the 12-lead ECG recordings of healthy and post-COVID (COVID-recovered) subjects to ascertain ECG changes after suffering from COVID-19. Method: We propose a shallow 1-D convolutional neural network (CNN) deep learning architecture, namely ECG-iCOVIDNet, to distinguish ECG data of post-COVID subjects and healthy subjects. Further, we employed ShAP technique to interpret ECG segments that are highlighted by the CNN model for the classification of ECG recordings into healthy and post-COVID subjects. Results: ECG data of 427 healthy and 105 post-COVID subjects were analyzed. Results show that the proposed ECG-iCOVIDNet model could classify the ECG recordings of healthy and post-COVID subjects better than the state-of-the-art deep learning models. The proposed model yields an F1-score of 100%. Conclusion: So far, we have not come across any other study with an in-depth ECG signal analysis of the COVID-recovered subjects. In this study, it is shown that the shallow ECG-iCOVIDNet CNN model performed good for distinguishing ECG signals of COVID-recovered subjects from those of healthy subjects. In line with the literature, this study confirms changes in the ECG signals of COVID-recovered patients that could be captured by the proposed CNN model. Successful deployment of such systems can help the doctors identify the changes in the ECG of the post-COVID subjects on time that can save many lives. © 2022 Elsevier Ltd","AI in ECG; CNN; COVID; Electrocardiogram (ECG); Interpretability; Post-COVID; Shapley additive exPlanations (ShAP)","Cardiology; Convolutional neural networks; Deep learning; Diseases; Recovery; Signal reconstruction; AI in electrocardiogram; Convolutional neural network; COVID; Electrocardiogram; Electrocardiogram recordings; Electrocardiogram signal; Interpretability; Post-COVID; Shapley; Shapley additive explanation; Electrocardiography; Article; artificial intelligence; binary classification; comparative study; controlled study; convolutional neural network; coronavirus disease 2019; data classification; deep learning; deep neural network; diagnostic accuracy; electrocardiogram; electrocardiography; female; heart ejection fraction; heart left ventricle failure; human; male; normal human; P wave; performance; procedures; signal processing; COVID-19; Electrocardiography; Humans; Neural Networks, Computer; Signal Processing, Computer-Assisted"
"Agrawal M., Peterson J.C., Griffiths T.L.","Using Machine Learning to Guide Cognitive Modeling: A Case Study in Moral Reasoning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099467528&partnerID=40&md5=427f3b9875858c6ec4376d653917f6e9","Large-scale behavioral datasets enable researchers to use complex machine learning algorithms to better predict human behavior, yet this increased predictive power does not always lead to a better understanding of the behavior in question. In this paper, we outline a data-driven, iterative procedure that allows cognitive scientists to use machine learning to generate models that are both interpretable and accurate. We demonstrate this method in the domain of moral decision-making, where standard experimental approaches often identify relevant principles that influence human judgments, but fail to generalize these findings to “real world” situations that place these principles in conflict. The recently released Moral Machine dataset allows us to build a powerful model that can predict the outcomes of these conflicts while remaining simple enough to explain the basis behind human decisions. © Cognitive Science Society: Creativity + Cognition + Computation, CogSci 2019.All rights reserved.","machine learning; moral psychology","Behavioral research; Iterative methods; Large dataset; Learning algorithms; Reinforcement learning; Case-studies; Cognitive model; Complex machines; Human behaviors; Large-scales; Machine learning algorithms; Machine-learning; Moral psychology; Moral reasoning; Predictive power; Decision making"
"Agrawal S., Sachdeva M., Mukherjee T.","Targeted Upskilling Framework based on Player Mistake Context in Online Skill Gaming Platforms","10.1145/3486001.3486234","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118311979&doi=10.1145%2f3486001.3486234&partnerID=40&md5=222e935d02fdfc26b7fa15d278300619","Recent proliferation of online platforms for Games of skill have made - an excellent source of recreation and relaxation, and the safest avenues for realising personal worth, respect, and recognition - readily accessible on the tip of a finger. A primary driver towards this enjoyment is the skill of a person in responding to complex game dynamics and states, which eventually impacts the game outcomes. Hence, from both player and platform perspectives, improving the player skills - or conversely, reducing player mistakes - is paramount to improve player experience and engagement. In this talk, we focus on unique personalized and near-real-time up-skilling framework depending on specific mistake contexts identified for the players. This framework leverages a suite of models to determine the correct action in a given game state. For a specific case-study of Rummy - a popular skill game in India - these are CNN-driven deep learning models. However, our framework can be plugged with any other suite, action set, and game state representations (depending on the game) within a broad construct of getting the best reference action set in a game state. Players' actions are then benchmarked w.r.t. the reference actions as adherence measures. A lower adherence essentially indicates deviations from correct actions, i.e., the mistakes. An explainable set of rules are accordingly derived from game features, which sets the contexts for these mistakes, leading to opportunities for targeted up-skilling. © 2021 ACM.",,"Action sets; Case-studies; Games of skills; Learning models; Near-real time; Online platforms; Player action; Player experience; Set of rules; State representation; Deep learning"
"Agrawal U., Wagner C., Garibaldi J.M., Soria D.","Fuzzy Integral Driven Ensemble Classification using A Priori Fuzzy Measures","10.1109/FUZZ-IEEE.2019.8858821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073779981&doi=10.1109%2fFUZZ-IEEE.2019.8858821&partnerID=40&md5=7c2e0cd669a032c58526895d6597a8fb","Aggregation operators are mathematical functions that enable the fusion of information from multiple sources. Fuzzy Integrals (FIs) are widely used aggregation operators, which combine information in respect to a Fuzzy Measure (FM) which captures the worth of both the individual sources and all their possible combinations. However, FIs suffer from the potential drawback of not fusing information according to the intuitively interpretable FM, leading to non-intuitive results. The latter is particularly relevant when a FM has been defined using external information (e.g. experts). In order to address this and provide an alternative to the FI, the Recursive Average (RAV) aggregation operator was recently proposed which enables intuitive data fusion in respect to a given FM. With an alternative fusion operator in place, in this paper, we define the concept of 'A Priori' FMs which are generated based on external information (e.g. classification accuracy) and thus provide an alternative to the traditional approaches of learning or manually specifying FMs. We proceed to develop one specific instance of such an a priori FM to support the decision level fusion step in ensemble classification. We evaluate the resulting approach by contrasting the performance of the ensemble classifiers for different FMs, including the recently introduced Uriz and the Sugeno λ-measure; as well as by employing both the Choquet FI and the RAV as possible fusion operators. Results are presented for 20 datasets from machine learning repositories and contextualised to the wider literature by comparing them to state-of-the-art ensemble classifiers such as Adaboost, Bagging, Random Forest and Majority Voting. © 2019 IEEE.",,"Adaptive boosting; Data fusion; Decision trees; Frequency modulation; Functions; Fuzzy systems; Image segmentation; Integral equations; Mathematical operators; Aggregation operator; Classification accuracy; Decision level fusion; Ensemble classification; External informations; Machine learning repository; Mathematical functions; Traditional approaches; Classification (of information)"
"Aguilar D.L., Perez M.A.M., Loyola-Gonzalez O., Choo K.R., Bucheli-Susarrey E.","Towards an interpretable autoencoder: A decision tree-based autoencoder and its application in anomaly detection","10.1109/TDSC.2022.3148331","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124181577&doi=10.1109%2fTDSC.2022.3148331&partnerID=40&md5=9cdf6c107c57971676aaed05db40c0c2","The importance of understanding and explaining the associated classification results in the utilization of artificial intelligence (AI) in many different practical applications has contributed to the trend of moving away from black-box AI towards explainable AI (XAI). In this paper, we propose the first interpretable autoencoder based on decision trees, which is designed to handle categorical data without the need to transform the data representation. Furthermore, our proposed interpretable autoencoder provides a natural explanation for experts in the application area. The experimental findings show that our proposed interpretable autoencoder is among the top-ranked anomaly detection algorithms, along with one-class SVM and Gaussian Mixture. More specifically, our proposal is on average 2\% below the best Area Under the Curve (AUC) result and 3\% over the other Average Precision scores, in comparison to One-class SVM, Isolation Forest, Local Outlier Factor, Elliptic Envelope, Gaussian Mixture Model, and eForest. IEEE","Anomaly detection; Anomaly detection; Autoencoder; Computational modeling; Computer architecture; Decision tree; Decision trees; Decoding; Encoding; Explainable artificial intelligence (XAI); Interpretable artificial intelligence; Neural networks","Artificial intelligence; Data mining; Decision trees; Gaussian distribution; Learning systems; Signal encoding; Anomaly detection; Auto encoders; Computational modelling; Encodings; Explainable artificial intelligence (XAI); Interpretable artificial intelligence; Neural-networks; One class-SVM; Tree-based; Computer architecture"
"Aguilar-Bonavides C., Sanchez-Arias R., Lanzas C.","Accurate prediction of major histocompatibility complex class II epitopes by sparse epresentation via ℓ1-minimization","10.1186/1756-0381-7-23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920830114&doi=10.1186%2f1756-0381-7-23&partnerID=40&md5=15318f4bd8e7b60611536829fea4b373","The major histocompatibility complex (MHC) is responsible for presenting antigens (epitopes) on the surface of antigen-presenting cells (APCs). When pathogen-derived epitopes are presented by MHC class II on an APC surface, T cells may be able to trigger an specific immune response. Prediction of MHC-II epitopes is particularly challenging because the open binding cleft of the MHC-II molecule allows epitopes to bind beyond the peptide binding groove; therefore, the molecule is capable of accommodating peptides of variable length. Among the methods proposed to predict MHC-II epitopes, artificial neural networks (ANNs) and support vector machines (SVMs) are the most effective methods. We propose a novel classification algorithm to predict MHC-II called sparse representation via ℓ1-minimization. We obtained a collection of experimentally confirmed MHC-II epitopes from the Immune Epitope Database and Analysis Resource (IEDB) and applied our ℓ1-minimization algorithm. To benchmark the performance of our proposed algorithm, we compared our predictions against a SVM classifier. We measured sensitivity, specificity abd accuracy; then we used Receiver Operating Characteristic (ROC) analysis to evaluate the performance of our method. The prediction performance of MHC-II epitopes of the ℓ1-minimization algorithm was generally comparable and, in some cases, superior to the standard SVM classification method and overcame the lack of robustness of other methods with respect to outliers. While our method consistently favoured DPPS encoding with the alleles tested, SVM showed a slightly better accuracy when ""11-factor"" encoding was used. Minimization has similar accuracy than SVM, and has additional advantages, such as overcoming the lack of robustness with respect to outliers. With ℓ1-minimization no model selection dependency is involved. © 2014 Aguilar-Bonavides et al.; licensee BioMed Central Ltd.","Classification algorithms; Epitope prediction; Immunoinformatics; Machine learning; MHC class II; Peptide binding; Sparse representation","major histocompatibility antigen class 2; algorithm; Article; artificial neural network; human; measurement accuracy; measurement precision; nonhuman; prediction; priority journal; process model; quantitative structure activity relation; sensitivity and specificity; sparse representation via minimization; support vector machine"
"Aguilar-Galindo F., Díaz-Tendero S.","Theoretical Insights into Vinyl Derivatives Adsorption on a Cu(100) Surface","10.1021/acs.jpcc.8b06142","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057562660&doi=10.1021%2facs.jpcc.8b06142&partnerID=40&md5=26d4ecefbd907ddb2216512469c5f5ec","Here, we present a thorough theoretical study of the adsorption of acrolein (ACO), acrylonitrile (ACN), and acrylamide (ACA) on Cu(100) surface. For this purpose, we have used the density functional theory, imposing periodic boundary conditions to have a correct description of the electronic band structure of the metal and including dispersion forces through two different schemes: the D2 method of Grimme and the vdW-DF. We have found several adsorption geometries. In all of them, the vinyl group together with the amide (in ACA), ciano (in ACN), and carbonyl (in ACO) groups, is highly involved. The highest adsorption energy is found for acrylamide, followed by acrolein and the lowest for acrylonitrile (depending on the level of theory employed ∼1.2, 1.0, and 0.9 eV, respectively). We show that a strong coupling between the π electronic system (both occupied and virtual orbitals) and the electronic levels of the metal is mainly responsible of the chemisorption. As a consequence, electronic density is transferred from the surface to the molecule, whose carbon atoms acquire a partial sp3 hybridization. Lone-pair orbitals of the cyano, amide, and carbonyl groups also play a role in the interaction. The simulations and following analysis allow to disentangle the nature of the interaction, which can be explained on the basis of a simple chemical picture: donation from the occupied lone pair and π orbitals of the molecule to the surface and backdonation from the surface to the π∗ orbital of the molecule (π-backbonding). © 2018 American Chemical Society.",,"Adsorption; Aldehydes; Amides; Artificial intelligence; Chemical analysis; Chemical bonds; Herbicides; Molecules; Van der Waals forces; Adsorption energies; Adsorption geometries; Electronic band structure; Electronic density; Electronic levels; Electronic systems; Periodic boundary conditions; Theoretical study; Density functional theory"
"Aguilar-Hidalgo D., Becerra-Alonso D., García-Morales D., Casares F.","Toward a study of gene regulatory constraints to morphological evolution of the Drosophila ocellar region","10.1007/s00427-016-0541-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962159648&doi=10.1007%2fs00427-016-0541-8&partnerID=40&md5=0eabfa27f465a62232a8542078c5ea58","The morphology and function of organs depend on coordinated changes in gene expression during development. These changes are controlled by transcription factors, signaling pathways, and their regulatory interactions, which are represented by gene regulatory networks (GRNs). Therefore, the structure of an organ GRN restricts the morphological and functional variations that the organ can experience—its potential morphospace. Therefore, two important questions arise when studying any GRN: what is the predicted available morphospace and what are the regulatory linkages that contribute the most to control morphological variation within this space. Here, we explore these questions by analyzing a small “three-node” GRN model that captures the Hh-driven regulatory interactions controlling a simple visual structure: the ocellar region of Drosophila. Analysis of the model predicts that random variation of model parameters results in a specific non-random distribution of morphological variants. Study of a limited sample of drosophilids and other dipterans finds a correspondence between the predicted phenotypic range and that found in nature. As an alternative to simulations, we apply Bayesian networks methods in order to identify the set of parameters with the largest contribution to morphological variation. Our results predict the potential morphological space of the ocellar complex and identify likely candidate processes to be responsible for ocellar morphological evolution using Bayesian networks. We further discuss the assumptions that the approach we have taken entails and their validity. © 2016, The Author(s).","Bayesian network; Evolution; Gene network; Hedgehog; Mathematical model; Ocellus; Patterning","Article; Bayes theorem; Diptera; Drosophila melanogaster; gene regulatory network; immunohistochemistry; machine learning; molecular evolution; morphology; nonhuman; phenotypic variation; priority journal; quantitative genetics; sensitivity analysis; simulation; steady state; anatomy and histology; animal; classification; Drosophila; genetic variation; genetics; molecular evolution; Drosophila protein; hedgehog protein, Drosophila; sonic hedgehog protein; Animals; Bayes Theorem; Drosophila; Drosophila Proteins; Evolution, Molecular; Gene Regulatory Networks; Genetic Variation; Hedgehog Proteins; Machine Learning"
"Aguilar-Palacios C., Munoz-Romero S., Rojo-Alvarez J.L.","Causal Quantification of Cannibalization during Promotional Sales in Grocery Retail","10.1109/ACCESS.2021.3062222","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101755246&doi=10.1109%2fACCESS.2021.3062222&partnerID=40&md5=2720a7a194c134d291dba26477f2faff","In food and grocery retail, sales cannibalisation during promotions occurs when a promoted product has a knock-on effect on the sales of a non-promoted one. The quantification of its effect is important for retailers, as cannibalisation can lead to wasted food and lost profits. The performance of promotions is ultimately dependent on their features but also on the characteristics of the stores, i.e. type of store or its location. Generally speaking, there is no homogeneous response to a promotion, and by extension to sales cannibalisation. Accordingly, in this paper we describe a framework to analyse the effects of cannibalisation due to individual promotions based on the relationship amongst their sales. The novelty of our work resides in understanding cannibalisation as a causal effect where the increase in sales of the promoted product is partly due to the decrease in sales of the non-promoted one. As such, we propose to use causal inference to measure the impact of cannibalisation due to promotions. Our method reviews each product that has been on promotion, searching for potential cannibals, given by products whose promotion have resulted in large sales uplifts, and for the fall-outs, given by those products experiencing a reduction in sales due to the cannibals. Then each cannibal-victim pair is analysed with Causal Impact, a time-series method which allows one to infer the causal effect of an intervention. We demonstrate the practical application of detecting cannibalisation on vast datasets of promotions within several stores and their many departments. To provide with an overview of the cannibalisation for entire departments and not simply for individual products, we build a directed graph. This is both unique and of utmost value to store managers and marketing teams. Additionally, we discuss in the Appendix the application of explainable forecasting to cannibalisation on a surrogate model. © 2013 IEEE.","Causal inference; interpretable machine learning (iML); retail promotions; sales cannibalization; supply chain","Chemical contamination; Directed graphs; Human resource management; Causal inferences; Knock-on effect; Large sales; Marketing team; Surrogate model; Time series method; Sales"
"Aguilar-Palacios C., Munoz-Romero S., Rojo-Alvarez J.L.","Cold-Start Promotional Sales Forecasting through Gradient Boosted-Based Contrastive Explanations","10.1109/ACCESS.2020.3012032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089577655&doi=10.1109%2fACCESS.2020.3012032&partnerID=40&md5=e45e58d49cf3ab8f30e7eb77197c5bde","Multiple Machine Learning solutions in Industry exist where interpretability is required. In retail, this is especially important when dealing with cold-start forecasting of promotional sales. In the planning phase of these promotions, retailers produce sales predictions that are scrutinised by both forecasting experts and managers. In this paper, we combine the predictive benefits of Gradient Boosted Decision Trees regressors and the interpretability of contrastive explanations. These are implicitly generated by the manner we shape data. Our method presents the cold-start forecasts in relation to the observed promotional sales of other products, which we call neighbours. They are selected based on a measure of closeness to the predicted promotion, which is derived from the variable importance calculated during the training of the regressors. With this information, the expert reviewer adjusts the cold-start prediction by simply varying the contribution of the neighbours. To validate our results we test our method on a surrogate model, as well as on real-market data. The results on the surrogate model demonstrate that our method is able to accurately identify the features that contribute to sales and then select the closest neighbours to produce a contrastive explanation. The results on real-market data also show that the proposed method performs at a similar level to widespread methods such as conventional CatBoost, NGBoost or AutoGluon, and has the advantage of providing interpretability. © 2013 IEEE.","Cold-start forecasting; contrastive explanations; interpretable ML (iML); retail promotions; supply chain","Commerce; Decision trees; Forecasting; Personnel training; Boosted decision trees; Interpretability; Multiple machine; Planning phase; Real markets; Sales forecasting; Surrogate model; Variable importances; Sales"
"Aguilar-Palacios C., Munoz-Romero S., Rojo-Alvarez J.L.","Forecasting Promotional Sales Within the Neighbourhood","10.1109/ACCESS.2019.2920380","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068347274&doi=10.1109%2fACCESS.2019.2920380&partnerID=40&md5=083e43af2ae33d8678ecec452f5689c6","Promotions are a widely used strategy to engage consumers and as such, retailers dedicate immense effort and resources to their planning and forecasting. This paper introduces a novel interpretable machine learning method specifically tailored to the automatic prediction of promotional sales in real-market applications. Particularly, we present fully automated weighted k-nearest neighbors where the distances are calculated based on a feature selection process that focuses on the similarity of promotional sales. The method learns online, thereby avoiding the model being retrained and redeployed. It is robust and able to infer the mechanisms leading to sales as demonstrated on detailed surrogate models. Also, to validate this method, real market data provided by a worldwide retailer have been used, covering numerous categories from three different countries and several types of stores. The algorithm is benchmarked against an ensemble of regression trees and the forecast provided by the retailer and it outperforms both on a merit figure composed not only by the mean absolute error but also by the error deviations used in the retail business. The proposed method significantly improves the accuracy of the forecast in many diverse categories and geographical locations, yielding significant and operative benefits for supply chains. Additionally, we briefly discuss in the Appendix how to deploy our method as a RESTful service in a production environment. © 2013 IEEE.","Automated feature selection; non-negative least squares; online learning; predictive models; promotions; supply chain; weighted k-nearest neighbors","Commerce; Feature extraction; Forecasting; Motion compensation; Nearest neighbor search; Sales; Supply chains; Trees (mathematics); Automated features; Least Square; Online learning; Predictive models; promotions; Weighted k-nearest neighbors; Learning systems"
"Aguilar-Ruiz J.S., Azuaje F., Riquelme J.C.","Data mining approaches to diffuse large B-cell lymphoma gene expression data interpretation","10.1007/978-3-540-30076-2_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048823913&doi=10.1007%2f978-3-540-30076-2_28&partnerID=40&md5=cb7656f5ed3b3071cf6f94a5781880fd","This paper presents a comprehensive study of gene expression patterns originating from a diffuse large B-cell lymphoma (DLBCL) database. It focuses on the implementation of feature selection and classification techniques. Thus, it firstly tackles the identification of relevant genes for the prediction of DLBCL types. It also allows the determination of key biomarkers to differentiate two subtypes of DLBCL samples: Activated B-Like and Germinal Centre B-Like DLBCL. Decision trees provide knowledge-based models to predict types and subtypes of DL-BCL. This research suggests that the data may be insufficient to accurately predict DLBCL types or even detect functionally relevant genes. However, these methods represent reliable and understandable tools to start thinking about possible interesting non-linear interdependencies. © Springer-Verlag Berlin Heidelberg 2004.",,"Data mining; Data warehouses; Decision trees; Forecasting; Genes; Oncology; Feature selection and classification; Gene Expression Data; Gene expression patterns; It focus; Gene expression"
"Aguilar-Saavedra J.A., Zaldívar B.","Jet tagging made easy","10.1140/epjc/s10052-020-8082-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086409692&doi=10.1140%2fepjc%2fs10052-020-8082-8&partnerID=40&md5=a641c9e1d94032f4093d0307a4c9b3c7","We develop taggers for multi-pronged jets that are simple functions of jet substructure (so-called ‘subjettiness’) variables. These taggers can be approximately decorrelated from the jet mass in a quite simple way. Specifically, we use a Logistic Regression Design (LoRD) which, even being one of the simplest machine learning classifiers, shows a performance which surpasses that of simple variables used by the ATLAS and CMS Collaborations and is not far from more complex models based on neural networks. Contrary to the latter, our method allows for an easy implementation of tagging tasks by providing a simple and interpretable analytical formula with already optimised parameters. © 2020, The Author(s).",,
"Aguilera J., Farías D.I.H., Montes-y-Gómez M., González L.C.","Detecting Traces of Self-harm in Social Media: A Simple and Interpretable Approach","10.1007/978-3-030-89820-5_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118329208&doi=10.1007%2f978-3-030-89820-5_16&partnerID=40&md5=ff5ec6d8ba4a0a300aadf694beb23d9a","Social networks have become the main means of communication and interaction between people. In them, users share information and opinions, but also their experiences, worries, and personal concerns. Because of this, there is a growing interest in analyzing this kind of content to identify people who commit self-harm, which is often one of the first signs of suicide risk. Recently, methods based on Deep Learning have shown good results in this task, however, they are opaque and do not facilitate the interpretation of decisions, something fundamental in health-related tasks. In this paper, we face the detection of self-harm in social media by applying a simple and interpretable one-class-classification approach, which, supported on the concept of the attraction force [1], produces its decisions considering both the relevance and distance between users. The results obtained in a benchmark dataset are encouraging, as they indicate a competitive performance with respect to state-of-the-art methods. Furthermore, taking advantage of the approach’s properties, we outline what could be a support tool for healthcare professionals for analyzing and monitoring self-harm behaviors in social networks. © 2021, Springer Nature Switzerland AG.",,"Benchmarking; Deep learning; Attraction force; Benchmark datasets; Classification approach; Communication and interaction; Competitive performance; Detecting trace; One-class Classification; Simple++; Social media; State-of-the-art methods; Social networking (online)"
"Aguirre-Zapata E., Morales H., Dagatti C.V., di Sciascio F., Amicarelli A.N.","Semi physical growth model of Lobesia botrana under laboratory conditions for Argentina's Cuyo region","10.1016/j.ecolmodel.2021.109803","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119932566&doi=10.1016%2fj.ecolmodel.2021.109803&partnerID=40&md5=178bad598f92a33dfa267e6b20229be1","Lobesia botrana is a quarantine pest from Argentina and other countries in the world. It causes damage to the vine in its different growth stages leading to losses in wine production. To develop pest control strategies based on knowledge of the moth, different mathematical models can be found in specific literature to predict its biological cycle, establish its relationship with environmental variables, describe the voltinism of the pest, among others. Based on the proposed models, it is possible to establish a minimum temperature threshold considering the development of the moth and the number of degrees’ days (DD) that must be accumulated for there to be a change of stage. Many of these models are empirical. They are limited because they do not consider some variables such as growth and mortality rates, also they lack a conceptual basis. This makes that professionals or institutions interested in the development of decision support systems (DSS) may not use them. This also prevents them from being easily extrapolated to other regions of the world. In this work, a semi-physical model based on first principles (FPBSM) is proposed to describe how the different growth stages of the vine moth change quantitatively throughout its normal development time under controlled and specific laboratory conditions for the Cuyo region in Argentina. The proposed model, based on a white box structure, considers important parameters in the development of the moth, such as growth and mortality rates. Opposite to the models reported in the literature, the proposed model is conceptually more simple, easy to calculate or adjust, and Its parameters are interpretable in the model's application context. The previous characteristics facilitate the proposal model's use by sectors interested in the development of DSS systems. The reported mathematical model has been validated with experimental data for three different temperature conditions. © 2021 Elsevier B.V.","European grape moth; FPBSM; Lobesia botrana; Mathematical modeling","Artificial intelligence; Population statistics; Wine; Argentina; Different growth stages; European grape moth; FPBSM; Growth models; Laboratory conditions; Lobesium botranum; Mathematical modeling; Model-based OPC; Mortality rate; Decision support systems; crop pest; growth rate; mortality; moth; numerical model; quarantine; vine; Argentina; Lobesia botrana"
"Agyemang B., Wu W.-P., Kpiebaareh M.Y., Lei Z., Nanor E., Chen L.","Multi-view self-attention for interpretable drug–target interaction prediction","10.1016/j.jbi.2020.103547","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090039212&doi=10.1016%2fj.jbi.2020.103547&partnerID=40&md5=eb9a78e9541e9384816694d6ee03ce30","The drug discovery stage is a vital aspect of the drug development process and forms part of the initial stages of the development pipeline. In recent times, machine learning-based methods are actively being used to model drug–target interactions for rational drug discovery due to the successful application of these methods in other domains. In machine learning approaches, the numerical representation of molecules is critical to the performance of the model. While significant progress has been made in molecular representation engineering, this has resulted in several descriptors for both targets and compounds. Also, the interpretability of model predictions is a vital feature that could have several pharmacological applications. In this study, we propose a self-attention-based multi-view representation learning approach for modeling drug–target interactions. We evaluated our approach using three benchmark kinase datasets and compared the proposed method to some baseline models. Our experimental results demonstrate the ability of our method to achieve competitive prediction performance and offer biologically plausible drug–target interaction interpretations. © 2020 Elsevier Inc.","Drug discovery; Drug–target interactions; Machine learning; Representation learning; Self-attention","Forecasting; Machine learning; Interaction prediction; Interpretability; Learning approach; Machine learning approaches; Model prediction; Molecular representations; Numerical representation; Prediction performance; Drug interactions; brigatinib; epidermal growth factor receptor; zanubrutinib; drug; Article; binding affinity; chemical interaction; controlled study; CPI Reg model; drug protein binding; drug target interaction; intermethod comparison; joint view self attention approach; mathematical computing; methodology; model; prediction; priority journal; protein structure; self attention based multi view representation learning approach; drug development; drug interaction; machine learning; Drug Development; Drug Discovery; Drug Interactions; Machine Learning; Pharmaceutical Preparations"
"Ahamad M.K., Bharti A.K.","Validation of Clustering Based Framework Using Unsupervised Machine Learning","10.1109/SASM51857.2021.9841205","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136260389&doi=10.1109%2fSASM51857.2021.9841205&partnerID=40&md5=eff59eefa814922744422949d0775c7f","K-Means algorithm of clustering is reflected in a more influential and widespread concept of mining in the research community. But, in spite of its acceptance, this method has few limitations associated with initialization problem of centroids to lead to surprise converge. Therefore, it is responsible for various shapes of the cluster and also affects the outlier. But the main problem of its method is that it is not capable to manage the huge dataset. In this paper, we have discussed how to overcome the limitations, where their performance is a measure based on experimental analysis by using the iris data set. In this paper, we proposed an efficient framework data mining using machine learning unsupervised technique k-means, this framework system which is implemented with MATLAB software, and NCSS 2021, SPSS tools for simulation to assess the training of iris, and heart disease datasets to measure the validity of clusters. Furthermore, proposed concept leads to the various directions of studies and exploration for research. © 2021 IEEE.","K-Means; Kernel function; Machine learning function; MATLAB; Metrics; NCSS2021; Particle swarm optimization; Performance; Principal component analysis","Data mining; K-means clustering; Machine learning; Particle swarm optimization (PSO); Principal component analysis; Swarm intelligence; K-means; Kernel function; Learning functions; Machine learning function; Machine-learning; Metric; Particle swarm; Particle swarm optimization; Performance; Principal-component analysis; Swarm optimization; MATLAB"
"Ahdal A.A., Prashar D., Rakhra M., Wadhawan A.","Machine Learning-Based Heart Patient Scanning, Visualization, and Monitoring","10.1109/ICCS54944.2021.00049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132855737&doi=10.1109%2fICCS54944.2021.00049&partnerID=40&md5=e489dc34c2d18ae88cf5e50b07b695ab","Heart diseases leading most causes of death globally according to World Health Organization cardiovascular or all heart related disease are responsible for 17.9 million death every year. An early detection and diagnosis of the disease is very important and maybe it's the key of cure. The major challenge is to predict the disease in early stages therefor most of scientists and researches focus on Machine learning techniques which have the capability of detection with accurate result for large and complex data and apply those techniques to help in health care. The purpose of this work is to detect heart diseases at early stage and avoid consequences by implementing different Machine Learning Algorithm for example, KNN Decision Tree (DT), Logistic Regression, SVM, Random Forest (RF), and Naïve Bayes (NB). © 2021 IEEE.","Cardiovascular Disease; Decision Tree; Heart Disease Prediction; Machine Learning","Cardiology; Decision trees; Diagnosis; Heart; Learning systems; Machine learning; Random forests; Cardiovascular disease; Causes of death; Detection and diagnosis; Heart disease; Heart disease prediction; Heart patients; Machine-learning; On-machines; Research focus; World Health Organization; Diseases"
"Ahlstrom M.L., Hadden G.D., Stroick G.R.","HAL: A HEURISTIC APPROACH TO SCHEMATIC GENERATION.",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021561551&partnerID=40&md5=016919fab61cf472518996a0dd1fbe5e","This investigation focuses on the suitability of rule-based expert systems for the problem of schematic generation. A rule-based expert system prototype has been developed that generates aesthetically pleasing and easily understandable schematics from MIMOLA hardware descriptions at the register-transfer level. This system is implemented in OPS5 and Lisp. Although it is possible to build schematic generators with rule-based expert system techniques, even illustrative systems require a significant knowledge acquisition effort, careful system design and substantial support in the underlying programming language.",,"ARTIFICIAL INTELLIGENCE - Expert Systems; COMPUTER PROGRAMMING LANGUAGES - LISP; HAL RULE-BASED SYSTEM; MIMOLA HARDWARE DESCRIPTIONS; OPS5 LANGUAGE; SCHEMATIC GENERATION; INTEGRATED CIRCUITS"
"Ahlstrom M.L., Hadden G.D., Stroick G.R.","EXPERT SYSTEM FOR THE GENERATION OF SCHEMATICS.",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021560932&partnerID=40&md5=5811a3de4052014d5b67cdc862b3c789","The Heuristically Augmented Layoutxx (HAL) prototype has been developed to serve as an intelligent interface for a front-end digital system design tool. The HAL system is a rule-based expert system which can produce a schematic drawing from a hardware description file. The system makes use of expert knowledge to produce a schematic for the designer which is aesthetically pleasing and easily understandable.",,"INTEGRATED CIRCUITS - Computer Aided Design; HEURISTICALLY AUGMENTED LAYOUT (HAL); IC LAYOUT; SCHEMATIC DRAWING; ARTIFICIAL INTELLIGENCE"
"Ahmad A., Asif A., Rajpoot N., Arif M., Minhas F.A.A.","Correlation Filters for Detection of Cellular Nuclei in Histopathology Images","10.1007/s10916-017-0863-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037031973&doi=10.1007%2fs10916-017-0863-8&partnerID=40&md5=78f398963773a39180b0ea5c07047dea","Nuclei detection in histology images is an essential part of computer aided diagnosis of cancers and tumors. It is a challenging task due to diverse and complicated structures of cells. In this work, we present an automated technique for detection of cellular nuclei in hematoxylin and eosin stained histopathology images. Our proposed approach is based on kernelized correlation filters. Correlation filters have been widely used in object detection and tracking applications but their strength has not been explored in the medical imaging domain up till now. Our experimental results show that the proposed scheme gives state of the art accuracy and can learn complex nuclear morphologies. Like deep learning approaches, the proposed filters do not require engineering of image features as they can operate directly on histopathology images without significant preprocessing. However, unlike deep learning methods, the large-margin correlation filters developed in this work are interpretable, computationally efficient and do not require specialized or expensive computing hardware. Availability: A cloud based webserver of the proposed method and its python implementation can be accessed at the following URL: http://faculty.pieas.edu.pk/fayyaz/software.html#corehist. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.","Cell detection; Correlation filters; Histopathology images; Kernelized correlation filters; Nuclei detection","Article; autoanalysis; cell nucleus; cell structure; cell tracking; classifier; comparative study; controlled study; histopathology; image analysis; image processing; kernelized correlation filter; mathematical computing; web browser; cell nucleus; computer assisted diagnosis; Fourier analysis; human; machine learning; pathology; procedures; Cell Nucleus; Fourier Analysis; Humans; Image Interpretation, Computer-Assisted; Machine Learning"
"Ahmad A.M., Minallah N., Ahmed N., Ahmad A.M., Fazal N.","Remote sensing based vegetation classification using machine learning Algorithms","10.1109/AECT47998.2020.9194217","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092333589&doi=10.1109%2fAECT47998.2020.9194217&partnerID=40&md5=0cee59291511ae5540a30ac02e2f251f","Vegetation is one of the most important part of an ecosystem. It is responsible for providing oxygen and gets in carbon dioxide, hence providing a suitable place for the human beings to live. The information about this vegetation is very critical. Using remote sensing, this information can be taken and gathered and later on used for different purposes. This paper aims to classify vegetation into different types and categories. Three machine learning algorithms i.e. K-means, Support Vector Machine (SVM) and Artificial Neural Networks (ANN) have been used because of their being the most popular and well known algorithms of the current time to classify vegetation. K-means being unsupervised classifier is used to compare it to two supervised classifiers i.e. SVM and ANN. Non-vegetation including buildings, roads, rivers etc. are also classified into their respective categories. This classification can be useful in many ways. They can be used by government agencies and authorities to get information about the yield of a specific crop e.g. tobacco, maize etc. This information could be very useful for gathering statistics of the crop and its location on map. These locations can be used for extracting the crops and for future planning regarding it. The information about buildings and roads can help in town planning for future. © 2020 IEEE.","K- means; Machine Learning; Neural networks; Support Vector Machine; Vegetation","Carbon dioxide; Classifiers; Crops; K-means clustering; Learning systems; Remote sensing; Support vector machines; Vegetation; Future planning; Government agencies; Human being; K-means; Supervised classifiers; Town planning; Vegetation classification; Learning algorithms"
"Ahmad F., Rizvi S.A.M.","Emotion based content credibility prediction model for Twitter social network",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082564952&partnerID=40&md5=d72508fbca8389692d8ea1226f52ba21","Twitter and other microblogging platforms are reportedly being used for propagating spams and other malicious content, which stand contrary to the vision of these platforms. This situation raises several ethical challenges in terms of deceiving people with pseudo information. Many a time, users of microblogging sites are held responsible for spreading spiteful information on social media, and they remain unknown in most of the cases. Tweet content may contain either genuine information or uncredible content. The uncredible content contains an abusive or absurd language for any caste, culture, religion or political party. This content needs to be filtered out so that it does not disturb the tranquility of nation in the long run. Therefore, filtering such content from Online Social Networks (OSNs) is the utmost requirement, and so, needs to be addressed. Earlier studies have focused on mainly feature-set belonging to content-based, topic-based and network-based categories. However, the potential of emotion-based features still remains to be explored in the domain. In this paper, IBM Watson and Meaning Cloud platforms have been used for evaluating emotions, sentiment and polarity scores in order to develop a classification model that will filter out all the uncredible content from the OSNs like Twitter. Initially, 35K tweets and associated features provided by Twitter were crawled. These tweets were further preprocessed and forwarded to six human experts for annotating it on a fivepointer credible class scores. The Multilayer perceptron, Naive Bayes, Random Forest and Support Vector Machine algorithms were used for developing a machine learning classification model for categorizing tweets into one of the given credibility classes. The acceptable level of accuracy, precision, recall, and f1 score is observed for all given credibility classes. © 2020 IJSTR.","Credibility; Emotions; Machine Learning; Sentiment; Social Media; Twitter",
"Ahmad F., Rizvi S.A.M.","Identification of Credibility Content Measures for Twitter and Sina-Weibo Social Networks","10.1007/978-3-030-30577-2_32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075274460&doi=10.1007%2f978-3-030-30577-2_32&partnerID=40&md5=5e572b5067ddac5c5ee093c39c8bf539","Twitter has been the most pivotal platform for sharing information, news and messages among the connected people. In the era of social media where a huge amount of information is being shared over the internet, the task of determining the authenticity, integrity, and nature of the information received has become significant for the users. So far, a number of rigorous efforts have been made on social networks such as Twitter and Sina Weibo to identify the specific features for assisting other users in determining the credibility of the available content. These features have been categorized into three categories, namely, content-based features, user-based features and hybrid features. This paper is an attempt to rigorously survey the approaches employed for the detection of rumors on social networks. Furthermore, we have highlighted various corpus, data collection methods, features responsible for finding and estimating credibility along with different machine learning techniques in order to assist the users working in this domain. © 2020, Springer Nature Switzerland AG.","Content-based features; Credibility analysis; Hybrid features; Online social networks; Rumor detection; User-based features","Learning systems; Content-based features; Credibility analysis; Hybrid features; On-line social networks; User-based features; Social networking (online)"
"Ahmad I., Yau K.-L.A., Ling M.H., Keoh S.L.","Trust and reputation management for securing collaboration in 5G access networks: The road ahead","10.1109/ACCESS.2020.2984318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083465582&doi=10.1109%2fACCESS.2020.2984318&partnerID=40&md5=fe91485dcfe789c2b67d3ded0deeb0bc","Trust represents the belief or perception of an entity, such as a mobile device or a node, in the extent to which future actions and reactions are appropriate in a collaborative relationship. Reputation represents the network-wide belief or perception of the trustworthiness of an entity. Each entity computes and assigns a trust or reputation value, which increases and decreases with the appropriateness of actions and reactions, to another entity in order to ensure a healthy collaborative relationship. Trust and reputation management (TRM) has been investigated to improve the security of traditional networks, particularly the access networks. In 5G, the access networks are multi-hop networks formed by entities which may not be trustable, and so such networks are prone to attacks, such as Sybil and crude attacks. TRM addresses such attacks to enhance the overall network performance, including reliability, scalability, and stability. Nevertheless, the investigation of TRM in 5G, which is the next-generation wireless networks, is still at its infancy. TRM must cater for the characteristics of 5G. Firstly, ultra-densification due to the exponential growth of mobile users and data traffic. Secondly, high heterogeneity due to the different characteristics of mobile users, such as different transmission characteristics (e.g., different transmission power) and different user equipment (e.g., laptops and smartphones). Thirdly, high variability due to the dynamicity of the entities' behaviors and operating environment. TRM must also cater for the core features of 5G (e.g., millimeter wave transmission, and device-to-device communication) and the core technologies of 5G (e.g., massive MIMO and beamforming, and network virtualization). In this paper, a review of TRM schemes in 5G and traditional networks, which can be leveraged to 5G, is presented. We also provide an insight on some of the important open issues and vulnerabilities in 5G networks that can be resolved using a TRM framework. © 2013 IEEE.","5G; artificial intelligence; cooperation; Next-generation networks; trust and reputation management","Millimeter waves; Mobile security; Mobile telecommunication systems; Network security; Wave transmission; Collaborative relationships; Device-to-Device communications; High heterogeneity; Network virtualization; Next-generation wireless network; Operating environment; Transmission characteristics; Trust and reputation managements; 5G mobile communication systems"
"Ahmad J., Javed F., Hayat M.","Intelligent computational model for classification of sub-Golgi protein using oversampling and fisher feature selection methods","10.1016/j.artmed.2017.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019267862&doi=10.1016%2fj.artmed.2017.05.001&partnerID=40&md5=ef455b132f1b3d92cff10ae7aaddd5f9","Golgi is one of the core proteins of a cell, constitutes in both plants and animals, which is involved in protein synthesis. Golgi is responsible for receiving and processing the macromolecules and trafficking of newly processed protein to its intended destination. Dysfunction in Golgi protein is expected to cause many neurodegenerative and inherited diseases that may be cured well if they are detected effectively and timely. Golgi protein is categorized into two parts cis-Golgi and trans-Golgi. The identification of Golgi protein via direct method is very hard due to limited available recognized structures. Therefore, the researchers divert their attention toward the sequences from structures. However, owing to technological advancement, exploration of huge amount of sequences was reported in the databases. So recognition of large amount of unprocessed data using conventional methods is very difficult. Therefore, the concept of intelligence was incorporated with computational model. Intelligence based computational model obtained reasonable results, but the gap of improvement is still under consideration. In this regard, an intelligent automatic recognition model is developed in order to enhance the true classification rate of sub-Golgi proteins. In this approach, discrete and evolutionary feature extraction methods are applied on the benchmark Golgi protein datasets to excerpt salient, propound and variant numerical descriptors. After that, an oversampling technique Syntactic Minority over Sampling Technique is employed to balance the data. Hybrid spaces are also generated with combination of these feature spaces. Further, Fisher feature selection method is utilized to reduce the extra noisy and redundant features from feature vector. Finally, k-nearest neighbor algorithm is used as learning hypothesis. Three distinct cross validation tests are used to examine the stability and efficiency of the proposed model. The predicted outcomes of proposed model are better than the existing models in the literature so far. Finally, it is anticipated that the proposed model will provide the foundation to pharmaceutical industry in drug design and research community to innovate new ideas in the area of computational biology and bioinformatics. © 2017 Elsevier B.V.","Bigram position specific scoring matrix; Dipeptide composition; Fisher feature selection; Golgi protein; k-nearest neighbor; Split pseudo amino acid composition","Bioinformatics; Biosynthesis; Computation theory; Computational methods; Motion compensation; Nearest neighbor search; Neurodegenerative diseases; Numerical methods; Pattern recognition; Plants (botany); Proteins; Dipeptide composition; Feature extraction methods; Intelligent computational model; K nearest neighbor algorithm; K-nearest neighbors; Position specific scoring matrix; Pseudo Amino Acid Compositions; Technological advancement; Feature extraction; protein derivative; protein; algorithm; amino acid composition; Article; artificial intelligence; Golgi complex; k nearest neighbor; priority journal; protein analysis; validation study; algorithm; animal; biology; Algorithms; Animals; Computational Biology; Golgi Apparatus; Proteins"
"Ahmad K., Maabreh M., Ghaly M., Khan K., Qadir J., Al-Fuqaha A.","Developing future human-centered smart cities: Critical analysis of smart city security, Data management, and Ethical challenges","10.1016/j.cosrev.2021.100452","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121968014&doi=10.1016%2fj.cosrev.2021.100452&partnerID=40&md5=203c708b33d037b72921c18a669803cd","As the globally increasing population drives rapid urbanization in various parts of the world, there is a great need to deliberate on the future of the cities worth living. In particular, as modern smart cities embrace more and more data-driven artificial intelligence services, it is worth remembering that (1) technology can facilitate prosperity, wellbeing, urban livability, or social justice, but only when it has the right analog complements (such as well-thought out policies, mature institutions, responsible governance); and (2) the ultimate objective of these smart cities is to facilitate and enhance human welfare and social flourishing. Researchers have shown that various technological business models and features can in fact contribute to social problems such as extremism, polarization, misinformation, and Internet addiction. In the light of these observations, addressing the philosophical and ethical questions involved in ensuring the security, safety, and interpretability of such AI algorithms that will form the technological bedrock of future cities assumes paramount importance. Globally there are calls for technology to be made more humane and human-centered. In this paper, we analyze and explore key challenges including security, robustness, interpretability, and ethical (data and algorithmic) challenges to a successful deployment of AI in human-centric applications, with a particular emphasis on the convergence of these concepts/challenges. We provide a detailed review of existing literature on these key challenges and analyze how one of these challenges may lead to others or help in solving other challenges. The paper also advises on the current limitations, pitfalls, and future directions of research in these domains, and how it can fill the current gaps and lead to better solutions. We believe such rigorous analysis will provide a baseline for future research in the domain. © 2021 Elsevier Inc.","Adversarial attacks; AI ethics; Data auditing; Data bias; Data management; Data ownership; Evasion attacks; Explainability; Interpretability; Machine learning; Privacy; Security; Smart cities; Trojan attacks","Digital storage; Ethical technology; Machine learning; Malware; Population statistics; Smart city; Adversarial attack; AI ethic; Data auditing; Data bias; Data ownership; Evasion attack; Explainability; Interpretability; Privacy; Security; Trojan attack; Trojans; Information management"
"Ahmad M.A., Overman S., Allen C., Kumar V., Teredesai A., Eckert C.","Software as a Medical Device: Regulating AI in Healthcare via Responsible AI","10.1145/3447548.3470823","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114920107&doi=10.1145%2f3447548.3470823&partnerID=40&md5=f86306856ae50d26b6172ce216587531","With the increased adoption of AI in healthcare, there is a growing recognition and demand to regulate AI in healthcare to avoid potential harm and unfair bias against vulnerable populations. Around a hundred governmental bodies and commissions as well as leaders in the tech sector have proposed principles to create responsible AI systems. However, most of these proposals are short on specifics which has led to charges of ethics washing. In this tutorial we offer a guide to help navigate through complex governmental regulations and explain the various constituent practical elements of a responsible AI system in healthcare in the light of proposed regulations. Additionally, we breakdown and emphasize that the recommendations from regulatory bodies like FDA or the EU are necessary but not sufficient elements of creating a responsible AI system. We elucidate how regulations and guidelines often focus on epistemic concerns to the detriment of practical concerns e.g., requirement for fairness without explicating what fairness constitutes for a use case. FDA's Software as a medical device document and EU's GDPR among other AI governance documents talk about the need for implementing sufficiently good machine learning practices. In this tutorial we elucidate what that would mean from a practical perspective for real world use cases in healthcare throughout the machine learning cycle i.e., Data Management, Data Specification, Feature Engineering, Model Evaluation, Model Specification, Model Explainability, Model Fairness, Reproducibility, checks for data leakage and model leakage. We note that conceptualizing responsible AI as a process rather than an end goal accords well with how AI systems are used in practice. We also discuss how a domain centric stakeholder perspective translates into balancing requirements for multiple competing optimization criteria. © 2021 Owner/Author.","ai in healthcare; explainable ai; fairness in machine learning; interpretable machine learning; responsible ai; xai","Balancing; Health care; Information management; Machine learning; Privacy by design; Specifications; Balancing requirements; Data specifications; Feature engineerings; Governmental regulations; Model specifications; Optimization criteria; Regulatory bodies; Reproducibilities; Data mining"
"Ahmad M.A., Eckert C., Teredesai A.","The challenge of imputation in explainable artificial intelligence models",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071046949&partnerID=40&md5=5faa07f740029ab8cfc20a01fa9d7ced","Explainable models in Artificial Intelligence are often employed to ensure transparency and accountability of AI systems. The fidelity of the explanations are dependent upon the algorithms used as well as on the fidelity of the data. Many real world datasets have missing values that can greatly influence explanation fidelity. The standard way to deal with such scenarios is imputation. This can, however, lead to situations where the imputed values may correspond to a setting which refer to counterfactuals. Acting on explanations from AI models with imputed values may lead to unsafe outcomes. In this paper, we explore different settings where AI models with imputation can be problematic and describe ways to address such scenarios. © 2019 CEUR-WS. All rights reserved.",,"AI systems; Counterfactuals; Missing values; Real-world datasets; Artificial intelligence"
"Ahmad M.A., Teredesai A., Eckert C.","Interpretable machine learning in healthcare","10.1109/ICHI.2018.00095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051141812&doi=10.1109%2fICHI.2018.00095&partnerID=40&md5=53f85ac5b64c300a83388c1d947f2273","This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare. © 2018 IEEE.","Explainable artificial intelligence; Interpretable machine learning","Artificial intelligence; Health care; Interpretability; Machine learning models; Learning systems"
"Ahmad M.W., Mourshed M., Yuce B., Rezgui Y.","Computational intelligence techniques for HVAC systems: A review","10.1007/s12273-016-0285-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971246176&doi=10.1007%2fs12273-016-0285-4&partnerID=40&md5=b9f9edbb4e2385536fc9a7bd1c22a99e","Buildings are responsible for 40% of global energy use and contribute towards 30% of the total CO2 emissions. The drive to reduce energy use and associated greenhouse gas emissions from buildings has acted as a catalyst in the development of advanced computational methods for energy efficient design, management and control of buildings and systems. Heating, ventilation and air-conditioning (HVAC) systems are the major source of energy consumption in buildings and ideal candidates for substantial reductions in energy demand. Significant advances have been made in the past decades on the application of computational intelligence (CI) techniques for HVAC design, control, management, optimization, and fault detection and diagnosis. This article presents a comprehensive and critical review on the theory and applications of CI techniques for prediction, optimization, control and diagnosis of HVAC systems. The analysis of trends reveals that the minimisation of energy consumption was the key optimization objective in the reviewed research, closely followed by the optimization of thermal comfort, indoor air quality and occupant preferences. Hardcoded Matlab program was the most widely used simulation tool, followed by TRNSYS, EnergyPlus, DOE-2, HVACSim+ and ESP-r. Metaheuristic algorithms were the preferred CI method for solving HVAC related problems and in particular genetic algorithms were applied in most of the studies. Despite the low number of studies focussing on multi-agent systems (MAS), as compared to the other CI techniques, interest in the technique is increasing due to their ability of dividing and conquering an HVAC optimization problem with enhanced overall performance. The paper also identifies prospective future advancements and research directions. © 2016, The Author(s).","buildings; computational intelligence; energy conservation; energy efficiency; heating; optimization; ventilation and airconditioning (HVAC)","Air conditioning; Air quality; Artificial intelligence; Buildings; Climate control; Computation theory; Design; Energy conservation; Energy efficiency; Energy utilization; Fault detection; Gas emissions; Genetic algorithms; Greenhouse gases; Heating; Historic preservation; Indoor air pollution; Intelligent agents; MATLAB; Multi agent systems; Quality control; Computational intelligence techniques; Energy-efficient design; Fault detection and diagnosis; Management and controls; Meta heuristic algorithm; Optimization problems; Substantial reduction; Ventilation and air conditioning; Optimization"
"Ahmad R., Kim D.-H.","A collaboration based context prediction in smart office","10.3233/AIS-150348","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948155634&doi=10.3233%2fAIS-150348&partnerID=40&md5=8a9bd6576b4c3aa3f162c7d037eacde8","Context prediction plays a vital role in an assistive ubiquitous environment. The environmental configuration in a ubiquitous environment is heavily dependent on the context of the events occurring in the environment. Current state of the art approaches utilize the user's history information for predicting the context of the events.When the user's history does not provide apposite contextual information for the observed activity/event at time t, the history based state of the art context prediction techniques fails to predict the appropriate future context. To overcome the gap of missing context information in the user's context history, we propose a Profile based Collaborative Context Prediction (PCCP) approach. PCCP is a predictive association rules based system which utilizes the history of similar users and collaborate among users of the ubiquitous environment. PCCP generates rules at high level of abstraction, human readable and understandable that helps in avoiding the underline details. To evaluate the PCCP, a smart office is considered as an experimental environment. Experiments are carried out on indigenous multi user smart office data set. Our experiments showed significant level of accuracy in both environments. Due to the understandability of output and higher accuracy of PCCP, it can be extended to assist the user in a smart environment. © 2015 - IOS Press and the authors. All rights reserved.","Collaboration; Context prediction; Keyword context awareness; Smart office","Artificial intelligence; Software engineering; Collaboration; Context predictions; Context- awareness; Experimental environment; High level of abstraction; Predictive association rule; Smart offices; State-of-the-art approach; Forecasting"
"Ahmad S.M.F., Jahromi M.Z.","Constructing accurate fuzzy classification systems: A new approach using weighted fuzzy rules","10.1109/CGIV.2007.31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449111204&doi=10.1109%2fCGIV.2007.31&partnerID=40&md5=1f2262dadc07a0cb16d0b98c08d11694","Different approaches to design fuzzy rule-based classification systems can be grouped into two main categories: descriptive and accurate. In the descriptive category, the emphasis is on the interpretability of the resulting classifier. The classifier is usually represented by a set of short fuzzy rules (i.e., with a few number of antecedent conditions) that make it a suitable tool for knowledge representation. In the accurate category, the generalization ability of the classifier is the main target in the design process and no attempt is made to use understandable fuzzy rules in constructing the rule base. In this paper, we propose a simple and efficient method to construct an accurate fuzzy classification system. We use rule-weight as a simple mechanism to tune the classifier and propose a new method of rule-weight specification for this purpose. Through computer simulations on some data sets from UCI repository, we show that the proposed scheme achieves better prediction accuracy compared with other fuzzy and non-fuzzy rule-based classification systems proposed in the past. © 2007 IEEE.","Classification; Fuzzy systems; Generalization accuracy; Ruleweight","Artificial intelligence; Classification (of information); Classifiers; Color image processing; Computational geometry; Computational methods; Computer graphics; Computer simulation; Computer systems; Fuzzy rules; Fuzzy sets; Fuzzy systems; Graphic methods; Information theory; Knowledge representation; Learning systems; Process design; Process engineering; Solute transport; Specifications; Visualization; Weighing; (I ,J) conditions; Data sets; design processes; Efficient methods; Fuzzy classification systems; Fuzzy rule based classification systems; generalization ability; Interpretability; new approaches; New methods; prediction accuracy; Rule base (IF THEN); UCI repository; Weighted fuzzy rules; Fuzzy logic"
"Ahmad W., Muhammad K., Glass H.J., Chatterjee S., Khan A., Hussain A.","Novel MLR-RF-Based Geospatial Techniques: A Comparison with OK","10.3390/ijgi11070371","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133637692&doi=10.3390%2fijgi11070371&partnerID=40&md5=12165c965c57a547d2307a5cb8406aad","Geostatistical estimation methods rely on experimental variograms that are mostly erratic, leading to subjective model fitting and assuming normal distribution during conditional simula-tions. In contrast, Machine Learning Algorithms (MLA) are (1) free of such limitations, (2) can in-corporate information from multiple sources and therefore emerge with increasing interest in real-time resource estimation and automation. However, MLAs need to be explored for robust learning of phenomena, better accuracy, and computational efficiency. This paper compares MLAs, i.e., Multiple Linear Regression (MLR) and Random Forest (RF), with Ordinary Kriging (OK). The techniques were applied to the publicly available Walkerlake dataset, while the exhaustive Walker Lake dataset was validated. The results of MLR were significant (p &lt; 10 × 10−5), with correlation coeffi-cients of 0.81 (R-square = 0.65) compared to 0.79 (R-square = 0.62) from the RF and OK methods. Additionally, MLR was automated (free from an intermediary step of variogram modelling as in OK), produced unbiased estimates, identified key samples representing different zones, and had higher computational efficiency. © 2022 by the authors. Li-censee MDPI, Basel, Switzerland.","geostatistics; interpretable machine learning; Machine Learning Algorithms (MLA); Ordinary Kriging (OK); random forest (RF); SHAP; spatial estimation",
"Ahmadi E., Weckman G.R., Masel D.T.","Decision making model to predict presence of coronary artery disease using neural network and C5.0 decision tree","10.1007/s12652-017-0499-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047562448&doi=10.1007%2fs12652-017-0499-z&partnerID=40&md5=39a5b786f9bc8e11b0ef936868e70e8c","Clinical decision support systems have always assisted physicians in diagnosing diseases. Coronary artery disease (CAD) is currently responsible for a large percentage of deaths, which motivated researchers to propose more accurate prediction models. This paper employs neural networks (NN) and a boosted C5.0 decision tree model to predict CAD for the well-known Cleveland Heart Disease dataset. We attempt to tune the optimal size and configuration of the neural networks and identify the insensitive features in both models, followed by assessing the effect of eliminating such features in the results. Both models are evaluated through ten experiments, each of which has different training and testing datasets, but with the same size. The most and the least important input features in each model are determined. The performance of the reduced dataset, i.e., the removed insignificant features contributing to the models, has been evaluated through statistical tests. Our results show that there is no significant difference between running the NN and C5.0 algorithms by initial dataset in terms of three performance criteria: positive prediction value (PPV), negative prediction value (NPV) and total accuracy value (TAV). Regarding the TAV criterion, the NN applied to the reduced dataset outperforms the C5.0 model with a 95% confidence interval. Finally, further discussion shows the trade-off between the NPV and PPV. © 2017, Springer-Verlag Berlin Heidelberg.","C5.0 decision tree; Coronary artery disease diagnosis; Decision making; Neural network","Artificial intelligence; Computer aided diagnosis; Decision support systems; Decision trees; Diseases; Economic and social effects; Forecasting; Heart; Neural networks; Statistical tests; Clinical decision support systems; Coronary artery disease; Coronary artery disease diagnosis; Decision making models; Decision tree modeling; Insensitive features; Performance criterion; Training and testing; Decision making"
"Ahmadi P., Gholampour I., Tabandeh M.","Cluster-based sparse topical coding for topic mining and document clustering","10.1007/s11634-017-0280-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014043381&doi=10.1007%2fs11634-017-0280-3&partnerID=40&md5=3dd9cf7c483a87dec7563dea7ee92ceb","In this paper, we introduce a document clustering method based on Sparse Topical Coding, called Cluster-based Sparse Topical Coding. Topic modeling is capable of improving textual document clustering by describing documents via bag-of-words models and projecting them into a topic space. The latent semantic descriptions derived by the topic model can be utilized as features in a clustering process. In our proposed method, document clustering and topic modeling are integrated in a unified framework in order to achieve the highest performance. This framework includes Sparse Topical Coding, which is responsible for topic mining, and K-means that discovers the latent clusters in documents collection. Experimental results on widely-used datasets show that our proposed method significantly outperforms the traditional and other topic model based clustering methods. Our method achieves from 4 to 39% improvement in clustering accuracy and from 2% to more than 44% improvement in normalized mutual information. © Springer-Verlag Berlin Heidelberg 2017.","Document clustering; K-means; Sparse topical coding; Topic model","Cluster analysis; Clustering algorithms; Codes (symbols); Information retrieval; Semantics; Bag-of-words models; Clustering accuracy; Clustering process; Document Clustering; K-means; Normalized mutual information; Sparse topical coding; Topic Modeling; Data mining"
"Ahmadi S.-A., Vivar G., Frei J., Nowoshilow S., Bardins S., Brandt T., Krafczyk S.","Towards computerized diagnosis of neurological stance disorders: data mining and machine learning of posturography and sway","10.1007/s00415-019-09458-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068856439&doi=10.1007%2fs00415-019-09458-y&partnerID=40&md5=0635ad1b879cf9239156be198ed43994","We perform classification, ranking and mapping of body sway parameters from static posturography data of patients using recent machine-learning and data-mining techniques. Body sway is measured in 293 individuals with the clinical diagnoses of acute unilateral vestibulopathy (AVS, n = 49), distal sensory polyneuropathy (PNP, n = 12), anterior lobe cerebellar atrophy (CA, n = 48), downbeat nystagmus syndrome (DN, n = 16), primary orthostatic tremor (OT, n = 25), Parkinson’s disease (PD, n = 27), phobic postural vertigo (PPV n = 59) and healthy controls (HC, n = 57). We classify disorders and rank sway features using supervised machine learning. We compute a continuous, human-interpretable 2D map of stance disorders using t-stochastic neighborhood embedding (t-SNE). Classification of eight diagnoses yielded 82.7% accuracy [95% CI (80.9%, 84.5%)]. Five (CA, PPV, AVS, HC, OT) were classified with a mean sensitivity and specificity of 88.4% and 97.1%, while three (PD, PNP, and DN) achieved a mean sensitivity of 53.7%. The most discriminative stance condition was ranked as “standing on foam-rubber, eyes closed”. Mapping of sway path features into 2D space revealed clear clusters among CA, PPV, AVS, HC and OT subjects. We confirm previous claims that machine learning can aid in classification of clinical sway patterns measured with static posturography. Given a standardized, long-term acquisition of quantitative patient databases, modern machine learning and data analysis techniques help in visualizing, understanding and utilizing high-dimensional sensor data from clinical routine. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Body sway; Machine learning; Neurological stance and gait disorders; Static posturography; Visualization","adult; Article; body equilibrium; cerebellum atrophy; classifier; cohort analysis; computer assisted diagnosis; controlled study; data mining; diagnostic accuracy; diagnostic test accuracy study; disease severity; female; human; major clinical study; male; neurologic gait disorder; nystagmus; Parkinson disease; polyneuropathy; positional vertigo; priority journal; sensitivity and specificity; stabilography; supervised machine learning; tremor; vestibular disorder; computer assisted diagnosis; data mining; machine learning; neurologic disease; pathophysiology; physiology; procedures; Adult; Cohort Studies; Data Mining; Diagnosis, Computer-Assisted; Female; Humans; Machine Learning; Male; Nervous System Diseases; Postural Balance"
"Ahmadian A., Lindsten F.","Likelihood-free Out-of-Distribution Detection with Invertible Generative Models",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125461759&partnerID=40&md5=4257fdd7dab8644acc950767dcbd1a72","Likelihood of generative models has been used traditionally as a score to detect atypical (Out-of-Distribution, OOD) inputs. However, several recent studies have found this approach to be highly unreliable, even with invertible generative models, where computing the likelihood is feasible. In this paper, we present a different framework for generative model-based OOD detection that employs the model in constructing a new representation space, instead of using it directly in computing typicality scores, where it is emphasized that the score function should be interpretable as the similarity between the input and training data in the new space. In practice, with a focus on invertible models, we propose to extract low-dimensional features (statistics) based on the model encoder and complexity of input images, and then use a One-Class SVM to score the data. Contrary to recently proposed OOD detection methods for generative models, our method does not require computing likelihood values. Consequently, it is much faster when using invertible models with iteratively approximated likelihood (e.g. iResNet), while it still has a performance competitive with other related methods. © 2021 International Joint Conferences on Artificial Intelligence. All rights reserved.",,"Artificial intelligence; Atypicals; Generative model; Input datas; Input image; Low dimensional; Model-based OPC; Representation space; Score function; Statistic-based; Training data; Iterative methods"
"Ahmadvand A., Sahijwani H., Choi J.I., Agichtein E.","Concet: Entity-aware topic classification for open-domain conversational agents","10.1145/3357384.3358048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075470748&doi=10.1145%2f3357384.3358048&partnerID=40&md5=e6ecc0e63589babaadccc71830adbb61","Identifying the topic (domain) of each user's utterance in open-domain conversational systems is a crucial step for all subsequent language understanding and response tasks. In particular, for complex domains, an utterance is often routed to a single component responsible for that domain. Thus, correctly mapping a user utterance to the right domain is critical. To address this problem, we introduce ConCET: a Concurrent Entity-aware conversational Topic classifier, which incorporates entity-type information together with the utterance content features. Specifically, ConCET utilizes entity information to enrich the utterance representation, combining character, word, and entity-type embeddings into a single representation. However, for rich domains with millions of available entities, unrealistic amounts of labeled training data would be required. To complement our model, we propose a simple and effective method for generating synthetic training data, to augment the typically limited amounts of labeled training data, using commonly available knowledge bases as to generate additional labeled utterances. We extensively evaluate ConCET and our proposed training method first on an openly available human-human conversational dataset called Self-Dialogue, to calibrate our approach against previous state-of-the-art methods; second, we evaluate ConCET on a large dataset of human-machine conversations with real users, collected as part of the Amazon Alexa Prize. Our results show that ConCET significantly improves topic classification performance on both datasets, including 8-10% improvements over state-of-the-art deep learning methods. We complement our quantitative results with detailed analysis of system performance, which could be used for further improvements of conversational agents. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Conversational Topic Classification; Entity-Aware conversation domain classification; Open-Domain Conversational Agents","Deep learning; Knowledge management; Large dataset; Conversational agents; Conversational systems; Conversational topics; Labeled training data; Language understanding; State-of-the-art methods; Synthetic training data; Topic Classification; Classification (of information)"
"Ahmed A., Hassan Z.R., Shabbir M.","Interpretable multi-scale graph descriptors via structural compression","10.1016/j.ins.2020.05.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085270076&doi=10.1016%2fj.ins.2020.05.032&partnerID=40&md5=ad974e63078977b71fabef079a22c66b","Graph representations that preserve relevant topological information allow the use of a rich machine learning toolset for data-driven network analytics. Some notable graph representations in the literature are fruitful in their respective applications but they either lack interpretability or are unable to effectively encode a graph's structure at both local and global scale. In this work, we propose the Higher-Order Structure Descriptor (HOSD): an interpretable graph descriptor that captures information about the patterns in a graph at multiple scales. Scaling is achieved using a novel graph compression technique that reveals successive higher-order structures. The proposed descriptor is invariant to node permutations due to its graph-theoretic nature. We analyze the HOSD algorithm for time complexity and also prove the NP-completeness of three interesting graph compression problems. A faster version, HOSD-Lite, is also presented to approximate HOSD on dense graphs. We showcase the interpretability of our model by discussing structural patterns found within real-world datasets using HOSD. HOSD and HOSD-Lite are evaluated on benchmark datasets for applicability to classification problems; results demonstrate that a simple random forest setup based on our representations competes well with the current state-of-the-art graph embeddings. © 2020 Elsevier Inc.","Graph classification; Graph compression; Graph embeddings","Classification (of information); Computational complexity; Decision trees; Graph algorithms; Graph theory; Benchmark datasets; Graph compressions; Graph representation; Higher-order structure; Real-world datasets; State of the art; Structural pattern; Topological information; Graph structures"
"Ahmed A., Krishnan V.V.G., Foroutan S.A., Touhiduzzaman M., Srivastava A., Wu Y., Hahn A., Sindhu S.","Cyber physical security analytics for anomalies in transmission protection systems","10.1109/IAS.2018.8544672","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059953045&doi=10.1109%2fIAS.2018.8544672&partnerID=40&md5=711621d38e5f3cda7306fada6e021b2b","Protection devices are considered to be the most critical components responsible for protecting the electrical grid. Due to recent technological advancements in the electrical grid, digitalization has played an influential role in integration of digital devices in protection systems. Incorporation of digital devices in protection systems has made Transmission Protection System more prone to vulnerabilities and cyber-attacks. A cyber attack exploiting protection devices aims to disrupt the normal operations by raising multiple false alarms on a large scale creating conflicting and confusing observation in the control center. Finding exact root cause(s) for the multiple alarms is important to solve this problem. The research presented in this paper imitates a cyber attack on the IEEE test system with industrial hardware relays in the loop, by manipulating the setting/logic design of protection devices in the system creating conflicting alarms in the control center. This paper presents a novel data analytics based approach combining signature-based method for detecting an intrusion in the cyber system and a deep learning algorithm for detecting a mal-operation in the physical system. Data gathered from the physical system through sensors such as Phasor Measurement Units (PMUs) and data acquired from cyber system through relay are analyzed by data analytics approach finding the root-cause of the observed events. The results of data analytics are further validated using the log data from protection devices. © 2018 IEEE","Cyber Security; Cyber-Physical Anomalies; Cyber-Physical Systems; Data Analytics; Deep Learning; Digital Protection; Transmission Protection Systems","Alarm systems; Crime; Cyber Physical System; Deep learning; Digital devices; Electric power transmission networks; Embedded systems; Industrial research; Learning algorithms; Network security; Phasor measurement units; Cyber physicals; Cyber security; Data analytics; Digital protection; Transmission protection; Computer crime"
"Ahmed A., Kanagal B., Pandey S., Josifovski V., Pueyo L.G., Yuan J.","Latent factor models with additive and hierarchically-smoothed user preferences","10.1145/2433396.2433445","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874261331&doi=10.1145%2f2433396.2433445&partnerID=40&md5=1e3c56c6d89ebc0195770d93bf04c8d9","Items in recommender systems are usually associated with annotated attributes: for e.g., brand and price for products; agency for news articles, etc. Such attributes are highly informative and must be exploited for accurate recommendation. While learning a user preference model over these attributes can result in an interpretable recommender system and can hands the cold start problem, it suffers from two major drawbacks: data sparsity and the inability to model random effects. On the other hand, latent-factor collaborative filtering models have shown great promise in recommender systems; however, its performance on rare items is poor. In this paper we propose a novel model LFUM, which provides the advantages of both of the above models. We learn user preferences (over the attributes) using a personalized Bayesian hierarchical model that uses a combination(additive model) of a globally learned preference model along with user-specific preferences. To combat data-sparsity, we smooth these preferences over the item-taxonomy using an efficient forward-filtering and backward-smoothing inference algorithm. Our inference algorithms can handle both discrete attributes (e.g., item brands) and continuous attributes (e.g., item prices). We combine the user preferences with the latent-factor models and train the resulting collaborative filtering system end-to-end using the successful BPR ranking algorithm. In our extensive experimental analysis, we show that our proposed model outperforms several commonly used baselines and we carry out an ablation study showing the benefits of each component of our model. © 2013 ACM.","inference; latent variable models; recomcollaborative filtering; recomfactor models; recommendation","Additive models; Backward-smoothing; Bayesian hierarchical model; Cold start problems; Collaborative filtering systems; Continuous attribute; Data sparsity; Discrete attributes; Experimental analysis; Forward-filtering; inference; Inference algorithm; Latent factor models; Latent variable models; News articles; Preference models; Random effects; Ranking algorithm; recommendation; User preference models; Algorithms; Computer supported cooperative work; Data mining; Hierarchical systems; Quality of service; Recommender systems; Websites; Inference engines"
"Ahmed A., Hong L., Smola A.J.","Nested Chinese Restaurant Franchise Processes: Applications to user tracking and document modeling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897526759&partnerID=40&md5=bce42aae0444505ac0b8ba3a44430ae2","Much natural data is hierarchical in nature. Moreover, this hierarchy is often shared between different instances. We introduce the nested Chinese Restaurant Franchise Process to obtain both hierarchical tree-structured representations for objects, akin to (but more general than) the nested Chinese Restaurant Process while sharing their structure akin to the Hierarchical Dirichlet Process. Moreover, by decoupling the structure generating part of the process from the components responsible for the observations, we are able to apply the same statistical approach to a variety of user generated data. In particular, we model the joint distribution of microblogs and locations for Twitter for users. This leads to a 40% reduction in location uncertainty relative to the best previously published results. Moreover, we model documents from the NIPS papers dataset, obtaining excellent perplexity relative to (hierarchical) Pachinko allocation and LDA. Copyright 2013 by the author(s).",,"Artificial intelligence; Software engineering; Document model; Hierarchical Dirichlet process; Joint distributions; Location uncertainty; Statistical approach; Tree-structured representation; User tracking; User-generated; Learning systems"
"Ahmed A., Low Y., Aly M., Josifovski V., Smola A.J.","Scalable distributed inference of dynamic user interests for behavioral targeting","10.1145/2020408.2020433","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052659792&doi=10.1145%2f2020408.2020433&partnerID=40&md5=7c1aa0ff0c8de6757276e5a1bd1ba9bb","Historical user activity is key for building user profiles to predict the user behavior and affinities in many web applications such as targeting of online advertising, content personalization and social recommendations. User profiles are temporal, and changes in a user's activity patterns are particularly useful for improved prediction and recommendation. For instance, an increased interest in car-related web pages may well suggest that the user might be shopping for a new vehicle. In this paper we present a comprehensive statistical framework for user profiling based on topic models which is able to capture such effects in a fully unsupervised fashion. Our method models topical interests of a user dynamically where both the user association with the topics and the topics themselves are allowed to vary over time, thus ensuring that the profiles remain current. We describe a streaming, distributed inference algorithm which is able to handle tens of millions of users. Our results show that our model contributes towards improved behavioral targeting of display advertising relative to baseline models that do not incorporate topical and/or temporal dependencies. As a side-effect our model yields human-understandable results which can be used in an intuitive fashion by advertisers. Copyright 2011 ACM.","Computational advertising; Distributed inference; Large-scale; Online inference; User modeling","Computational advertisings; Distributed inference; Large-scale; Online inference; User Modeling; Data mining; Inference engines; Marketing; Models; User interfaces; World Wide Web; Behavioral research"
"Ahmed A., Xing E.","Dynamic non-parametric mixture models and the recurrent Chinese restaurant process: With applications to evolutionary clustering","10.1137/1.9781611972788.20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-52649163923&doi=10.1137%2f1.9781611972788.20&partnerID=40&md5=18fbb8408fe630bd73bd2446bfef4484","Clustering is an important data mining task for exploration and visualization of different data types like news stories, scientific publications, weblogs, etc. Due to the evolving nature of these data, evolutionary clustering, also known as dynamic clustering, has recently emerged to cope with the challenges of mining temporally smooth clusters over time. A good evolutionary clustering algorithm should be able to fit the data well at each time epoch, and at the same time results in a smooth cluster evolution that provides the data analyst with a coherent and easily interpretable model. In this paper we introduce the temporal Dirichlet process mixture model (TDPM) as a framework for evolutionary clustering. TDPM is a generalization of the DPM framework for clustering that automatically grows the number of clusters with the data. In our framework, the data is divided into epochs; all data points inside the same epoch are assumed to be fully exchangeable, whereas the temporal order is maintained across epochs. Moreover, The number of clusters in each epoch is unbounded: the clusters can retain, die out or emerge over time, and the actual parameterization of each cluster can also evolve over time in a Markovian fashion. We give a detailed and intuitive construction of this framework using the recurrent Chinese restaurant process (RCRP) metaphor, as well as a Gibbs sampling algorithm to carry out posterior inference in order to determine the optimal cluster evolution. We demonstrate our model over simulated data by using it to build an infinite dynamic mixture of Gaussian factors, and over real dataset by using it to build a simple non-parametric dynamic clustering-topic model and apply it to analyze the NIPS12 document collection. Copyright © by SIAM.",,"Cluster analysis; Data mining; Data visualization; Evolutionary algorithms; Inference engines; Mixtures; Document collection; Dynamic clustering; Evolutionary clustering; Mixture of Gaussians; Non-parametric mixture models; Number of clusters; Scientific publications; Temporal dirichlet process; Clustering algorithms"
"Ahmed A.A.M., Jui S.J.J., Chowdhury M.A.I., Ahmed O., Sutradha A.","The development of dissolved oxygen forecast model using hybrid machine learning algorithm with hydro-meteorological variables","10.1007/s11356-022-22601-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137209338&doi=10.1007%2fs11356-022-22601-z&partnerID=40&md5=a480e904a497927f0fcbc09d34eceeef","Dissolved oxygen (DO) forecasting is essential for aquatic managers responsible for maintaining ecosystem health and the management of water bodies affected by water quality parameters. This paper aims to forecast dissolved oxygen (DO) concentration using a multivariate adaptive regression spline (MARS) hybrid model coupled with maximum overlap discrete wavelet transformation (MODWT) as a feature decomposition approach for Surma River water using a set of water quality hydro-meteorological variables. The proposed hybrid model is compared with numerous machine learning methods, namely Bayesian ridge regression (BNR), k-nearest neighbourhood (KNN), kernel ridge regression (KRR), random forest (RF), and support vector regression (SVR). The investigational results show that the proposed model of MODWT-MARS has a better prediction than the comparing benchmark models and individual standalone counter parts. The result shows that the hybrid algorithms (i.e. MODWT-MARS) outperformed the other models (r = 0.981, WI = 0.990, RMAE = 2.47%, and MAE = 0.089). This hybrid method may serve to forecast water quality variables with fewer predictor variables. © 2022, The Author(s).","Bangladesh; Dissolved oxygen; Forecasting; Hybrid model; MARS; MODWT; Surma River",
"Ahmed A.F., Sherif M.A., Moussallem D., Ngonga Ngomo A.-C.","Multilingual Verbalization and Summarization for Explainable Link Discovery","10.1016/j.datak.2021.101874","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102647141&doi=10.1016%2fj.datak.2021.101874&partnerID=40&md5=8ad693337b72abba522a4bfdf27cae4b","The number and size of datasets abiding by the Linked Data paradigm increase every day. Discovering links between these datasets is thus central to achieving the vision behind the Data Web. Declarative Link Discovery (LD) frameworks rely on complex Link Specification (LS) to express the conditions under which two resources should be linked. Understanding such LS is not a trivial task for non-expert users. Particularly when such users are interested in generating LS to match their needs. Even if the user applies a machine learning algorithm for the automatic generation of the required LS, the challenge of explaining the resultant LS persists. Hence, providing explainable LS is the key challenge to enable users who are unfamiliar with underlying LS technologies to use them effectively and efficiently. In this paper, we extend our previous work (Ahmed et al., 2019) by proposing a generic multilingual approach that allows verbalization of LS in many languages, i.e., converts LS into understandable natural language text. In this work, we ported our LS verbalization framework into German and Spanish, in addition to English language. Our adequacy and fluency evaluations show that our approach can generate complete and easily understandable natural language descriptions even by lay users. Moreover, we devised an experimental neural approach for improving the quality of our generated texts. Our neural approach achieves promising results in terms of BLEU, METEOR and chrF++. © 2021 Elsevier B.V.","Link discovery; Link specification; NLG; NLP; Text summarization; Verbalization","Learning algorithms; Machine learning; Turing machines; Automatic Generation; English languages; Expert users; Link Discovery; Multilingual approach; Natural language text; Natural languages; Number and size; Natural language processing systems"
"Ahmed A.F., Sherif M.A., Ngomo A.-C.N.","LSVS: Link Specification Verbalization and Summarization","10.1007/978-3-030-23281-8_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068327686&doi=10.1007%2f978-3-030-23281-8_6&partnerID=40&md5=6650eeea4c378b03243e3efa727a94fb","An increasing number and size of datasets abiding by the Linked Data paradigm are published everyday. Discovering links between these datasets is thus central to achieve the vision behind the Data Web. Declarative Link Discovery (LD) frameworks rely on complex Link Specification (LS) to express the conditions under which two resources should be linked. Understanding such LS is not a trivial task for non-expert users, particularly when such users are interested in generating LS to match their needs. Even if the user applies a machine learning algorithm for the automatic generation of the required LS, the challenge of explaining the resultant LS persists. Hence, providing explainable LS is the key challenge to enable users who are unfamiliar with underlying LS technologies to use them effectively and efficiently. In this paper, we address this problem by proposing a generic approach that allows a LS to be verbalized, i.e., converted into understandable natural language. We propose a summarization approach to the verbalized LS based on the selectivity of the underlying LS. Our adequacy and fluency evaluations show that our approach can generate complete and easily understandable natural language descriptions even by lay users. © 2019, Springer Nature Switzerland AG.","Link discovery; Link specification; NLP; Open linked data; Text summarization; Verbalization","Data handling; Information systems; Information use; Learning algorithms; Linked data; Machine learning; Specifications; Automatic Generation; Expert users; Generic approach; Link Discovery; Natural languages; Number and size; Text summarization; Verbalization; Natural language processing systems"
"Ahmed A.H., Hicks S., Riegler M.A., Elmokashfi A.","Predicting High Delays in Mobile Broadband Networks","10.1109/ACCESS.2021.3138695","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122094544&doi=10.1109%2fACCESS.2021.3138695&partnerID=40&md5=0dab766430a93158698a6a8e51a0be96","The number of applications that run over mobile networks, expecting bounded end-to-end delay, is increasing steadily. However, the stochastic and shared nature of the wireless medium makes providing such guarantees challenging. Using several network interfaces simultaneously can help address fluctuating delays, provided that transport protocols can switch between them in a timely manner. Today's protocols are mostly closed-loop and thus require at least one round trip before reacting to increased delay. This paper examines whether jumps in round trip times (RTTs) have a pattern that can be predicted beforehand. Using per second RTT measurements from hundreds of probes in two Long Term Evolution (LTE) cellular networks, we train an ensemble of classifiers to detect increases in delay. We construct a parsimonious explainable model that provides an accuracy of 80% and does not appear to be specific to a particular mobile operator. Further, we examine whether our model can be extended to 5G using a small dataset with extra 5G metadata, resulting in an accuracy of 88%. Our model indicates that RTTs are long-range correlated and shows that radio measurements of channel occupancy are accurate predictors of the onset of high delays. These results suggest that it is feasible to build an open-loop control system for multiplexing among several interfaces to proactively bound delays. © 2013 IEEE.","5G; Delay; LTE; machine learning; prediction","5G mobile communication systems; Broadband networks; Closed loop control systems; Long Term Evolution (LTE); Machine learning; Wireless networks; Closed-loop; Delay; End to end delay; Long-term evolution; Mobile broadband; Protocol cans; Round-trip-time; Stochastics; Transport protocols; Wireless media; Stochastic systems"
"Ahmed Asif Fuad K., Martin P.-E., Giot R., Bourqui R., Benois-Pineau J., Zemmari A.","Features Understanding in 3D CNNs for Actions Recognition in Video","10.1109/IPTA50016.2020.9286629","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099300586&doi=10.1109%2fIPTA50016.2020.9286629&partnerID=40&md5=37ec297145b09923e1c1675fb5be4465","Human Action Recognition is one of the key tasks in video understanding. Deep Convolutional Neural Networks (CNN) are often used for this purpose. Although they usually perform impressively, their decision interpretation remains challenging. We propose a novel visual CNN features understanding technique. Its objective is to find salient features that played a key role in decision making of the network. The technique only uses the features from the last convolutional layer before the fully connected layers of a trained model and builds an importance map of features. The map is propagated to the original frame thus highlighting the regions in them that contribute to the final decision. The method is fast as it does not require gradient computation as many state-of-the-art methods do. Proposed technique is applied to the Twin Spatio-Temporal 3D Convolutional Neural Network (TSTCNN), designed for Table Tennis Actions recognition. Features visualization is performed at the RGB and Optical flow branches of the network. Obtained results are compared to other visualization techniques both in terms of human understanding and similarity metrics. The metrics show that generated maps are similar to those obtained with known Grad-CAM method, e.g. Pearson Correlation Coefficient between the maps generated of RGB data for Grad-CAM and our method is 0.7±0.05 and 0.72±0.06 on Optical Flow data. © 2020 IEEE.","3D convolutions; Action classification; Explainable Deep Learning; Table Tennis; Video indexing","Computation theory; Convolution; Decision making; Deep neural networks; Optical correlation; Optical flows; Sports; Visualization; Gradient computation; Human understanding; Human-action recognition; Pearson correlation coefficients; Similarity metrics; State-of-the-art methods; Video understanding; Visualization technique; Convolutional neural networks"
"Ahmed Benyahia A., Hajjam A., Hilaire V., Hajjam M., Andres E.","E-Care telemonitoring system: Extend the platform","10.1109/IISA.2013.6623725","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889054032&doi=10.1109%2fIISA.2013.6623725&partnerID=40&md5=d84ca7cac4c3e857b6c1863e68326f81","In most developed countries; telemonitoring is increasingly used, due to the increasing of life expectancy and chronic diseases that accompany them. Indeed, chronic diseases are responsible for health care costs increasingly growing and remote monitoring systems provide a mean to monitor patients and their needs in the comfort of their own homes, with minimal costs. In older systems, the data were collected and sent directly to the medical experts to be interpreted. With technological advancements, software and systems have been developed for on-site data processing. In this paper, we present E-Care a telemonitoring platform that combines the semantic web and artificial intelligence. E-Care is based on generic ontologies and a decision support system. Ontologies provide knowledge for decision support. The decision support system is based on an inference engine; this engine is used for monitoring the health of the patient and the detection of abnormal situations. System reactions consist in alarms and recommendations for informing physicians. E-Care has a generic open architecture; in this paper we extend the platform by adding a module for auscultation sounds in this architecture. © 2013 IEEE.","Alert detection; Auscultation sounds; Decision support system; Ontologies; Rule base inference; Telemonitoring","Developed countries; Life expectancies; Open architecture; Remote monitoring system; Rule base; Technological advancement; Tele-monitoring; Telemonitoring systems; Artificial intelligence; Computer architecture; Data processing; Decision support systems; Diseases; Ontology; Patient monitoring; Telecommunication services"
"Ahmed Benyahia A., Hajjam A., Andres E., Hajjam M., Hilaire V.","Including other system in E-care telemonitoring platform","10.3233/978-1-61499-276-9-115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894285937&doi=10.3233%2f978-1-61499-276-9-115&partnerID=40&md5=822aba9a1a7f37e742bb1f302dd9e2b8","Nowadays, telemonitoring systems are increasingly used, due to the increasing of life expectancy and chronic diseases. Indeed, chronic diseases and disabilities due to advancing age are responsible for health care costs increasingly growing. Telemonitoring systems provide a low cost way to monitor patients and their needs in the comfort of their own homes. In first systems, the data were collected then sent directly to physicians to be interpreted. Nowadays, thanks to technological advancements, software and systems have been developed to process data, on a simple computer or even smartphone. In this paper, we present e-Care telemonitoring system that combines the semantic web and expert system. E-Care is based on generic ontologies and a decision support system. The decision support system uses ontologies as knowledge base and an inference engine to detect abnormal situations. E-Care platform has a generic open architecture, which cans include other knowledge coming from other systems. We'll show how to integrate data of auscultation sounds in this architecture. © 2013 The authors and IOS Press. All rights reserved.","alert detection; auscultation sounds; decision support system; Ontologies; rule base inference; telemonitoring","Artificial intelligence; Diseases; Expert systems; mHealth; Ontology; Patient monitoring; Telecommunication services; Chronic disease; Health care costs; Life expectancies; Open architecture; Rule base; Technological advancement; Tele-monitoring; Telemonitoring systems; Decision support systems; computer assisted diagnosis; conference paper; controlled vocabulary; decision support system; heart auscultation; human; information retrieval; methodology; natural language processing; organization and management; system analysis; telemedicine; Decision Support Systems, Clinical; Decision Support Techniques; Diagnosis, Computer-Assisted; Heart Auscultation; Humans; Information Storage and Retrieval; Natural Language Processing; Systems Integration; Telemedicine; Vocabulary, Controlled"
"Ahmed F., Straub J.","Initial Work on the Development of a Hardware-Based Gradient Descent Trained Expert System","10.3390/systems10050160","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140625616&doi=10.3390%2fsystems10050160&partnerID=40&md5=6ff92095b2fede6865cccb54f8eaef71","Prior work has introduced a form of explainable artificial intelligence that is able to precisely explain, in a human-understandable form, why it makes decisions. It is also able to learn to make better decisions without potentially learning illegal or invalid considerations. This defensible system is based on fractional value rule-fact expert systems and the use of gradient descent training to optimize rule weightings. This software system has demonstrated efficacy for many applications; however, it utilizes iterative processing and thus does not have a deterministic completion time. It also requires comparatively expensive general-purpose computing hardware to run on. This paper builds on prior work in the development of hardware-based expert systems and presents and assesses the efficacy of a hardware implementation of this system. It characterizes its performance and discusses its utility and trade-offs for several application domains. © 2022 by the authors.","artificial intelligence; electronic; expert system; gradient descent; hardware-based; rule-fact network",
"Ahmed H.O.","17.16 GOPS\W Sustainable FLS-based wireless sensor network for surveillance system using FPGA","10.1109/ICNS52807.2021.9441628","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107614253&doi=10.1109%2fICNS52807.2021.9441628&partnerID=40&md5=4705a52ea435a6853ca8680555348b84","Enhancing the sustainability of Wireless Sensor Networks (WSN) using different Artificial Intelligence (AI) approaches is one of the promising research topics nowadays, especially for battery-based applications such as border surveillance monitoring. In this paper, we proposed a Sustainable Systolic Wireless Sensor Network for Surveillance System (SS-WSN-SS) processing unit, which consists of two Fuzzy Logic System (FLS) processing cores. The first core is responsible for the event decision-making process based on the received data from a Lidar Sensor and an ultrasonic sensor. The second core is responsible for the detection and correction of any thermo-cognitive dysfunction that could reduce the reliability of the WSN system and could be assumed as a primitive electronics somatosensory cortex unit. The proposed SS-WSN-SS architecture could significantly decrease the power consumption at the processing unit level, sensory data acquisition level, and the wireless interfacing level as well. The proposed SS-WSN-SS processing unit architecture has been designed using VHDL, and the targeted FPGA chip was the Intel Cyclone V 5CGXFC9D6F27C7. Also, the proposed SS-WSN-SS processing unit obtained a Power efficiency of 17.16 GOPS\W at a maximum operating frequency of 263.5 MHz, while draining only 50.87 mW as a core dynamic thermal power dissipation loss and dissipates about 27.33 mW as an I/O thermal power dissipation loss. © 2021 IEEE.","Approximate computing; Cyber Physical System (CPS); Fuzzy logic system; Reconfigurable computing; Sustainable computing; Wireless Sensor Network (WSN)","Artificial intelligence; Computer architecture; Data acquisition; Decision making; Electric losses; Field programmable gate arrays (FPGA); Fuzzy logic; Monitoring; Network architecture; Optical radar; Security systems; Storms; Ultrasonic applications; Border surveillance; Decision making process; Fuzzy logic system; Maximum operating frequency; Power efficiency; Processing units; Somatosensory cortex; Surveillance systems; Wireless sensor networks"
"Ahmed H.O., Ghoneima M., Dessouky M.","Systolic-based pyramidal neuron accelerator blocks for convolutional neural network","10.1016/j.mejo.2019.04.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066433224&doi=10.1016%2fj.mejo.2019.04.017&partnerID=40&md5=08f27e0fc0bb514cc20637b3e0f14ea3","The dramatic evolution in the Deep Learning (DL) algorithms required to alter the silicon architecture fabric of the conventional parallel processing units to increase the efficiency of accelerationg the enormous feature data while achieving reasonable low power consumption levels, especially for the Convolutional Neural Networks (CNN). In this paper, three proposed Pyramidal Neuron Accelerator Architecture (PNAA) units have been designed and optimized for accelerating the convolutional layer of the Convolutional Neural Networks (CNN). The three proposed PNAA units are suggested to replace the conventional generic embedded Digital Signal Processing (DSP) blocks in the silicon architecture fabric of the FPGA chips, that are responsible for the dot-operation functions. The three proposed PNAA units represent the intensively-used neuron operations for the most common kernel filter dimensions in CNN systems as a proof of concept. The proposed PNAA units have been compiled using the TSMC 130 nm technology using the Synopsys DC compiler software. The Front-End analysis for different characteristic PVTs showed that the maximum achieved processing speed for the proposed PNAA units could reach a computational speed of 20.9 Giga Operation per Seconds (GPOS) at a frequency of 409.84 MHz and a predicted power consumption equal to 58.729 mW. © 2019 Elsevier Ltd","Architectures optimization; Computational intelligence; Concurrent computation","Artificial intelligence; Computer architecture; Convolution; Deep learning; Digital signal processing; Electric power utilization; Energy efficiency; Image coding; Low power electronics; Network architecture; Neurons; Accelerator architectures; Computational speed; Concurrent computation; Convolutional neural network; Digital signal processing (DSP); Filter dimensions; Low-power consumption; Parallel processing; Neural networks"
"Ahmed I., Jeon G., Piccialli F.","From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0: A Survey on What, How, and Where","10.1109/TII.2022.3146552","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124093314&doi=10.1109%2fTII.2022.3146552&partnerID=40&md5=e2560c01bd58b59a3cdce298a02917fd","Nowadays, Industry 4.0 can be considered a reality, a paradigm integrating modern technologies and innovations. Artificial intelligence (AI) can be considered the leading component of the industrial transformation enabling intelligent machines to execute tasks autonomously such as self-monitoring, interpretation, diagnosis, and analysis. AI-based methodologies (especially machine learning and deep learning support manufacturers and industries in predicting their maintenance needs and reducing downtime. Explainable artificial intelligence (XAI) studies and designs approaches, algorithms and tools producing human-understandable explanations of AI-based systems information and decisions. This article presents a comprehensive survey of AI and XAI-based methods adopted in the Industry 4.0 scenario. First, we briefly discuss different technologies enabling Industry 4.0. Then, we present an in-depth investigation of the main methods used in the literature: we also provide the details of what, how, why, and where these methods have been applied for Industry 4.0. Furthermore, we illustrate the opportunities and challenges that elicit future research directions toward responsible or human-centric AI and XAI systems, essential for adopting high-stakes industry applications. © 2005-2012 IEEE.","Artificial intelligence (AI); cloud computing; cyber-physical system; explainable artificial intelligence (XAI); Industry 4.0; Internet of Things (IoT)","Deep learning; Embedded systems; Hidden Markov models; Industry 4.0; Intelligent robots; Surveys; Cloud-computing; Explainable artificial intelligence; Hidden-Markov models; Industrial transformations; Industry 40; Intelligent machine; Modern technologies; Monitoring interpretation; Self-monitoring; Service robots; Cyber Physical System"
"Ahmed I., Kumara I., Reshadat V., Kayes A.S.M., van den Heuvel W.-J., Tamburri D.A.","Travel time prediction and explanation with spatio-temporal features: A comparative study","10.3390/electronics11010106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121822454&doi=10.3390%2felectronics11010106&partnerID=40&md5=8813a157c0f06a68123e63ed1dd3882f","Travel time information is used as input or auxiliary data for tasks such as dynamic navigation, infrastructure planning, congestion control, and accident detection. Various data-driven Travel Time Prediction (TTP) methods have been proposed in recent years. One of the most challenging tasks in TTP is developing and selecting the most appropriate prediction algorithm. The existing studies that empirically compare different TTP models only use a few models with specific features. More-over, there is a lack of research on explaining TTPs made by black-box models. Such explanations can help to tune and apply TTP methods successfully. To fill these gaps in the current TTP literature, using three data sets, we compare three types of TTP methods (ensemble tree-based learning, deep neural networks, and hybrid models) and ten different prediction algorithms overall. Furthermore, we apply XAI (Explainable Artificial Intelligence) methods (SHAP and LIME) to understand and interpret models’ predictions. The prediction accuracy and reliability for all models are evaluated and compared. We observed that the ensemble learning methods, i.e., XGBoost and LightGBM, are the best performing models over the three data sets, and XAI methods can adequately explain how various spatial and temporal features influence travel time. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Explainable AI; Hybrid models; LightGBM; LSTM; SHAP and LIME; Spatio-temporal; Travel time prediction; XAI; XGBoost",
"Ahmed I., Aljahdali S., Khan M.S., Kaddoura S.","Classification of parkinson disease based on patient’s voice signal using machine learning","10.32604/iasc.2022.022037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119887721&doi=10.32604%2fiasc.2022.022037&partnerID=40&md5=beb801913ffa586f170c96c995b966b1","Parkinson’s disease (PD) is a nervous system disorder first described as a neurological condition in 1817. It is one of the more prevalent diseases in the elderly, and Alzheimer’s is the second most common neurodegenerative illness. It impacts the patient’s movement. Symptoms start gradually with tremors, stiffness in movement, and speech and voice disorders. Researches proved that 89% of patients with Parkinson’s has speech disorder including uncertain articulation, hoarse and breathy voice and monotone pitch. The cause behind this voice change is the reduction of dopamine due to damage of neurons in the substantia nigra responsible for dopamine production. In this work, Parkinson’s disease is classified with the help of human voice signals. Six different machine learning (ML) algorithms are used in the classification: Stochastic Gradient Descent (SGD) Classifier, Extreme Gradient Boosting (XGB) Classifier, Logistic Regression Classifier, Random Forest Classifier, K-Nearest Neighbour (KNN) Classifier, and Decision Tree (DT) Classifier. This research aims to classify Parkinson’s disease using human voice signals and extract essential features to reduce the complexity of the dataset. Then, human voice signals are analyzed to check the voice intensity and spectrum for PD patients. Then, machine learning classifiers are applied to classify the PD patients based on the extracted features. The results show that SGD-Classifier has 91% accuracy, XGB-Classifier has 95% accuracy, Logistic Regression has 91% accuracy, Random Forest shows 97% accuracy, KNN shows 95% accuracy, and Decision Tree has 95% accuracy. Hence, Random Forest has the highest accuracy. The disease can be studied more by looking for more characteristics of PD patients to enhance its proper use in the medical field. Â© 2021, Tech Science Press. All rights reserved.","Decision tree classifier; KNN-classifier; Logistic regression; Parkinson disease; Random forest; SGD-classifier; XGB-classifier",
"Ahmed J., Junior A.D.N., Kilinc C., Pan D., I Riu J.R., Gustafsson J.","Using Blackbox ML Techniques to Diagnose QoE Problems for an IPTV Service","10.1109/NOMS47738.2020.9110375","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086755609&doi=10.1109%2fNOMS47738.2020.9110375&partnerID=40&md5=4233adf2aa4cdb35454d3a18e7b27922","IPTV services continue to disrupt the traditional cable-based TV delivery and grow in popularity. However, they also face challenges related to the delivery of stable and problem free video streams to the end users while minimizing downtime due to the inherent complexity and uncertainty associated with the IP based networks. In this work, we share our experience on addressing this service assurance problem for a commercial IPTV service provider. The key focus here is on the root cause inference aspects of the service assurance aimed at providing QoE problem hints with localization in the delivery network to the technicians of network service provider. To solve this problem, we investigate various ML-based model interpretability approaches to extract insights from predictive models built for the alarm generation. Key focus is on Blackbox model interpretability approaches namely LIME and SHAP with their associated pros and cons while at the same time comparing these techniques to the popular model agnostic (i.e., Whitebox) way of explainability based on XGB. One of the findings is that SHAP Blackbox technique is most suitable for diagnosing IPTV QoE problems. However, having the downside of long computational time, for cases where the predictive model is not tree-based such as XGB or Random Forest. © 2020 IEEE.","Bayesian Network (BN); Internet Protocol Television (IPTV); LIME (Local Interpretable Model-agnostic Explanations); Machine Learning (ML); Prediction; Quality of Experience (QoE); Service Assurance; SHAP (SHapley Additive exPlanations); STB (Set-top-box)","Decision trees; Internet service providers; Lime; Television broadcasting; Alarm generation; Computational time; Inherent complexity; IP-based networks; Network service providers; Predictive modeling; Predictive models; Service assurance; IPTV"
"Ahmed M., Hossain M.S., Ul Islam R., Andersson K.","Explainable Text Classification Model for COVID-19 Fake News Detection","10.22667/JISIS.2022.05.31.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132973856&doi=10.22667%2fJISIS.2022.05.31.051&partnerID=40&md5=2214ddcd129a1e25a70c76417e50e0e8","Artificial intelligence has achieved notable advances across many applications, and the field is recently concerned with developing novel methods to explain machine learning models. Deep neural networks deliver the best performance accuracy in different domains, such as text categorization, image classification, and speech recognition. Since the neural network models are black-box types, they lack transparency and explainability in predicting results. During the COVID-19 pandemic, Fake News Detection is a challenging research problem as it endangers the lives of many online users by providing misinformation. Therefore, the transparency and explainability of COVID-19 fake news classification are necessary for building the trustworthiness of model prediction. We proposed an integrated LIME-BiLSTM model where BiLSTM assures classification accuracy, and LIME ensures transparency and explainability. In this integrated model, since LIME behaves similarly to the original model and explains the prediction, the proposed model becomes comprehensible. The performance of this model in terms of explainability is measured by using Kendall’s tau correlation coefficient. We also employ several machine learning models and provide a comparison of their performances. Therefore, we analyzed and compared the computation overhead of our proposed model with the other methods because the model takes the integrated strategy. © 2022, Innovative Information Science and Technology Research Group. All rights reserved.","BiLSTM; COVID-19; Explainable AI; fake news; LIME",
"Ahmed M., Zubair S.","Explainable Artificial Intelligence in Sustainable Smart Healthcare","10.1007/978-3-030-96630-0_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128615527&doi=10.1007%2f978-3-030-96630-0_12&partnerID=40&md5=4792783026faadc42f5fc75449b275bf","Artificial Intelligence (AI) is the capability of a system to execute tasks similar to decisions taken by human intelligence. AI has been certainly the hotspot for Internet of Health Things (IoHT) and has brought revolutionary changes in the health community. But yet the healthcare providers and the researchers’ demand for explanation of the resulting predictions made by the system on the basis of the health data trained in the Machine Learning model was not satisfied. Thus, the field of Explainable Artificial Intelligence (XAI) has been explored by the researcher community to provide explanation to the predictions made by the machines and ensure accuracy in the absolute healthcare infrastructure. Since blindly relying on the decisions made by the machine for saving a human soul without proper understanding of the underlying logic is inappropriate, in this condition XAI assists the medical care team to understand the logic and counter check the decisions before implementing on the patient for a better cause. Our aim is to highlight the reasons of adopting XAI in the healthcare domain in this book chapter and discuss the basic concept behind it on how it can contribute towards reliant AI-based solutions to the healthcare. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Artificial intelligence (AI); Big data; Cyber-attacks; Cybersecurity; Explainable artificial intelligence (XAI); Healthcare system; Human intelligence; Internet of health things (IoHT); Machine learning",
"Ahmed M., Shuai C., Ahmed M.","Analysis of energy consumption and greenhouse gas emissions trend in China, India, the USA, and Russia","10.1007/s13762-022-04159-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128411277&doi=10.1007%2fs13762-022-04159-y&partnerID=40&md5=cfdadf37ddba7ed6480bdd3961ffcef9","With the growth of industries and population, the need for energy consumption has increased, which has inevitably increased greenhouse gas emissions. Further use of fossil fuel for energy consumption exacerbates the situation making it one of the major issues for climate change. China, India, the USA, and Russia are the world’s leading countries in energy consumption and emissions and are responsible for climate change. These countries account for 54% of carbon dioxide (CO2) emissions in the global environment. This paper investigates the energy consumption of China, India, the USA, and Russia and its trend in greenhouse gas emissions. Using four available datasets from 1980 to 2018 for China, India, USA, and 1992 to 2018 for Russia, we employed three advanced machine learning algorithms (support vector machine, artificial neural network, and long-short term memory) and verified its predicted capability with actual greenhouse gas emissions. The obtained results were evaluated with three statistical metrics (route mean square, mean absolute percentage error, and mean bias error). The predicted results with three machine learning algorithms were very close to actual greenhouse gas emissions. Besides, we forecasted the trend of greenhouse gas emissions in these countries from 2019 to 2023. The forecasted results with the long-short term memory model confirm an increase in CO2, methane, and Nitrous oxide (N2O) emissions in the case of China and India; in contrast, the results indicate a slowdown of CO2, methane, and N2O emissions in the USA and Russia. © 2022, The Author(s) under exclusive licence to Iranian Society of Environmentalists (IRSEN) and Science and Research Branch, Islamic Azad University.","CO2 emissions; Energy consumption; Machine learning algorithms; Methane emissions; N2O emissions","Brain; Carbon dioxide; Climate change; Fossil fuels; Gas emissions; Greenhouse gases; Learning algorithms; Methane; Neural networks; Nitrogen oxides; Population statistics; Support vector machines; CO 2 emission; Emissions trends; Energy emissions; Energy-consumption; Global environment; Greenhouse gas emissions; Machine learning algorithms; Methane emissions; N2O emission; Support vectors machine; Energy utilization"
"Ahmed M., Seraj R., Islam S.M.S.","The k-means algorithm: A comprehensive survey and performance evaluation","10.3390/electronics9081295","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090372567&doi=10.3390%2felectronics9081295&partnerID=40&md5=cc01e9ed9ff107b264bf5238614d18e9","The k-means clustering algorithm is considered one of the most powerful and popular data mining algorithms in the research community. However, despite its popularity, the algorithm has certain limitations, including problems associated with random initialization of the centroids which leads to unexpected convergence. Additionally, such a clustering algorithm requires the number of clusters to be defined beforehand, which is responsible for different cluster shapes and outlier effects. A fundamental problem of the k-means algorithm is its inability to handle various data types. This paper provides a structured and synoptic overview of research conducted on the k-means algorithm to overcome such shortcomings. Variants of the k-means algorithms including their recent developments are discussed, where their effectiveness is investigated based on the experimental analysis of a variety of datasets. The detailed experimental analysis along with a thorough comparison among different k-means clustering algorithms differentiates our work compared to other existing survey papers. Furthermore, it outlines a clear and thorough understanding of the k-means algorithm along with its different research directions. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Categorical attributes; Clustering; Cyber security; Healthcare; Initialization; K-means; Unsupervised learning",
"Ahmed M.S., Tazwar M.T., Khan H., Roy S., Iqbal J., Rabiul Alam M.G., Hassan M.R., Hassan M.M.","Yield Response of Different Rice Ecotypes to Meteorological, Agro-Chemical, and Soil Physiographic Factors for Interpretable Precision Agriculture Using Extreme Gradient Boosting and Support Vector Regression","10.1155/2022/5305353","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139490887&doi=10.1155%2f2022%2f5305353&partnerID=40&md5=91ebeaa7d8d3c6f433b56bbc7949f169","The food security of more than half of the world's population depends on rice production which is one of the key objectives of precision agriculture. The traditional rice almanac used astronomical and climate factors to estimate yield response. However, this research integrated meteorological, agro-chemical, and soil physiographic factors for yield response prediction. Besides, the impact of those factors on the production of three major rice ecotypes has also been studied in this research. Moreover, this study found a different set of those factors with respect to the yield response of different rice ecotypes. Machine learning algorithms named Extreme Gradient Boosting (XGBoost) and Support Vector Regression (SVR) have been used for predicting the yield response. The SVR shows better results than XGBoost for predicting the yield of the Aus rice ecotype, whereas XGBoost performs better for forecasting the yield of the Aman and Boro rice ecotypes. The result shows that the root mean squared error (RMSE) of three different ecotypes are in between 9.38% and 24.37% and that of R-squared values are between 89.74% and 99.13% on two different machine learning algorithms. Moreover, the explainability of the models is also shown in this study with the help of the explainable artificial intelligence (XAI) model called Local Interpretable Model-Agnostic Explanations (LIME). © 2022 Md. Sabbir Ahmed et al.",,"Adaptive boosting; Food supply; Forecasting; Lime; Machine learning; Mean square error; Agro-chemicals; Food security; Gradient boosting; Key objective; Machine learning algorithms; Precision Agriculture; Rice production; Support vector regressions; World population; Yield response; Precision agriculture"
"Ahmed R., Karypis G.","Algorithms for mining the evolution of conserved relational states in dynamic networks","10.1007/s10115-012-0537-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869094022&doi=10.1007%2fs10115-012-0537-2&partnerID=40&md5=2b372c6966e2fae4ed304e98ff09aad1","Dynamic networks have recently being recognized as a powerful abstraction to model and represent the temporal changes and dynamic aspects of the data underlying many complex systems. Significant insights regarding the stable relational patterns among the entities can be gained by analyzing temporal evolution of the complex entity relations. This can help identify the transitions from one conserved state to the next and may provide evidence to the existence of external factors that are responsible for changing the stable relational patterns in these networks. This paper presents a new data mining method that analyzes the time-persistent relations or states between the entities of the dynamic networks and captures all maximal non-redundant evolution paths of the stable relational states. Experimental results based on multiple datasets from real-world applications show that the method is efficient and scalable. © 2012 Springer-Verlag London Limited.","Dynamic network; Evolution; Relational state","Complex networks; Complex entities; Data mining methods; Dynamic network; Evolution; External factors; Multiple data sets; Relational state; Temporal evolution; Data mining"
"Ahmed R., Karypis G.","Algorithms for mining the evolution of conserved relational states in dynamic networks","10.1109/ICDM.2011.20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857151939&doi=10.1109%2fICDM.2011.20&partnerID=40&md5=3029f4e4a36e866f7243a323b8f48643","Dynamic networks have recently being recognized as a powerful abstraction to model and represent the temporal changes and dynamic aspects of the data underlying many complex systems. Significant insights regarding the stable relational patterns among the entities can be gained by analyzing temporal evolution of the complex entity relations. This can help identify the transitions from one conserved state to the next and may provide evidence to the existence of external factors that are responsible for changing the stable relational patterns in these networks. This paper presents a new data mining method that analyzes the time-persistent relations or states between the entities of the dynamic networks and captures all maximal non-redundant evolution paths of the stable relational states. Experimental results based on multiple datasets from real world applications show that the method is efficient and scalable. © 2011 IEEE.","Dynamic network; Evolution; Relational state","Complex entities; Data mining methods; Data sets; Dynamic aspects; Dynamic network; Evolution; External factors; Real-world application; Relational state; Temporal change; Temporal evolution; Data mining"
"Ahmed S., Amin S.E., Elarif T.","Navigation and cooperative control for nanorobots in the bloodstream environment based on swarm intelligence","10.1115/DETC2015-46506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979034778&doi=10.1115%2fDETC2015-46506&partnerID=40&md5=39ddfa382f57d660e2508c9c4275d6dd","In this paper, an innovative technique was tested to solve the path-planning problem of swarm nanorobots' navigation within the human environment. Blood elements were treated as obstacles to nanorobot movement. Blood flow was also factored into the movement problem, as was the environment's physical properties, including blood viscosity and density, both of which can potentially affect nanorobot behavior. To account for all these considerations in a human body environment, two algorithms were combined, yielding a single algorithm responsible for the self-organized control of nanorobots to avoid obstacles during their movement trajectory. The technique is based on modification of the Particle Swarm Optimization algorithm, referred to as the MPSO algorithm which is classified as a swarm intelligence algorithm, and modification of the Obstacle Avoidance Algorithm, referred to as the MOA algorithm. The proposed MPSO algorithm generated the best locations in a given operational area enabling nanorobots to detect the target areas. The proposed MOA algorithm allowed nanorobots to efficiently avoid collision with blood elements. The simulation results show that the combined MPSO-MOA algorithm safely routes all nanorobots past blood elements while navigating within the human body. © Copyright 2015 by ASME.",,"Algorithms; Artificial intelligence; Birds; Blood; Design; Embedded systems; Motion planning; Nanorobots; Optimization; Particle swarm optimization (PSO); Robots; Viscosity; Co-operative control; Innovative techniques; Movement trajectories; Obstacle avoidance algorithms; Particle swarm optimization algorithm; Path planning problems; Self-organized controls; Swarm intelligence algorithms; Nanorobotics"
"Ahmed T., Wahid M.F., Hasan M.J.","Combining Deep Convolutional Neural Network with Support Vector Machine to Classify Microscopic Bacteria Images","10.1109/ECACE.2019.8679397","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064620319&doi=10.1109%2fECACE.2019.8679397&partnerID=40&md5=f40f78d390c4f4ee9581a27658f0351a","Microorganisms such as Bacteria are responsible for the contamination of numerous infectious diseases such as Cholera, Botulism, Gonorrhoea, Lyme disease, Strep throat, Tuberculosis and so on. Therefore, proper identification and classification of bacteria is essential to prevent the outbreak of such life-threatening diseases. But manual identification and classification of bacteria from microscopic image samples requires professional individuals and reasonable amount of time. However, the process could be automated with the implementation of artificial intelligence (AI) and computer-vision technologies. An effectively trained AI could efficiently classify bacteria and save a large amount of time as well as human-effort. In this paper, a unique approach has been investigated to classify bacteria from microscopic image samples. An AI system has been developed by combining a Deep Convolutional Neural Network (DCNN) with Support Vector Machine (SVM) to perform this operation. Using the transfer-learning method, the Inception V3 DCNN architecture has been modified and retrained with more than 800 image samples of seven separate bacteria species, which are 80% of the image-dataset. The features extracted by the retrained DCNN were then used to train a Support Vector Machine (SVM) classifier. The hybrid network was then tested on the rest 20% of images and the network efficiently classified the images of seven individual kinds of bacteria samples with accuracy-level of around 96%. © 2019 IEEE.","Bacteria classification; Deep Convolutional Neural Network (DCNN); Inception V3 model; Microscopic image; SVM; Transfer learning","Convolution; Deep neural networks; Diseases; Image classification; Neural networks; Support vector machines; Computer vision technology; Convolutional neural network; Hybrid network; Infectious disease; Manual identification; Microscopic image; Transfer learning; Transfer learning methods; Bacteria"
"Ahmed U., Lin J.C.-W., Tomasiello S., Srivastava G.","An Explainable Mental Health Fuzzy Deep Active Learning Technique","10.1109/FUZZ-IEEE55066.2022.9882622","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138786758&doi=10.1109%2fFUZZ-IEEE55066.2022.9882622&partnerID=40&md5=d5879194777897a45bd1b6daa5f7c6a2","In this study, we present a fuzzy contrast-based model that classifies mental patient authored text into different symptoms by using an attention network for position-weighted words. Then, the mental data are labeled using the trained embedding. After that, the lexicons of the attention network are extended to allow the use of transfer learning methods. Our proposed approach classifies weighted attention words using similarity as well as contrast sets. The fuzzy model then classifies mental health data into different groups. To illustrate the performance of the proposed model, the approach is compared with the non-embedding as well as standard approaches. From the demonstrated results, the feature vector has a high Receiver Operating Characteristic Curve (ROC)-curve of 0.82 for 9 different symptom problems. © 2022 IEEE.","Constraint sets; Deep learning; Fuzzy System; Human intervention","Deep learning; Fuzzy systems; Active Learning; Constraint set; Deep learning; Embeddings; Fuzzy modeling; Health data; Human intervention; Learning techniques; Mental health; Transfer learning methods; Embeddings"
"Ahmed U., Lin J.C., Srivastava G.","Multi-Aspect Deep Active Attention Network for Healthcare Explainable Adoption","10.1109/JBHI.2022.3204633","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137911255&doi=10.1109%2fJBHI.2022.3204633&partnerID=40&md5=1e94e0e9377d9decf6af444c451d4053","Depression is a serious illness that significantly affects the lives of those affected. Recent studies have looked at the possibility of detecting and diagnosing this mental disorder using user-generated data from various forms of online media. Therefore, we addressed the issue of detecting sadness in social media by focusing on terms in personal remarks. To overcome the limitations in classifying depression texts, this study aims to develop attention networks that use covert levels of self-attention. Since nodes/words can express the properties/emotions of their neighbors, this paper naturally assigns each node in a neighborhood its weight without performing costly matrix operations such as similarity or network architecture knowledge. The paper extended the emotion lexicon by using hypernyms. For this reason, our method is superior to the performance of the other designs. According to the results of our experiments, the emotion lexicon combined with an attention network achieves a ROC of 0.87 while maintaining its interpretability and transparency level. Subsequently, the learned embedding is used to display the contribution of each symptom to the activated word, and the psychiatrist is polled to obtain his qualitative agreement with this representation. By using unlabeled forum language, the method increases the rate at which depression symptoms can be identified from information in Internet forums. IEEE","adaptive treatments; Deep learning; Depression; Explainability; Internet-delivered interventions; Media; Medical diagnostic imaging; natural language processing; Neural networks; Social networking (online); Training; word sense identification","Deep learning; Diagnosis; Medical imaging; Natural language processing systems; Network architecture; Social networking (online); Adaptive treatment; Deep learning; Depression; Explainability; Internet-delivered intervention; Language processing; Medical diagnostic imaging; Medium; Natural language processing; Natural languages; Neural-networks; Social networking (online); Word sense; Word sense identification; Neural networks"
"Ahmed U., Lin J.C., Srivastava G.","Social Media Multiaspect Detection by Using Unsupervised Deep Active Attention","10.1109/TCSS.2022.3183283","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133700313&doi=10.1109%2fTCSS.2022.3183283&partnerID=40&md5=90c2675ef21ac69825690815c859979f","Depression is a severe medical condition that substantially impacts people&#x2019;s daily lives. Recently, researchers have examined user-generated data from social media platforms to detect and diagnose this mental illness. As a result, in this paper we have focused on phrases used in personal remarks to solve recognizing grief on social media. This research aims to develop generalized attention networks (GATs) that employ masked self-attention layers to overcome the depression text categorization problem. The networks distribute weight to each node in a neighborhood based on neighbors&#x2019; properties/emotions without using expensive matrix operations like similarity or architectural knowledge. This study expands the emotional vocabulary through the use of hypernyms. As a result, our architecture outperforms the competition. Our experimental results show that the emotion lexicon combined with an attention network achieves receiver operating characteristic (ROC)-0.87 while staying interpretable and transparent. After obtaining qualitative agreement from the psychiatrist, the learned embedding is used to show the contribution of each symptom to the activated word. By utilizing unlabeled forum text, the approach increases the rate of detecting depression symptoms from online data. IEEE","Behavior; Computer architecture; cyber-physical social intelligence; Data mining; deep active attention; Depression; Mental health; Semantics; social media; Social networking (online); Training","Computer architecture; Cyber Physical System; Data mining; Diseases; Network architecture; Social networking (online); Text processing; Behavior; Cybe-physical social intelligence; Cyber physicals; Deep active attention; Depression; Mental health; Multiaspect; Social intelligence; Social media; Social networking (online); Semantics"
"Ahmed U., Lin J.C.","Deep Explainable Hate Speech Active Learning on Social-Media Data","10.1109/TCSS.2022.3165136","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129210012&doi=10.1109%2fTCSS.2022.3165136&partnerID=40&md5=6d9dd94894a4705a956c2bbccdf9e16f","Hate speech is demonstrably aimed at social tension and violence. Detection becomes increasingly difficult as overlapping emotional feelings occur. However, there are still several unresolved issues with informal and indirect targeting of negative communication, including sarcasm, misrepresentation, and praise for the target's or society's immoral behavior. In this study, we proposed a method for instance selection based on attention network visualization. The goal is to categorize, modify, and expand the number of training instances. To this end, we first used the lexicons of hate speech and online forums to train the embedding using transfer learning. Then, we used synonym expansion to the semantic vectors. The active learning approach was used to train the task using the result-label pairs. The entropy-based selection and visualization techniques help select unlabeled text for each active learning cycle. The approach is improved, and the number of training instances is increased to improve the model's accuracy. The active learning cycles are repeated until all unlabeled texts are converted to labeled text. The semantic embedding and lexicon expansion improve the model receiver operating characteristics (ROCs) from 0.89 to 0.91. The bidirectional LSTM with attention and active learning achieved 0.90 for precision-recall. The learned model can visualize the position-weighted terms to illustrate why hate speech is classified. IEEE","Artificial intelligence; Deep learning; Depression; ethnic hate; explainable machine learning (ML); Feature extraction; Hate speech; hate speech detection.; Internet; Medical services; Social networking (online)","Long short-term memory; Semantics; Social networking (online); Speech; Speech recognition; Visualization; Deep learning; Depression; Ethnic hate; Explainable machine learning; Features extraction; Hate speech; Hate speech detection.; Medical services; Social networking (online); Speech detection; Feature extraction"
"Ahmed U., Lin J.C.-W., Srivastava G.","Fuzzy Explainable Attention-based Deep Active Learning on Mental-Health Data","10.1109/FUZZ45933.2021.9494423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114687193&doi=10.1109%2fFUZZ45933.2021.9494423&partnerID=40&md5=e10f04a3ee677b51bcf04f2729a05d00","In this paper, we propose a fuzzy classification deep attention-based model that expands emotional lexicons by using linguistic properties of actual patient authored texts. The active learning methods can expand the trained dataset and fuzzy rules over some time. As a result, the model itself can reduce its labeling efforts for mental health application. Thus, the designed model can solve issues related to vocabulary sizes per class, data sources, methods of creation, and create a baseline for human performance levels. This paper also gives fuzzy explainability by visualizing weighted words. Our proposed method uses a similarity-based method that includes a subset of unstructured data as the training set. Next, using an active learning mechanism cycle, our method updates the training model using new training points. This cycle is repeatedly performed until an optimal solution is reached. The designed model also converts all unlabeled texts into the training set. Our in-depth experimental results show that the emotion-based expansion enhances the testing accuracy and helps to build quality rules. © 2021 IEEE.","adaptive treatments; Internet-delivered interventions; NLP; text clustering; word sense identification","Classification (of information); Deep learning; Fuzzy inference; Fuzzy systems; Active learning methods; Fuzzy classification; Human performance; Linguistic properties; Optimal solutions; Similarity-Based Methods; Testing accuracy; Unstructured data; Learning systems"
"Ahmed W., Veluthandath A.V., Rowe D.J., Madsen J., Clark H.W., Postle A.D., Wilkinson J.S., Murugan G.S.","Prediction of Neonatal Respiratory Distress Biomarker Concentration by Application of Machine Learning to Mid-Infrared Spectra","10.3390/s22051744","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125075923&doi=10.3390%2fs22051744&partnerID=40&md5=14cd9b9acd08ff3cf3aae55fa1d822e0","The authors of this study developed the use of attenuated total reflectance Fourier transform infrared spectroscopy (ATR–FTIR) combined with machine learning as a point-of-care (POC) diagnostic platform, considering neonatal respiratory distress syndrome (nRDS), for which no POC currently exists, as an example. nRDS can be diagnosed by a ratio of less than 2.2 of two nRDS biomarkers, lecithin and sphingomyelin (L/S ratio), and in this study, ATR–FTIR spectra were recorded from L/S ratios of between 1.0 and 3.4, which were generated using purified reagents. The calibration of principal component (PCR) and partial least squares (PLSR) regression models was performed using 155 raw baselined and second derivative spectra prior to predicting the concentration of a further 104 spectra. A three-factor PLSR model of second derivative spectra best predicted L/S ratios across the full range (R2: 0.967; MSE: 0.014). The L/S ratios from 1.0 to 3.4 were predicted with a prediction interval of +0.29, −0.37 when using a second derivative spectra PLSR model and had a mean prediction interval of +0.26, −0.34 around the L/S 2.2 region. These results support the validity of combining ATR–FTIR with machine learning to develop a point-of-care device for detecting and quantifying any biomarker with an interpretable mid-infrared spectrum. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","ATR–FTIR; Machine learning; Neonatal respiratory distress syndrome; Point-of-care devices; Spectroscopy","Forecasting; Fourier transform infrared spectroscopy; Infrared devices; Least squares approximations; Machine learning; Phospholipids; Regression analysis; Attenuated total reflectance Fourier transform infrared spectroscopy; Mid-infrared spectra; Neonatal respiratory distress syndromes; Partial least squares models; Point of care; Point-of-care device; Prediction interval; S ratio; Second derivative spectra; Spectra's; Biomarkers; biological marker; human; infrared spectroscopy; least square analysis; machine learning; neonatal respiratory distress syndrome; newborn; procedures; Biomarkers; Humans; Infant, Newborn; Least-Squares Analysis; Machine Learning; Respiratory Distress Syndrome, Newborn; Spectroscopy, Fourier Transform Infrared"
"Ahmed W.A.M., El-Halees A.M.","Arabic Opinion Mining Using Parallel Decision Trees","10.1109/PICICT.2017.28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032262583&doi=10.1109%2fPICICT.2017.28&partnerID=40&md5=e48cf6460a8347c6206c4c53af610c33","Opinion mining is an interested area of research, which epitomize the customer reviews of a product or service and express whether the opinions are positive or negative. Various methods have been proposed as classifiers for opinion mining such as Naïve Bayesian, and Support vector machine, these methods classify opinion without giving us the reasons about why the instance opinion is classified to certain class. Therefore, in our work, we investigate opinion mining of Arabic text at the document level, by applying decision trees classification classifier to have clear, understandable rule, also we apply parallel decision trees classifiers to have efficient results. We applied parallel decision trees on two Arabic corpus of text documents by using parallel implementation of RapidMiner tools. In case of applying parallel decision tree family on OCA we get the best results of accuracy (93.83%), f-measure (93.22) and consumed time 42 Sec at thread 4, one of the resulted rule is Urdu language lines. In case of applying parallel decision tree family on BHA we get the best results of accuracy (90.63%), f-measure (82.29) and consumed time 219 Sec at thread 4, one of the resulted rule is Urdu language lines. © 2017 IEEE.","Arabic text; Classification; Decision tree; Machine learning; Opinion Extraction; Opinion mining; Parallel Decision Tree; Sentiment Analysis; Sentiment Classification","Classification (of information); Data mining; Information retrieval systems; Learning systems; Text processing; Arabic texts; Opinion extraction; Opinion mining; Parallel decision trees; Sentiment analysis; Sentiment classification; Decision trees"
"Ahmed Z.U., Sun K., Shelly M., Mu L.","Explainable artificial intelligence (XAI) for exploring spatial variability of lung and bronchus cancer (LBC) mortality rates in the contiguous USA","10.1038/s41598-021-03198-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121486671&doi=10.1038%2fs41598-021-03198-8&partnerID=40&md5=1c7709852180a00fade1a2701789f8b4","Machine learning (ML) has demonstrated promise in predicting mortality; however, understanding spatial variation in risk factor contributions to mortality rate requires explainability. We applied explainable artificial intelligence (XAI) on a stack-ensemble machine learning model framework to explore and visualize the spatial distribution of the contributions of known risk factors to lung and bronchus cancer (LBC) mortality rates in the conterminous United States. We used five base-learners—generalized linear model (GLM), random forest (RF), Gradient boosting machine (GBM), extreme Gradient boosting machine (XGBoost), and Deep Neural Network (DNN) for developing stack-ensemble models. Then we applied several model-agnostic approaches to interpret and visualize the stack ensemble model's output in global and local scales (at the county level). The stack ensemble generally performs better than all the base learners and three spatial regression models. A permutation-based feature importance technique ranked smoking prevalence as the most important predictor, followed by poverty and elevation. However, the impact of these risk factors on LBC mortality rates varies spatially. This is the first study to use ensemble machine learning with explainable algorithms to explore and visualize the spatial heterogeneity of the relationships between LBC mortality and risk factors in the contiguous USA. © 2021, The Author(s).",,"bronchus tumor; epidemiology; female; forecasting; human; lung tumor; machine learning; male; mortality; risk factor; spatial regression; statistical model; United States; Bronchial Neoplasms; Female; Forecasting; Humans; Lung Neoplasms; Machine Learning; Male; Models, Statistical; Risk Factors; Spatial Regression; United States"
"Ahmetoğlu A., Alpaydın E.","Hierarchical mixtures of generators for adversarial learning","10.1109/ICPR48806.2021.9413249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110411379&doi=10.1109%2fICPR48806.2021.9413249&partnerID=40&md5=b76a7246878ab7b4fbca243144afce73","Generative adversarial networks (GANs) are deep neural networks that allow us to sample from an arbitrary probability distribution without explicitly estimating the distribution. There is a generator that takes a latent vector as input and transforms it into a valid sample from the distribution. There is also a discriminator that is trained to discriminate such fake samples from true samples of the distribution; at the same time, the generator is trained to generate fakes that the discriminator cannot tell apart from the true samples. Instead of learning a global generator, a recent approach involves training multiple generators each responsible from one part of the distribution. In this work, we review such approaches and propose the hierarchical mixture of generators, inspired from the hierarchical mixture of experts model, that learns a tree structure implementing a hierarchical clustering with soft splits in the decision nodes and local generators in the leaves. Since the generators are combined softly, the whole model is continuous and can be trained using gradient-based optimization, just like the original GAN model. Our experiments on five image data sets, namely, MNIST, FashionMNIST, UTZap50K, Oxford Flowers, and CelebA, show that our proposed model generates samples of high quality and diversity in terms of popular GAN evaluation metrics. The learned hierarchical structure also leads to knowledge extraction. © 2020 IEEE",,"Deep learning; Deep neural networks; Mixtures; Pattern recognition; Probability distributions; Trees (mathematics); Adversarial learning; Adversarial networks; Arbitrary probability distribution; Gradient-based optimization; Hierarchical mixture of experts models; Hierarchical mixtures; Hierarchical structures; Knowledge extraction; Hierarchical clustering"
"Ahn I., Gwon H., Kang H., Kim Y., Seo H., Choi H., Cho H.N., Kim M., Jun T.J., Kim Y.-H.","Machine learning-based hospital discharge prediction for patients with cardiovascular diseases:development and usability study","10.2196/32662","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120166300&doi=10.2196%2f32662&partnerID=40&md5=eb9530ff1c173218c68d9f0984f2cd24","Background: Effective resource management in hospitals can improve the quality of medical services by reducing labor-intensive burdens on staff, decreasing inpatient waiting time, and securing the optimal treatment time. The use of hospital processes requires effective bed management; a stay in the hospital that is longer than the optimal treatment time hinders bed management. Therefore, predicting a patient's hospitalization period may support the making of judicious decisions regarding bed management. Objective: First, this study aims to develop a machine learning (ML)-based predictive model for predicting the discharge probability of inpatients with cardiovascular diseases (CVDs). Second, we aim to assess the outcome of the predictive model and explain the primary risk factors of inpatients for patient-specific care. Finally, we aim to evaluate whether our ML-based predictive model helps manage bed scheduling efficiently and detects long-term inpatients in advance to improve the use of hospital processes and enhance the quality of medical services. Methods: We set up the cohort criteria and extracted the data from CardioNet, a manually curated database that specializes in CVDs. We processed the data to create a suitable data set by reindexing the date-index, integrating the present features with past features from the previous 3 years, and imputing missing values. Subsequently, we trained the ML-based predictive models and evaluated them to find an elaborate model. Finally, we predicted the discharge probability within 3 days and explained the outcomes of the model by identifying, quantifying, and visualizing its features. Results: We experimented with 5 ML-based models using 5 cross-validations. Extreme gradient boosting, which was selected as the final model, accomplished an average area under the receiver operating characteristic curve score that was 0.865 higher than that of the other models (ie, logistic regression, random forest, support vector machine, and multilayer perceptron). Furthermore, we performed feature reduction, represented the feature importance, and assessed prediction outcomes. One of the outcomes, the individual explainer, provides a discharge score during hospitalization and a daily feature influence score to the medical team and patients. Finally, we visualized simulated bed management to use the outcomes. Conclusions: In this study, we propose an individual explainer based on an ML-based predictive model, which provides the discharge probability and relative contributions of individual features. Our model can assist medical teams and patients in identifying individual and common risk factors in CVDs and can support hospital administrators in improving the management of hospital beds and other resources. © 2021 Eesti Rakenduslingvistika Uhingu Aastaraamat. All rights reserved.","Bed management; Cardiovascular diseases; Discharge prediction; Electronic health records; Explainable artificial intelligence",
"Ahn K.U., Park C.S., Kim K.-J., Kim D.-W., Chae C.-U.","Hybrid model using Bayesian neural network for variable refrigerant flow system","10.1080/19401493.2021.1992013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121529763&doi=10.1080%2f19401493.2021.1992013&partnerID=40&md5=82bcaa1eb0d22bb81a19eccd316ddd0a","This study introduces a hybrid model that combines physics and machine learning (ML) models to describe the behaviour of variable refrigerant flow (VRF) systems. The standalone ML model was developed with identical data and conditions for comparison between the hybrid and ML models. A Bayesian neural network (BNN) was used for both the models, and the predictive abilities and uncertainties were investigated. For the experimental dataset, the predictive performances of both models were similar. For example, the predictive performance of the hybrid and ML models showed mean absolute error of 0.73 and 0.78 kW, respectively. However, the epistemic uncertainty of the hybrid model quantified using the BNN was 36.4% lower than that of the ML model. A parametric study showed that the hybrid model combined with the physics model can achieve better generalization performance than the ML model, yielding results that are more reliable and physically explainable. © 2021 International Building Performance Simulation Association (IBPSA).","Bayesian neural network; hybrid model; Machine learning; variable refrigerant flow system","Bayesian networks; Neural networks; Refrigerants; Uncertainty analysis; Bayesian neural networks; Condition; Flow systems; Hybrid model; Machine learning models; Predictive abilities; Predictive performance; Predictive uncertainty; Refrigerant flow; Variable refrigerant flow system; Machine learning"
"Ahn M., Jun S.C., Yeom H.G., Cho H.","Editorial: Deep Learning in Brain-Computer Interface","10.3389/fnhum.2022.927567","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131755913&doi=10.3389%2ffnhum.2022.927567&partnerID=40&md5=7370a33d472d02dfb5701589fa35abfa",[No abstract available],"brain-computer interface; data augmentation; deep learning; explainable artificial intelligence; machine learning; transfer learning","artificial intelligence; deep learning; Editorial; functional near-infrared spectroscopy; human; k nearest neighbor; transfer of learning"
"Ahn S., Kim J., Park S.Y., Cho S.","Explaining Deep Learning-based Traffic Classification using A Genetic Algorithm","10.1109/ACCESS.2020.3048348","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099111031&doi=10.1109%2fACCESS.2020.3048348&partnerID=40&md5=3d4bc6a77e218a3ceb74dfb4846007e1","Traffic classification is widely used in various network functions such as software-defined networking and network intrusion detection systems. Many traffic classification methods have been proposed for classifying encrypted traffic by utilizing a deep learning model without inspecting the packet payload. However, they have an important challenge in that the mechanism of deep learning is inexplicable. A malfunction of the deep learning model may occur if the training dataset includes malicious or erroneous data. Explainable artificial intelligence (XAI) can give some insight for improving the deep learning model by explaining the cause of the malfunction. In this paper, we propose a method for explaining the working mechanism of deep-learning-based traffic classification as a method of XAI based on a genetic algorithm. We describe the mechanism of the deep-learning-based traffic classifier by quantifying the importance of each feature. In addition, we leverage the genetic algorithm to generate a feature selection mask that selects important features in the entire feature set. To demonstrate the proposed explanation method, we implemented a deep-learning-based traffic classifier with an accuracy of approximately 97.24%. In addition, we present the importance of each feature derived from the proposed explanation method by defining the dominance rate. CCBY","Data models; Deep learning; Deep learning; Explainable artificial intelligence (XAI); Feature extraction; Genetic algorithm; Genetic algorithms; Machine learning; Payloads; Quality of service; Traffic classification","Genetic algorithms; Intrusion detection; Learning algorithms; Learning systems; Encrypted traffic; Important features; Network functions; Network intrusion detection systems; Traffic classification; Traffic classifiers; Training dataset; Working mechanisms; Deep learning"
"Ahn Y., Yan M., Lin Y.-R., Chung W.-T., Hwa R.","Tribe or Not? Critical Inspection of Group Differences Using TribalGram","10.1145/3484509","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127755199&doi=10.1145%2f3484509&partnerID=40&md5=c9f861b569e06d810a6ec27b980ce7a6","With the rise of AI and data mining techniques, group profiling and group-level analysis have been increasingly used in many domains, including policy making and direct marketing. In some cases, the statistics extracted from data may provide insights to a group's shared characteristics; in others, the group-level analysis can lead to problems, including stereotyping and systematic oppression. How can analytic tools facilitate a more conscientious process in group analysis? In this work, we identify a set of accountable group analytics design guidelines to explicate the needs for group differentiation and preventing overgeneralization of a group. Following the design guidelines, we develop TribalGram, a visual analytic suite that leverages interpretable machine learning algorithms and visualization to offer inference assessment, model explanation, data corroboration, and sense-making. Through the interviews with domain experts, we showcase how our design and tools can bring a richer understanding of ""groups""mined from the data. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.","contrastive explanation; Group analysis; group difference; group profiling; interpretable machine learning; visual analytics","Data mining; Data visualization; Design; Inference engines; Learning algorithms; Machine learning; Contrastive explanation; Data-mining techniques; Direct marketing; Group analysis; Group differences; Group level; Group profiling; Interpretable machine learning; Policy making; Visual analytics; Visualization"
"Ahne A., Fagherazzi G., Tannier X., Czernichow T., Orchard F.","Improving Diabetes-Related Biomedical Literature Exploration in the Clinical Decision-making Process via Interactive Classification and Topic Discovery: Methodology Development Study","10.2196/27434","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123651727&doi=10.2196%2f27434&partnerID=40&md5=829c61485e54a8528c6f341783104e87","Background: The amount of available textual health data such as scientific and biomedical literature is constantly growing and becoming more and more challenging for health professionals to properly summarize those data and practice evidence-based clinical decision making. Moreover, the exploration of unstructured health text data is challenging for professionals without computer science knowledge due to limited time, resources, and skills. Current tools to explore text data lack ease of use, require high computational efforts, and incorporate domain knowledge and focus on topics of interest with difficulty. Objective: We developed a methodology able to explore and target topics of interest via an interactive user interface for health professionals with limited computer science knowledge. We aim to reach near state-of-the-art performance while reducing memory consumption, increasing scalability, and minimizing user interaction effort to improve the clinical decision-making process. The performance was evaluated on diabetes-related abstracts from PubMed. Methods: The methodology consists of 4 parts: (1) a novel interpretable hierarchical clustering of documents where each node is defined by headwords (words that best represent the documents in the node), (2) an efficient classification system to target topics, (3) minimized user interaction effort through active learning, and (4) a visual user interface. We evaluated our approach on 50,911 diabetes-related abstracts providing a hierarchical Medical Subject Headings (MeSH) structure, a unique identifier for a topic. Hierarchical clustering performance was compared against the implementation in the machine learning library scikit-learn. On a subset of 2000 randomly chosen diabetes abstracts, our active learning strategy was compared against 3 other strategies: random selection of training instances, uncertainty sampling that chooses instances about which the model is most uncertain, and an expected gradient length strategy based on convolutional neural networks (CNNs). Results: For the hierarchical clustering performance, we achieved an F1 score of 0.73 compared to 0.76 achieved by scikit-learn. Concerning active learning performance, after 200 chosen training samples based on these strategies, the weighted F1 score of all MeSH codes resulted in a satisfying 0.62 F1 score using our approach, 0.61 using the uncertainty strategy, 0.63 using the CNN, and 0.45 using the random strategy. Moreover, our methodology showed a constant low memory use with increased number of documents. Conclusions: We proposed an easy-to-use tool for health professionals with limited computer science knowledge who combine their domain knowledge with topic exploration and target specific topics of interest while improving transparency. Furthermore, our approach is memory efficient and highly parallelizable, making it interesting for large Big Data sets. This approach can be used by health professionals to gain deep insights into biomedical literature to ultimately improve the evidence-based clinical decision making process. © 2022 Journal of Medical Internet Research. All rights reserved.","Active learning; Classification; Clinical decision making; Clinical decision support; Digital health; Evidence-based medicine; Hierarchical clustering; Medical informatics; Memory consumption; Natural language processing; Transparency","Article; clinical decision making; clinical decision support system; convolutional neural network; data analysis; data processing; data science; diabetes mellitus; health practitioner; hierarchical clustering; human; learning algorithm; machine learning; medical literature; Medical Subject Headings; natural language processing; clinical decision making; diabetes mellitus; Medical Subject Headings; Medline; Clinical Decision-Making; Diabetes Mellitus; Humans; Medical Subject Headings; Neural Networks, Computer; PubMed"
"Ahrazem Dfuf I., Forte Perez-Minayo J., Mira McWilliams J.M., Gonzalez Fernandez C.","Variable Importance Analysis in Imbalanced Datasets: A New Approach","10.1109/ACCESS.2020.3008416","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089223584&doi=10.1109%2fACCESS.2020.3008416&partnerID=40&md5=914cecfa69fb01b0a2334ddcae26470d","Decision-making using machine learning requires a deep understanding of the model under analysis. Variable importance analysis provides the tools to assess the importance of input variables when dealing with complex interactions, making the machine learning model more interpretable and computationally more efficient. In classification problems with imbalanced datasets, this task is even more challenging. In this article, we present two variable importance techniques, a nonparametric solution, called mh - χ2, and a parametric method based on Global Sensitivity Analysis. The mh - χ2 employs a multivariate continuous response framework to deal with the multiclass classification problem. Based on the permutation importance framework, the proposed mh - χ2 algorithm captures the dissimilarities between the distribution of misclassification errors generated by the base learner, Conditional Inference Tree, before and after permuting the values of the input variable under analysis. The GSA solution is based on the Covariance decomposition methodology for multivariate output models. Both solutions will be assessed in a comparative study of several Random Forest-based techniques with emphasis in the multiclass classification problem with different imbalanced scenarios. We apply the proposed techniques in two real application cases in order first, to quantify the importance of the 35 companies listed in the Spanish market index IBEX35 on the economic, political and social uncertainties reflected in economic newspapers in Spain during the first quadrimester of 2020 due to the COVID-19 pandemic and second, to assess the impact of energy factors on the occurrence of spike prices on the Spanish electricity market. © 2013 IEEE.","Covid-19 pandemic; electricity market; global sensitivity analysis; multiclass classification problem; multivariate response scenario; variable importance analysis","Commerce; Decision making; Decision trees; Inference engines; Machine learning; Sensitivity analysis; Uncertainty analysis; Conditional inference; Covariance decompositions; Global sensitivity analysis; Machine learning models; Misclassification error; Multiclass classification problems; Spanish electricity markets; Variable importances; Classification (of information)"
"Åhs F., Mozelius P., Dobslaw F.","Artificial intelligence supported cognitive behavioral therapy for treatment of speech anxiety in virtual reality environments","10.34190/EAIR.20.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097823046&doi=10.34190%2fEAIR.20.030&partnerID=40&md5=4cc5507d5507f3e7e05d3aac182ef52a","Cognitive behavioral therapy (CBT) has become a successful treatment to improve management of stress and anxiety in social situations. One of the most widespread social anxiety disorders is speech anxiety, and there are also studies reporting that speech anxiety is increasing among younger adults. An emerging trend in CBT treatment is virtual reality (VR), a technology that today also could involve the use of artificial intelligence. The aim of this position paper is to present and discuss the idea of using explainable artificial intelligence to improve CBT treatment of speech anxiety in virtual reality environments. The proposed CBT and VR concept builds upon identification of individuals for whom a scientifically grounded treatment can be predicted to have a larger effect than the average. The identification of these individuals should be conducted with the use of Explainable artificial intelligence (XAI). However, the effect of providing XAI-based information on actual treatment outcome has not been fully investigated and established. To better understand how AI-based information can strengthen CBT, it would be valuable to investigate how much confidence individuals undergoing treatment can have in information that is derived from XAI applications. If XAI-derived information is trusted to the same extent as traditional information coming from psychologists, this could open up for CBT design. Furthermore, the VR-treatment should be grounded in learning theory and cognitive psychology with an emphasis on promotion of inhibitory learning. A commercial application should be used for stimuli presentation in the VR-head-set based on various scenarios that simulates real-world situations. The main objective of the VR-treatment is to promote inhibitory learning by disproving catastrophic beliefs through exposure to distressful speech situations. Outcomes of the treatment should primarily be measured by the Public Speaking Anxiety Scale, but also involve an assessment of social anxiety with the use of Liebowitz’s Social Anxiety Scale. © ECIAIR 2020.All right reserved.","Artificial intelligence; Cognitive behavioral therapy; Explainable artificial intelligence; Speech anxiety; Virtual reality","Agricultural robots; Robotics; Speech; Cognitive psychology; Cognitive-behavioral therapies; Commercial applications; Identification of individuals; Real world situations; Social anxieties; Treatment outcomes; Virtual-reality environment; Virtual reality"
"Ahsan M.M., Nazim R., Siddique Z., Huebner P.","Detection of covid-19 patients from ct scan and chest x-ray data using modified mobilenetv2 and lime","10.3390/healthcare9091099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113747043&doi=10.3390%2fhealthcare9091099&partnerID=40&md5=5d93872707e5b8bd9f9177b1c66d9586","The COVID-19 global pandemic caused by the widespread transmission of the novel coron-avirus (SARS-CoV-2) has become one of modern history’s most challenging issues from a healthcare perspective. At its dawn, still without a vaccine, contagion containment strategies remained most effective in preventing the disease’s spread. Patient isolation has been primarily driven by the results of polymerase chain reaction (PCR) testing, but its initial reach was challenged by low availability and high cost, especially in developing countries. As a means of taking advantage of a preexisting infrastructure for respiratory disease diagnosis, researchers have proposed COVID-19 patient screening based on the results of Chest Computerized Tomography (CT) and Chest Radiographs (X-ray). When paired with artificial-intelligence-and deep-learning-based approaches for analysis, early studies have achieved a comparatively high accuracy in diagnosing the disease. Considering the opportunity to further explore these methods, we implement six different Deep Convolutional Neural Networks (Deep CNN) models—VGG16, MobileNetV2, InceptionResNetV2, ResNet50, ResNet101, and VGG19—and use a mixed dataset of CT and X-ray images to classify COVID-19 patients. Preliminary results showed that a modified MobileNetV2 model performs best with an accuracy of 95 ± 1.12% (AUC = 0.816). Notably, a high performance was also observed for the VGG16 model, outperforming several previously proposed models with an accuracy of 98.5 ± 1.19% on the X-ray dataset. Our findings are supported by recent works in the academic literature, which also uphold the higher performance of MobileNetV2 when X-ray, CT, and their mixed datasets are considered. Lastly, we further explain the process of feature extraction using Local Interpretable Model-Agnostic Explanations (LIME), which contributes to a better understanding of what features in CT/X-ray images characterize the onset of COVID-19. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Chest X-ray; Coronavirus; COVID-19; CT scan; Deep learning; Explainable AI; Imbalanced data; Mixed-data; SARS-CoV-2; Small data",
"Ahsan M.N.I., Nahian T., Kafi A.A., Hossain Md.I., Shah F.M.","An ensemble approach to detect review spam using hybrid machine learning technique","10.1109/ICCITECHN.2016.7860229","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016201307&doi=10.1109%2fICCITECHN.2016.7860229&partnerID=40&md5=564ad2e1a2c5094ecf2e2f944d7f3dcc","Online reviews are becoming one of the vital components of e-commerce in recent years as so many people consider having different opinions prior to buying online products or apprehending any online service. Nowadays, in the era of web 2.0, it is completely understandable that people rely on online reviews more than ever while taking a decision. However, guaranteeing the authenticity of these sensitive and valuable information is hardly visible. Due to fulfill some immoral benefits, many people post fake review or fabricated opinion to uphold or devalue a certain product or service which certainly hampers the ingenuousness of the real fact. To detect fake reviews, many methodologies were introduced by harvesting the obvious content features, rating consistency, empirical conditions, helpfulness voting etc. The most of them are supervised models which mostly rely on pseudo fake reviews and the scarcity of good quality largescale labeled dataset is still a hindrance. In this paper, we introduce an ensemble learning approach which combines two different types of learning methods (active and supervised) by creating a hybrid dataset of both real-life and pseudo reviews. This model holds 3 different filtering phases that is based on KL and JS distance, TF-IDF features and n-gram features of the review content. It achieves phenomenal results while working on almost 3600 reviews from different domains. In the best case, the precision, recall and f-score are above 95% and the accuracy it achieved is slightly above 88%. In the process, about 2000 reviews were manually labeled. After evaluating and comparing the results with other successful methods, it is quite clear that this detecting method is efficient and very promising. © 2016 IEEE.","Fake review; Machine learning; Review spam detection; Spam detection; Spam review","Artificial intelligence; Detecting methods; Different domains; Ensemble approaches; Ensemble learning approach; Hybrid machine learning; Learning methods; Review spam; Spam detection; Learning systems"
"Ahuja V.","Explainable Artificial Intelligence: Guardian for Cancer Care","10.1201/9781003172772-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133242880&doi=10.1201%2f9781003172772-5&partnerID=40&md5=5895fa1118e65df41fff6b19b620c98f",[No abstract available],,
"Ai L., Muggleton S.H., Hocquette C., Gromowski M., Schmid U.","Beneficial and harmful explanatory machine learning","10.1007/s10994-020-05941-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102509030&doi=10.1007%2fs10994-020-05941-0&partnerID=40&md5=3b780a16d9b0cb76ce2ffa395d3ba073","Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie’s definition of ultra-strong machine learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work to our knowledge has examined the potential harmfulness of machine’s involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. Our quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning. © 2021, The Author(s).","Comprehensibility; Explainable AI; Inductive logic programming; Ultra-strong machine learning","Deep learning; Turing machines; Beneficial effects; Classification tasks; Cognitive science; Human comprehensions; Human learning; Human performance; Task performance; Two person game; Learning systems"
"Ai Q., Zhang Y., Bi K., Chen X., Bruce Croft W.","Learning a hierarchical embedding model for personalized product search","10.1145/3077136.3080813","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029376715&doi=10.1145%2f3077136.3080813&partnerID=40&md5=c957024b8c483c52fa4428607465901f","Product search is an important part of online shopping. In contrast to many search tasks, the objectives of product search are not con.ned to retrieving relevant products. Instead, it focuses on .nding items that satisfy the needs of individuals and lead to a user purchase.The unique characteristics of product search make search personalization essential for both customers and e-shopping companies. Purchase behavior is highly personal in online shopping and users o.en provide rich feedback about their decisions (e.g. product reviews). However, the severe mismatch found in the language of queries, products and users make traditional retrieval models based on bag-of-words assumptions less suitable for personalization in product search. In this paper, we propose a hierarchical embedding model to learn semantic representations for entities (i.e. words, products, users and queries) from different levels with their associated language data. Our contributions are three-fold: (1) our work is one of the initial studies on personalized product search; (2) our hierarchical embedding model is the .rst latent space model that jointly learns distributed representations for queries, products and users with a deep neural network; (3) each component of our network is designed as a generative model so that the whole structure is explainable and extendable. Following the methodology of previous studies, we constructed personalized product search benchmarks with Amazon product data. Experiments show that our hierarchical embedding model significantly outperforms existing product search baselines on multiple benchmark datasets. © 2017 Copyright held by the owner/author(s).","Latent Space Model; Personalization; Product Search; Representation Learning","Customer satisfaction; Deep neural networks; Electronic commerce; Information retrieval; Semantics; Benchmark datasets; Distributed representation; Latent space models; Personalizations; Personalized products; Product Search; Representation Learning; Semantic representation; Deep learning"
"Ai S., Du R., Fan W.","Discovering and sharing experiential knowledge through active mining and friendly delivery: A design of ekdss",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888212470&partnerID=40&md5=583c0650209e3a18b3123e39e5b565aa","As a nontrivial process of identifying valid, novel, potentially useful, and ultimately understandable experiences and best practices, discovering experiential knowledge is of significant value. And as a transfer of experiential knowledge via channels and interactions between relevant people, sharing experiential knowledge is also very important. To build an effective experiential knowledge discovering and sharing system is necessary for a firm to substantially improve its decision processes and thereby its performance in a knowledge-based economy. In this study, we design an experiential knowledge discovering and sharing system (EKDSS), in which active mining and friendly delivery are two focuses. Active mining is performed by the professional knowledge management workers at an experiential knowledge center through a monitored project management process and a required reporting procedure. Friendly delivery is carried out by the same staff through a monitored distribution process and a required planning procedure. We address the application of EKDSS. Hopefully, this system will significantly improve experiential knowledge discovering and sharing. We also point out some directions for further work. Copyright © (2008) by Computers & Industrial Engineering.","Best practices; Data mining; Experiential knowledge; Knowledge discovery; Knowledge sharing; System design","Best practices; Distribution process; Experiential knowledge; Knowledge based economy; Knowledge-sharing; Planning procedure; Professional knowledge; Project management process; Data mining; Industrial engineering; Knowledge management; Management science; Project management; Systems analysis; Human resource management"
"Aicardi C., Fothergill B.T., Rainey S., Stahl B.C., Harris E.","Accompanying technology development in the Human Brain Project: From foresight to ethics management","10.1016/j.futures.2018.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040775439&doi=10.1016%2fj.futures.2018.01.005&partnerID=40&md5=3119ce384d4774fc8f1bf2c823a81750","This paper addresses the question of managing the existential risk potential of general Artificial Intelligence (AI), as well as the more near-term yet hazardous and disruptive implications of specialised AI, from the perspective of a particular research project that could make a significant contribution to the development of Artificial Intelligence (AI): the Human Brain Project (HBP), a ten-year Future and Emerging Technologies Flagship of the European Commission. The HBP aims to create a digital research infrastructure for brain science, cognitive neuroscience, and brain-inspired computing. This paper builds on work undertaken in the HBP's Ethics and Society subproject (SP12). Collaborators from two activities in SP12, Foresight and Researcher Awareness on the one hand, and Ethics Management on the other, use the case of machine intelligence to illustrate key aspects of the dynamic processes through which questions of ethics and society, including existential risks, are approached in the organisational context of the HBP. The overall aim of the paper is to provide practice-based evidence, enriched by self-reflexive assessment of the approach used and its limitations, for guiding policy makers and communities who are, and will be, engaging with such questions. © 2018 The Authors","Artificial intelligence; Ethics management; Foresight; Human Brain Project; Responsible research and innovation","artificial intelligence; ethics; future prospect; innovation; research work; technological development"
"Aiello E.M., Wu Z., Christofides P.D., Toffanin C., Cobelli C., Magni L.","Improving diabetes conventional therapy via machine learning modeling","10.23919/acc.2019.8815347","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072296925&doi=10.23919%2facc.2019.8815347&partnerID=40&md5=86873a1293f21ee76214aedf229e11db","Glucose is a major source of energy for the human body and it is essential that blood glucose levels are maintained within a safe range. Type 1 Diabetes (T1D) is a metabolic disorder characterized by the deficiency of insulin, a hormone which is secreted by the pancreas and is responsible for blood glucose regulation. Thus, T1D patients need exogenous insulin injections to keep the blood glucose level within a safe range. However, the post-prandial (PP) glucose regulation remains a challenging issue for diabetes treatment. In order to improve PP glucose concentrations, a data-driven modeling approach to adjust the meal-related insulin dose is proposed. Specifically, an individualized regression model able to correct the meal bolus computed with the conventional therapy is developed in order to handle the inter-patient variability characterising T1D patients that may affect PP glucose regulation. Moreover, the proposed approach exploits specific models for different day periods on the basis of the intra-day variability of insulin sensitivity. The individualized therapy is validated both on nominal and perturbed scenarios by using the UVA/PADOVA simulator, which is accepted by the FDA as a substitute for pre-clinical animal trials, and the results of a case study are reported. © 2019 American Automatic Control Council.",,
"Aighuraibawi A.H.B., Abdullah R., Manickam S., Alyasseri Z.A.A.","Detection of ICMPv6-based DDoS attacks using anomaly based intrusion detection system: A comprehensive review","10.11591/ijece.v11i6.pp5216-5228","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111168198&doi=10.11591%2fijece.v11i6.pp5216-5228&partnerID=40&md5=f095e889c3ecd34283767ccb32ddf408","Security network systems have been an increasingly important discipline since the implementation of preliminary stages of Internet Protocol version 6 (IPv6) for exploiting by attackers. IPv6 has an improved protocol in terms of security as it brought new functionalities, procedures, i.e., Internet Control Message Protocol version 6 (ICMPv6). The ICMPv6 protocol is considered to be very important and represents the backbone of the IPv6, which is also responsible to send and receive messages in IPv6. However, IPv6 Inherited many attacks from the previous internet protocol version 4 (IPv4) such as distributed denial of service (DDoS) attacks. DDoS is a thorny problem on the internet, being one of the most prominent attacks affecting a network result in tremendous economic damage to individuals as well as organizations. In this paper, an exhaustive evaluation and analysis are conducted anomaly detection DDoS attacks against ICMPv6 messages, in addition, explained anomaly detection types to ICMPv6 DDoS flooding attacks in IPv6 networks. Proposed using feature selection technique based on bio-inspired algorithms for selecting an optimal solution which selects subset to have a positive impact of the detection accuracy ICMPv6 DDoS attack. The review outlines the features and protection constraints of IPv6 intrusion detection systems focusing mainly on DDoS attacks. © 2021 Institute of Advanced Engineering and Science. All rights reserved.","Anomaly detection; DDoS attack; ICMPv6; IPv6; Machine learning",
"Aiken A., Hellerstein J.M., Widom J.","Static Analysis Techniques for Predicting the Behavior of Active Database Rules","10.1145/202106.202107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029277484&doi=10.1145%2f202106.202107&partnerID=40&md5=c4ba9700c4f2367e7af23d242648cfba","This article gives methods for statically analyzing sets of active database rules to determine if the rules are 1995 guaranteed to terminate, (2) guaranteed to produce a unique final database state, and (3) guaranteed to produce a unique stream of observable actions. If the analysis determines that one of these properties is not guaranteed, it isolates the rules responsible for the problem and determines criteria that, if satisfied, guarantee the property. The analysis methods are presented in the context of the Starburst Rule System. © 1995, ACM. All rights reserved.","active database systems; confluence; database rule processing; static analysis; termination","Algorithms; Artificial intelligence; Computer hardware description languages; Computer software; Data handling; Software engineering; Systems analysis; Active database rules; Active databases; Program verification; Static analysis; Validation; Database systems"
"Aillaud P., Lequeux J., Mathe J., Huet L., Lallemand C., Liandrat O., Sebastien N., Kurzrock F., Schmutz N.","Day-ahead forecasting of regional photovoltaic production using deep learning","10.1109/PVSC45281.2020.9300538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099544111&doi=10.1109%2fPVSC45281.2020.9300538&partnerID=40&md5=0bb3b70226d44bb5e50ce72a6e01da20","Power production based on solar energy is directly related to the state of the atmosphere. As the atmospheric state is undergone, this connection makes the solar energy a non-dispatchable source as opposed to controllable renewable sources such as hydroelectricity. In a context of growing photovoltaic generation, accurate forecast tools at regional scale are then increasingly important to grid operators. Indeed, forecasts allow getting information about future production over the next minutes, hours and days. Forecasting tools offer the possibility of a better grid management strategy specifically for transmission system operator (TSO) that are responsible for balancing renewable power production. High forecast accuracy could also lead to reduced costs for energy trading. In light of this situation, this study focuses on the development and analysis of a regional forecasting tool based on a deep learning approach. The selected model consists in a combination of a convolutional neural network (CNN) with a long short-term memory architecture (LSTM). The CNN layers allow extracting spatial features from Numerical Weather Prediction outputs while the LSTM part supports the temporal relationship. The day ahead regional forecast for Germany is chosen as a case study. The CNN-LSTM is compared to the classical Random Forest model known to be one of the reference techniques for this kind of problematic. Simpler deep learning models are also tested to validate the improvement brought by the CNN-LSTM architecture. All the comparisons are based on the classical root mean squared error (RMSE) metrics. The main result of this study shows that CNN-LSTM model can improve forecast accuracy when compared to state-of-the-art Random Forest. As expected, this improvement is strongly correlated to the amount of historical data which must cover several years according to the sensitivity study realized in this work. © 2020 IEEE.","CNN; day-ahead; deep learning; forecast; LSTM; NWP; Random Forest","Convolutional neural networks; Decision trees; Electric power transmission; Long short-term memory; Mean square error; Memory architecture; Network architecture; Random forests; Solar energy; Solar power generation; Weather forecasting; Numerical weather prediction; Photovoltaic generation; Photovoltaic productions; Random forest modeling; Renewable power production; Root mean squared errors; Temporal relationships; Transmission system operators; Deep learning"
"Ain Q.U., Al-Sahaf H., Xue B., Zhang M.","Genetic programming for automatic skin cancer image classification","10.1016/j.eswa.2022.116680","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125466814&doi=10.1016%2fj.eswa.2022.116680&partnerID=40&md5=cd46064e13d52110eb9641a1840aff0f","Developing a computer-aided diagnostic system for detecting various types of skin malignancies from images has attracted many researchers. However, analyzing the behaviors of algorithms is as important as developing new systems in order to establish the effectiveness of a system in real-time situations which impacts greatly how well it can assist the dermatologist in making a diagnosis. Unlike many machine learning approaches such as Artificial Neural Networks, Genetic Programming (GP) automatically evolves models with its dynamic representation and flexibility. This study aims at analyzing recently developed GP-based approaches to skin image classification. These approaches have utilized the intrinsic feature selection and feature construction ability of GP to effectively construct informative features from a variety of pre-extracted features. These features encompass local, global, texture, color and multi-scale image properties of skin images. The performance of these GP methods is assessed using two real-world skin image datasets captured from standard camera and specialized instruments, and compared with six commonly used classification algorithms as well as existing GP methods. The results reveal that these constructed features greatly help improve the performance of the machine learning classification algorithms. Unlike “black-box” algorithms like deep neural networks, GP models are interpretable, therefore, our analysis shows that these methods can help dermatologists identify prominent skin image features. Further, it can help researchers identify suitable feature extraction methods for images captured from a specific instrument. Being fast, these methods can be deployed for making a quick and effective diagnosis in actual clinic situations. © 2022 Elsevier Ltd","Dimensionality reduction; Feature construction; Feature selection; Genetic programming; Image classification","Classification (of information); Deep neural networks; Dermatology; Genetic algorithms; Genetic programming; Image analysis; Image classification; Real time systems; Textures; Classification algorithm; Computer aided diagnostics; Diagnostic systems; Dimensionality reduction; Feature construction; Features selection; Images classification; Performance; Skin cancers; Skin images; Feature extraction"
"Ain Q.U., Al-Sahaf H., Xue B., Zhang M.","Automatically Diagnosing Skin Cancers From Multimodality Images Using Two-Stage Genetic Programming","10.1109/TCYB.2022.3182474","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134226740&doi=10.1109%2fTCYB.2022.3182474&partnerID=40&md5=4e7ed2ed416300f8a70fe88fcee9397a","Developing a computer-aided diagnostic system for detecting various skin malignancies from images has attracted many researchers. Unlike many machine-learning approaches, such as artificial neural networks, genetic programming (GP) automatically evolves models with flexible representation. GP successfully provides effective solutions using its intrinsic ability to select prominent features (i.e., feature selection) and build new features (i.e., feature construction). Existing approaches have utilized GP to construct new features from the complete set of original features and the set of operators. However, the complete set of features may contain redundant or irrelevant features that do not provide useful information for classification. This study aims to develop a two-stage GP method, where the first stage selects prominent features, and the second stage constructs new features from these selected features and operators, such as multiplication in a wrapper approach to improve the classification performance. To include local, global, texture, color, and multiscale image properties of skin images, GP selects and constructs features extracted from local binary patterns and pyramid-structured wavelet decomposition. The accuracy of this GP method is assessed using two real-world skin image datasets captured from the standard camera and specialized instruments, and compared with commonly used classification algorithms, three state of the art, and an existing embedded GP method. The results reveal that this new approach of feature selection and feature construction effectively helps improve the performance of the machine-learning classification algorithms. Unlike other black-box models, the evolved models by GP are interpretable; therefore, the proposed method can assist dermatologists to identify prominent features, which has been shown by further analysis on the evolved models. IEEE","Feature construction; feature selection; genetic programming (GP); image classification; skin cancer images","Classification (of information); Computer aided diagnosis; Dermatology; Diseases; Feature Selection; Genetic algorithms; Image classification; Learning algorithms; Local binary pattern; Neural networks; Textures; Wavelet decomposition; Classification algorithm; Complete sets; Feature construction; Features selection; Genetic programming; Images classification; Prominent features; Skin cancer image; Skin cancers; Skin images; Genetic programming"
"Aiolli F., Giollo M.","A study on the writer identification task for paleographic document analysis","10.2316/P.2011.717-115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958096883&doi=10.2316%2fP.2011.717-115&partnerID=40&md5=cbad154a1f90a61c5b0a56158490e134","The subject of paleography is the study of ancient documents. In particular, the paleographer's aim is to locate a document in a cultural environment and chronological interval in the past. Automatic writer identification is then a desirable tool for a paleographer as she/he gains useful information about the document at hand. However, the paleographer is often interested in methods which can be easily interpretable by humans. In this paper, we apply some state-of-the-art techniques devised for modern documents to the paleographic domain. Moreover, we propose new techniques and document representations with the aim at producing more understandable representation of a writing style. Experimental results have been performed on a large dataset of paleographic images and demonstrate the feasibility of the proposed approach, and the suitability of this tool on helping the paleographer's work.","Biometrics; Intelligent data analysis; Medieval document analysis; Pattern recognition; Writer identification","Ancient documents; Cultural environment; Data sets; Document analysis; Document Representation; Intelligent data analysis; Writer identification; Writing style; Biometrics; Data reduction; Artificial intelligence"
"Aisbett J., Gibbon G.","A cognitive model based on representations that are spatial functions",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-30144431850&partnerID=40&md5=69e77bf5f7989cd7bc7083269c36a729","This paper outlines a cognitive model in which internal representations are spatial functions, and in which the associated process model is governed by distance in psychological space. Motivation for the model comes from the role of similarity judgements in human reasoning, and the apparent ability of humans to create task-dependent features about the concepts used in reasoning. Motivation also comes from the promise that neuroimages might be interpretable in terms of the conceptual tasks in which the person was engaged at the time of imaging. The creation of task-dependent features to aid problem solving is demonstrated in a categorisation task.","Categorisation; Classification; Cognitive model; Cognitive representation; Cortical maps","Artificial intelligence; Magnetic resonance imaging; Mathematical models; Neural networks; Problem solving; Vectors; Cognitive model; Cognitive representation; Cortical maps; Cognitive systems"
"Aishwarya Gowda A.G., Su H.-K., Kuo W.-K.","Monitoring Panic Situations in Students and Alerting Using Artificial Intelligence","10.18494/SAM3927","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138998496&doi=10.18494%2fSAM3927&partnerID=40&md5=57307da5e23515056c6f22b2414d24bd","The role of artificial intelligence (AI) in human monitoring and recognition has been upgraded with different solutions to address difficulties in various industries. This sensing-based technology can have a considerable impact on students' life by helping parents and teachers understand and realize students' panic situations. Several students are losing lives owing to unbearable panic attacks and the tough decisions they make during their panic phase. In the current education system, students face panic due to fear of exams, failure in exams, bullying, and so forth. If this situation is recognized and communicated early, it can help elders prevent the dangerous decisions taken by students. AI can play a major role in monitoring day-to-day activities by identifying their panic attacks and alerting the responsible persons. This is achieved by sensing and monitoring facial expressions and human behavioral patterns. The AI system also influences their mind by enabling professional counselors to provide counseling on their mobile phones to soothe their minds and avoid harsh thoughts. As the AI system is made to be controlled within a private network to enhance privacy, the choice of alerting personal and counselor options can be availed by their parents. This AI-technology-based system not only saves students from panic but also promotes a positive personality. According to several studies, students across the globe are victims of panic situations. Hence, using this expression sensing system the right way can save thousands of lives across the globe. © 2022 M Y U Scientific Publishing Division. All rights reserved.","artificial intelligence; facial expressions; human behavioral patterns; panic; peer pressure","Artificial intelligence; Behavioral research; Artificial intelligence systems; Behavioral patterns; Facial Expressions; Human behavioral pattern; Human monitoring; Human recognition; Panic; Panic attacks; Peer pressure; Teachers'; Students"
"Aishwarya Gowda A.G., Su H.-K., Kuo W.-K., Santoso H.D.","Monitoring and Alerting Panic Situations in Students Using Artificial Intelligence","10.1109/ECEI53102.2022.9829506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136122126&doi=10.1109%2fECEI53102.2022.9829506&partnerID=40&md5=a6d2e444c720c25c244afde0402cdc42","The role of Artificial Intelligence in human monitoring and recognition is taking advanced steps on every progress. This technology makes a greater impact on student's life in helping parents and teachers understand and realize their panic situations. Several students are losing lives due to unbearable panic attacks and tough decisions they make due to their panic phase. In the current education system, many students face panic due to fear of exams, failure in exams, bullying, and so on. If this situation is recognized at its initial stage, it can help the guardians to save them from any dangerous decisions or situations. The AI can play a major role in monitoring the day-to-day activities by identifying their panic and alerting the responsible person. This is achieved by monitoring facial expressions and human behavioral patterns. The AI system also equips their mind by providing the counseled content on their mobile phones provided by professional counselors to soothe their minds and avoid harsh thoughts. As the AI system is made to control within a private network to enhance privacy, the choice of alerting personal and counseling options can be availed by their parents. This Artificial Intelligence technology-based system not only saves students from panic but also promotes a positive personality. According to several studies, students across the globe are victims of panic situations. Hence, using this system the right way can save hundreds of lives across the globe. © 2022 IEEE.","Artificial Intelligence; Facial Expressions; Human Behavioral Patterns; Panic; Peer Pressure","Artificial intelligence; Behavioral research; AI systems; Behavioral patterns; Facial Expressions; Human behavioral pattern; Human monitoring; Human recognition; Panic; Panic attacks; Peer pressure; Teachers'; Students"
"Aitken M., Toreini E., Carmichael P., Coopamootoo K., Elliott K., van Moorsel A.","Establishing a social licence for Financial Technology: Reflections on the role of the private sector in pursuing ethical data practices","10.1177/2053951720908892","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081301849&doi=10.1177%2f2053951720908892&partnerID=40&md5=803f79cce4acb3a041ac695a757b0d78","Current attention directed at ethical dimensions of data and Artificial Intelligence have led to increasing recognition of the need to secure and maintain public support for uses (and reuses) of people’s data. This is essential to establish a “Social Licence” for current and future practices. The notion of a “Social Licence” recognises that there can be meaningful differences between what is legally permissible and what is socially acceptable. Establishing a Social Licence entails public engagement to build relationships of trust and ensure that practices align with public values. While the concept of the Social Licence is well-established in other sectors – notably in relation to extractive industries – it has only very recently begun to be discussed in relation to digital innovation and data-intensive industries. This article therefore draws on existing literature relating to the Social Licence in extractive industries to explore the potential approaches needed to establish a Social Licence for emerging data-intensive industries. Additionally, it draws on well-established literature relating to trust (from psychology and organisational science) to examine the relevance of trust, and trustworthiness, for emerging practices in data-intensive industries. In doing so the article considers the extent to which pursuing a Social Licence might complement regulation and inform codes of practice to place ethical and social considerations at the heart of industry practice. We focus on one key industry: Financial Technology. We demonstrate the importance of combining technical and social approaches to address ethical challenges in data-intensive innovation (particularly relating to Artificial Intelligence) and to establish relationships of trust to underpin a Social Licence for Financial Technology. Such approaches are needed across all areas and industries of data-intensive innovation to complement regulation and inform the development of ethical codes of practice. This is important to underpin culture change and to move beyond rhetorical commitments to develop best practice putting ethics at the heart of innovation. © The Author(s) 2020.","data; ethics; Financial Technology; responsible artificial intelligence; social licence; trust",
"Aïvodji U., Ferry J., Gambs S., Huguet M.-J., Siala M.","Leveraging Integer Linear Programming to Learn Optimal Fair Rule Lists","10.1007/978-3-031-08011-1_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132975428&doi=10.1007%2f978-3-031-08011-1_9&partnerID=40&md5=bb381f7b9c555979b5c35ebb363bfb59","Fairness and interpretability are fundamental requirements for the development of responsible machine learning. However, learning optimal interpretable models under fairness constraints has been identified as a major challenge. In this paper, we investigate and improve on a state-of-the-art exact learning algorithm, called CORELS, which learns rule lists that are certifiably optimal in terms of accuracy and sparsity. Statistical fairness metrics have been integrated incrementally into CORELS in the literature. This paper demonstrates the limitations of such an approach for exploring the search space efficiently before proposing an Integer Linear Programming method, leveraging accuracy, sparsity and fairness jointly for better pruning. Our thorough experiments show clear benefits of our approach regarding the exploration of the search space. © 2022, Springer Nature Switzerland AG.","Fairness; Interpretability; Machine learning; Rule lists","Integer programming; Learning algorithms; Exact learning; Fairness; Fairness constraints; Integer Linear Programming; Interpretability; Learn+; Machine-learning; Rule list; Search spaces; State of the art; Machine learning"
"Aïvodji U., Ferry J., Gambs S., Huguet M.-J., Siala M.","FairCORELS, an Open-Source Library for Learning Fair Rule Lists","10.1145/3459637.3481965","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119200824&doi=10.1145%2f3459637.3481965&partnerID=40&md5=a0688e6c3cd8d030461d06ad41202fe7","FairCORELS is an open-source Python module for building fair rule lists. It is a multi-objective variant of CORELS, a branch-and-bound algorithm to learn certifiably optimal rule lists. FairCORELS supports six statistical fairness metrics, proposes several exploration parameters and leverages on the fairness constraints to prune the search space efficiently. It can easily generate sets of accuracy-fairness trade-offs. The models learnt are interpretable by design and a sparsity parameter can be used to control their length. © 2021 ACM.","fairness; interpretability; machine learning","Branch and bound method; Machine learning; Open source software; Branch-and-bound algorithms; Fairness; Fairness constraints; Interpretability; Learn+; Multi objective; Open-source; Open-source libraries; Search spaces; Trade off; Economic and social effects"
"Ajalloeian A., Moosavi-Dezfooli S.-M., Vlachos M., Frossard P.","On Smoothed Explanations: Quality and Robustness","10.1145/3511808.3557409","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140853627&doi=10.1145%2f3511808.3557409&partnerID=40&md5=a25bb975698e36a18edab603f50ef452","Explanation methods highlight the importance of the input features in taking a predictive decision, and represent a solution to increase the transparency and trustworthiness in machine learning and deep neural networks (DNNs). However, explanation methods can be easily manipulated generating misleading explanations particularly under visually imperceptible adversarial perturbations. Recent work has identified the decision surface geometry of DNNs as the main cause of this phenomenon. To make explanation methods more robust against adversarially crafted perturbations, recent research has promoted several smoothing approaches. These approaches smooth either the explanation map or the decision surface. In this work, we initiate a very thorough evaluation of the quality and robustness of the explanations offered by smoothing approaches. Different properties are evaluated. We present settings in which the smoothed explanations are both better, and worse, than the explanations derived by the commonly-used (non-smoothed) Gradient explanation method. By making the connection with the literature on adversarial attacks, we demonstrate that such smoothed explanations are robust primarily against additive attacks. However, a combination of additive and non-additive attacks can still manipulate these explanations, revealing important shortcomings in their robustness properties. © 2022 ACM.","adversarial robustness; explainable ai; gradient-based explanations; neural networks; robust explanations; transparency","Additives; Deep neural networks; Quality control; Additive attacks; Adversarial robustness; Decision surfaces; Explainable ai; Gradient based; Gradient-based explanation; Input features; Machine-learning; Neural-networks; Robust explanation; Transparency"
"Ajiboye A., Babalola K.","Determining mice sex from chest X-rays using deep learning","10.1109/CYBERNIGERIA51635.2021.9428822","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107509435&doi=10.1109%2fCYBERNIGERIA51635.2021.9428822&partnerID=40&md5=a557c990fd4795ade64f21195a487ecd","This Following on from work by Babalola et al. It is shown that the sex of mice can be determined from x-ray images of the chest region alone using convolutional neural networks. The anatomical differences that may be responsible for this is further sinvestigated, as it may be useful in determining phenotype changes caused by knocking out genes - hence in understanding genotype-phenotype effects. Our results indicate that the cervical vertebrae may play an important role in the ability of our convolutional neural network to classify the sex of mice correctly using only x-rays of the chest region. © 2021 IEEE.","CNN; Genotype; Knockout; Phenotype","Computers; Convolution; Convolutional neural networks; Mammals; Cervical vertebra; Chest x-rays; X-ray image; Deep learning"
"Ajiboye A.R., Arshah R.A., Qin H.","A novel approach to efficient exploration and mining of students' data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941084002&partnerID=40&md5=447d93460349f1c2ec96b81df4ae99c5","Several undergraduate students most especially the new students do have an unacceptable Grade Point Average (GPA) at the end of their first year. Research has shown that during this period, stress by this set of students sometime responsible for their poor performance as they are faced with many challenges. Having knowledge about their previous academic knowledge can bring about a workable solution to prevent this trend. In this paper, a novel approach that efficiently explores students' data for the prediction of their performance based on their historical data is proposed. A prediction system is designed in line with the concept of the Unified Modelling Language (UML) and it is implemented using PHP; while MySQL serves as the back-end. The developed model is tested and a more satisfactory result is achieved when compared with similar models emanated from using machine learning techniques. The resulting outputs of this study unveil the academic achievement of each student prior to their first registration in the university. The model developed is also found to be useful for efficient planning and quick decision making that can ensure a sustainable educational growth. © 2005 - 2015 JATIT & LLS. All rights reserved.","Data exploration; Educational data mining; Predictive model; Student's achievement; UML",
"Ajjour F., Kurdy M.B.","Face recognition for online users’ authentication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082202153&partnerID=40&md5=2b727b4c9d03365403b3d0e2e140b133","In the last decade, advancement in Artificial Intelligence attracted a lot of experts that lead to massive growth and advancement in all human life aspects. Therefore, one of the key fields to point at, which attracted a lot of attention and development lately, is Face Recognition. In recent years, Face Recognition tends to be one of the most widely used technologies in many different domains and workspaces, such as emotional recognition, security, health sector, marketing, and retail, etc. this approach will consist of an online system with real-time functionality (close to real-time), that will be responsible for the declaration of users to be recognized later. Based on the recognition results, the system will then grant the users the needed authentication. In this research, various different challenges related to the development and the use of Face Recognition, including the variations in light conditions, camera resolution, processing power, facial changes over time, number of users to be recognized, etc... During this work, “Viola and Jones” and “MTCNN” were used for face detection, and “FaceNet” was applied for facial features extraction. Also, similarity neural network (Similarity Net) has been created to regress similarity percent between user’s features’ vectors, beside it has been trained on user’s features by exploiting the Euclidian distance between embeddings. This approach was tested on a group of datasets - personal, Kaggle and LFW dataset. The tests returned 100% successful recognitions on personal and Kaggle dataset, and 99.5% on LFW dataset. © 2005 – ongoing JATIT & LLS.","CLAHE (Contrast Limited Adaptive Histogram); CNN (Convolutional Neural Network); Embeddings; Face Authentication; Face Detection; Face Recognition; Facenet; Feature Vectors; Histogram Equalization; Kaggle (Website for AI Contests); LFW (Labelled Faces in The Wild); MTCNN (Multi-Task Cascaded Convolutional Neural Networks for Face Detection)",
"Akagi T., Masuda K., Kuwada E., Takeshita K., Kawakatsu T., Ariizumi T., Kubo Y., Ushijima K., Uchida S.","Genome-wide cis-decoding for expression design in tomato using cistrome data and explainable deep learning","10.1093/plcell/koac079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131106159&doi=10.1093%2fplcell%2fkoac079&partnerID=40&md5=d5443d61ce3c34842b57df4dbeb2fdae","In the evolutionary history of plants, variation in cis-regulatory elements (CREs) resulting in diversification of gene expression has played a central role in driving the evolution of lineage-specific traits. However, it is difficult to predict expression behaviors from CRE patterns to properly harness them, mainly because the biological processes are complex. In this study, we used cistrome datasets and explainable convolutional neural network (CNN) frameworks to predict genome-wide expression patterns in tomato (Solanum lycopersicum) fruit from the DNA sequences in gene regulatory regions. By fixing the effects of trans-acting factors using single cell-type spatiotemporal transcriptome data for the response variables, we developed a prediction model for crucial expression patterns in the initiation of tomato fruit ripening. Feature visualization of the CNNs identified nucleotide residues critical to the objective expression pattern in each gene, and their effects were validated experimentally in ripening tomato fruit. This cis-decoding framework will not only contribute to the understanding of the regulatory networks derived from CREs and transcription factor interactions, but also provides a flexible means of designing alleles for optimized expression. © 2022 The Author(s). Published by Oxford University Press on behalf of American Society of Plant Biologists.",,"plant protein; transcription factor; fruit; gene expression regulation; genetics; metabolism; regulatory sequence; tomato; Deep Learning; Fruit; Gene Expression Regulation, Plant; Lycopersicon esculentum; Plant Proteins; Regulatory Sequences, Nucleic Acid; Transcription Factors"
"Akagi T., Onishi M., Masuda K., Kuroki R., Baba K., Takeshita K., Suzuki T., Niikawa T., Uchida S., Ise T.","Explainable deep learning reproduces a 'Professional eye' on the diagnosis of internal disorders in persimmon fruit","10.1093/pcp/pcaa111","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099072828&doi=10.1093%2fpcp%2fpcaa111&partnerID=40&md5=5da539e1fa9a33b383c96745bdc05b26","Recent rapid progress in deep neural network techniques has allowed recognition and classification of various objects, often exceeding the performance of the human eye. In plant biology and crop sciences, some deep neural network frameworks have been applied mainly for effective and rapid phenotyping. In this study, beyond simple optimizations of phenotyping, we propose an application of deep neural networks to make an image-based internal disorder diagnosis that is hard even for experts, and to visualize the reasons behind each diagnosis to provide biological interpretations. Here, we exemplified classification of calyx-end cracking in persimmon fruit by using five convolutional neural network models with various layer structures and examined potential analytical options involved in the diagnostic qualities. With 3,173 visible RGB images from the fruit apex side, the neural networks successfully made the binary classification of each degree of disorder, with up to 90% accuracy. Furthermore, feature visualizations, such as Grad-CAM and LRP, visualize the regions of the image that contribute to the diagnosis. They suggest that specific patterns of color unevenness, such as in the fruit peripheral area, can be indexes of calyx-end cracking. These results not only provided novel insights into indexes of fruit internal disorders but also proposed the potential applicability of deep neural networks in plant biology. © 2020 The Author(s) 2020. Published by Oxford University Press on behalf of Japanese Society of Plant Physiologists. All rights reserved.","Artificial intelligence; Backpropagation; Convolutional neural network; Image diagnosis; Physiological disorder","anatomy and histology; computer assisted diagnosis; Diospyros; flower; fruit; plant disease; Deep Learning; Diospyros; Flowers; Fruit; Image Interpretation, Computer-Assisted; Neural Networks, Computer; Plant Diseases"
"Akamine Y., Ueda Y., Ueno Y., Sofue K., Murakami T., Yoneyama M., Obara M., Van Cauteren M.","Application of hierarchical clustering to multi-parametric MR in prostate: Differentiation of tumor and normal tissue with high accuracy","10.1016/j.mri.2020.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091258999&doi=10.1016%2fj.mri.2020.09.011&partnerID=40&md5=4b1eaeb67cc24e620b557ef56148b107","Purpose: Hierarchical clustering (HC), an unsupervised machine learning (ML) technique, was applied to multi-parametric MR (mp-MR) for prostate cancer (PCa). The aim of this study is to demonstrate HC can diagnose PCa in a straightforward interpretable way, in contrast to deep learning (DL) techniques. Methods: HC was constructed using mp-MR including intravoxel incoherent motion, diffusion kurtosis imaging, and dynamic contrast-enhanced MRI from 40 tumor and normal tissues in peripheral zone (PZ) and 23 tumor and normal tissues in transition zone (TZ). HC model was optimized by assessing the combinations of several dissimilarity and linkage methods. Goodness of HC model was validated by internal methods. Results: Accuracy for differentiating tumor and normal tissue by optimal HC model was 96.3% in PZ and 97.8% in TZ, comparable to current clinical standards. Relationship between input (DWI and permeability parameters) and output (tumor and normal tissue cluster) was shown by heat maps, consistent with literature. Conclusion: HC can accurately differentiate PCa and normal tissue, comparable to state-of-the-art diffusion based parameters. Contrary to DL techniques, HC is an operator-independent ML technique producing results that can be interpreted such that the results can be knowledgeably judged. © 2020 Elsevier Inc.","Cluster analysis; Hierarchical clustering; Interpretability; Machine learning; Multi-parametric magnetic resonance imaging; Prostate cancer","adult; aged; Article; controlled study; diagnostic accuracy; diagnostic imaging; diffusion kurtosis imaging; dynamic contrast-enhanced magnetic resonance imaging; hierarchical clustering; human; human tissue; image processing; intermethod comparison; intravoxel incoherent motion; major clinical study; male; middle aged; multiparametric magnetic resonance imaging; priority journal; process optimization; prostate cancer; retrospective study; tumor differentiation; cluster analysis; cytology; diagnostic imaging; nuclear magnetic resonance imaging; pathology; procedures; prostate; prostate tumor; sensitivity and specificity; unsupervised machine learning; Aged; Cluster Analysis; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Prostate; Prostatic Neoplasms; Sensitivity and Specificity; Unsupervised Machine Learning"
"Akanmu A.A., Olayiwola J., Ogunseiju O., McFeeters D.","Cyber-physical postural training system for construction workers","10.1016/j.autcon.2020.103272","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085250970&doi=10.1016%2fj.autcon.2020.103272&partnerID=40&md5=2603b56272d854e22bd962cc1f2b7edf","Risks of work-related musculoskeletal injuries can be reduced by adequately training construction workers on performing work in safe postures. Traditional training approaches provide limited support for performing work tasks and receiving real-time feedback. This paper describes a cyber-physical postural training environment where workers can practice to perform work with reduced ergonomic risks. The proposed system uses wearable sensors, Vive trackers, machine learning and virtual reality to track body kinematics, and engagement with physical construction resources, and provides feedback via an interactive user interface. The postural training system was developed for training workers engaged in wood frame construction. User study of the effectiveness of the feedback and user interface was conducted. Findings showed that the user interface was perceived as convenient with limited interference with the workspace. The feedback was understandable in learning risks associated with participant's postures. Further research will involve conducting formative workload evaluation of the user interface and developing a reinforcement learning model for adapting the feedback based on the state of learning. © 2020","Cyber-physical systems; Postural training; Virtual reality; Wearable sensors; Work-related musculoskeletal disorders","Cyber Physical System; Feedback; Interface states; Learning systems; Occupational risks; Reinforcement learning; Wooden construction; Construction resource; Construction workers; Interactive user interfaces; Musculo-skeletal injuries; Real-time feedback; Reinforcement learning models; Wood frame construction; Workload evaluation; User interfaces"
"Akata Z., Balliet D., De Rijke M., Dignum F., Dignum V., Eiben G., Fokkens A., Grossi D., Hindriks K., Hoos H., Hung H., Jonker C., Monz C., Neerincx M., Oliehoek F., Prakken H., Schlobach S., Van Der Gaag L., Van Harmelen F., Van Hoof H., Van Riemsdijk B., Van Wynsberghe A., Verbrugge R., Verheij B., Vossen P., Welling M.","A Research Agenda for Hybrid Intelligence: Augmenting Human Intellect with Collaborative, Adaptive, Responsible, and Explainable Artificial Intelligence","10.1109/MC.2020.2996587","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089339059&doi=10.1109%2fMC.2020.2996587&partnerID=40&md5=f2a1931d3ba0e87d381ccceda324a87f","We define hybrid intelligence (HI) as the combination of human and machine intelligence, augmenting human intellect and capabilities instead of replacing them and achieving goals that were unreachable by either humans or machines. HI is an important new research focus for artificial intelligence, and we set a research agenda for HI by formulating four challenges. © 1970-2012 IEEE.",,"Computer science; Computers; Hybrid intelligence; Machine intelligence; Research agenda; Research focus; Artificial intelligence"
"Akatsuka J., Yamamoto Y., Sekine T., Numata Y., Morikawa H., Tsutsumi K., Yanagi M., Endo Y., Takeda H., Hayashi T., Ueki M., Tamiya G., Maeda I., Fukumoto M., Shimizu A., Tsuzuki T., Kimura G., Kondo Y.","Illuminating clues of cancer buried in prostate MR image: Deep learning and expert approaches","10.3390/biom9110673","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074388438&doi=10.3390%2fbiom9110673&partnerID=40&md5=9908bb85e11771d2ee4f03b0b368eb4f","Deep learning algorithms have achieved great success in cancer image classification. However, it is imperative to understand the differences between the deep learning and human approaches. Using an explainable model, we aimed to compare the deep learning-focused regions of magnetic resonance (MR) images with cancerous locations identified by radiologists and pathologists. First, 307 prostate MR images were classified using a well-established deep neural network without locational information of cancers. Subsequently, we assessed whether the deep learning-focused regions overlapped the radiologist-identified targets. Furthermore, pathologists provided histopathological diagnoses on 896 pathological images, and we compared the deep learning-focused regions with the genuine cancer locations through 3D reconstruction of pathological images. The area under the curve (AUC) for MR images classification was sufficiently high (AUC = 0.90, 95% confidence interval 0.87-0.94). Deep learning-focused regions overlapped radiologist-identified targets by 70.5% and pathologist-identified cancer locations by 72.1%. Lymphocyte aggregation and dilated prostatic ducts were observed in non-cancerous regions focused by deep learning. Deep learning algorithms can achieve highly accurate image classification without necessarily identifying radiological targets or cancer locations. Deep learning may find clues that can help a clinical diagnosis even if the cancer is not visible. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Black box; Deep learning; MRI; Pathology; Prostate cancer","prostate specific antigen; aged; Article; cancer grading; cancer staging; contrast enhancement; controlled study; convolutional neural network; deep learning; diagnostic imaging; diagnostic test accuracy study; false positive result; Gleason score; histopathology; human; human tissue; image analysis; image processing; image reconstruction; major clinical study; male; male genital tract parameters; nuclear magnetic resonance imaging; prostate biopsy; prostate cancer; prostate specific antigen density; prostate volume; prostatectomy; radiologist; receiver operating characteristic; scoring system; tumor localization; diagnostic imaging; procedures; prostate tumor; Aged; Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Prostatic Neoplasms"
"Akbar A., Kousiouris G., Pervaiz H., Sancho J., Ta-Shma P., Carrez F., Moessner K.","Real-time probabilistic data fusion for large-scale IoT applications","10.1109/ACCESS.2018.2804623","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041823510&doi=10.1109%2fACCESS.2018.2804623&partnerID=40&md5=bc70ec0faa1b3d2ec5cd3404b9f1b545","Internet of Things (IoT) data analytics is underpinning numerous applications, however, the task is still challenging predominantly due to heterogeneous IoT data streams, unreliable networks, and ever increasing size of the data. In this context, we propose a two-layer architecture for analyzing IoT data. The first layer provides a generic interface using a service oriented gateway to ingest data from multiple interfaces and IoT systems, store it in a scalable manner and analyze it in real-time to extract high-level events; whereas second layer is responsible for probabilistic fusion of these high-level events. In the second layer, we extend state-of-the-art event processing using Bayesian networks in order to take uncertainty into account while detecting complex events. We implement our proposed solution using open source components optimized for large-scale applications. We demonstrate our solution on real-world use-case in the domain of intelligent transportation system where we analyzed traffic, weather, and social media data streams from Madrid city in order to predict probability of congestion in real-time. The performance of the system is evaluated qualitatively using a web-interface where traffic administrators can provide the feedback about the quality of predictions and quantitatively using F-measure with an accuracy of over 80%. © 2013 IEEE.","Complex event processing; Data analysis; Intelligent transportation systems; Internet of things; Real-time systems","Bayesian networks; Complex networks; Data fusion; Data handling; Data mining; Data reduction; Gateways (computer networks); Information analysis; Intelligent systems; Intelligent vehicle highway systems; Interactive computer systems; Internet of things; Meteorology; Probabilistic logics; Quality control; Transportation; Uncertainty analysis; Bayes method; Bayesian Networks (bns); Complex event processing; Intelligent transportation systems; Large-scale applications; Probabilistic data fusion; Quality of predictions; Uncertainty; Real time systems"
"Akbar S., Hayat M., Iqbal M., Jan M.A.","iACP-GAEnsC: Evolutionary genetic algorithm based ensemble classification of anticancer peptides by utilizing hybrid feature space","10.1016/j.artmed.2017.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021241117&doi=10.1016%2fj.artmed.2017.06.008&partnerID=40&md5=3814081ae224bd5fec0a8b77c3a1521f","Cancer is a fatal disease, responsible for one-quarter of all deaths in developed countries. Traditional anticancer therapies such as, chemotherapy and radiation, are highly expensive, susceptible to errors and ineffective techniques. These conventional techniques induce severe side-effects on human cells. Due to perilous impact of cancer, the development of an accurate and highly efficient intelligent computational model is desirable for identification of anticancer peptides. In this paper, evolutionary intelligent genetic algorithm-based ensemble model, ‘iACP-GAEnsC’, is proposed for the identification of anticancer peptides. In this model, the protein sequences are formulated, using three different discrete feature representation methods, i.e., amphiphilic Pseudo amino acid composition, g-Gap dipeptide composition, and Reduce amino acid alphabet composition. The performance of the extracted feature spaces are investigated separately and then merged to exhibit the significance of hybridization. In addition, the predicted results of individual classifiers are combined together, using optimized genetic algorithm and simple majority technique in order to enhance the true classification rate. It is observed that genetic algorithm-based ensemble classification outperforms than individual classifiers as well as simple majority voting base ensemble. The performance of genetic algorithm-based ensemble classification is highly reported on hybrid feature space, with an accuracy of 96.45%. In comparison to the existing techniques, ‘iACP-GAEnsC’ model has achieved remarkable improvement in terms of various performance metrics. Based on the simulation results, it is observed that ‘iACP-GAEnsC’ model might be a leading tool in the field of drug design and proteomics for researchers. © 2017 Elsevier B.V.","Am-PseAAC; Anticancer; Genetic algorithm; Majority voting; SVM","Amino acids; Chemotherapy; Diseases; Evolutionary algorithms; Genetic algorithms; Molecular biology; Peptides; Proteins; Anticancer; Conventional techniques; Ensemble classification; Feature representation; Intelligent computational model; Intelligent genetic algorithm; Majority voting; Pseudo Amino Acid Compositions; Bioinformatics; amphiphilic pseudo amino acid; anticancer peptide; antifreeze protein; antineoplastic agent; pseudo g gap dipeptide; reduced amino acid alphabet; unclassified drug; amino acid; antineoplastic agent; peptide; amino acid composition; amino acid sequence; Article; cellular distribution; classification algorithm; controlled study; developed country; DNA recombination; drug design; evolutionary algorithm; genetic algorithm; machine learning; mathematical model; pattern recognition; prediction; priority journal; proteomics; support vector machine; algorithm; biology; computer simulation; human; neoplasm; sequence analysis; Algorithms; Amino Acids; Antineoplastic Agents; Computational Biology; Computer Simulation; Humans; Neoplasms; Peptides; Sequence Analysis, Protein"
"Akbari A., Martinez J., Jafari R.","A Meta-Learning Approach for Fast Personalization of Modality Translation Models in Wearable Physiological Sensing","10.1109/JBHI.2021.3105055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113230919&doi=10.1109%2fJBHI.2021.3105055&partnerID=40&md5=5c725a0fdc1e7d2e0122dc4bb3ade8b1","Modality translation grants diagnostic value to wearable devices by translating signals collected from low-power sensors to their highly-interpretable counterparts that are more familiar to healthcare providers. For instance, bio-impedance (Bio-Z) is a conveniently collected modality for measuring physiological parameters but is not highly interpretable. Thus, translating it to a well-known modality such as electrocardiogram (ECG) improves the usability of Bio-Z in wearables. Deep learning solutions are well-suited for this task given complex relationships between modalities generated by distinct processes. However, current algorithms usually train a single model for all users that results in ignoring cross-user variations. Retraining for new users usually requires collecting abundant labeled data, which is challenging in healthcare applications. In this paper, we build a modality translation framework to translate Bio-Z to ECG by learning personalized user information without training several independent architectures. Furthermore, our framework is able to adapt to new users in testing using very few samples. We design a meta-learning framework that contains shared and user-specific parameters to account for user differences while learning from the similarity amongst user signals. In this model, a meta-learner approximated by a neural network learns how to learn user-specific parameters and can efficiently update them in testing. Our experiments show that the proposed model reduces the percentage root mean square difference (PRD) by 41% compared to training a single model for all users and by 36% compared to training independent models for each user. When adapting the model to new users, our model outperforms fine-tuning a pre-trained model through back-propagation by 40% using as few as two new samples in testing. © 2013 IEEE.","bioimpedance; meta learning; Modality translation; wearable physiological sensing","Backpropagation; Deep learning; Electrocardiography; Health care; Learning systems; Mean square error; Physiology; Wearable technology; Complex relationships; Health care application; Health care providers; Meta-learning approach; Meta-learning frameworks; Physiological parameters; Physiological sensing; Root mean square errors; Physiological models; accuracy; algorithm; Article; blood glucose monitoring; blood pressure measurement; breathing rate; calibration; conceptual framework; convolutional neural network; electrocardiogram; health care personnel; health data; heart cycle; heart rate; human; impedance; knowledge; machine learning; mathematical model; mechanosensing; Meta Learning; modality translation framework; nerve cell network; parameters; photoelectric plethysmography; prediction; root mean squared error; training; Wearable Physiological Sensing; electrocardiography; electronic device; Algorithms; Electrocardiography; Humans; Neural Networks, Computer; Wearable Electronic Devices"
"Akbarzadeh Khorshidi H., Hassani-Mahmooei B., Haffari G.","An Interpretable Algorithm on Post-injury Health Service Utilization Patterns to Predict Injury Outcomes","10.1007/s10926-019-09863-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074584745&doi=10.1007%2fs10926-019-09863-0&partnerID=40&md5=60552c4e4072e2838456c2bf9656d938","Purpose Post-injury health service utilization (HSU) contributes to injury outcomes, but limited studies investigated their relationship. This study aims to group injured patients in transport accidents based on minimal historical information of their HSU so that the groups are meaningfully associated with the outcome of interest. Methods The data include 20,692 injured patients who had compensation claims over 3 years. We propose a hybrid approach, combining unsupervised and supervised machine learning methods. Based on the first week post-injury data, we identify a proper clustering of patients best associated with total cost to recovery, as well as the discovery of HSU patterns. This allows developing models to accurately predict the outcome of interest using the discovered patterns. Furthermore, we propose to use decision tree classifiers to accurately classify future patients into the discovered clusters using their first week post-injury information. Results Our hybrid approach has identified eight patient groups. The compactness of the resulted clusters, assessed by Average Silhouette Width metric, is 0.71 indicating well-defined clusters. The resulted patient groups are highly predictive of injury outcomes. They improve the cost predictability more than twice in comparison with predictors such as gender, age and injury type. These groups also have substantial association with patients’ recovery. The transparency and interpretability of decision trees allow integrating the resulting classification rules conveniently in operational processes. Conclusions This study provides a framework to discover knowledge and useful insights for health service providers and policy makers to control injury outcomes, and consequently to reduce the severity of transport accidents. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Accident injury; Clustering; Decision tree; Health service utilization; Interpretable modeling; Pattern recognition","algorithm; cluster analysis; compensation; health service; human; injury; Algorithms; Cluster Analysis; Compensation and Redress; Health Services; Humans; Wounds and Injuries"
"Akbarzadeh S., Ahderom S., Alameh K.","A statistical approach to provide explainable convolutional neural network parameter optimization","10.2991/ijcis.d.191219.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078248102&doi=10.2991%2fijcis.d.191219.001&partnerID=40&md5=e13e3d0feafbc6e63e442e68604ec599","Algorithms based on convolutional neural networks (CNNs) have been great attention in image processing due to their ability to find patterns and recognize objects in a wide range of scientific and industrial applications. Finding the best network and optimizing its hyperparameters for a specific application are central challenges for CNNs. Most state-of-the-art CNNs are manually designed, while techniques for automatically finding the best architecture and hyperparameters are computationally intensive, and hence, there is a need to severely limit their search space. This paper proposes a fast statistical method for CNN parameter optimization, which can be applied in many CNN applications and provides more explainable results. The authors specifically applied Taguchi based experimental designs for network optimization in a basic network, a simplified Inception network and a simplified Resnet network, and conducted a comparison analysis to assess their respective performance and then to select the hyperparameters and networks that facilitate faster training and provide better accuracy. The results show that up to a 6% increase in classification accuracy can be achieved after parameter optimization. © 2019 The Authors. Published by Atlantis Press SARL.","Convolutional neural network; Deep learning; Design of experiment; Hyperparameter; Optimization; Taguchi","Deep learning; Deep neural networks; Design of experiments; Image processing; Neural networks; Optimization; Taguchi methods; Classification accuracy; Comparison analysis; Convolutional neural network; Hyper-parameter; Network optimization; Parameter optimization; State of the art; Statistical approach; Convolution"
"Akben S.B.","A novel multi-objective optimization method: Input values effect on responses (IVER)","10.1002/nme.6511","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091499049&doi=10.1002%2fnme.6511&partnerID=40&md5=f9894d46aa28297e096f51808ea9ea1a","In this study, a novel multi-objective optimization method based on the best effect of unique input (independent variable) values on responses (dependent variables) was proposed. The proposed method was compared with optimization using Derringer & Suich function that is still the most used. The comparison was made using the response values measured in real experiments and available in the literature. The advantages of the proposed method such as not needing the polynomial model aiming to predict the response values, no parameter selection problem, being able to offer optimum range instead of single optimum value, being suitable for use with existing experimental designs and being simple and interpretable were demonstrated as a result of comparison. It was also suggested how the proposed method will be effective according to experimental designs, and application for the users' application was presented. © 2020 John Wiley & Sons Ltd","coefficients of fit; data mining; modeling; multi-objective optimization; prediction","Statistics; Dependent variables; Independent variables; Input values; Optimum value; Parameter selection; Polynomial modeling; Multiobjective optimization"
"Akdemir B., Doʇan S., Aksoy M.H., Canli E., Özgören M.","Artificial frame filling using adaptive neural fuzzy inference system for particle image velocimetry dataset","10.1117/12.2179689","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925428034&doi=10.1117%2f12.2179689&partnerID=40&md5=1cb39bb86636c9ed973f251666f98054","Liquid behaviors are very important for many areas especially for Mechanical Engineering. Fast camera is a way to observe and search the liquid behaviors. Camera traces the dust or colored markers travelling in the liquid and takes many pictures in a second as possible as. Every image has large data structure due to resolution. For fast liquid velocity,v there is not easy to evaluate or make a fluent frame after the taken images. Artificial intelligence has much popularity in science to solve the nonlinear problems. Adaptive neural fuzzy inference system is a common artificial intelligence in literature. Any particle velocity in a liquid has two dimension speed and its derivatives. Adaptive Neural Fuzzy Inference System has been used to create an artificial frame between previous and post frames as offline. Adaptive neural fuzzy inference system uses velocities and vorticities to create a crossing point vector between previous and post points. In this study, Adaptive Neural Fuzzy Inference System has been used to fill virtual frames among the real frames in order to improve image continuity. So this evaluation makes the images much understandable at chaotic or vorticity points. After executed adaptive neural fuzzy inference system, the image dataset increase two times and has a sequence as virtual and real, respectively. The obtained success is evaluated using R2 testing and mean squared error. R2 testing has a statistical importance about similarity and 0.82, 0.81, 0.85 and 0.8 were obtained for velocities and derivatives, respectively. © 2015 SPIE.","Artificial neural network; frame; Particle image velocimetry; sphere; vector","Artificial intelligence; Cameras; Flow visualization; Fuzzy logic; Fuzzy systems; Image processing; Liquids; Mean square error; Neural networks; Spheres; Vectors; Velocimeters; Velocity; Velocity control; Velocity measurement; Vorticity; Adaptive neural fuzzy inference system (ANFIS); frame; Liquid behavior; Liquid velocities; Mean squared error; Nonlinear problems; Particle image velocimetries; Particle velocities; Fuzzy inference"
"Akgobek O.","A hybrid approach for improving the accuracy of classification algorithms in data mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862564076&partnerID=40&md5=6364921d8f200f457e4aef7216950166","Classification and rule induction are two important methods/processes to extract knowledge from data. In rule induction, the representation of knowledge is defined as IF-THEN rules which are easily understandable and applicable by problem-domain experts. Classification is to organize a large data set objects into predefined classes, described by a set of attributes, using supervised learning methods. The objective of this study is to present a new classification algorithm, RES (Rule Extraction System), for automatic knowledge acquisition in data mining. It aims at eliminating the pitfalls and the disadvantages of the techniques and algorithms currently in use. The proposed algorithm makes use of the direct rule extraction approach, rather than the decision tree. For this purpose, it uses a set of examples to induce general rules. In this study, the rule base is created through the knowledge discovery by employing RES algorithm, a data mining technique, on the sample sets of the Wisconsin Breast Cancer, Ljubljana Breast Cancer, Dermatology, Hepatitis, Iris, Tic-Tac-Toe, Nursery, Lympograph, CRX and Diabetes, which are real life data and commonly used in the machine learning. In terms of the accuracy rate, the results of this study were compared to the results of the algorithms widely used in this field, such as C4.5, NavieBayes, PART, CN2, CORE, GA-SVM. The proposed algorithm showed promising results. © Sila Science.","Attribute selection; Classification; Data mining; Knowledge acquisition","Accuracy rate; Attribute selection; Automatic knowledge acquisition; Breast Cancer; Classification algorithm; Data mining techniques; Hybrid approach; If-then rules; Large data; Pre-defined class; Problem-domain; Real life data; Rule base; Rule extraction; Rule induction; Sample sets; Supervised learning methods; WISCONSIN; Classification (of information); Decision trees; Diseases; Genetic algorithms; Knowledge acquisition; Knowledge representation; Learning systems; Data mining"
"Akhavian R., Behzadan A.H.","Evaluation of queuing systems for knowledge-based simulation of construction processes","10.1016/j.autcon.2014.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905658355&doi=10.1016%2fj.autcon.2014.07.007&partnerID=40&md5=63289775154cbd6d50e11f56dc14dabb","During the course of a construction project, there are many situations in which formation of waiting lines or queues is inevitable. The effect of resource delays in queues on the overall project completion time and cost has motivated researchers to employ simulation for analysis of queuing systems in order to identify the best operational strategies to reduce the time wasted in queues. Providing proper and timely input data with high spatial and temporal accuracy for queuing systems simulation enhances the reliability of decisions made based upon the simulation output. Hence, the presented paper describes a methodology for collecting and mining of spatio-temporal data corresponding to the interactions of queue entities to extract computer interpretable knowledge for simulation input modeling. The developed framework was validated using empirical datasets collected from a series of experiments. The extracted relevant knowledge from the queuing system entities was used to update corresponding simulation models. © 2014 Elsevier B.V.","Construction; Data mining; Discrete event simulation; Infrastructure; Knowledge; Queue; Queue discipline; Simulation; Ultra wideband; Wireless data collection","Construction; Discrete event simulation; Knowledge based systems; Queueing networks; Queueing theory; Ultra-wideband (UWB); Data mining; Discrete event simulation; Ultra-wideband (UWB); Infrastructure; Knowledge; Queue; Queue disciplines; Simulation; Wireless data collection; Data mining; Construction"
"Akhavian R., Behzadan A.H.","Knowledge-based simulation modeling of construction fleet operations using multimodal-process data mining","10.1061/(ASCE)CO.1943-7862.0000775","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886507375&doi=10.1061%2f%28ASCE%29CO.1943-7862.0000775&partnerID=40&md5=73fce2cc7bcadcdc27ccdec1d82376ae","In order to develop a realistic simulation model, it is critical to provide the model with factual input data based on the interactions and events that take place between real entities. However, the existing trend in simulation of construction fleet activities is based on estimating input parameters such as activity durations using expert judgments and assumptions. Not only may such estimations not be precise, but project dynamics can influence model parameters beyond expectation. Therefore, the simulation model may not be a proper and reliable representation of the real engineering system. In order to alleviate these issues and improve the current practice of construction simulation, a thorough approach is needed that enables the integration of field data into simulation modeling and systematic refinement of the resulting models. This paper describes the latest efforts by authors to design and test a novel methodology for multimodal-process data capturing, fusion, and mining that provides a solid basis for automated generation and refinement of simulation models that realistically represent construction fleet operations. Different modes of operational data are collected and fused to facilitate the discovery of operational knowledge required to create realistic simulation models. The developed algorithms are validated using laboratory scale experiments and analytical results are also provided. The main contribution of this research to the body of knowledge is that it lays the foundation to systematically investigate whether it is possible to robustly discover computer-interpretable knowledge patterns from heterogeneous field data in order to create or refine realistic simulation models from complex, unstructured, and evolving operations such as heavy construction and infrastructure projects. © 2013 American Society of Civil Engineers.","Construction; Data driven; Data fusion; Data mining; Earthmoving; Heavy equipment; Knowledge discovery; Real time; Simulation","Data driven; Earthmoving; Heavy equipment; Real time; Simulation; Construction; Construction equipment; Data fusion; Data mining; Estimation; Knowledge based systems; Parameter estimation; Computer simulation"
"Akhavizadegan F., Ansarifar J., Wang L., Huber I., Archontoulis S.V.","A time-dependent parameter estimation framework for crop modeling","10.1038/s41598-021-90835-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107403375&doi=10.1038%2fs41598-021-90835-x&partnerID=40&md5=948cbe0c8f207da7a906a7c1cfeaf164","The performance of crop models in simulating various aspects of the cropping system is sensitive to parameter calibration. Parameter estimation is challenging, especially for time-dependent parameters such as cultivar parameters with 2–3 years of lifespan. Manual calibration of the parameters is time-consuming, requires expertise, and is prone to error. This research develops a new automated framework to estimate time-dependent parameters for crop models using a parallel Bayesian optimization algorithm. This approach integrates the power of optimization and machine learning with prior agronomic knowledge. To test the proposed time-dependent parameter estimation method, we simulated historical yield increase (from 1985 to 2018) in 25 environments in the US Corn Belt with APSIM. Then we compared yield simulation results and nine parameter estimates from our proposed parallel Bayesian framework, with Bayesian optimization and manual calibration. Results indicated that parameters calibrated using the proposed framework achieved an 11.6% reduction in the prediction error over Bayesian optimization and a 52.1% reduction over manual calibration. We also trained nine machine learning models for yield prediction and found that none of them was able to outperform the proposed method in terms of root mean square error and R2. The most significant contribution of the new automated framework for time-dependent parameter estimation is its capability to find close-to-optimal parameters for the crop model. The proposed approach also produced explainable insight into cultivar traits’ trends over 34 years (1985–2018). © 2021, The Author(s).",,"algorithm; article; calibration; crop; cultivar; machine learning; nonhuman; prediction; simulation"
"Akhmedova S., Stanovov V., Vishnevskaya S., Miyajima C., Kamiya Y.","Automatically generated data mining tools for complex system operator's condition detection using non-contact vital sensing","10.1587/transcom.2020HMI0001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107857124&doi=10.1587%2ftranscom.2020HMI0001&partnerID=40&md5=1aa13afa5303b027a14e65c922073113","This study is focused on the automated detection of a complex system operator's condition. For example, in this study a person's reaction while listening to music (or not listening at all) was determined. For this purpose various well-known data mining tools as well as ones developed by authors were used. To be more specific, the following techniques were developed and applied for the mentioned problems: artificial neural networks and fuzzy rule-based classifiers. The neural networks were generated by two modifications of the Differential Evolution algorithm based on the NSGA and MOEA/D schemes, proposed for solving multiobjective optimization problems. Fuzzy logic systems were generated by the population-based algorithm called Co-Operation of Biology Related Algorithms or COBRA. However, firstly each person's state was monitored. Thus, databases for problems described in this study were obtained by using non-contact Doppler sensors. Experimental results demonstrated that automatically generated neural networks and fuzzy rule-based classifiers can properly determine the human condition and reaction. Besides, proposed approaches outperformed alternative data mining tools. However, it was established that fuzzy rule-based classifiers are more accurate and interpretable than neural networks. Thus, they can be used for solving more complex problems related to the automated detection of an operator's condition. © 2021 The Institute of Electronics, Information and Communication Engineers","Classification; Fuzzy logic; Neural networks; Non-contact vital sensing","Biology; Complex networks; Evolutionary algorithms; Fuzzy inference; Fuzzy logic; Fuzzy neural networks; Fuzzy rules; Multiobjective optimization; Automated detection; Automatically generated; Condition detection; Differential evolution algorithms; Fuzzy logic system; Fuzzy rule-based classifier; Multi-objective optimization problem; Population-based algorithm; Data mining"
"Akhmetov I., Pak A., Ualiyeva I., Gelbukh A.","Highly language-independent word lemmatization using a machine-learning classifier","10.13053/CYS-24-3-3775","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095717240&doi=10.13053%2fCYS-24-3-3775&partnerID=40&md5=7ce91e0fc1544b56a0ce5e01b51cd019","Lemmatization is a process of finding the base morphological form (lemma) of a word. It is an important step in many natural language processing, information retrieval, and information extraction tasks, among others. We present an open-source language-independent lemmatizer based on the Random Forest classification model. This model is a supervised machine-learning algorithm with decision trees that are constructed corresponding to the grammatical features of the language. This lemmatizer does not require any manual work for hard-coding of the rules, and at the same time it is simple and interpretable. We compare the performance of our lemmatizer with that of the UDPipe lemmatizer on twenty-two out of twenty-five languages we work on for which UDPipe has models. Our lemmatization method shows good performance on different languages from various language groups, and it is easily extensible to other languages. The source code of our lemmatizer is publicly available. © 2020 Instituto Politecnico Nacional. All rights reserved.","Decision Tree classifier; Lemmatization; Natural language processing; Random Forest classifier; Text preprocessing",
"Akhouri S., Girdhar K.","Data management for M2M communication using telecom mediation systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887196432&partnerID=40&md5=c5a378cd615918cf6b1adcc6778c5aa9","Mediation systems are an integral part of a telecom carrier's Business Support System / Operational Support System (BSS / OSS) landscape. They are responsible for collecting, transforming and consolidating massive volumes of data from a diverse set of Network Elements (NE) across a range of protocols. Traditionally, mediation systems support revenue management functions such as charging and billing. Unlike standard data warehousing applications that are based on Extract-Transform-Lead (ETL) paradigm, mediation systems are focused on rapid processing of events in real-time or near real-time. This paper proposes the use of mediation system as a data management platform for Machine-to-Machine (M2M) communication. Mediation systems inherently deliver scalable solutions for handling huge volumes of sensor data. They can connect across a diverse set of M2M communication protocols to collect, consolidate, process sensor data and feed it downstream to decision support systems for actionable intelligence. This paper briefly explores the integration of mediation systems with stream mining applications to classify and cluster data-streams.","Analytics; Anomaly detection; BSS/OSS; Clustering; M2M; Mediation; Revenue management; Sensor data management; Stream data mining; Telecommunication","Analytics; Anomaly detection; Clustering; M2M; Mediation; Revenue management; Sensor data management; Stream data mining; Artificial intelligence; Data warehouses; Decision support systems; Sensors; Telecommunication; Information management"
"Akinbolajo O., Li H.","Evaluating neural network methods for brain hemorrhage identification and classification from computed tomography imagery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120951070&partnerID=40&md5=93c07bd0076dc68b719e97dc77b2f4b0","The need for accurate classification of brain hemorrhages has drawn increasing attention in recent years as a critical component in the early detection and treatment of brain hemorrhages. This is not only important for improving accurate diagnosis, but also helpful in reducing the tedious tasks carried out by radiologists when they try to accurately diagnose brain hemorrhage. Many medical practitioners believe that misdiagnosis or late diagnosis of brain hemorrhage is responsible for the increased fatality rates of brain hemorrhage patients. This research investigates the effectiveness of selected machine learning models for binary classification of brain hemorrhage from Computed Tomography (CT) images. The traditional AlexNet convolutional neural network (CNN), which is known for its effectiveness with image detection, is used together with other neural network models, i.e., MobileNet, VGG-16, and VGG-19. The model parameters of AlexNet, MobileNet, VGG-16, and VGG-19 are modified, trained, and validated. The VGG-16 and VGG-19 show significantly better accuracy compared to AlexNet and MobileNet. This research also conducts model evaluation with publically available dataset to compare the effectiveness and accuracy of these neural networks in detecting hemorrhages from CT images. The results and additional tests carried out shows that deep learning models have the capacity to accurately identify and classify brain hemorrhages. © 2021 IISE Annual Conference and Expo 2021. All rights reserved.","Brain hemorrhage; Computed tomography; Convolutional neural networks; Machine learning","Convolution; Convolutional neural networks; Deep learning; Diagnosis; Image classification; Binary classification; Brain hemorrhage; Computed tomography images; Convolutional neural network; Critical component; Fatality rates; Haemorrage; Machine learning models; Medical practitioner; Neural network method; Computerized tomography"
"Akinbulire T., Falcon R., Abielmona R., Schwartz H.","Responding to illegal, unreported and unregulated fishing with evolutionary multi-objective optimization","10.1109/CIVEMSA.2018.8439981","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053147297&doi=10.1109%2fCIVEMSA.2018.8439981&partnerID=40&md5=a4f6eaceae0dd5decca366abf7e7f454","Illegal, unreported and unregulated (IUU) fishing is largely responsible for dwindling fish stocks and marine habitat destruction. It is estimated that IUU fishing accounts for about 30% of all fishing activity worldwide, both on open oceans and within national exclusive economic zones. Responding to IUU fishing incidents is of paramount importance to law enforcement and marine environment protection organizations. This paper employs Evolutionary Multi-Objective Optimization (EMOO) to automatically generate a set of promising candidate responses once an IUU fishing event has been identified. Four EMOO algorithms will explore the trade-off among three conflicting decision objectives, namely (1) the proximity to the target (IUU fishing vessel), (2) the total cost of the response for all engaged assets and (3) the probability of confirming the detection of the offending vessel inside the fishing zone, which is important for prosecution purposes. We illustrate the proposed methodology with a simulated scenario along the Canadian Atlantic coast and discuss some of the automatically generated responses that are offered to the decision maker for their consideration. To the best of our knowledge, this is the first time EMOO techniques have been applied to respond to IUU fishing incidents. © 2018 IEEE.",,"Artificial intelligence; Crime; Decision making; Economic and social effects; Electronic trading; Environmental regulations; Fisheries; Fishing vessels; Virtual reality; Atlantic coasts; Automatically generated; Decision objectives; Evolutionary multiobjective optimization; Exclusive economic zones; Fishing activities; Habitat destruction; Marine environment protection; Multiobjective optimization"
"Akita H., Nakago K., Komatsu T., Sugawara Y., Maeda S.-I., Baba Y., Kashima H.","BayesGrad: Explaining predictions of graph convolutional networks","10.1007/978-3-030-04221-9_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059059881&doi=10.1007%2f978-3-030-04221-9_8&partnerID=40&md5=23520243e2f8d52ee6b18c5faf9c9fd5","Recent advances in graph convolutional networks have significantly improved the performance of chemical predictions, raising a new research question: “how do we explain the predictions of graph convolutional networks?” A possible approach to answer this question is to visualize evidence substructures responsible for the predictions. For chemical property prediction tasks, the sample size of the training data is often small and/or a label imbalance problem occurs, where a few samples belong to a single class and the majority of samples belong to the other classes. This can lead to uncertainty related to the learned parameters of the machine learning model. To address this uncertainty, we propose BayesGrad, utilizing the Bayesian predictive distribution, to define the importance of each node in an input graph, which is computed efficiently using the dropout technique. We demonstrate that BayesGrad successfully visualizes the substructures responsible for the label prediction in the artificial experiment, even when the sample size is small. Furthermore, we use a real dataset to evaluate the effectiveness of the visualization. The basic idea of BayesGrad is not limited to graph-structured data and can be applied to other data types. © 2018, Springer Nature Switzerland AG.","Cheminformatics; Deep learning; Graph convolution; Interpretability; Machine learning","Artificial intelligence; Convolution; Deep learning; Learning systems; Cheminformatics; Convolutional networks; Graph structured data; Interpretability; Machine learning models; Predictive distributions; Property predictions; Research questions; Forecasting"
"Akman D.V., Malekipirbazari M., Yenice Z.D., Yeo A., Adhikari N., Wong Y.K., Abbasi B., Gumus A.T.","k-best feature selection and ranking via stochastic approximation","10.1016/j.eswa.2022.118864","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139068377&doi=10.1016%2fj.eswa.2022.118864&partnerID=40&md5=6fc3bc0a4c8fb4185c508ad218d76eee","This study presents SPFSR, a novel stochastic approximation approach for performing simultaneous k-best feature ranking (FR) and feature selection (FS) based on Simultaneous Perturbation Stochastic Approximation (SPSA) with Barzilai and Borwein (BB) non-monotone gains. SPFSR is a wrapper-based method which may be used in conjunction with any given classifier or regressor with respect to any suitable corresponding performance metric. Numerical experiments are performed on 47 public datasets which contain both classification and regression problems, with the mean accuracy and R2 reported from four different classifiers and four different regressors respectively. In over 80% of classification experiments and over 85% of regression experiments SPFSR provided a statistically significant improvement or equivalent performance compared to existing, well-known FR techniques. Furthermore, SPFSR obtained a better classification accuracy and R-squared on average compared to utilising the entire feature set. © 2022 Elsevier Ltd","Barzilai and Borwein method; Explainable artificial intelligence; Feature ranking; Feature selection; Stochastic approximation","Approximation theory; Classification (of information); Feature Selection; Optimization; Approximation approach; Barzilai and borwein method; Explainable artificial intelligence; Feature ranking; Features selection; Performance metrices; Ranking selection; Selection based; Simultaneous perturbation stochastic approximation; Stochastic approximations; Stochastic systems"
"Akolekar H.D., Sandberg R.D., Hutchins N., Michelassi V., Laskowski G.","Machine-learnt turbulence closures for low-pressure turbines with unsteady inflow conditions","10.1115/1.4043907","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080974430&doi=10.1115%2f1.4043907&partnerID=40&md5=563dccfed1e36f80110de9e24e7452cb","The design of low-pressure turbines (LPTs) must account for the losses generated by the unsteady interaction with the upstream blade row. The estimation of such unsteady wake-induced losses requires the accurate prediction of the incoming wake dynamics and decay. Existing linear turbulence closures (stress-strain relationships), however, do not offer an accurate prediction of the wake mixing. Therefore, machine-learnt, nonlinear turbulence closures (models) have been developed for LPT flows with unsteady inflow conditions using a zonal-based model development approach, with an aim to enhance the wake mixing prediction for unsteady Reynolds-averaged Navier-Stokes calculations. High-fidelity time-averaged and phase-lock averaged data at a realistic isentropic Reynolds number and two reduced frequencies, i.e., with discrete incoming wakes and with wake “fogging,” have been used as reference data for a machine learning algorithm based on gene expression programing to develop models. Models developed via phase-lock averaged data were able to capture the effect of certain prominent physical phenomena in LPTs such as wake-wake interactions, whereas models based on the time-averaged data could not. Correlations with the flow physics lead to a set of models that can effectively enhance the wake mixing prediction across the entire LPT domain for both cases. Based on a newly developed error metric, the developed models have reduced the a priori error over the Boussinesq approximation on average by 45%. This study thus aids blade designers in selecting the appropriate nonlinear closures capable of mimicking the physical mechanisms responsible for loss generation. Copyright © 2019 by ASME",,"Forecasting; Gene expression; Learning algorithms; Locks (fasteners); Machine learning; Mixing; Navier Stokes equations; Reynolds number; Stress-strain curves; Turbomachine blades; Turbulence; Turing machines; Boussinesq approximations; Low-pressure turbines; Non-linear turbulence; Stress-strain relationships; Turbulence closures; Unsteady interaction; Unsteady reynolds-averaged navier-stokes; Wake-wake interactions; Wakes"
"Akpinar M.T., Karabacak M.E.","Data mining applications in civil aviation sector: State-of-art review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021082155&partnerID=40&md5=23e08da4600ee2084efa99750ba28a59","Data mining is the process of analyzing data from different perspectives and summarizing it into useful information take place in the scope of business intelligence. Nowadays increasing of the globalization and integration of the world, transportation sector is one of the most required area that needs to be used for data analysis. Purely, planes equipped with flight recording data typically record up to 500 variables of data - described in these flight data recordings are time, altitude, vertical acceleration, and heading-per second for the duration the plane is being operated. Airline companies may use data mining in order to fuel cost optimization, planning take into consideration weather conditions, passenger analysis, cargo optimization, airport situation revenue per flight, profit per flight, cost per seat or more detailed one catering and handling expenses per seat. In this article, the current applications of data mining in civil aviation sector are reviewed based on certain critical factors including airlines, airports, cargo, passenger, efficiency and safety. The critical analysis of 63 empirical studies reveals that the usage of data mining in airlines sector is still in its early stages and the ability of these studies to generate knowledge may not be sufficient. It's tried to subhead papers and classified them subject by subject and also yearly. The intention of this study is that this is going to guide that sheds light on this field studies and staff. And also we want to put forth the tendency of civil aviation sector and evaluation progress of airlines in business intelligence concept. Given these findings, it can be suggested that there is more to dig for in order to obtain more managerially interpretable and acceptable results in further studies. Also, recommendations are made for other potentials of other business intelligence tools or data mining techniques research. Copyright © 2017 held by the authors.","Airlines; Business intelligence; Civil aviation; Data mining","Air transportation; Aircraft seats; Airport passenger transportation; Airports; Civil aviation; Competitive intelligence; Cost benefit analysis; Information analysis; Transportation; Aviation sector; Critical analysis; Critical factors; Data mining applications; Empirical studies; Recording data; Transportation sector; Vertical accelerations; Data mining"
"Akpinaroglu D., Ruffolo J.A., Mahajan S.P., Gray J.J.","Simultaneous prediction of antibody backbone and side-chain conformations with deep learning","10.1371/journal.pone.0258173","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132080557&doi=10.1371%2fjournal.pone.0258173&partnerID=40&md5=db25e724e5f98f94a7ab1ba3ca47e8bd","Antibody engineering is becoming increasingly popular in medicine for the development of diagnostics and immunotherapies. Antibody function relies largely on the recognition and binding of antigenic epitopes via the loops in the complementarity determining regions. Hence, accurate high-resolution modeling of these loops is essential for effective antibody engineering and design. Deep learning methods have previously been shown to effectively predict antibody backbone structures described as a set of inter-residue distances and orientations. However, antigen binding is also dependent on the specific conformations of surface side-chains. To address this shortcoming, we created DeepSCAb: a deep learning method that predicts inter-residue geometries as well as side-chain dihedrals of the antibody variable fragment. The network requires only sequence as input, rendering it particularly useful for antibodies without any known backbone conformations. Rotamer predictions use an interpretable self-attention layer, which learns to identify structurally conserved anchor positions across several species. We evaluate the performance of the model for discriminating near-native structures from sets of decoys and find that DeepSCAb outperforms similar methods lacking side-chain context. When compared to alternative rotamer repacking methods, which require an input backbone structure, DeepSCAb predicts side-chain conformations competitively. Our findings suggest that DeepSCAb improves antibody structure prediction with accurate side-chain modeling and is adaptable to applications in docking of antibody-antigen complexes and design of new therapeutic antibody sequences. © 2022 Akpinaroglu et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"antibody structure; antigen antibody complex; antigen binding; article; attention; deep learning; geometry; prediction; protein conformation; protein conformation; structural homology; Antigen-Antibody Complex; Deep Learning; Protein Conformation; Structural Homology, Protein"
"Akramizadeh A., Farjami A.A., Khaloozadeh H.","Nonlinear Hammerstein model identification using genetic algorithm","10.1109/ICAIS.2002.1048126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962878136&doi=10.1109%2fICAIS.2002.1048126&partnerID=40&md5=3769646fd474de69c4555fe34a736aae","In this paper, a new approach to nonlinear system identification using evolutionary LMS algorithm is proposed. The system in our method consists of a static nonlinear function in series with a dynamic linear transfer function, which the literature refers to them as Hammerstein models. The identified nonlinear function can be one of the hyperbolic functions or a general format of (ax+b) or a combination of them. The genetic algorithm is responsible for finding the correct structure and parameters of the nonlinear function, and the number of zeros and poles of the linear transfer function as well. In order to speed up the convergence process, we use a kind of dynamic mutation rate that increases with respect to the generation passed while the fitness remains unchanged. For the linear identification algorithm we prefer to parameterize the problem as ARMA and apply the traditional LMS algorithm. AIC is the fitness function evaluator of the GA chromosomes, using both the total error and estimated order of the linear section. Two different simulations show the effectiveness of our method. In the simulation two hard nonlinear functions, saturation and dead-zone, were used and show that despite of the small amount of information, which is limited to input-output signals, our approach can considerably identify the systems. © 2002 IEEE.","Genetic algorithm; Hammerstein; Nonlinear system; System identification","Algorithms; Artificial intelligence; Evolutionary algorithms; Functions; Genetic algorithms; Hyperbolic functions; Identification (control systems); Nonlinear systems; Religious buildings; Transfer functions; Amount of information; Convergence process; Dynamic mutation rate; Fitness functions; Hammerstein; Identification algorithms; Linear transfer function; Nonlinear functions; Parameter estimation"
"Aksakalli V., D. Yenice Z., Malekipirbazari M., Kargar K.","Feature selection using stochastic approximation with Barzilai and Borwein non-monotone gains","10.1016/j.cor.2021.105334","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104730087&doi=10.1016%2fj.cor.2021.105334&partnerID=40&md5=28cdfab6ca6c8b0e11b6b13432c68d93","With recent emergence of machine learning problems with massive number of features, feature selection (FS) has become an ever-increasingly important tool to mitigate the effects of the so-called curse of dimensionality. FS aims to eliminate redundant and irrelevant features for models that are faster to train, easier to understand, and less prone to overfitting. This study presents a wrapper FS method based on Simultaneous Perturbation Stochastic Approximation (SPSA) with Barzilai and Borwein (BB) non-monotone gains within a pseudo-gradient descent framework wherein performance is measured via cross-validation. We illustrate that SPSA with BB gains (SPSA-BB) provides dramatic improvements in terms of the number of iterations for convergence with minimal degradation in cross-validated error performance over the current state-of-the art approach with monotone gains (SPSA-MON). In addition, SPSA-BB requires only one internal parameter and therefore it eliminates the need for careful fine-tuning of numerous other internal parameters as in SPSA-MON or comparable meta-heuristic FS methods such as genetic algorithms (GA). Our particular implementation includes gradient averaging as well as gain smoothing for better convergence properties. We present computational experiments on various public datasets with Nearest Neighbors and Naive Bayes classifiers as wrappers. We present comparisons of SPSA-BB against full set of features, SPSA-MON, as well as seven popular meta-heuristics based FS algorithms including GA and particle swarm optimization. Our results indicate that SPSA-BB converges to a good feature set in about 50 iterations on the average regardless of the number of features (whether a dozen or more than 1000 features) and its performance is quite competitive. SPSA-BB can be considered extremely fast for a wrapper method and therefore it stands as a high-performing new feature selection method that is also computationally feasible in practice. © 2021 Elsevier Ltd","Barzilai and Borwein method; Explainable artificial intelligence; Feature selection; Genetic algorithm; Gradient descent; Stochastic approximation","Approximation theory; Classification (of information); Genetic algorithms; Gradient methods; Heuristic algorithms; Heuristic methods; Particle swarm optimization (PSO); Stochastic systems; Traffic signals; Computational experiment; Curse of dimensionality; Feature selection methods; Machine learning problem; Naive Bayes classifiers; Simultaneous perturbation stochastic approximation; State-of-the-art approach; Stochastic approximations; Feature extraction"
"Aksar B., Schwaller B., Aaziz O., Leung V.J., Brandt J., Egele M., Coskun A.K.","E2EWatch: An End-to-End Anomaly Diagnosis Framework for Production HPC Systems","10.1007/978-3-030-85665-6_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115137861&doi=10.1007%2f978-3-030-85665-6_5&partnerID=40&md5=52ddceccc4ae1c076ff998bfba2403de","In today’s High-Performance Computing (HPC) systems, application performance variations are among the most vital challenges as they adversely affect system efficiency, application performance, and cost. System administrators need to identify the anomalies that are responsible for performance variation and take mitigating actions. One can perform manual root-cause analysis on telemetry data collected by HPC monitoring infrastructures to analyze performance variations. However, manual analysis methods are time-intensive and limited in impact due to the increasing complexity of HPC systems and terabyte/day-sized telemetry data. State-of-the-art approaches use machine learning-based methods to diagnose performance anomalies automatically. This paper deploys an end-to-end machine learning framework that diagnoses performance anomalies on compute nodes on a 1488-node production HPC system. We demonstrate job and node-level anomaly diagnosis results with the Grafana frontend interface at runtime. Furthermore, we discuss challenges and design decisions for the deployment. © 2021, Springer Nature Switzerland AG.","Anomaly diagnosis; HPC; Machine learning; Telemetry","Telemetering equipment; Anomaly diagnosis; Application performance; High performance computing systems; Performance anomaly; Performance variations; Root cause analysis; State-of-the-art approach; System administrators; Machine learning"
"Aksoy B., Salman O.K.M., Ekrem Ö.","Detection of Turkish Sign Language Using Deep Learning and Image Processing Methods","10.1080/08839514.2021.1982184","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115417790&doi=10.1080%2f08839514.2021.1982184&partnerID=40&md5=912a09b1a8c38a546a3eb45a758af06d","Sign language is a physical language that enables people with disabilities to communicate using hand and facial gestures. For this reason, it is very important for people with disabilities to express themselves freely in society and to make the sign language understandable to everyone. In this study, the data set was created by taking 10223 images for 29 letters in the Turkish Sign Language Alphabet. Images are made suitable for education by using image enhancement techniques. In the final stage of the study, classification processes on images were carried out by using CapsNet, AlexNet and ResNet-50, DenseNet, VGG16, Xception, InceptionV3, NasNet, EfficentNet, Hitnet, Squeezenet architectures and TSLNet, which was designed for the study. When the deep learning models were examined, it was found that CapsNet and TSLNet models were the most successful models with 99.7% and 99.6% accuracy rates, respectively. © 2021 Taylor & Francis.",,"Deep learning; Accuracy rate; Classification process; Data set; Facial gestures; Hand gesture; Image processing - methods; Learning models; People with disabilities; Sign language; Turkishs; Image enhancement"
"Akter R., Doan V.-S., Zainudin A., Kim D.-S.","An Explainable Multi-Task Learning Approach for RF-based UAV Surveillance Systems","10.1109/ICUFN55119.2022.9829629","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135245352&doi=10.1109%2fICUFN55119.2022.9829629&partnerID=40&md5=a6b2df614c27c6dbca12e7e408172d08","Unmanned aerial vehicles or drones are ubiquitous among people, which can lead to technological, security, and community safety issues that must be addressed, monitored, and avoided. Intelligence services are always on the search for potential technology and intelligent systems that can identify drones. A potential drone surveillance system must be capable of detecting, localizing, identifying, recognizing the modes, and combating unauthorized drones. In this paper, we introduce a Multi-Task Learning (MTL) neural network for drone detection, identification, and drone mode detection using Radio Frequency (RF) signals. Due to the semantic abstraction of the drone RF signals, a single-task learning method can not fully meet the demands of the current anti-drone system. Moreover, executing each of the tasks, such as drone detection, type identification, and activity recognition, individually takes longer time, which is not applicable in a real-time drone surveillance system. Therefore, this paper proposes an MTL approach leveraging convolution layers to perform three tasks in parallel. A cross-entropy loss function used as the objective function optimization to improve the accuracy of the multiple tasks. The empirical results shows that the proposed MTL model achieve a better recognition accuracy compared to the existing solutions. © 2022 IEEE.","Convolution neural network; drone detection and classification; multi-task learning neural network; radio frequency signal","Aircraft detection; Antennas; Convolution; Deep learning; Intelligent systems; Learning systems; Linearization; Monitoring; Security systems; Semantics; Convolution neural network; Drone detection and classification; Learning approach; Learning neural networks; Multi-task learning neural network; Multitask learning; Radiofrequencies; Radiofrequency signals; Surveillance systems; UAV surveillance; Drones"
"Akther A., Ushakov Y., Balanov A.G., Savel’ev S.E.","Deterministic modeling of the diffusive memristor","10.1063/5.0056239","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109416182&doi=10.1063%2f5.0056239&partnerID=40&md5=e4a4a8c38589261664f01d16ad2d3f07","Recently developed diffusive memristors have gathered a large amount of research attention due to their unique property to exhibit a variety of spiking regimes reminiscent to that found in biological cells, which creates a great potential for their application in neuromorphic systems of artificial intelligence and unconventional computing. These devices are known to produce a huge range of interesting phenomena through the interplay of regular, chaotic, and stochastic behavior. However, the character of these interplays as well as the instabilities responsible for different dynamical regimes are still poorly studied because of the difficulties in analyzing the complex stochastic dynamics of the memristive devices. In this paper, we introduce a new deterministic model justified from the Fokker-Planck description to capture the noise-driven dynamics that noise has been known to produce in the diffusive memristor. This allows us to apply bifurcation theory to reveal the instabilities and the description of the transition between the dynamical regimes. © 2021 Author(s).",,"article; noise"
"Akula A.R., Wang K., Liu C., Saba-Sadiya S., Lu H., Todorovic S., Chai J., Zhu S.-C.","CX-ToM: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition models","10.1016/j.isci.2021.103581","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122257956&doi=10.1016%2fj.isci.2021.103581&partnerID=40&md5=dedcf21ff9848f9d34c7067f2575d0a2","We propose CX-ToM, short for counterfactual explanations with theory-of-mind, a new explainable AI (XAI) framework for explaining decisions made by a deep convolutional neural network (CNN). In contrast to the current methods in XAI that generate explanations as a single shot response, we pose explanation as an iterative communication process, i.e., dialogue between the machine and human user. More concretely, our CX-ToM framework generates a sequence of explanations in a dialogue by mediating the differences between the minds of the machine and human user. To do this, we use Theory of Mind (ToM) which helps us in explicitly modeling the human's intention, the machine's mind as inferred by the human, as well as human's mind as inferred by the machine. Moreover, most state-of-the-art XAI frameworks provide attention (or heat map) based explanations. In our work, we show that these attention-based explanations are not sufficient for increasing human trust in the underlying CNN model. In CX-ToM, we instead use counterfactual explanations called fault-lines which we define as follows: given an input image I for which a CNN classification model M predicts class cpred, a fault-line identifies the minimal semantic-level features (e.g., stripes on zebra), referred to as explainable concepts, that need to be added to or deleted from I to alter the classification category of I by M to another specified class calt. Extensive experiments verify our hypotheses, demonstrating that our CX-ToM significantly outperforms the state-of-the-art XAI models. © 2021 The Author(s)","Artificial intelligence; Computer science; Human-computer interaction",
"Akula A.R., Wang S., Zhu S.-C.","CoCoX: Generating conceptual and counterfactual explanations via fault-lines",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098439493&partnerID=40&md5=370282d52061b2d8ed23b8a99902d27c","We present CoCoX (short for Conceptual and Counterfactual Explanations), a model for explaining decisions made by a deep convolutional neural network (CNN). In Cognitive Psychology, the factors (or semantic-level features) that humans zoom in on when they imagine an alternative to a model prediction are often referred to as fault-lines. Motivated by this, our CoCoX model explains decisions made by a CNN using fault-lines. Specifically, given an input image I for which a CNN classification model M predicts class cpred, our fault-line based explanation identifies the minimal semantic-level features (e.g., stripes on zebra, pointed ears of dog), referred to as explainable concepts, that need to be added to or deleted from I in order to alter the classification category of I by M to another specified class calt. We argue that, due to the conceptual and counterfactual nature of fault-lines, our CoCoX explanations are practical and more natural for both expert and non-expert users to understand the internal workings of complex deep learning models. Extensive quantitative and qualitative experiments verify our hypotheses, showing that CoCoX significantly outperforms the state-of-the-art explainable AI models. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Deep learning; Deep neural networks; Semantics; Classification models; Cognitive psychology; Learning models; Model prediction; Nature of faults; Qualitative experiments; Semantic levels; State of the art; Convolutional neural networks"
"Akula R., Garibay I.","Interpretable multi-head self-attention architecture for sarcasm detection in social media","10.3390/e23040394","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103465993&doi=10.3390%2fe23040394&partnerID=40&md5=f2650d7d5f41afe83dc45d98d144863a","With the online presence of more than half the world population, social media plays a very important role in the lives of individuals as well as businesses alike. Social media enables businesses to advertise their products, build brand value, and reach out to their customers. To leverage these social media platforms, it is important for businesses to process customer feedback in the form of posts and tweets. Sentiment analysis is the process of identifying the emotion, either positive, negative or neutral, associated with these social media texts. The presence of sarcasm in texts is the main hindrance in the performance of sentiment analysis. Sarcasm is a linguistic expression often used to communicate the opposite of what is said, usually something that is very unpleasant, with an intention to insult or ridicule. Inherent ambiguity in sarcastic expressions make sarcasm detection very difficult. In this work, we focus on detecting sarcasm in textual conversations from various social networking platforms and online media. To this end, we develop an interpretable deep learning model using multi-head self-attention and gated recurrent units. The multi-head self-attention module aids in identifying crucial sarcastic cue-words from the input, and the recurrent units learn long-range dependencies between these cue-words to better classify the input text. We show the effectiveness of our approach by achieving state-of-the-art results on multiple datasets from social networking platforms and online media. Models trained using our proposed approach are easily interpretable and enable identifying sarcastic cues in the input text which contribute to the final classification score. We visualize the learned attention weights on a few sample input texts to showcase the effectiveness and interpretability of our model. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Interpretability; Sarcasm detection; Self-attention; Social media analysis",
"Akula R., Garibay I.","Explainable Detection of Sarcasm in Social Media",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124142326&partnerID=40&md5=88f2447e2f08a21023701092e699b164","Sarcasm is a linguistic expression often used to communicate the opposite of what is said, usually something that is very unpleasant with an intention to insult or ridicule. Inherent ambiguity in sarcastic expressions makes sarcasm detection very difficult. In this work, we focus on detecting sarcasm in textual conversations, written in English, from various social networking platforms and online media. To this end, we develop an interpretable deep learning model using multi-head self-attention and gated recurrent units. We show the effectiveness and interpretability of our approach by achieving state-of-the-art results on datasets from social networking platforms, online discussion forum and political dialogues. © 2021 Association for Computational Linguistics.",,"Computational linguistics; Deep learning; Interpretability; Learning models; Linguistic expressions; Online discussion forums; Online medium; Social media; Social-networking; State of the art; Social networking (online)"
"Akula R., Garibay I.","Ethical AI for Social Good","10.1007/978-3-030-90963-5_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119872249&doi=10.1007%2f978-3-030-90963-5_28&partnerID=40&md5=748f2bb558e0d9044410221580225002","The concept of AI for Social Good(AI4SG) is gaining momentum in both information societies and the AI community. Through all the advancement of AI-based solutions, it can solve societal issues effectively. To date, however, there is only a rudimentary grasp of what constitutes AI socially beneficial in principle, what constitutes AI4SG in reality, and what are the policies and regulations needed to ensure it. This paper fills the vacuum by addressing the ethical aspects that are critical for future AI4SG efforts. Some of these characteristics are new to AI, while others have greater importance due to its usage. © 2021, Springer Nature Switzerland AG.","Ai for social good; Artificial intelligence; Equitable; Ethics; Fairness; Human centered AI; Responsible AI","Computers; Ethical technology; Ai for social good; Equitable; Fairness; Gaining momentum; Human centered AI; Information society; Policy and regulation; Responsible AI; Societal issues; Artificial intelligence"
"Akulich F., Anahideh H., Sheyyab M., Ambre D.","Explainable predictive modeling for limited spectral data","10.1016/j.chemolab.2022.104572","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129396706&doi=10.1016%2fj.chemolab.2022.104572&partnerID=40&md5=3343501a3fe15f1508ef4190c21fdc8b","Feature selection of high-dimensional labeled data with limited observations is critical for making powerful predictive modeling accessible, scalable, and interpretable for domain experts. Spectroscopy data, which records the interaction between matter and electromagnetic radiation, particularly holds a lot of information in a single sample. Since acquiring such high-dimensional data is a complex task, it is crucial to exploit the best analytical tools to extract necessary information. In this paper, we investigate the most commonly used feature selection techniques and introduce applying recent explainable AI techniques to interpret the prediction outcomes of high-dimensional and limited spectral data. Interpretation of the prediction outcome is beneficial for the domain experts as it ensures the transparency and faithfulness of the ML models to the domain knowledge. Due to the instrument resolution limitations, pinpointing important regions of the spectroscopy data creates a pathway to optimize the data collection process through the miniaturization of the spectrometer device. Reducing the device size and power and therefore cost is a requirement for the real-world deployment of such a sensor-to-prediction system as a whole. Furthermore, we consider a wide range of machine learning models that have been proven to be successful for the prediction of the Cetane Number of fuels. We specifically design three different scenarios to ensure that the evaluation of ML models is robust for the real-time practice of the developed methodologies and to uncover the hidden effect of noise sources on the final outcome. The evaluation is performed for both the full model and reduced models using different feature selection techniques on a real dataset. Finally, we propose a correctness metric for the feature selection techniques to assess the conformance of the selected subset of features to the domain expertise. As a result, the Support Vector Regression yields better prediction accuracy and generalization power as it leads to less complex and computationally more efficient than model Neural Network. More importantly, using the reduced subset of features from original data creates a pathway to deploying less complex, scalable, and explainable prediction models. © 2022 Elsevier B.V.","Explainable AI; Feature selection; Interpretability; Predictive modeling; Spectral data","algorithm; Article; artificial intelligence; artificial neural network; decision tree; electromagnetic radiation; feature selection; Fourier transform infrared spectroscopy; infrared spectroscopy; k nearest neighbor; learning algorithm; machine learning; mathematical analysis; mean squared error; near infrared spectroscopy; nonhuman; prediction; predictive model; principal component analysis; random forest; regression model; spectroscopy; support vector machine"
"Akundi P., Coyle F.P.","The semantic Web applied to data organization for decision making",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878132057&partnerID=40&md5=4e82ca8f7c9e62991dd9e72f56dd8fce","Which comes first, scrubbing the data or organizing the data? Data organization appears the obvious choice. It is no surprise that the value of data is directly proportional to the information we extract from it. Email is the most widely used communication channel yet the least organized data repository in business organizations. Emails are an invaluable resource for documenting requirements, project milestones, and checklists to make data analysis a smoother process. With the vast number of communication tools available for the business user, the desire to simplify the receipt of email content without losing information is a challenge. Emails exhibit data redundancy, search constraints, and filtering problems when trying to find valuable information. There are a number of studies into data mining emails for analysis which may apply data cleansing and data quality techniques, but the relationships between the information still requires scrutiny by the analyst once it is put back into an understandable format. Now think semantically, step out of the database and organize the data logically and structure it in a way that makes sense for the organization. Take it a step further and apply an intelligent framework to generate topic maps from these emails for ease of analysis. This paper will outline the value of organizing data semantically and then illustrate the value of extracting data intelligently for documentation and analysis.","Business process; Email organization; N3; RDF; Semantic enterprise; Semantic routing; Semantic web; Social networking; XML","Business organizations; Business Process; Communication tools; Filtering problems; N3; Project milestones; RDF; Semantic routing; Electronic mail; Information filtering; Information systems; Project management; Semantic Web; Social networking (online); XML; Data mining"
"Akusok A., Gritsenko A., Miche Y., Björk K.-M., Nian R., Lauren P., Lendasse A.","Adding reliability to ELM forecasts by confidence intervals","10.1016/j.neucom.2016.09.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998775194&doi=10.1016%2fj.neucom.2016.09.021&partnerID=40&md5=e363e1b058f01b698954608a3e209abd","This paper proposes a way of providing transparent and interpretable results for ELM models by adding confidence intervals to the predicted outputs. In supervised learning, outputs are often random variables because they may depend on information that is unavailable, due to the presence of noise, or the projection function itself may be stochastic. Probability distribution of outputs is input dependent, and the observed output values are samples from that distribution. However, ELM predicts deterministic outputs. The proposed method addresses that problem by estimating predictive Confidence Intervals (CIs) at a confidence level α, such that random output values fall between these intervals with probability α. Assuming that the outputs are normally distributed, only a standard deviation is needed to compute CIs of a predicted output (the predicted output itself is a mean). Our method provides CIs for ELM predictions by estimating standard deviation of a random output for a particular input sample. It shows good results on both toy and real skin segmentation datasets, and compares well with the existing Confidence-weighted ELM methods. On a toy dataset, the predicted CIs accurately represent the variable variance of outputs. On a real dataset, CIs improve the precision of a classification task at a cost of recall. © 2016 Elsevier B.V.","Big data; Confidence; Confidence interval; Extreme learning machines; Regression; Skin segmentation","Big data; Classification (of information); Financial data processing; Learning systems; Normal distribution; Statistics; Stochastic systems; Confidence; Confidence interval; Extreme learning machine; Regression; Skin segmentation; Probability distributions; algorithm; Article; confidence interval; extreme learning machine; forecasting; intermethod comparison; machine learning; mathematical computing; prediction; priority journal; skin color"
"Akuzawa K., Iwasawa Y., Matsuo Y.","Adversarial Invariant Feature Learning with Accuracy Constraint for Domain Generalization","10.1007/978-3-030-46147-8_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084852601&doi=10.1007%2f978-3-030-46147-8_19&partnerID=40&md5=e1cfd0522fa1d582fb0691e5439d51a0","Learning domain-invariant representation is a dominant approach for domain generalization (DG), where we need to build a classifier that is robust toward domain shifts. However, previous domain-invariance-based methods overlooked the underlying dependency of classes on domains, which is responsible for the trade-off between classification accuracy and domain invariance. Because the primary purpose of DG is to classify unseen domains rather than the invariance itself, the improvement of the invariance can negatively affect DG performance under this trade-off. To overcome the problem, this study first expands the analysis of the trade-off by Xie et al. [33], and provides the notion of accuracy-constrained domain invariance, which means the maximum domain invariance within a range that does not interfere with accuracy. We then propose a novel method adversarial feature learning with accuracy constraint (AFLAC), which explicitly leads to that invariance on adversarial training. Empirical validations show that the performance of AFLAC is superior to that of domain-invariance-based methods on both synthetic and three real-world datasets, supporting the importance of considering the dependency and the efficacy of the proposed method. © Springer Nature Switzerland AG 2020.","Adversarial training; Domain generalization; Invariant feature learning; Transfer learning","Economic and social effects; Classification accuracy; Constrained domain; Empirical validation; Feature learning; Invariant features; Invariant representation; Real-world datasets; Trade off; Machine learning"
"Al Buhussain A., De Grande R.E., Boukerche A.","Performance analysis of bio-inspired scheduling algorithms for cloud environments","10.1109/IPDPSW.2016.186","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991619807&doi=10.1109%2fIPDPSW.2016.186&partnerID=40&md5=0309f80d905645f82daf468aadffac17","Cloud computing environments mainly focus on the delivery of resources, platforms, and applications as services to users over the Internet. Cloud promises users access to as many resources as they need, making use of an elastic provisioning of resources. The cloud technology has gained popularity in recent years as the new paradigm in the IT industry. The number of users of Cloud services has been increasing steadily, so the need for efficient task scheduling is crucial for maintaining performance. In this particular case, a scheduler is responsible for assigning tasks to virtual machines efficiently, it is expected to adapt to changes along with defined demand. In this paper, we present a comparative performance study on bio-inspired scheduling algorithms: Ant Colony Optimization (ACO) and Honey Bee Optimization (HBO). A networking scheduling algorithm, Random Biased Sampling, is also evaluated. Those algorithms show the ability of self-managing and adapting to changes in the environment. The experimental results have shown that ACO performs better when computation power is set as the objective, and HBO shows better scheduling when the objective mainly relies on costs. © 2016 IEEE.","Bio-inspired Algorithms; Scheduling; Swarm Optimization","Ant colony optimization; Artificial intelligence; Optimization; Scheduling; Ant Colony Optimization (ACO); Bio-inspired algorithms; Cloud computing environments; Cloud environments; Cloud technologies; Comparative performance; Performance analysis; Swarm optimization; Scheduling algorithms"
"Al Farabi K.M., Sarkhel S., Dey S., Venugopal D.","Fine-Grained Explanations Using Markov Logic","10.1007/978-3-030-46147-8_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084835528&doi=10.1007%2f978-3-030-46147-8_37&partnerID=40&md5=50bb127a89355f60e87d9ac0368a6a80","Explaining the results of Machine learning algorithms is crucial given the rapid growth and potential applicability of these methods in critical domains including healthcare, defense, autonomous driving, etc. In this paper, we address this problem in the context of Markov Logic Networks (MLNs) which are highly expressive statistical relational models that combine first-order logic with probabilistic graphical models. MLNs in general are known to be interpretable models, i.e., MLNs can be understood more easily by humans as compared to models learned by approaches such as deep learning. However, at the same time, it is not straightforward to obtain human-understandable explanations specific to an observed inference result (e.g. marginal probability estimate). This is because, the MLN provides a lifted interpretation, one that generalizes to all possible worlds/instantiations, which are not query/evidence specific. In this paper, we extract grounded-explanations, i.e., explanations defined w.r.t specific inference queries and observed evidence. We extract these explanations from importance weights defined over the MLN formulas that encode the contribution of formulas towards the final inference results. We validate our approach in real world problems related to analyzing reviews from Yelp, and show through user-studies that our explanations are richer than state-of-the-art non-relational explainers such as LIME. © Springer Nature Switzerland AG 2020.",,"Computer circuits; Deep learning; Learning algorithms; Lime; Markov processes; Probabilistic logics; Autonomous driving; First order logic; Importance weights; Marginal probability; Markov logic networks; Probabilistic graphical models; Real-world problem; Relational Model; Learning systems"
"Al Hammadi A.Y., Yeun C.Y., Damiani E., Yoo P.D., Hu J., Yeun H.K., Yim M.-S.","Explainable artificial intelligence to evaluate industrial internal security using EEG signals in IoT framework","10.1016/j.adhoc.2021.102641","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112393109&doi=10.1016%2fj.adhoc.2021.102641&partnerID=40&md5=90318a27f158437923fe7cd53615e3f9","Industrial insider threat detection has consistently been a popular field of research. To help detect potential insider threats, the emotional states of humans are identified through a wide range of physiological signals including the galvanic skin response, electrocardiogram, and electroencephalogram (EEG). This paper presents an insider risk assessment system as a fitness for duty security evaluation using EEG brainwave signals with explainable deep learning and machine learning algorithms to classify abnormal EEG signals indicating a potential insider threat and evaluating fitness for duty. The system is designed to be cost-effective by using an Emotiv Insight EEG device with five electrodes. In this study, the data from 17 people in different emotional states were collected. The different levels of emotions were mapped and classified into four risk levels, namely low, normal, medium, and high. The data were collected while the subjects were presented with different images from the scientific international affective picture system. The collected EEG signals were preprocessed to eliminate noise from physical movements and blinking. The data were then used to train self-feature learning of two- and one-dimensional convolutional neural networks, Adaptive Boosting, random forest, and K-nearest neighbors models; the proposed method yielded classification accuracies of 96, 75, 97, 94 and 81%, respectively. © 2021 Elsevier B.V.","Deep learning; EEG sensor; Explainable artificial intelligence (XAI); Fitness for duty; Industrial insider; Insider threat; Internet of things (IoT); Security evaluation; Two-dimensional CNN","Convolutional neural networks; Cost effectiveness; Decision trees; Deep learning; Electroencephalography; Electrophysiology; Industrial research; Internet of things; Learning algorithms; Learning systems; Nearest neighbor search; Physiological models; Risk assessment; Classification accuracy; Electro-encephalogram (EEG); Fitness for duties; Galvanic skin response; Insider threat detections; K-nearest neighbors; Physiological signals; Security evaluation; Biomedical signal processing"
"Al Maruf A., Nayem M.A.-A., Haque M.M., Jiyad Z.M., Rashid A.M.O., Khanam F.","A Survey on Personality Prediction","10.1145/3542954.3543012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136806800&doi=10.1145%2f3542954.3543012&partnerID=40&md5=2c34a25b30c0e3ac304756142a399d11","Personality is an important psychological structure that is responsible for individual differences between people. Social networking site has become the most universally used communication and interaction platform for all people over the past few years. By predicting the personality of a person could be useful in several areas like personalized recommendations, strategies of marketing, business intelligence, sociology, human resources management, and mental diagnosis. Predicting personality with the help of social media data is an encouraging approach as this method does not require any questionnaires to be filled by users. The Big Five test is an approach where asks some questions and predicts the personality of humans of the following categories neuroticism, extraversion, conscientiousness, agreeableness, and openness to experience. According to the reply of persons, it can easily generalize into 5 personality categories. The outcome of this paper is to show a comparative study of various strategies and algorithms used in this field. We will inspect ways to use the new technology to future development in this area also understand human personality and contribute to the development of personality theory. © 2022 ACM.","Big five personality traits; Classifiers; Deep learning; Machine learning; Personality models; Personality prediction; Text","Behavioral research; Deep learning; Forecasting; Learning systems; Big five; Big five personality trait; Deep learning; Individual Differences; Machine-learning; Personality modeling; Personality predictions; Personality traits; Social-networking; Text; Surveys"
"Al P.","(E)-Trust and Its Function: Why We Shouldn't Apply Trust and Trustworthiness to Human–AI Relations","10.1111/japp.12613","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136491211&doi=10.1111%2fjapp.12613&partnerID=40&md5=bc02308c8248ebb3dc5181ee361edbec","With an increasing use of artificial intelligence (AI) systems, theorists have analyzed and argued for the promotion of trust in AI and trustworthy AI. Critics have objected that AI does not have the characteristics to be an appropriate subject for trust. However, this argumentation is open to counterarguments. Firstly, rejecting trust in AI denies the trust attitudes that some people experience. Secondly, we can trust other non-human entities, such as animals and institutions, so why can we not trust AI systems? Finally, human–AI trust is criticized based on a conception of human–human trust, which does not recognize the distinctiveness of the human–AI relationship. This article aims to refute these counterarguments based on the genealogical analyses of ‘trust’ and ‘trustworthiness’ of Karen Jones and Thomas Simpson, who show that trust and trustworthiness help to overcome vulnerabilities. This function of trust gives reason to use human–human trust as a standard. For this function, it is important that trustees are responsive to trust. While animals and institutions could be responsive, narrow AI systems are unable to be responsive to trust. Therefore, we should not apply trust to AI and instead direct our trust to those who can be responsive to and held responsible for our trust. © 2022 Society for Applied Philosophy.",,
"Al Qazlan T.A., Hamdi-Cherif A., Kara-Mohamed C.","State of the art of fuzzy methods for gene regulatory networks inference","10.1155/2015/148010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927126843&doi=10.1155%2f2015%2f148010&partnerID=40&md5=80e7a19d2be2f90b92d54deb205ecd65","To address one of the most challenging issues at the cellular level, this paper surveys the fuzzy methods used in gene regulatory networks (GRNs) inference. GRNs represent causal relationships between genes that have a direct influence, trough protein production, on the life and the development of living organisms and provide a useful contribution to the understanding of the cellular functions as well as the mechanisms of diseases. Fuzzy systems are based on handling imprecise knowledge, such as biological information. They provide viable computational tools for inferring GRNs from gene expression data, thus contributing to the discovery of gene interactions responsible for specific diseases and/or ad hoc correcting therapies. Increasing computational power and high throughput technologies have provided powerful means to manage these challenging digital ecosystems at different levels from cell to society globally. The main aim of this paper is to report, present, and discuss the main contributions of this multidisciplinary field in a coherent and structured framework. © 2015 Tuqyah Abdullah Al Qazlan et al.",,"conceptual framework; data mining; fuzzy logic; fuzzy system; gene expression; gene regulatory network; human; microarray analysis; nonhuman; protein analysis; Review; animal; biological model; biology; gene regulatory network; genetics; procedures; trends; Animals; Computational Biology; Fuzzy Logic; Gene Regulatory Networks; Humans; Models, Genetic"
"Al Shorman A.R., Faris H., Castillo P.A., Merelo J.J., Al-Madi N.","The influence of input data standardization methods on the prediction accuracy of genetic programming generated classifiers","10.5220/0006959000790085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059121872&doi=10.5220%2f0006959000790085&partnerID=40&md5=e509ff8f2d2775e9537531d6da930339","Genetic programming (GP) is a powerful classification technique. It is interpretable and it can dynamically build very complex expressions that maximize or minimize some fitness functions. It has a capacity to model very complex problems in the area of Machine Learning, Data Mining and Pattern Recognition. Nevertheless, GP has a high computational complexity time. On the other side, data standardization is one of the most important pre-processing steps in machine learning. The purpose of this step is to unify the scale of all input features to have equal contribution to the model. The objective of this paper is to investigate the influence of input data standardization methods on GP, and how it affects its prediction accuracy. Six different methods of input data standardization were checked in order to determine which one allows to achieve the most accurate result with lowest computational cost. The simulations have been implemented on ten benchmarked datasets with three different scenarios (varying the population size and number of generations). The results showed that the computational efficiency of GP is highly enhanced when coupled with some standardization methods, specifically Min-Max method for scenario I and Vector method for scenario II, and scenario III. Whereas, Manhattan and Z-Score methods had the worst results for all three scenarios. Copyright 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Classification; Genetic programming; Preprocessing; Standardization methods","Classification (of information); Computational efficiency; Data mining; Genetic algorithms; Input output programs; Machine learning; Pattern recognition; Population statistics; Standardization; Classification technique; Computational costs; Data standardization; Fitness functions; Population sizes; Pre-processing step; Prediction accuracy; Preprocessing; Genetic programming"
"Al taweraqi N., King R.D.","Improved prediction of gene expression through integrating cell signalling models with machine learning","10.1186/s12859-022-04787-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135566660&doi=10.1186%2fs12859-022-04787-8&partnerID=40&md5=2664f91b23c985bf2d32fe499b0197fe","Background: A key problem in bioinformatics is that of predicting gene expression levels. There are two broad approaches: use of mechanistic models that aim to directly simulate the underlying biology, and use of machine learning (ML) to empirically predict expression levels from descriptors of the experiments. There are advantages and disadvantages to both approaches: mechanistic models more directly reflect the underlying biological causation, but do not directly utilize the available empirical data; while ML methods do not fully utilize existing biological knowledge. Results: Here, we investigate overcoming these disadvantages by integrating mechanistic cell signalling models with ML. Our approach to integration is to augment ML with similarity features (attributes) computed from cell signalling models. Seven sets of different similarity feature were generated using graph theory. Each set of features was in turn used to learn multi-target regression models. All the features have significantly improved accuracy over the baseline model - without the similarity features. Finally, the seven multi-target regression models were stacked together to form an overall prediction model that was significantly better than the baseline on 95% of genes on an independent test set. The similarity features enable this stacking model to provide interpretable knowledge about cancer, e.g. the role of ERBB3 in the MCF7 breast cancer cell line. Conclusion: Integrating mechanistic models as graphs helps to both improve the predictive results of machine learning models, and to provide biological knowledge about genes that can help in building state-of-the-art mechanistic models. © 2022, The Author(s).","Gene expression; Machine learning; Multi-target regression","Cell culture; Cell signaling; Diseases; Forecasting; Graph theory; Machine learning; Regression analysis; Broader Approach; Cells signaling; Gene expression levels; Genes expression; Machine-learning; Mechanistic models; Multi-target regression; Multi-targets; Regression modelling; Target regression; Gene expression; biology; gene expression; human; machine learning; neoplasm; procedures; Computational Biology; Gene Expression; Humans; Machine Learning; Neoplasms"
"Al W.A., Yun I.D.","Actor-Critic Reinforcement Learning for Automatic Left Atrial Appendage Segmentation","10.1109/BIBM.2018.8621575","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062487073&doi=10.1109%2fBIBM.2018.8621575&partnerID=40&md5=939a50e2ced0eba9f72ec8c2396bc813","Left atrial appendage (LAA) is a major thrombus formation site, potentially responsible for atrial fibrillation (AF)associated stroke. In analyzing the risk factor of the AF-patients, diagnosing the LAA anatomy plays a significant role. Therefore, an automatic segmentation of the LAA can facilitate an accelerated AF diagnosis. It can also help physicians in preprocedural planning of LAA closure, which is an implant-based strategy to prevent thromboembolism in LAA. However, the high anatomic variation of the LAA, and leaking through the adjacent left superior pulmonary vein yield major challenges in LAA segmentation. With some prior works generally relying on a manual annotation of a bounding box, fully automated segmentation approach is rare to be found. In this paper, we propose a fully automatic LAA segmentation method powered by an actor-critic reinforcement learning agent where the agent proposes necessary segmentation seeds to perform a geodesic distance-based segmentation. The proposed method could resolve all the major challenges of LAA segmentation. To the best of our knowledge, this is the first automated method for LAA segmentation. Compared to the previous methods, it performs the segmentation with a significantly greater efficiency, taking only 7.6 seconds. © 2018 IEEE.","Actor-critic; automatic; left atrial appendage; reinforcement learning; segmentation","Bioinformatics; Diagnosis; Image segmentation; Machine learning; Actor critic; Actor-critic reinforcement learning; Atrial fibrillation; automatic; Automatic segmentations; left atrial appendage; Preprocedural planning; Segmentation methods; Reinforcement learning"
"Al Zoubi O., Misaki M., Tsuchiyagaito A., Zotev V., White E., Paulus M., Bodurka J.","Machine Learning Evidence for Sex Differences Consistently Influences Resting-State Functional Magnetic Resonance Imaging Fluctuations Across Multiple Independently Acquired Data Sets","10.1089/brain.2020.0878","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130632219&doi=10.1089%2fbrain.2020.0878&partnerID=40&md5=f65bc4e0adbf971eb7da1466e7d52192","Background/Introduction: Sex classification using functional connectivity from resting-state functional magnetic resonance imaging (rs-fMRI) has shown promising results. This suggested that sex difference might also be embedded in the blood-oxygen-level-dependent properties such as the amplitude of low-frequency fluctuation (ALFF) and the fraction of ALFF (fALFF). This study comprehensively investigates sex differences using a reliable and explainable machine learning (ML) pipeline. Five independent cohorts of rs-fMRI with over than 5500 samples were used to assess sex classification performance and map the spatial distribution of the important brain regions. Methods: Five rs-fMRI samples were used to extract ALFF and fALFF features from predefined brain parcellations and then were fed into an unbiased and explainable ML pipeline with a wide range of methods. The pipeline comprehensively assessed unbiased performance for within-sample and across-sample validation. In addition, the parcellation effect, classifier selection, scanning length, spatial distribution, reproducibility, and feature importance were analyzed and evaluated thoroughly in the study. Results: The results demonstrated high sex classification accuracies from healthy adults (area under the curve >0.89), while degrading for nonhealthy subjects. Sex classification showed moderate to good intraclass correlation coefficient based on parcellation. Linear classifiers outperform nonlinear classifiers. Sex differences could be detected even with a short rs-fMRI scan (e.g., 2 min). The spatial distribution of important features overlaps with previous results from studies. Discussion: Sex differences are consistent in rs-fMRI and should be considered seriously in any study design, analysis, or interpretation. Features that discriminate males and females were found to be distributed across several different brain regions, suggesting a complex mosaic for sex differences in rs-fMRI. The presented study unraveled that sex differences are embedded in the blood-oxygen-level dependent (BOLD) and can be predicted using unbiased and explainable machine learning pipeline. The study revealed that psychiatric disorders and demographics might influence the BOLD signal and interact with the classification of sex. The spatial distribution of the important features presented here supports the notion that the brain is a mosaic of male and female features. The findings emphasize the importance of controlling for sex when conducting brain imaging analysis. In addition, the presented framework can be adapted to classify other variables from resting-state BOLD signals. © 2022, Mary Ann Liebert, Inc., publishers.","Classification; Deep learning; fMRI; Machine learning; Resting state; Sex","oxygen; adult; brain; brain mapping; diagnostic imaging; female; human; machine learning; male; nuclear magnetic resonance imaging; procedures; reproducibility; sexual characteristics; Adult; Brain; Brain Mapping; Female; Humans; Machine Learning; Magnetic Resonance Imaging; Male; Oxygen; Reproducibility of Results; Sex Characteristics"
"Alaa A.M., Gurdasani D., Harris A.L., Rashbass J., van der Schaar M.","Machine learning to guide the use of adjuvant therapies for breast cancer","10.1038/s42256-021-00353-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112718017&doi=10.1038%2fs42256-021-00353-8&partnerID=40&md5=0875c2ef56b926a358818e726a89d019","Accurate prediction of the individualized survival benefit of adjuvant therapy is key to making informed therapeutic decisions for patients with early invasive breast cancer. Machine learning technologies can enable accurate prognostication of patient outcomes under different treatment options by modelling complex interactions between risk factors in a data-driven fashion. Here, we use an automated and interpretable machine learning algorithm to develop a breast cancer prognostication and treatment benefit prediction model—Adjutorium—using data from large-scale cohorts of nearly one million women captured in the national cancer registries of the United Kingdom and the United States. We trained and internally validated the Adjutorium model on 395,862 patients from the UK National Cancer Registration and Analysis Service (NCRAS), and then externally validated the model among 571,635 patients from the US Surveillance, Epidemiology, and End Results (SEER) programme. Adjutorium exhibited significantly improved accuracy compared to the major prognostic tool in current clinical use (PREDICT v2.1) in both internal and external validation. Importantly, our model substantially improved accuracy in specific subgroups known to be under-served by existing models. Adjutorium is currently implemented as a web-based decision support tool (https://vanderschaar-lab.com/adjutorium/) to aid decisions on adjuvant therapy in women with early breast cancer, and can be publicly accessed by patients and clinicians worldwide. © 2021, The Author(s), under exclusive licence to Springer Nature Limited.",,"Decision support systems; Diseases; Forecasting; HTTP; Learning algorithms; Patient treatment; Predictive analytics; Risk assessment; Accurate prediction; Adjuvant therapy; Cancer registries; Decision support tools; Different treatments; Early breast cancer; Machine learning technology; Prediction model; Machine learning"
"Alaa A.M., van der Schaar M.","Demystifying black-box models with symbolic metamodels",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078483653&partnerID=40&md5=e624a02d5e67fdb84876eed77ec60513","Understanding the predictions of a machine learning model can be as crucial as the model's accuracy in many application domains. However, the black-box nature of most highly-accurate (complex) models is a major hindrance to their interpretability. To address this issue, we introduce the symbolic metamodeling framework - a general methodology for interpreting predictions by converting “black-box” models into “white-box” functions that are understandable to human subjects. A symbolic metamodel is a model of a model, i.e., a surrogate model of a trained (machine learning) model expressed through a succinct symbolic expression that comprises familiar mathematical functions and can be subjected to symbolic manipulation. We parameterize metamodels using Meijer G-functions - a class of complex-valued contour integrals that depend on real-valued parameters, and whose solutions reduce to familiar algebraic, analytic and closed-form functions for different parameter settings. This parameterization enables efficient optimization of metamodels via gradient descent, and allows discovering the functional forms learned by a model with minimal a priori assumptions. We show that symbolic metamodeling provides a generalized framework for model interpretation - many common forms of model explanation can be analytically derived from a symbolic metamodel. © 2019 Neural information processing systems foundation. All rights reserved.",,"Gradient methods; Machine learning; Turing machines; General methodologies; Machine learning models; Mathematical functions; Meijer g functions; Model interpretations; Parameter setting; Symbolic expression; Symbolic manipulation; Functions"
"ALABDULWAHHAB K.M., SAMI W., MEHMOOD T., MEO S.A., ALASBALI T.A., ALWADANI F.A.","Automated detection of diabetic retinopathy using machine learning classifiers","10.26355/eurrev_202101_24615","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101399414&doi=10.26355%2feurrev_202101_24615&partnerID=40&md5=5c6f11fd7800b8b9592b14d05e35d1a5","OBJECTIVE: Diabetic Retinopathy (DR) is a highly threatening microvascular complication of diabetes mellitus. Diabetic patients must be screened annually for DR; however, it is practically not viable due to the high volume of patients, lack of resources, economic burden, and cost of the screening procedure. The use of machine learning (ML) classifiers in medical science is an emerging frontier and can help in assisted diagnosis. The few available proposed models perform best when used in similar population cohorts and their external validation has been questioned. Therefore, the purpose of our research is to classify the DR using different ML methods on Saudi diabetic data, propose the best method based on accuracy and identify the most discriminative interpretable features using the socio-demographic and clinical information. PATIENTS AND METHODS: This cross-sectional study was conducted among 327 diabetic patients in Almajmaah, Saudi Arabia. Socio-demographic and clinical data were collected using a systematic random sampling technique. For DR classification, ML algorithm including, linear discriminant analysis, support vector machine, K nearest neighbor, random forest and its variate ranger random forest classifiers were used through cross-validation resampling procedure. RESULTS: In classifying DR, ranger random forest outperforms the other methods by accurately classifying 86% of the DR patients on the test data. HbA1c (p<0.001) and duration of diabetes (p<0.001) were the most influential risk factor that best discriminated the DR patients. Other influential risk factors were the body mass index (p<0.001), age-onset (p<0.001), age (p<0.001), systolic blood pressure (p<0.05), and the use of medication (p<0.05) that significantly discriminated the DR patients. CONCLUSIONS: Based on the present study findings, integrating ophthalmology and ML can transform diagnosing the disease pattern that can help generate a compelling clinical effect. ML can be used as an added tool for clinical decision- making and must not be the sole substitute for a clinician. We will work to examine the classification performance of multi-class data using more sophisticated ML methods. © 2021 Verduci Editore s.r.l. All rights reserved.","Diabetic retinopathy; Machine learning; Ranger random forest","hemoglobin A1c; adult; Article; automation; body mass; clinical decision making; cohort analysis; controlled study; cross-sectional study; demography; diabetic patient; diabetic retinopathy; diagnostic accuracy; diagnostic test accuracy study; discriminant analysis; disease classification; disease duration; drug use; female; human; k nearest neighbor; machine learning; major clinical study; male; medical information; onset age; random forest; random sample; ranger random forest classifier; risk factor; Saudi; Saudi Arabia; support vector machine; systolic blood pressure; validation process; diabetic retinopathy; Automation; Cross-Sectional Studies; Diabetic Retinopathy; Female; Humans; Machine Learning; Male; Saudi Arabia"
"Alabi R.O., Youssef O., Pirinen M., Elmusrati M., Mäkitie A.A., Leivo I., Almangush A.","Machine learning in oral squamous cell carcinoma: Current status, clinical concerns and prospects for future—A systematic review","10.1016/j.artmed.2021.102060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103775829&doi=10.1016%2fj.artmed.2021.102060&partnerID=40&md5=d20888880e9ff7afc880a6b1fec8d86e","Background: Oral cancer can show heterogenous patterns of behavior. For proper and effective management of oral cancer, early diagnosis and accurate prediction of prognosis are important. To achieve this, artificial intelligence (AI) or its subfield, machine learning, has been touted for its potential to revolutionize cancer management through improved diagnostic precision and prediction of outcomes. Yet, to date, it has made only few contributions to actual medical practice or patient care. Objectives: This study provides a systematic review of diagnostic and prognostic application of machine learning in oral squamous cell carcinoma (OSCC) and also highlights some of the limitations and concerns of clinicians towards the implementation of machine learning-based models for daily clinical practice. Data sources: We searched OvidMedline, PubMed, Scopus, Web of Science, and Institute of Electrical and Electronics Engineers (IEEE) databases from inception until February 2020 for articles that used machine learning for diagnostic or prognostic purposes of OSCC. Eligibility criteria: Only original studies that examined the application of machine learning models for prognostic and/or diagnostic purposes were considered. Data extraction: Independent extraction of articles was done by two researchers (A.R. & O.Y) using predefine study selection criteria. We used the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) in the searching and screening processes. We also used Prediction model Risk of Bias Assessment Tool (PROBAST) for assessing the risk of bias (ROB) and quality of included studies. Results: A total of 41 studies were published to have used machine learning to aid in the diagnosis/or prognosis of OSCC. The majority of these studies used the support vector machine (SVM) and artificial neural network (ANN) algorithms as machine learning techniques. Their specificity ranged from 0.57 to 1.00, sensitivity from 0.70 to 1.00, and accuracy from 63.4 % to 100.0 % in these studies. The main limitations and concerns can be grouped as either the challenges inherent to the science of machine learning or relating to the clinical implementations. Conclusion: Machine learning models have been reported to show promising performances for diagnostic and prognostic analyses in studies of oral cancer. These models should be developed to further enhance explainability, interpretability, and externally validated for generalizability in order to be safely integrated into daily clinical practices. Also, regulatory frameworks for the adoption of these models in clinical practices are necessary. © 2021 Elsevier B.V.","Explainable AI; Machine learning; Oral squamous cell carcinoma; Systematic review","Diagnosis; Diseases; Extraction; Forecasting; Neural networks; Predictive analytics; Risk assessment; Search engines; Support vector machines; Effective management; Eligibility criterion; Institute of Electrical and Electronics Engineers; Machine learning models; Machine learning techniques; Oral squamous cell carcinomata; Prognostic analysis; Regulatory frameworks; Learning systems; accuracy; artificial neural network; autofluorescence; automation; Bayesian network; cancer diagnosis; cancer prognosis; cancer recurrence; cancer staging; clinical practice; confocal laser scanning microscopy; convolutional neural network; decision tree; deep neural network; disease specific survival; feature selection; fuzzy system; histopathology; human; immunohistochemistry; k nearest neighbor; lymph node metastasis; machine learning; mouth cancer; mouth squamous cell carcinoma; multilayer perceptron; overall survival; priority journal; random forest; relevance vector machine; Review; sensitivity and specificity; support vector machine; survival rate; systematic review; tongue carcinoma; artificial intelligence; head and neck tumor; machine learning; meta analysis; mouth tumor; squamous cell carcinoma; Artificial Intelligence; Carcinoma, Squamous Cell; Head and Neck Neoplasms; Humans; Machine Learning; Mouth Neoplasms; Squamous Cell Carcinoma of Head and Neck"
"Alabied S., Hamomd O., Daraz A., Gu F., Ball A.D.","Fault diagnosis of centrifugal pumps based on the intrinsic time-scale decomposition of motor current signals","10.23919/IConAC.2017.8082027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040012373&doi=10.23919%2fIConAC.2017.8082027&partnerID=40&md5=60315f4df38fb0792eed873395fd8199","Centrifugal pumps are widely used in various manufacturing processes, such as power plants, and chemistry. However, pump problems are responsible for large amount of the maintenance budget. An early detection of such problems would provide timely information to take appropriate preventive actions. This paper investigates the application of Machine Learning Techniques (MLT) in monitoring and diagnosing fault in centrifugal pump. In particular, the focus is on utilising motor current signals since they can be measured remotely for easy and low-cost deployment. Moreover, because the signals are usually produced by a nonlinear process and contaminated by various noises, it is difficult to obtain accurate diagnostic features with conventional signal processing methods such as Fourier spectrum and wavelet transforms as they rely heavily on standard basis functions and often capture limited nonlinear weak fault signatures. Therefore, a data-driven method: Intrinsic Time-scale Decomposition (ITD) is adopted in this study to process motor current signals from different pump fault cases. The results indicate that the proposed ITD technique is an effective method for extracting useful diagnostic information, leading to accurate diagnosis by combining the RMS values of the first Proper Rotation Component (PRC) with the raw signal RMS values. © 2017 Chinese Automation and Computing Society in the UK - CACSUK.","Centrifugal Pump; Current Signal Analysis; Intrinsic Time-scale Decomposition","Automation; Budget control; Centrifugal pumps; Fault detection; Learning systems; Mathematical transformations; Pumps; Signal processing; Time measurement; Wavelet transforms; Current signal analysis; Data-driven methods; Intrinsic time-scale decompositions; Machine learning techniques; Maintenance budgets; Manufacturing process; Motor current signals; Rotation components; Electric fault currents"
"AL-Abri H.H., Kumar B., Mani J.","Improving Fraud Detection Mechanism in Financial Banking Sectors Using Data Mining Techniques","10.1007/978-981-33-4299-6_70","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105911280&doi=10.1007%2f978-981-33-4299-6_70&partnerID=40&md5=7cbed18532e00bf388a3b1419df9d0fc","The banking sector is facing increasing risks of fraud and malpractices. Thus, adopting new methods and approaches to detect, prevent, and predict frauds is essential. Data Mining (DM) techniques are the latest methods which are primarily responsible for ensuring data integrity, when are used in the way of machine learning approach. This paper essentially discusses how can the banking sector takes the advantage of one of these methods is a logistic regression. It is an effective classification algorithm that gives high accuracy for detecting binary problems and through the outcomes it can predict irregular transactions. The paper offers this algorithm effectively by presenting suggested developed model, as a part of a framework for the development of a fraud detection mechanism. Finally, the paper will test the suggested model by using a LR algorithm for proving its performance. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Agile approach; Confusion matrix; Cross-industry process for data mining (CRISP-DM) methodology; Data mining; Fraud detection; Logistic regression; Machine learning","Banking; Crime; Intelligent computing; Logistic regression; Banking sectors; Binary problems; Classification algorithm; Data integrity; Developed model; Fraud detection; High-accuracy; Machine learning approaches; Data mining"
"Aladsani M.A., Burton H., Abdullah S.A., Wallace J.W.","Explainable Machine Learning Model for Predicting Drift Capacity of Reinforced Concrete Walls","10.14359/51734484","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131300644&doi=10.14359%2f51734484&partnerID=40&md5=8b4012b977703e3450bc72eb5fbf1e60","The ability to predict the drift capacity of reinforced concrete structural walls is critical to the seismic design process. The accuracy of such predictions has implications for construction costs, seismic safety, and reliability. However, the inability of an empirical model to capture any nonlinearity that exists between the drift capacity and different influencing variables can negatively impact the predictive performance. This study proposes a drift capacity prediction model for special structural walls based on the extreme gradient boosting machine-learning algorithm and a data set of 164 special boundary element wall tests. The efficiency of the proposed model is evaluated using a nested cross-validation approach, and the results reveal its superior predictive capabilities relative to the empirical equation adopted in ACI 318-19. To overcome the lack of interpretability of the model, SHapley Additive exPlanations are used to examine the relative individual and interactive effects of the different input variables on the drift capacity. © 2022 American Concrete Institute. All rights reserved.","artificial intelligence; drift capacity; extreme gradient boosting; machine learning; reinforced concrete walls; special boundary elements","Adaptive boosting; Machine learning; Reinforced concrete; Seismic design; Seismology; Statistical tests; Boundary elements; Design-process; Drift capacity; Extreme gradient boosting; Gradient boosting; Machine learning models; Reinforced concrete structural walls; Reinforced concrete wall; Special boundaries; Special boundary element; Forecasting"
"Alaff A.J.I., Mukhairez H.H.A., Kose U.","An Explainable Artificial Intelligence Model for Detecting COVID-19 with Twitter Text Classification: Turkey Case","10.1007/978-981-33-4084-8_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104763429&doi=10.1007%2f978-981-33-4084-8_9&partnerID=40&md5=98824461b02f2511dd91a1680dc2cb2a","In December 2019, the first case of the coronavirus was reported, specifically in Wuhan, in China, and the virus began to spread very quickly until it reached more than 3 million cases around the world. But with the lack of technology and medical equipment and the existence of low health awareness in many countries, there is an intense research to combat with that massive problem. In this context, objective of this paper is to discover the spread of the virus according to the countries, by following what users around the world publish on social networking sites, especially on Twitter. In detail, we proposed an explainable artificial intelligence (XAI)-based text classification model that depends on three main steps to discover approximate numbers of infects, by checking the symptoms that published within Twitter posts. A dataset publishing cases of infected and the accompanying symptoms of more than 112 k cases was considered and a model with Naïve Bayes was trained through a comparative work with eight different classifiers. Naïve Bayes is a transparent machine learning technique so it is easier to use probabilistic relations between inputs and the outputs to explain results. Eventually, that XAI-based Naïve Bayes model reached to the highest accuracy rate with 93.6%. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","COVID-19; Explainable artificial intelligence; Machine learning; Text classification; Twitter feeds",
"Alagukumar S., Kathirvalavakumar T.","Classifying Microarray Gene Expression Cancer Data Using Statistical Feature Selection and Machine Learning Methods","10.1007/978-981-16-9416-5_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133655354&doi=10.1007%2f978-981-16-9416-5_5&partnerID=40&md5=3fc861385bc33175fc6cf7c19d6668df","Objective: A breast microarray data is a repository of thousands of gene expressions with different strengths of each cancer cell. It is necessary to detect the genes which are responsible for cancer growth. The proposed work aims to identify a statistical test for extracting the differentially expressed genes from a microarray gene expression and a suitable classifier for classifying the gene as diseased and control genes. Method: Cancerous genes are identified by six statistical tests, namely Welch test, analysis of variance (ANOVA) test, Wilcoxon signed rank sum test, Kruskal–Wallis, linear model for microarray (LIMMA), and F-test using their p-values. The identified cancer genes are used to classify cancer patients using seven classifiers, namely linear discriminant analysis (LDA), K-nearest neighbor, Naïve Bayesian, linear support vector machine, support vector machine with radial basis function, C5.0, and C5.0 with boosting technique. Performance is evaluated using accuracy, sensitivity, and specificity. Result: The microarray breast cancer dataset of 32 cancer patients and 28 non-cancer patients is considered in the experiment. Microarray contains 25,575 numbers of genes for each patient. When LIMMA test is used to extract differentially expressed cancer genes and KNN is used for classification, the maximum classification accuracy 100% is obtained. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Breast cancer; Classification; Gene expression data; Microarray; Statistical test","Analysis of variance (ANOVA); Discriminant analysis; Diseases; Gene expression; K-means clustering; Nearest neighbor search; Radial basis function networks; Support vector machines; Breast Cancer; Cancer data; Cancer genes; Cancer patients; Features selection; Gene Expression Data; Linear modeling; Machine learning methods; Microarray gene expression; Statistical features; Statistical tests"
"Alagundi S., Palanisamy T.","Neural network prediction of joint shear strength of exterior beam-column joint","10.1016/j.istruc.2022.01.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123837959&doi=10.1016%2fj.istruc.2022.01.013&partnerID=40&md5=5ef1508c4705823248a337e71122123b","Beam-Column joints are the critical locations in the reinforced concrete structures as they experience a massive amount of deformations under earthquake. The shear failure of the beam column joint should be avoided for the safety of the structure. In the present study, prediction of joint shear capacity of exterior Beam-column joint is proposed using artificial neural network (ANN). Experimental investigations performed by different authors have been examined and used to prepare the data sets for training, testing and validating the neural network. Parameters responsible for the shear strength of the exterior Beam-Column Joints are identified and the artificial neural network model is proposed to predict the joint shear strength. Input parameters for the ANN model are width and depth of the joint, concrete compressive strength, length of beam, top and bottom longitudinal reinforcement in the beam, yield strength of longitudinal reinforcement in beam, ratio of beam to column depth, joint Shear reinforcement index, beam bar index and column load index. The performance of the neural network model is evaluated by the statistical relations like Coefficient of correlation, Root mean square error and Scatter index. The proposed model is compared with an empirical formula and different equations suggested by the design codes. The results show that the proposed neural network model can effectively predict the joint shear strength of the Exterior Beam-Column joint. © 2022 Institution of Structural Engineers","Artificial neural network; Beam-Column joint; Earthquake; Joint shear strength; Machine learning",
"Alahmad Y., Daradkeh T., Agarwal A.","Proactive Failure-Aware Task Scheduling Framework for Cloud Computing","10.1109/ACCESS.2021.3101147","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111611035&doi=10.1109%2fACCESS.2021.3101147&partnerID=40&md5=57858caa1882b1a6c64aef9815c5dc32","Cloud computing is a widely adopted platform for executing tasks of different application types that belong to the end users. In the cloud, application task is prone to failure for several reasons, such as software bug or exception, virtual or physical infrastructure failure. Cloud service providers are responsible for managing availability of scheduled computing tasks in order to provide high level QoS for their customers. Protecting task against failure is a challenging and not a trivial mission due to dynamic, heterogeneous and large distributed structure of the cloud environment. The existing works in the literature focus on task failure prediction and neglect the remedy (post) actions. In this work, we first study and analyze three publicly available large cluster datasets from Google, Alibaba, and Trinity, to characterize task failure in cloud computing platform. We then propose a failure-aware task scheduling framework that can predict the termination status for a set of given tasks during the runtime, and take the appropriate remedy actions. The framework uses deep learning methods named Artificial and Convolutional Neural Network, ANN and CNN, for different prediction purposes. In addition, we formalize the actions selection problem as Integer Linear Programming (ILP) model and propose a heuristic optimization solution that aims to minimize the failure probability of tasks and their resources usage. The results show ANN and CNN can achieve prediction accuracy of up to 94% and 92%, respectively using Google dataset. Moreover, the framework can protect up to 40% of tasks that are predicted as failed using Alibaba dataset by taking the appropriate remedy actions, and hence save many of cluster's resources such as CPU and RAM. © 2013 IEEE.","cloud computing; deep learning; Task failure prediction; task scheduling","Application programs; Cloud computing; Convolutional neural networks; Deep learning; Failure (mechanical); Forecasting; Integer programming; Large dataset; Learning systems; Multitasking; Cloud computing platforms; Cloud service providers; Distributed structures; Failure Probability; Heuristic optimization; Infrastructure failures; Integer linear programming models; Prediction accuracy; Cluster computing"
"Alam A., Khan M.N., Khan J., Lee Y.-K.","IntelliBVR - Intelligent large-scale video retrieval for objects and events utilizing distributed deep-learning and semantic approaches","10.1109/BigComp48618.2020.0-103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084374626&doi=10.1109%2fBigComp48618.2020.0-103&partnerID=40&md5=27a9e6944ac8a9db5e128507d846eaf0","There is increasing reliance on the intelligent CCTV systems for effective analysis and interpretation of the streaming data to recognize activities and to ensure public safety. Monitoring videos captured by surveillance cameras is always a challenging and time-consuming task. There is a need for automated analysis using computer vision methods in order to extract spatial and temporal features to assist the authorities. Once videos are processed using computer vision technologies, another issue is how to index the extracted low-level features to search, analyze, and browse? How to bridge the semantic gap between the low-level features in Euclidean space and temporal relation across videos in a multi-stream environment? Similarly, how to deal with petascale video in the cloud while extracting the low-level and high-level features? In order to address such issues, in this paper, we propose a layered architecture for large-scale distributed intelligent video retrieval while exploiting deep-learning and semantic approaches called IntelliBVR. The base layer is responsible for large-scale video data curation. The second and the third layer is supposed to process and annotate videos, respectively while using deep learning on the top of distributed in-memory computing engine. Finally, the knowledge curation layer, where the extracted low-level and high-level features are mapped to the proposed ontology so that it can be searched and retrieved using semantic rich queries. Finally, we implement and show results, which project the effectiveness of IntelliBVR. © 2020 IEEE.","Deep-learning; Distributed computing; Intelligent video search; Ontology; Spark; Video analytics; Video ontology","Big data; Computer vision; Data curation; Security systems; Semantics; Automated analysis; Computer vision technology; High-level features; Knowledge curation; Layered architecture; Low-level features; Surveillance cameras; Time-consuming tasks; Deep learning"
"Alam A.M., Gurbuz A.C., Kurum M.","SMAP Radiometer RFI Prediction with Deep Learning using Antenna Counts","10.1109/IGARSS46834.2022.9884010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140383181&doi=10.1109%2fIGARSS46834.2022.9884010&partnerID=40&md5=66906b5b40a92e4edf9e03560eac3a8f","Soil Moisture Active Passive (SMAP) is a NASA's earth observing satellite which is used for global scale soil moisture measurement and differentiating frozen/thawed state. It is employed in 1400-1427 MHz protected band which uses L-Band radiometer for the quantification. But increasing number of wireless equipment such as air surveillance radar signals and 5G communication are making it harder to protect the radiometer microwave sensing in this secured spectrum. These technologies are responsible for the Radio Frequency Interference (RFI) in SMAP's passive observation. In this study, a novel deep learning architecture is developed that uses convolutional neural network (CNN) to predict RFI. Our model uses SMAP's level 1A raw antenna counts as well as level 1B quality flags to dynamically label these antenna raw measurements as RFI contaminated and RFI free footprints. This example study shows around 94% accuracy in detecting RFI and such result may recommend a lucrative technique in detecting RFI. © 2022 IEEE.","Deep Learning; Radiometer; RFI; SMAP",
"Alam H., Kumar A.","Multi-lingual author identification and linguistic feature extraction - A machine learning approach","10.1109/THS.2013.6699035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893285618&doi=10.1109%2fTHS.2013.6699035&partnerID=40&md5=f5a77dd91e34a2428eb20563039354e9","Internet based services have emerged as one of the most effective platform to express and exchange views. Most of these services allow anonymous postings. Lately, it has been observed that anonymous postings responsible to instigate violence or cause panic. Some studies have been made to identify authors for such blogs, mostly target to English postings. Current author identification systems do not employ rich morphological features for languages such as Arabic (Modern Standard Arabic). In this study we develop a novel semantic feature to aid author identification system for Arabic. To completely exploit rich morphology of Arabic, we used parse tree intelligently as features. The overall approach uses language-specific NLP parsers, lexicons, semantic processing, thematic role assignment, semantic heuristics, and machine learning techniques to rapidly train systems for the subtleties mentioned above for Arabic. Our system identifies authors on the basis of stylistic and linguistic similarities between the author's existing works and the unidentified text in the form of online blogs and articles. We use support vector machine (SVM) to identify authors based on these novel features. Our approach yields accuracy of 98% in law and order and terrorism related Arabic blogs. © 2013 IEEE.","Author Identification; Feature Extraction; NLP; Semantic Features; Support Vector Machine","Author identification; Internet-based services; Linguistic similarities; Machine learning approaches; Machine learning techniques; Morphological features; NLP; Semantic features; Feature extraction; Internet; National security; Natural language processing systems; Security systems; Semantics; Support vector machines; Linguistics"
"Alam M., Ul Hussain S.","Sequence to sequence networks for Roman-Urdu to Urdu transliteration","10.1109/INMIC.2017.8289449","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050591827&doi=10.1109%2fINMIC.2017.8289449&partnerID=40&md5=56321c10a1ec6db806141a006350eb51","Neural Machine Translation models have replaced the conventional phrase based statistical translation methods since the former takes a generic, scalable, data-driven approach rather than relying on manual, hand-crafted features. The neural machine translation system is based on one neural network that is composed of two parts, one that is responsible for input language sentence and other part that handles the desired output language sentence. This model based on encoder-decoder architecture also takes as input the distributed representations of the source language which enriches the learnt dependencies and gives a warm start to the network. In this work, we transform Roman-Urdu to Urdu transliteration into sequence to sequence learning problem. To this end, we make the following contributions. We create the first ever parallel corpora of Roman-Urdu to Urdu, create the first ever distributed representation of Roman-Urdu and present the first neural machine translation model that transliterates text from Roman-Urdu to Urdu language. Our model has achieved the state-of-the-art results using BLEU as the evaluation metric. Precisely, our model is able to correctly predict sentences up to length 10 while achieving BLEU score of 48.6 on the test set. We are hopeful that our model and our results shall serve as the baseline for further work in the domain of neural machine translation for Roman-Urdu to Urdu using distributed representation. © 2017 IEEE.","deep learning; distributed representation; natural language processing; neural network; parallel corpora; sequence to sequence models","Computational linguistics; Computer aided language translation; Deep learning; Neural networks; Data-driven approach; Distributed representation; Encoder-decoder architecture; Machine translation models; Machine translation systems; Parallel corpora; Sequence models; Statistical translation; Natural language processing systems"
"Alam M., Coulet A., Napoli A., Smaïl-Tabbone M.","Formal Concept Analysis applied to transcriptomic data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922702018&partnerID=40&md5=f14687d81cc0d0d84a64464206fd5923","Identifying functions shared by genes responsible for cancer is a challenging task. This paper describes the preparation work for applying Formal Concept Analysis (FCA) to complex biological data. We present here a preliminary experiment using these data on a core context with the addition of domain knowledge. The resulting concept lattices are explored and some interesting concepts are discussed. Our study shows how FCA can help the domain experts in the exploration of complex data. © 2012 by the paper authors.","Formal Concept Analysis; Knowledge discovery; Transcriptomic data","Data mining; Information analysis; Biological data; Complex data; Concept Lattices; Domain experts; Domain knowledge; Formal concept analyses (FCA); Transcriptomic data; Formal concept analysis"
"Alam M.M., Ali M.O., Shahjalal M., Chung B., Jang Y.M.","Optimal Energy Management among Multiple Households with Integrated Shared Energy Storage System (ESS)","10.1109/ICUFN49451.2021.9528536","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115606156&doi=10.1109%2fICUFN49451.2021.9528536&partnerID=40&md5=f261f9d831227b5a76fe22c4955040c6","The integration of artificial intelligence with home energy management systems (HEMS) due to the development of advanced metering infrastructure is a promising scheme to improve the usage of renewable energy in a residential application. In the paper, energy management among multiple co-operative households with PV-Storage integrated generation system in a home micro-grid in the presence of short-term prediction of power generation and consumption is studied. In such a home microgrid system, the central energy storage system (C.ESS) is considered that is connected with multiple household and PV panels. The key parameters that are responsible for optimum scheduling of C.ESS are forecasted PV power generation, forecasted household energy consumption, dynamic state of charge (SOC), and base level of energy consumption. In this paper, firstly, the prediction of short-term generation and consumption based on the long short-term memory (LSTM) algorithm is done. Then, this forecasted data is used as the constraint to the control algorithm for optimum scheduling. Therefore, the amount of power that will be supplied from C.ESS is also determined for properly utilizing the stored energy. The simulation results of the proposed scheme show the robustness and effectiveness in the home microgrid environment. © 2021 IEEE.","Central energy storage system (CESS); control system; energy management; LSTM","Advanced metering infrastructures; Charging (batteries); Digital storage; Electric power transmission networks; Energy management systems; Energy storage; Energy utilization; Long short-term memory; Microgrids; Photovoltaic cells; Scheduling; Scheduling algorithms; Smart power grids; Storage management; Central energy storage system; Generation systems; Home energy management systems; Microgrid; Optimal energy; Power- generations; Renewable energies; Residential application; Short term prediction; Storage systems; Energy management"
"Alam M.M., Mohiuddin K., Islam M.K., Hassan M., Hoque M.A.-U., Allayear S.M.","A machine learning approach to analyze and reduce features to a significant number for employee’s turn over prediction model","10.1007/978-3-030-01177-2_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057102886&doi=10.1007%2f978-3-030-01177-2_11&partnerID=40&md5=607e22f5ff45b4cedee24a2446fbf76f","Turnover of employee considers as one of the major issue that every company faces. Especially, if the employee has advance skills at his/her working field, then the company faces great loss during that period. To find out the most dominant reasons of employee attrition, we approach by determining features and using machine learning algorithms where features have been processed and reduced beforehand. We have proposed a new model where particular attributes of employee turnover have been selected and adjusted accordingly. In first phase of our reduction method, Sequential Backward Selection Algorithm (SBS) has been used to reduce the features from a higher number to a relatively smaller significant number. After that Chi2 and Random Forest importance algorithm have been used together for the second phase of reduction to determine the common important features by both of the algorithms which can be considered as the foremost features that lead to employee turnover. Our two steps feature selection technique confirms that there are mainly three features that are responsible for employee’s departure. Later, these selected minimal features have been tested with state of the art algorithms of machine learning, such as Decision Tree, Random Forest, Support Vector Machine, Multi-layer Perceptron (MLP), K-Nearest Neighbor (kNN) and Gaussian Naïve Bayes. Lastly, the test result has been visualized by 3D representation to learn the features that are precisely involved for the employee’s turnover. © Springer Nature Switzerland AG 2019.","Chi2; Component; Decision tree; kNN; Machine learning; MLP; Naïve bayes; Predictive model; Random forest; SBS; SVM","Antimony; Data mining; Decision trees; Feature extraction; Intelligent computing; Learning systems; Nearest neighbor search; Personnel; Support vector machines; Chi2; Component; Feature selection techniques; K nearest neighbor (KNN); Machine learning approaches; Predictive modeling; Random forests; State-of-the-art algorithms; Learning algorithms"
"Alam M.U., Baldvinsson J.R., Wang Y.","Exploring LRP and Grad-CAM visualization to interpret multi-label-multi-class pathology prediction using chest radiography","10.1109/CBMS55023.2022.00052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137920000&doi=10.1109%2fCBMS55023.2022.00052&partnerID=40&md5=24cd7b5f604fa0b675c962f157139925","The area of interpretable deep neural networks has received increased attention in recent years due to the need for transparency in various fields, including medicine, healthcare, stock market analysis, compliance with legislation, and law. Layer-wise Relevance Propagation (LRP) and Gradient-weighted Class Activation Mapping (Grad-CAM) are two widely used algorithms to interpret deep neural networks. In this work, we investigated the applicability of these two algorithms in the sensitive application area of interpreting chest radiography images. In order to get a more nuanced and balanced outcome, we use a multi-label classification-based dataset and analyze the model prediction by visualizing the outcome of LRP and Grad-CAM on the chest radiography images. The results show that LRP provides more granular heatmaps than Grad-CAM when applied to the CheXpert dataset classification model. We posit that this is due to the inherent construction difference of these algorithms (LRP is layer-wise accumulation, whereas Grad-CAM focuses primarily on the final sections in the model's architecture). Both can be useful for understanding the classification from a micro or macro level to get a superior and interpretable clinical decision support system. © 2022 IEEE.","Chest X-ray; Clinical Decision Support System; Deep Learning; Grad-CAM; Interpretability; LRP; Visualization","Classification (of information); Decision support systems; Multilayer neural networks; Visualization; Activation mapping; Chest radiography; Chest X-ray; Clinical decision support systems; Deep learning; Gradient-weighted class activation mapping; Interpretability; Layer-wise; Layer-wise relevance propagation; Radiography images; Deep neural networks"
"Alam R., Peden D.B., Lach J.C.","Wearable Respiration Monitoring: Interpretable Inference with Context and Sensor Biomarkers","10.1109/JBHI.2020.3035776","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098791588&doi=10.1109%2fJBHI.2020.3035776&partnerID=40&md5=24af41fea247279bda9ec4945ccc9134","Continuous monitoring of breathing rate (BR), minute ventilation (VE), and other respiratory parameters could transform care for and empower patients with chronic cardio-pulmonary conditions, such as asthma. However, the clinical standard for measuring respiration, namely Spirometry, is hardly suitable for continuous use. Wearables can track many physiological signals, like ECG and motion, yet respiration tracking faces many challenges. In this work, we infer respiratory parameters from wearable ECG and wrist motion signals. We propose a modular and generalizable classification-regression pipeline to utilize available context information, such as physical activity, in learning context-conditioned inference models. Novel morphological and power domain features from the wearable ECG are extracted to use with these models. Exploratory feature selection methods are incorporated in this pipeline to discover application-driven interpretable biomarkers. Using data from 15 subjects, we evaluate two implementations of the proposed inference pipeline: for BR and VE. Each implementation compares generalized linear model, random forest, support vector machine, Gaussian process regression, and neighborhood component analysis as regression models. Permutation, regularization, and relevance determination methods are used to rank the ECG features to identify robust ECG biomarkers across models and activities. This work demonstrates the potential of wearable sensors not only in continuous monitoring, but also in designing biomarker-driven preventive measures. © 2013 IEEE.","Asthma; Biomarkers; Breathing rate; Classification-Regression; Context; ECG; IMU; Interpretability; Minute ventilation; Respiration; Wearable","Biomarkers; Classification (of information); Decision trees; Electrocardiography; Monitoring; Motion tracking; Pipelines; Respirators; Support vector machines; Support vector regression; Classification regression; Continuous monitoring; Feature selection methods; Gaussian process regression; Generalized linear model; Neighborhood component analysis; Respiration monitoring; Respiratory parameters; Wearable sensors; biological marker; biological marker; adult; Article; artificial intelligence; artificial neural network; artificial ventilation; asthma; bootstrapping; breathing rate; classification algorithm; electrocardiogram; electrocardiography; electroencephalography; female; heart rate variability; hospital readmission; hospitalization; human; human experiment; learning; lung minute volume; machine learning; male; mathematical model; neighborhood; nerve cell network; normal distribution; normal human; peak inspiratory flow; photoelectric plethysmography; physical activity; pipeline; plethysmography; principal component analysis; random forest; signal noise ratio; signal processing; spirometry; steady state; support vector machine; tidal volume; wrist; breathing; breathing rate; electronic device; physiologic monitoring; wrist; Biomarkers; Humans; Monitoring, Physiologic; Respiration; Respiratory Rate; Wearable Electronic Devices; Wrist"
"Alam T.M., Shaukat K., Hameed I.A., Luo S., Sarwar M.U., Shabbir S., Li J., Khushi M.","An investigation of credit card default prediction in the imbalanced datasets","10.1109/ACCESS.2020.3033784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102836976&doi=10.1109%2fACCESS.2020.3033784&partnerID=40&md5=bc665840b899f74351616e1765c9c0c6","Financial threats are displaying a trend about the credit risk of commercial banks as the incredible improvement in the financial industry has arisen. In this way, one of the biggest threats faces by commercial banks is the risk prediction of credit clients. Recent studies mostly focus on enhancing the classifier performance for credit card default prediction rather than an interpretable model. In classification problems, an imbalanced dataset is also crucial to improve the performance of the model because most of the cases lied in one class, and only a few examples are in other categories. Traditional statistical approaches are not suitable to deal with imbalanced data. In this study, a model is developed for credit default prediction by employing various credit-related datasets. There is often a significant difference between the minimum and maximum values in different features, so Min-Max normalization is used to scale the features within one range. Data level resampling techniques are employed to overcome the problem of the data imbalance. Various undersampling and oversampling methods are used to resolve the issue of class imbalance. Different machine learning models are also employed to obtain efficient results. We developed the hypothesis of whether developed models using different machine learning techniques are significantly the same or different and whether resampling techniques significantly improves the performance of the proposed models. One-way Analysis of Variance is a hypothesis-testing technique, used to test the significance of the results. The split method is utilized to validate the results in which data has split into training and test sets. The results on imbalanced datasets show the accuracy of 66.9% on Taiwan clients credit dataset, 70.7% on South German clients credit dataset, and 65% on Belgium clients credit dataset. Conversely, the results using our proposed methods significantly improve the accuracy of 89% on Taiwan clients credit dataset, 84.6% on South German clients credit dataset, and 87.1% on Belgium clients credit dataset. The results show that the performance of classifiers is better on the balanced dataset as compared to the imbalanced dataset. It is also observed that the performance of data oversampling techniques are better than undersampling techniques. Overall, the Gradient Boosted Decision Tree method performs better than other traditional machine learning classifiers. The Gradient Boosted Decision Tree method gives the best results while utilizing the K-means SMOTE oversampling method. Using one-way ANOVA, the null hypothesis was rejected by a p-value <0.001, hence confirming that the proposed model improved performance is statistical significance. The interpretable model is also deployed on the web to ease the different stakeholders. This model will help commercial banks, financial organizations, loan institutes, and other decision-makers to predict the loan defaulter earlier. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Credit card default model; Customer credit risk; Gradient boosted decision tree; Imbalanced data; Interpretable model; Machine learning","Decision making; Decision trees; Finance; Forecasting; Machine learning; Risk assessment; Sampling; Testing; Boosted decision trees; Classifier performance; Financial organizations; Machine learning models; Machine learning techniques; Oversampling technique; Performance of classifier; Statistical significance; Classification (of information)"
"Alam W., Tayara H., Chong K.T.","XG-ac4C: identification of N4-acetylcytidine (ac4C) in mRNA using eXtreme gradient boosting with electron-ion interaction pseudopotentials","10.1038/s41598-020-77824-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096955499&doi=10.1038%2fs41598-020-77824-2&partnerID=40&md5=2776b91a5c0b040e808950fff0818803","N4-acetylcytidine (ac4C) is a post-transcriptional modification in mRNA which plays a major role in the stability and regulation of mRNA translation. The working mechanism of ac4C modification in mRNA is still unclear and traditional laboratory experiments are time-consuming and expensive. Therefore, we propose an XG-ac4C machine learning model based on the eXtreme Gradient Boost classifier for the identification of ac4C sites. The XG-ac4C model uses a combination of electron-ion interaction pseudopotentials and electron-ion interaction pseudopotentials of trinucleotide of the nucleotides in ac4C sites. Moreover, Shapley additive explanations and local interpretable model-agnostic explanations are applied to understand the importance of features and their contribution to the final prediction outcome. The obtained results demonstrate that XG-ac4C outperforms existing state-of-the-art methods. In more detail, the proposed model improves the area under the precision-recall curve by 9.4% and 9.6% in cross-validation and independent tests, respectively. Finally, a user-friendly web server based on the proposed model for ac4C site identification is made freely available at http://nsclbio.jbnu.ac.kr/tools/xgac4c/. © 2020, The Author(s).",,"cytidine; messenger RNA; N-acetylcytidine; algorithm; electron; machine learning; metabolism; receiver operating characteristic; theoretical model; Algorithms; Cytidine; Electrons; Machine Learning; Models, Theoretical; RNA, Messenger; ROC Curve"
"Alamanos A.","Simple hydro-economic tools for supporting small water supply agencies on sustainable irrigation water management","10.2166/ws.2021.318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125591430&doi=10.2166%2fws.2021.318&partnerID=40&md5=4bab0dd52283b15dbe7d1493c1a547a8","In the Mediterranean countries, agriculture poses challenges in terms of its production expectations, resources availability, pollution, general management and implementation of economic tools (e.g. full cost of irrigation water, according to the Water Framework Directive). This study attempts to provide useful approaches for small water supply agencies facing multiple management, funding, environmental, and practical issues. A representative case in Central Greece is examined, in order to describe the situation in understandable terms supporting sustainable management. Simple hydro-economic tools were used to address these challenges; water balance, profits from agriculture, water value, water quality, management strategies, and full cost of irrigation water were simulated and incorporated into a decision support system (DSS), using Multi Criteria Analysis (MCA), involving experts on water resources management and local policymakers. This is the first hydro-economic study designed for a Greek rural agency, aiming to improve and encourage integrated monitoring and management at multiple levels, communicating more efficient water use approaches to local irrigation management communities. © 2022 IWA Publishing. All rights reserved.","Decision support system; Full cost of irrigation water; Hydro-economic modelling; Irrigation water efficiency; Pinios; Water resources management","Artificial intelligence; Cost benefit analysis; Costs; Decision making; Economic analysis; Environmental regulations; Irrigation; Sustainable development; Water conservation; Water management; Water pollution; Water quality; Water supply; Economic tools; Full cost of irrigation water; Full costs; Hydro-economic modelling; Irrigation water efficiency; Irrigation waters; Pinios; Water efficiency'; Water resources management; Decision support systems; water; decision support system; sustainability; water management; water supply; water use; agricultural land; agriculture; Article; cost; decision support system; drip irrigation; fluid balance; Greece; water conservation; water management; water quality; water supply; Central Greece; Greece"
"Alamelu Mangai M., Jegadeeshwaran R., Sugumaran V.","Vibration based condition monitoring of a brake system using statistical features with logit boost and simple logistic algorithm","10.23940/ijpe.18.01.p1.18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042307590&doi=10.23940%2fijpe.18.01.p1.18&partnerID=40&md5=401b1dca0111d5ad2f05fb29673eb5f1","Brakes are responsible for the stability of the vehicle. Brake failure is one of the key elements where more attention is required. Normally, a brake system failure is not an instantaneous process. It is caused by faults due to reasons like wear, mechanical fade, and oil leak, which started long before the failure progresses. Hence, it is essential to build a model that can recognize the condition from the signal. Condition monitoring is one such supervision approach, which continuously monitors the system and gives characteristics data. These data can be analysed and the condition of the component can be extracted using a machine learning approach. This study focuses on one such machine learning approach using the vibration characteristics of the brake system. The machine learning approach was carried out using feature extraction and feature classification. The statistical information extracted from the vibration signals under various fault conditions were used as features. The features were classified using machine learning algorithms, namely, Simple logistics, Logit boost and Multinominal Regression. Results were compared and discussed. The Logit boost algorithm, which produced 98.91 % classification accuracy, has been suggested as an effective approach for the brake fault diagnosis study. © 2018 Totem Publisher, Inc. All rights reserved.","Fault diagnosis; Feature classification; Logistics; Logit boost; Machine learning; Multinomial regression","Artificial intelligence; Brakes; Computer aided diagnosis; Condition monitoring; Data mining; Failure analysis; Fault detection; Learning algorithms; Learning systems; Logistics; Monitoring; Systems engineering; Classification accuracy; Effective approaches; Feature classification; Logit boost; Machine learning approaches; Multinomials; Statistical information; Vibration characteristics; Classification (of information)"
"Alamelu M.T.M., Jegadeeshwaran R., Sakthivel G., Saravanakumar D., Ivakumar R.","Vibration-Based Fault Detection in a Hydraulic Brake System using Artificial Immune Recognition System with Statistical Features","10.1109/iSES50453.2020.00049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106595656&doi=10.1109%2fiSES50453.2020.00049&partnerID=40&md5=67ac8b9bfb0235e0fc9429c78ed7c5df","In an automobile, the brake is an essential part responsible for the control of the vehicle. Any failure in the brake system impacts the vehicle's motion. It will generate frequent catastrophic effects on the safety of the vehicle cum passenger. Thus the brake system plays a vital role in an automobile and hence condition monitoring of the brake system is necessitated. Vibration-based condition monitoring techniques are gaining momentum. This study is one such attempt to categorize the faults that occur in a hydraulic brake system through vibration analysis. In this research, the performance of an artificial intelligence technique called Artificial Immune Recognition System for brake fault diagnosis has been reported. A hydraulic brake system test setup was fabricated. The vibration signals under good and faulty conditions of a brake system were acquired using a piezoelectric transducer. The statistical parameters were extracted from the vibration signals. The best features were identified using an attribute evaluator. The selected features were then classified using various versions of the Artificial Immune Recognition System (AIRS). The classification accuracy of such artificial intelligence techniques has been reported and discussed. © 2020 IEEE.","AIRS 1; AIRS 2; AIRS 2 Parallel; attribute evaluator; Confusion matrix; Statistical features","Artificial intelligence; Automobile electronic equipment; Condition monitoring; Hydraulic brakes; Vibration analysis; Artificial immune recognition system; Artificial intelligence techniques; Catastrophic effects; Classification accuracy; Hydraulic brake systems; Monitoring techniques; Statistical features; Statistical parameters; Fault detection"
"Alami H., Lehoux P., Auclair Y., de Guise M., Gagnon M.-P., Shaw J., Roy D., Fleet R., Ahmed M.A.A., Fortin J.-P.","Artificial intelligence and health technology assessment: Anticipating a new level of complexity","10.2196/17707","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088233421&doi=10.2196%2f17707&partnerID=40&md5=6ad3a91ec9934609c6d2c032100d7118","Artificial intelligence (AI) is seen as a strategic lever to improve access, quality, and efficiency of care and services and to build learning and value-based health systems. Many studies have examined the technical performance of AI within an experimental context. These studies provide limited insights into the issues that its use in a real-world context of care and services raises. To help decision makers address these issues in a systemic and holistic manner, this viewpoint paper relies on the health technology assessment core model to contrast the expectations of the health sector toward the use of AI with the risks that should be mitigated for its responsible deployment. The analysis adopts the perspective of payers (ie, health system organizations and agencies) because of their central role in regulating, financing, and reimbursing novel technologies. This paper suggests that AI-based systems should be seen as a health system transformation lever, rather than a discrete set of technological devices. Their use could bring significant changes and impacts at several levels: technological, clinical, human and cognitive (patient and clinician), professional and organizational, economic, legal, and ethical. The assessment of AI’s value proposition should thus go beyond technical performance and cost logic by performing a holistic analysis of its value in a real-world context of care and services. To guide AI development, generate knowledge, and draw lessons that can be translated into action, the right political, regulatory, organizational, clinical, and technological conditions for innovation should be created as a first step. © Hassane Alami, Pascale Lehoux, Yannick Auclair, Michèle de Guise, Marie-Pierre Gagnon, James Shaw, Denis Roy, Richard Fleet, Mohamed Ali Ag Ahmed, Jean-Paul Fortin.","Artificial intelligence; EHealth; Health care; Health services; Health technology assessment; Medical device; Patient","artificial intelligence; biomedical technology assessment; cognition; economic aspect; health service; legal aspect; medical ethics; organization; reproducibility; Review; risk management; telehealth; artificial intelligence; biomedical technology assessment; human; medical technology; procedures; Artificial Intelligence; Biomedical Technology; Humans; Technology Assessment, Biomedical"
"Alami H., Alami H., Rivard L., Rivard L., Lehoux P., Lehoux P., Hoffman S.J., Hoffman S.J., Cadeddu S.B.M., Cadeddu S.B.M., Savoldelli M., Samri M.A., Ag Ahmed M.A., Fleet R., Fleet R., Fleet R., Fortin J.-P., Fortin J.-P.","Artificial intelligence in health care: Laying the Foundation for Responsible, sustainable, and inclusive innovation in low- And middle-income countries","10.1186/s12992-020-00584-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086989097&doi=10.1186%2fs12992-020-00584-1&partnerID=40&md5=79a24dde456526d4bca3b81e911ed54b","The World Health Organization and other institutions are considering Artificial Intelligence (AI) as a technology that can potentially address some health system gaps, especially the reduction of global health inequalities in low- and middle-income countries (LMICs). However, because most AI-based health applications are developed and implemented in high-income countries, their use in LMICs contexts is recent and there is a lack of robust local evaluations to guide decision-making in low-resource settings. After discussing the potential benefits as well as the risks and challenges raised by AI-based health care, we propose five building blocks to guide the development and implementation of more responsible, sustainable, and inclusive AI health care technologies in LMICs. © 2020 The Author(s).","Artificial intelligence; Digital health; Global health; Human security; Innovation; Low- and middle-income countries; Public health; Universal health coverage","artificial intelligence; decision making; developing world; health care; health services; innovation; risk assessment; sustainability; article; artificial intelligence; decision making; health insurance; high income country; human; middle income country; public health; World Health Organization; developing country; global health; health care facility; health care planning; income; poverty; World Health Organization; Artificial Intelligence; Developing Countries; Global Health; Health Facilities; Health Resources; Humans; Income; Poverty; World Health Organization"
"Alamri A., Sun Z., Cristea A.I., Senthilnathan G., Shi L., Stewart C.","Is MOOC learning different for dropouts? A visually-driven, multi-granularity explanatory ML approach","10.1007/978-3-030-49663-0_42","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086235000&doi=10.1007%2f978-3-030-49663-0_42&partnerID=40&md5=bea0b5009ed5fe7477b5b77179aa8bfc","Millions of people have enrolled and enrol (especially in the Covid-19 pandemic world) in MOOCs. However, the retention rate of learners is notoriously low. The majority of the research work on this issue focuses on predicting the dropout rate, but very few use explainable learning patterns as part of this analysis. However, visual representation of learning patterns could provide deeper insights into learners’ behaviour across different courses, whilst numerical analyses can – and arguably, should – be used to confirm the latter. Thus, this paper proposes and compares different granularity visualisations for learning patterns (based on clickstream data) for both course completers and non-completers. In the large-scale MOOCs we analysed, across various domains, our fine-grained, fish-eye visualisation approach showed that non-completers are more likely to jump forward in their learning sessions, often on a ‘catch-up’ path, whilst completers exhibit linear behaviour. For coarser, bird-eye granularity visualisation, we observed learners’ transition between types of learning activity, obtaining typed transition graphs. The results, backed up by statistical significance analysis and machine learning, provide insights for course instructors to maintain engagement of learners by adapting the course design to not just ‘dry’ predicted values, but explainable, visually viable paths extracted. © Springer Nature Switzerland AG 2020.","Behavioural pattern; Learning analytics; Machine learning; MOOCs; Visualisation","Computer aided instruction; Intelligent vehicle highway systems; Visualization; Different granularities; Learning Activity; Learning patterns; Learning sessions; Multi-granularity; Statistical significance; Transition graphs; Visual representations; Curricula"
"Alamri L.H., Almuslim R.S., Alotibi M.S., Alkadi D.K., Ullah Khan I., Aslam N.","Predicting Student Academic Performance using Support Vector Machine and Random Forest","10.1145/3446590.3446607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106177965&doi=10.1145%2f3446590.3446607&partnerID=40&md5=39ecab976813a77ecce67164bd355b59","The use of machine learning and data mining in the educational field to predict student performance, known as educational data mining, which has always been an important study area. An early prediction of student performance may help the responsible entities to provide solutions to the students with low performance. Student performance in the final exam could be affected by many factors (e.g., previous assignment grades, social life, parents' job, and absence frequency). This paper aims to predict student academics performance to enhance the performance of educational organizations to get better academic results of their students. In this paper, classification algorithms and techniques applied were Support Vector Machines (SVM) and Random Forest (RF). Binary classification and regression techniques have been applied with both SVM and RF. In this work, we predict the final grade of mathematics course and Portuguese language course, the dataset consists of 369 and 649 records, respectively. The experimental results for both SVM and RF algorithms applied to both datasets showed that the accuracy in the case of binary classification achieves a superior accurate prediction reach to 93%, while in regression, the lowest RMSE is 1.13 in case of RF. © 2020 ACM.","Machine learning; Random Forest; Student performance; Supervised learning; Support vector machine","Classification (of information); Data mining; Decision trees; Forecasting; Industrial management; Random forests; Support vector machines; Academic performance; Accurate prediction; Binary classification; Classification algorithm; Educational data mining; Educational organizations; Portuguese languages; Regression techniques; Students"
"Alamri R., Alharbi B.","Explainable Student Performance Prediction Models: A Systematic Review","10.1109/ACCESS.2021.3061368","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101742107&doi=10.1109%2fACCESS.2021.3061368&partnerID=40&md5=3a03f1e9d6dd679a26b6e1db58f07411","Successful prediction of student performance has significant impact to many stakeholders, including students, teachers and educational institutes. In this domain, it is equally important to have accurate and explainable predictions, where accuracy refers to the correctness of the predicted value, and explainability refers to the understandability of the prediction made. In this systematic review, we investigate explainable models of student performance prediction from 2015 to 2020. We analyze and synthesize primary studies, and group them based on nine dimensions. Our analysis revealed the need for more studies on explainable student performance prediction models, where both accuracy and explainability are properly quantified and evaluated. © 2013 IEEE.","Explainable artificial intelligence; explainable machine learning; student performance models; systematic literature review","Forecasting; Students; Educational Institutes; Student performance; Systematic Review; Understandability; Predictive analytics"
"Alamu O.A., Pandya D.A., Warner O., Debacker I.","Esp data analytics: Use of deep autoencoders for intelligent surveillanceof electric submersible pumps",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086268361&partnerID=40&md5=08dfa66d2eab6415d6a43f9586823ecf","Objectives/Scope: Electric Submersible Pump (ESP) account for over 60% of artificial lift methods usedglobally and contribute significantly to the CAPEX and OPEX of a project. They tend to be the leastreliable component in the system with an average life-span of 2 years. This paper demonstrates how artificialintelligence was used to unlock insights from sensor data around an ESP to understand the operatingconditions which lead to a trip and failure of these systems. Methods, Procedures, Process: Autoencoders were used for the detection of anomalous behavior in anESP and the determination of the root cause of an anomalous event. Autoencoders are neural networkstrained to reconstruct input data. They have an encoding and decoding section, the encoder compresses theinput vector, while the decoder reconstructs the original input from the compressed vector. This processallows the network to understand the patterns in a dataset. We trained the network on stable operating datafrom a 2-years historical data dump of 97 sensors. This allowed the model to understand the patterns ofstability in an ESP. Results, Observations, Conclusions: The autoencoder was developed using the Python programminglanguage along with the Keras deep learning framework. It had 7 layers with the exponential linear unitas the activation function for training. During reconstruction, the autoencoder never produces a perfectreconstruction of input data, it, however, performs a good reconstruction on data similar to what it wastrained on. In our case, the model reconstructs stable data well and struggles with unstable data. Thereconstruction error is used to distinguish a normal event from an anomalous event because it increasesprior to an event and reduces as the system returns to stability. During the historical time period, the ESPexperienced 5 major trips, three of them were due to gas locks while the other two were due to electricalissues. The model was able to detect the gas locks on average 5 hrs in advance and electrical issues severaldays in advance before the actual events. The top ten sensors responsible for each event were determinedbased on the relative magnitude of the individual sensor reconstruction errors, the validity of this outputwas confirmed by the Subject Matter Expert. © 2020, Offshore Technology Conference.",,"Data Analytics; Decoding; Deep learning; Input output programs; Learning systems; Locks (fasteners); Offshore oil well production; Signal encoding; Submersible pumps; Submersibles; Activation functions; Anomalous behavior; Artificial lift methods; Electric submersible pumps; Encoding and decoding; Learning frameworks; Sensor reconstruction; Subject matter experts; Offshore technology"
"Alanazi A.H., Cradock A., Rainford L.","Development of lumbar spine MRI referrals vetting models using machine learning and deep learning algorithms: Comparison models vs healthcare professionals","10.1016/j.radi.2022.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132785925&doi=10.1016%2fj.radi.2022.05.005&partnerID=40&md5=07a0c8dc1e246fd107881e139a5ea55f","Introduction: Referrals vetting is a necessary daily task to ensure the appropriateness of radiology referrals. Vetting requires extensive clinical knowledge and may challenge those responsible. This study aims to develop AI models to automate the vetting process and to compare their performance with healthcare professionals. Methods: 1020 lumbar spine MRI referrals were collected retrospectively from two Irish hospitals. Three expert MRI radiographers classified the referrals into indicated or not indicated for scanning based on iRefer guidelines. The reference label for each referral was assigned based on the majority voting. The corpus was divided into two datasets, one for the models' development with 920 referrals, and one included 100 referrals used as a held-out for the final comparison of the AI models versus national and international MRI radiographers. Three traditional models were developed: SVM, LR, RF, and two deep neural models, including CNN and Bi-LSTM. For the traditional models, four vectorisation techniques applied: BoW, bigrams, trigrams, and TF-IDF. A textual data augmentation technique was applied to investigate the influence of data augmentation on the models' performances. Results: RF with BoW achieved the highest AUC reaching 0.99. CNN model outperformed Bi-LSTM with AUC = 0.98. With the augmented dataset, the performance significantly improved with an increase in F1 scores ranging from 1% to 7%. All models outperformed the national and international radiographers when compared on the hold-out dataset. Conclusion: The models assigned the referrals' appropriateness with higher accuracies than the national and international radiographers. Applying data augmentation significantly improved the models' performances. Implications for practice: The outcomes suggest that the use of AI for checking referrals' eligibility could serve as a supporting tool to improve the referrals' management in radiology departments. © 2022 The Author(s)","Deep learning; Machine learning; Magnetic resonance imaging; Natural language processing; Referrals' appropriateness","article; controlled study; deep learning; eligibility; human; lumbar spine; machine learning; multicenter study; natural language processing; nuclear magnetic resonance imaging; patient referral; practice guideline; radiographer; radiology department; retrospective study; algorithm; health care delivery; machine learning; nuclear magnetic resonance imaging; Algorithms; Deep Learning; Delivery of Health Care; Humans; Machine Learning; Magnetic Resonance Imaging; Referral and Consultation; Retrospective Studies"
"Alani M.M.","BotStop : Packet-based efficient and explainable IoT botnet detection using machine learning","10.1016/j.comcom.2022.06.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133439578&doi=10.1016%2fj.comcom.2022.06.039&partnerID=40&md5=04f14ead882c9838e8b0f7972c59a973","The rapid increase in the adoption of the Internet of Things has increased the attack surface of these devices, encouraging malicious actors to target these devices. Vulnerable Internet of Things devices are susceptible to botnet infections that give attackers control over these devices from where they can launch attacks on other targets. In this paper, we present an efficient packet-based botnet detection system based on explainable machine learning. Our proposed approach also focuses on feature selection to produce a data set with only seven features to train a machine learning classifier that achieves very high accuracy. Testing the proposed system demonstrates an accuracy exceeding 99% relying on these seven selected characteristics extracted from the network packets. The proposed model is explained using Shapley additive explanation to provide transparency to the classifier prediction process. © 2022 Elsevier B.V.","Botnet; Explainable machine learning; Intrusion detection; IoT","Botnet; Classification (of information); Internet of things; Intrusion detection; Botnet detections; Botnets; Data set; Detection system; Explainable machine learning; Features selection; Intrusion-Detection; IoT; Machine-learning; Packet-based; Machine learning"
"Alani M.M., Miri A.","Towards an Explainable Universal Feature Set for IoT Intrusion Detection","10.3390/s22155690","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136340806&doi=10.3390%2fs22155690&partnerID=40&md5=17097e4e9aa8c78b4ab6a0edb9155dca","As IoT devices’ adoption grows rapidly, security plays an important role in our daily lives. As part of the effort to counter these security threats in recent years, many IoT intrusion detection datasets were presented, such as TON_IoT, BoT-IoT, and Aposemat IoT-23. These datasets were used to build many machine learning-based IoT intrusion detection models. In this research, we present an explainable and efficient method for selecting the most effective universal features from IoT intrusion detection datasets that can help in producing highly-accurate and efficient machine learning-based intrusion detection systems. The proposed method was applied to TON_IoT, Aposemat IoT-23, and IoT-ID datasets and resulted in the selection of six universal network-flow features. The proposed method was tested and produced a high accuracy of 99.62% with a prediction time reduced by up to 70%. To provide better insight into the operation of the classifier, a Shapley additive explanation was used to explain the selected features and to prove the alignment of the explanation with current attack techniques. © 2022 by the authors.","dataset; intrusion detection; IoT; machine-learning; security","Feature extraction; Internet of things; Daily lives; Dataset; Features sets; Highly accurate; Intrusion detection models; Intrusion-Detection; IoT; Machine-learning; Security; Security threats; Intrusion detection; algorithm; machine learning; Algorithms; Machine Learning"
"Alani M.M., Awad A.I.","PAIRED: An Explainable Lightweight Android Malware Detection System","10.1109/ACCESS.2022.3189645","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134232226&doi=10.1109%2fACCESS.2022.3189645&partnerID=40&md5=d35433cc9a71a2a57a5614cb7aa57562","With approximately 2 billion active devices, the Android operating system tops all other operating systems in terms of the number of devices using it. Android has gained wide popularity not only as a smartphone operating system, but also as an operating system for vehicles, tablets, smart appliances, and Internet of Things devices. Consequently, security challenges have arisen with the rapid adoption of the Android operating system. Thousands of malicious applications have been created and are being downloaded by unsuspecting users. This paper presents a lightweight Android malware detection system based on explainable machine learning. The proposed system uses the features extracted from applications to identify malicious and benign malware. The proposed system is tested, showing an accuracy exceeding 98% while maintaining its small footprint on the device. In addition, the classifier model is explained using Shapley Additive Explanation (SHAP) values. © 2013 IEEE.","Android; machine learning; malware; malware detection; XAI","Android (operating system); Android malware; Learning systems; Mobile security; Smartphones; Android; Features extraction; Machine-learning; Malware detection; Malwares; Operating system; Security; Smart phones; XAI; Feature extraction"
"Alanjary M., Kronmiller B., Adamek M., Blin K., Weber T., Huson D., Philmus B., Ziemert N.","The Antibiotic Resistant Target Seeker (ARTS), an exploration engine for antibiotic cluster prioritization and novel drug target discovery","10.1093/nar/gkx360","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023202714&doi=10.1093%2fnar%2fgkx360&partnerID=40&md5=3c782637968711f2cf85dc971e8c2a73","With the rise of multi-drug resistant pathogens and the decline in number of potential new antibiotics in development there is a fervent need to reinvigorate the natural products discovery pipeline. Most antibiotics are derived from secondary metabolites produced by microorganisms and plants. To avoid suicide, an antibiotic producer harbors resistance genes often found within the same biosynthetic gene cluster (BGC) responsible for manufacturing the antibiotic. Existing mining tools are excellent at detecting BGCs or resistant genes in general, but provide little help in prioritizing and identifying gene clusters for compounds active against specific and novel targets. Here we introduce the 'Antibiotic Resistant Target Seeker' (ARTS) available at https://arts.ziemertlab.com. ARTS allows for specific and efficient genome mining for antibiotics with interesting and novel targets. The aim of this web server is to automate the screening of large amounts of sequence data and to focus on the most promising strains that produce antibiotics with new modes of action. ARTS integrates target directed genome mining methods, antibiotic gene cluster predictions and 'essential gene screening' to provide an interactive page for rapid identification of known and putative targets in BGCs. © 2017 The Author(s).",,"cephamycin; erythromycin; novobiocin; rifampicin; salinosporamide A; streptolydigin; thiolactomycin; vancomycin; antiinfective agent; Actinobacteria; antibiotic resistance; Article; autoanalysis; bacterial genome; bacterial strain; data mining; drug targeting; gene cluster; gene duplication; gene transfer; molecular phylogeny; nonhuman; prediction; priority journal; screening; sequence analysis; web browser; antibiotic resistance; biosynthesis; drug development; genetics; Internet; software; Actinobacteria; Anti-Bacterial Agents; Biosynthetic Pathways; Data Mining; Drug Discovery; Drug Resistance, Bacterial; Genome, Bacterial; Internet; Software"
"Alaoui E.A.A., Nassiri K., Tekouabou S.C.K.","Explainable Machine Learning Model for Performance Prediction MAC Layer in WSNs","10.1007/978-3-031-15191-0_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137569335&doi=10.1007%2f978-3-031-15191-0_23&partnerID=40&md5=117bb6d65f2ac04080e7ddaa2d3a6524","Wireless Sensor Networks (WSNs) are used to gather data in a variety of sectors, including smart factories, smart buildings, and so on, to monitor surroundings. Different medium access control (MAC) protocols are accessible to sensor nodes for wireless communications in such contexts, and they are critical to improving network performance. The proposed MAC layer protocols for WSNs are all geared on achieving high packet reception rates. The MAC protocol is adopted and utilized throughout the lifespan of the network, even if its performance degrades over time. Based on the packet reception rate, we use supervised machine learning approaches to forecast the performance of the CSMA/CA MAC protocol in this study. Our method consists of three steps: data gathering trials, offline modeling, and performance assessment. According to our findings, the XGBoost (eXtreme Gradient Boosting) prediction model is the most effective supervised machine learning approach for improving network performance at the MAC layer. In addition, we explain predictions using the SHAP (SHapley Additive exPlanations) approach. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","MAC protocols; Machine learning; Shap value; Wireless sensor networks","Adaptive boosting; Internet protocols; Medium access control; Network performance; Sensor nodes; Supervised learning; Machine learning approaches; Machine learning models; Machine-learning; Medium access control layer; Medium access control protocols; Packet Reception Rate; Performance; Performance prediction; Shap value; Supervised machine learning; Forecasting"
"Alaoui E.A.A., Tekouabou S.C.K., Maleh Y., Nayyar A.","Towards to intelligent routing for DTN protocols using machine learning techniques","10.1016/j.simpat.2021.102475","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124203157&doi=10.1016%2fj.simpat.2021.102475&partnerID=40&md5=a6dc059b172ac7bc9b386bf6be47af18","The communication protocols of wireless networks have experienced great advances in recent years, specifically with the evolution of new technologies such as the Internet of Things (IoT). However, certain problems remain unsolved, in particular for wireless networks, and more specifically for DTN networks, which represent a major challenge in terms of DTN routing. This paper aims to design an intelligent routing system based on machine learning techniques, the use of which represents another possibility to classify bundles that have arrived at the destination successfully or not. These networks occasionally carry out an evaluation which makes it possible to choose the type of routing corresponding to a given situation. It then minimizes the unnecessary information of the entries and performs the classification of the data. Despite the problems cited, our challenge is to design an intelligent routing mechanism that is able to classify bundles that have arrived and those that have not arrived at their destination. The smart routing system uses machine learning as a main tool to design our system. Indeed, various Machine Learning techniques, such as Bagging and Boosting, have been used to classify whether bundles have arrived at their destination successfully or not. Machine Learning now enables us to learn directly from data rather than human expertise, resulting in higher accuracy. We utilized the SMOTE technique to balance the two groups of data, which allows us to collect the equal amount of samples for each class. We also included techniques for interpreting complicated Machine Learning Models to understand the reasoning for model decisions, such as SHAP values. Results show an overall accuracy of 80% for the Random Forest (RF) and ExtraTrees Classifier (ET). © 2022 Elsevier B.V.","Delay Tolerant Networks (DTN); DTN protocols; Game theory; Internet of things (IoT); Interpretable machine learning; Machine learning; Shap values","Classification (of information); Decision trees; Delay tolerant networks; Game theory; Internet of things; Internet protocols; Learning algorithms; Wireless networks; Communications protocols; Delay tolerant network; Delay tolerant network protocol; Delay tolerant network routing; Intelligent routing; Internet of thing; Interpretable machine learning; Machine learning techniques; Routing system; Shap value; Machine learning"
"Alarab I., Prakoonwit S.","Effect of data resampling on feature importance in imbalanced blockchain data: Comparison studies of resampling techniques","10.1016/j.dsm.2022.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132690777&doi=10.1016%2fj.dsm.2022.04.003&partnerID=40&md5=094ca66e03cc803926c3c0cc4714dfe5","Cryptocurrency blockchain data encounter a class-imbalance problem due to only a few known labels of illicit or fraudulent activities in the blockchain network. For this purpose, we seek to compare various resampling methods applied to two highly imbalanced datasets derived from the blockchain of Bitcoin and Ethereum after further dimensionality reductions, which is different from previous studies on these datasets. Firstly, we study the performance of various classical supervised learning methods to classify illicit transactions or accounts on Bitcoin or Ethereum datasets, respectively. Consequently, we apply various resampling techniques to these datasets using the best performing learning algorithm on each of these datasets. Subsequently, we study the feature importance of the given models, wherein the resampled datasets directly influenced on the explainability of the model. Our main finding is that undersampling using the edited nearest-neighbour technique has attained an accuracy of more than 99% on the given datasets by removing the noisy data points from the whole dataset. Moreover, the best-performing learning algorithms have shown superior performance after feature reduction on these datasets in comparison to their original studies. The matchless contribution lies in discussing the effect of the data resampling on feature importance which is interconnected with explainable artificial intelligence (XAI) techniques. © 2022 Xi'an Jiaotong University","Bitcoin blockchain; Cryptocurrency data; Ethereum blockchain; Resampling techniques",
"AL-Aswadi F.N., Chan H.Y., Gan K.H.","Extracting Semantic Concepts and Relations from Scientific Publications by Using Deep Learning","10.1007/978-3-030-70713-2_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105512365&doi=10.1007%2f978-3-030-70713-2_35&partnerID=40&md5=0d31ffcfa17b2385902988b4f4f3d2a5","With the large volume of unstructured data that increases constantly on the web, the motivation of representing the knowledge in this data in the machine-understandable form is increased. Ontology is one of the major cornerstones of representing the information in a more meaningful way on the semantic Web. The current ontology repositories are quite limited either for their scope or for currentness. In addition, the current ontology extraction systems have many shortcomings and drawbacks, such as using a small dataset, depending on a large amount predefined patterns to extract semantic relations, and extracting a very few types of relations. The aim of this paper is to introduce a proposal of automatically extracting semantic concepts and relations from scientific publications. This paper introduces a novel relevance measurement for concepts, and it suggests new types of semantic relations. Also, it points out of using deep learning (DL) models for semantic relation extraction. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Concept extraction; Deep learning; Ontology construction; Relevance measurements; Semantic relation discovery","Extraction; Large dataset; Ontology; Ontology Extraction; Ontology repositories; Scientific publications; Semantic concept; Semantic relation extractions; Semantic relations; Types of relations; Unstructured data; Deep learning"
"Alattar F., Shaalan K.","Using Artificial Intelligence to Understand What Causes Sentiment Changes on Social Media","10.1109/ACCESS.2021.3073657","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104630381&doi=10.1109%2fACCESS.2021.3073657&partnerID=40&md5=0893fd008c065578f6c01dcd3981cd4d","Sentiment Analysis tools allow decision-makers to monitor changes of opinions on social media towards entities, events, products, solutions, and services. These tools provide dashboards for tracking positive, negative, and neutral sentiments for platforms like Twitter where millions of users express their opinions on various topics. However, so far, these tools do not automatically extract reasons for sentiment variations, and that makes it difficult to conclude necessary actions by decision-makers. In this paper, we first compare performance of various Sentiment Analysis classifiers for short texts to select the top performer. Then we present a Filtered-LDA framework that significantly outperformed existing methods of interpreting sentiment variations on Twitter. The framework utilizes cascaded LDA Models with multiple settings of hyperparameters to capture candidate reasons that cause sentiment changes. Then it applies a filter to remove tweets that discuss old topics, followed by a Topic Model with a high Coherence Score to extract Emerging Topics that are interpretable by a human. Finally, a novel Twitter's sentiment reasoning dashboard is introduced to display the most representative tweet for each candidate reason. © 2013 IEEE.","Artificial Intelligence; Emerging Topic Detection; FB-LDA; Filtered-LDA; interpreting sentiment variations; Machine Learning; opinion reason mining; Sentiment Analysis; Sentiment Reasoning; Sentiment Spikes; Topic Model","Artificial intelligence; Decision making; Social networking (online); Decision makers; Emerging topics; Hyperparameters; Lda models; Short texts; Social media; Topic Modeling; Sentiment analysis"
"Alawadi S., Mera D., Fernández-Delgado M., Alkhabbas F., Olsson C.M., Davidsson P.","A comparison of machine learning algorithms for forecasting indoor temperature in smart buildings","10.1007/s12667-020-00376-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078337875&doi=10.1007%2fs12667-020-00376-x&partnerID=40&md5=c2f5ba1752a7106576e1dff284dcbf3f","The international community has largely recognized that the Earth’s climate is changing. Mitigating its global effects requires international actions. The European Union (EU) is leading several initiatives focused on reducing the problems. Specifically, the Climate Action tries to both decrease EU greenhouse gas emissions and improve energy efficiency by reducing the amount of primary energy consumed, and it has pointed to the development of efficient building energy management systems as key. In traditional buildings, households are responsible for continuously monitoring and controlling the installed Heating, Ventilation, and Air Conditioning (HVAC) system. Unnecessary energy consumption might occur due to, for example, forgetting devices turned on, which overwhelms users due to the need to tune the devices manually. Nowadays, smart buildings are automating this process by automatically tuning HVAC systems according to user preferences in order to improve user satisfaction and optimize energy consumption. Towards achieving this goal, in this paper, we compare 36 Machine Learning algorithms that could be used to forecast indoor temperature in a smart building. More specifically, we run experiments using real data to compare their accuracy in terms of R-coefficient and Root Mean Squared Error and their performance in terms of Friedman rank. The results reveal that the ExtraTrees regressor has obtained the highest average accuracy (0.97%) and performance (0,058%) over all horizons. © 2020, The Author(s).","Energy efficiency; Internet of Things; Machine Learning; Smart buildings; Time series prediction","Air conditioning; Buildings; Earth (planet); Energy management systems; Energy utilization; Forecasting; Gas emissions; Greenhouse gases; HVAC; Intelligent buildings; Internet of things; Learning algorithms; Learning systems; Machine learning; Mean square error; Space heating; Three term control systems; Efficient buildings; Indoor temperature; International community; Monitoring and controlling; Root mean squared errors; Time series prediction; Traditional buildings; User satisfaction; Energy efficiency"
"Al-Azawi R.J., Al-Saidi N.M.G., Jalab H.A., Kahtan H., Ibrahim R.W.","Efficient Classification of COVID-19 CT Scans By Using Q-transform Model For feature Extraction","10.7717/peerj-cs.553","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109153149&doi=10.7717%2fpeerj-cs.553&partnerID=40&md5=68e1e601eb6edb01422f6d3664d2564e","The exponential growth in computer technology throughout the past two decades has facilitated the development of advanced image analysis techniques which aid the field of medical imaging. CT is a widely used medical screening method used to obtain high resolution images of the human body. CT has been proven useful in the screening of the virus that is responsible for the COVID-19 pandemic by allowing physicians to rule out suspected infections based on the appearance of the lungs from the CT scan. Based on this, we hereby propose an intelligent yet efficient CT scan-based COVID-19 classification algorithm that is able to discriminate negative from positive cases by evaluating the appearance of lungs. The algorithm is comprised of four main steps: preprocessing, features extraction, features reduction, and classification. In preprocessing, we employ the contrast limited adaptive histogram equalization (CLAHE) to adjust the contrast of the image to enhance the details of the input image. We then apply the q-transform method to extract features from the CT scan. This method measures the grey level intensity of the pixels which reflects the features of the image. In the feature reduction step, we measure the mean, skewness and standard deviation to reduce overhead and improve the efficiency of the algorithm. Finally, “k-nearest neighbor”, “decision tree”, and “support vector machine” are used as classifiers to classify the cases. The experimental results show accuracy rates of 98%, 98%, and 98.25% for each of the classifiers, respectively. It is therefore concluded that the proposed method is efficient, accurate, and flexible. Overall, we are confident that the proposed algorithm is capable of achieving a high classification accuracy under different scenarios, which makes it suitable for implementation in real-world applications. Copyright 2021 Al-Azawi et al.","Classification; COVID-19; CT scans; Feature extraction; Features reduction; k-nearest neighbor; Machine learning; q-transform; Support vector machine","Biomedical signal processing; Decision trees; Diagnosis; Extraction; Image enhancement; Medical imaging; Nearest neighbor search; Statistical methods; Support vector machines; Classification accuracy; Classification algorithm; Contrast Limited Adaptive Histogram Equalization (CLAHE); Features extraction; Features reductions; High resolution image; Image analysis techniques; K-nearest neighbors; Computerized tomography"
"Albano A., Busch P.","Data mining for higher education fundraising",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074091885&partnerID=40&md5=42ecf5c68f14b383f8b0754e4157dceb","Higher education fundraising in Australia is typically conducted by a university office called 'Advancement'. This office is responsible for encouraging and securing the university's fundraising and development activities. While investment in alumni giving is consistent and increasing in Australia currently, much can still be done to increase the effectiveness and success of fundraising activities - specifically what is referred to as the annual fund. One appropriate technique to help in this mission is data mining where even high level data analysis can provide great insight into the data. Clustering algorithms can group donors based on shared characteristics and hopefully help the advancement team at the university in understanding their alumni and donors, what engages these groups and how an initial gift can be used to build long term relationships. By improving donor understanding and developing donor profiles the advancement team can personalize their targeted messaging and enhance donor engagement. The use of cluster analysis can also improve the effectiveness of annual fund raising campaigns and returns on investment by using data driven strategies. For best insights the underlying data must be clean, consistent and available; this is a challenge of implementing data driven strategies. © 2019 International Business Information Management Association (IBIMA).","Australia; Data Mining; Fund Raising; Higher Education",
"Albaradei S., Albaradei A., Alsaedi A., Uludag M., Thafar M.A., Gojobori T., Essack M., Gao X.","MetastaSite: Predicting metastasis to different sites using deep learning with gene expression data","10.3389/fmolb.2022.913602","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135449419&doi=10.3389%2ffmolb.2022.913602&partnerID=40&md5=1a0eb720f28748ab64986c6c57368389","Deep learning has massive potential in predicting phenotype from different omics profiles. However, deep neural networks are viewed as black boxes, providing predictions without explanation. Therefore, the requirements for these models to become interpretable are increasing, especially in the medical field. Here we propose a computational framework that takes the gene expression profile of any primary cancer sample and predicts whether patients’ samples are primary (localized) or metastasized to the brain, bone, lung, or liver based on deep learning architecture. Specifically, we first constructed an AutoEncoder framework to learn the non-linear relationship between genes, and then DeepLIFT was applied to calculate genes’ importance scores. Next, to mine the top essential genes that can distinguish the primary and metastasized tumors, we iteratively added ten top-ranked genes based upon their importance score to train a DNN model. Then we trained a final multi-class DNN that uses the output from the previous part as an input and predicts whether samples are primary or metastasized to the brain, bone, lung, or liver. The prediction performances ranged from AUC of 0.93–0.82. We further designed the model’s workflow to provide a second functionality beyond metastasis site prediction, i.e., to identify the biological functions that the DL model uses to perform the prediction. To our knowledge, this is the first multi-class DNN model developed for the generic prediction of metastasis to various sites. Copyright © 2022 Albaradei, Albaradei, Alsaedi, Uludag, Thafar, Gojobori, Essack and Gao.","artificial intelligence; clinical decision-making; deep learning; gene expression; machine learning; metastasis; metastasis site",
"al-Bashiti M.K., Naser M.Z.","Verifying domain knowledge and theories on Fire-induced spalling of concrete through eXplainable artificial intelligence","10.1016/j.conbuildmat.2022.128648","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135504808&doi=10.1016%2fj.conbuildmat.2022.128648&partnerID=40&md5=53a26727e3c16d534258e11d1bb08cb0","This paper adopts eXplainable Artificial Intelligence (XAI) to identify the key factors influencing fire-induced spalling of concrete and to extract new insights into the phenomenon of spalling by investigating over 640 fire tests. In this pursuit, an XAI model was developed, validated, and then augmented with two explainability measures, namely, Shapley Additive exPlanations (SHAP) and Local eXplainable model-agnostic explanations (LIME). The proposed XAI model not only can predict the fire-induced spalling with high accuracy (i.e., >92 %) but can also articulate the reasoning behind its predictions (as in, the proposed model can specify the rationale for each prediction instance); thus, providing us with valuable insights into the factors, as well as relationships between these factors, leading to spalling. Our findings indicate that there are eight key factors that heavily govern spalling: 1) presence of Polypropylene fibers, 2) degree of moisture content, 3) heating rate, 4) maximum exposure temperature, 5) silica fume/binder ratio, 6) sand/binder ratio, 7) water/binder ratio and 8) fly ash/binder ratio. While these factors were also listed by the majority of the existing spalling theories, the contribution of each factor seems to vary significantly and, most importantly, was not quantified for the most part. Thus, the validated model was then utilized to contrast and quantify the spalling-based knowledge domain and theories as collected by some of the most cited studies in this domain. © 2022 Elsevier Ltd","Concrete; eXplainable AI; Fire; Spalling","Artificial intelligence; Concretes; Fires; Fly ash; Forecasting; Polypropylenes; Silica fume; Spalling; Binder ratio; Concrete; Domain knowledge; Domain theory; Explainable AI; Fire tests; Fire-induced spalling; High-accuracy; Key factors; Shapley; Lime"
"Al-Behadili H.N.K., Sagban R., Ku-Mahamud K.R.","Adaptive parameter control strategy for ant-miner classification algorithm","10.11591/ijeei.v8i1.1423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083985504&doi=10.11591%2fijeei.v8i1.1423&partnerID=40&md5=ece570ee385f2042e279dfd37186d45f","Pruning is the popular framework for preventing the dilemma of overfitting noisy data. This paper presents a new hybrid Ant-Miner classification algorithm and ant colony system (ACS), called ACS-AntMiner. A key aspect of this algorithm is the selection of an appropriate number of terms to be included in the classification rule. ACS-AntMiner introduces a new parameter called importance rate (IR) which is a pre-pruning criterion based on the probability (heuristic and pheromone) amount. This criterion is responsible for adding only the important terms to each rule, thus discarding noisy data. The ACS algorithm is designed to optimize the IR parameter during the learning process of the Ant-Miner algorithm. The performance of the proposed classifier is compared with related ant-mining classifiers, namely, Ant-Miner, CAnt-Miner, TACO-Miner, and Ant-Miner with a hybrid pruner across several datasets. Experimental results show that the proposed classifier significantly outperforms the other ant-mining classifiers. Copyright © 2019 Institute of Advanced Engineering and Science.","Ant Colony Optimization; Data Mining; Metaheuristic; Parameter Control; Rule Induction; Swarm Intelligent",
"AL-Behadili H.N.K., Ku-Mahamud K.R., Sagban R.","Hybrid ant colony optimization and genetic algorithm for rule induction","10.3844/JCSSP.2020.1019.1028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089807473&doi=10.3844%2fJCSSP.2020.1019.1028&partnerID=40&md5=64a00da3e9d821108ac2efd714361fd8","In this study, a hybrid rule-based classifier namely, ant colony optimization/genetic algorithm ACO/GA is introduced to improve the classification accuracy of Ant-Miner classifier by using GA. The Ant-Miner classifier is efficient, useful and commonly used for solving rule-based classification problems in data mining. Ant-Miner, which is an ACO variant, suffers from local optimization problem which affects its performance. In our proposed hybrid ACO/GA algorithm, the ACO is responsible for generating classification rules and the GA improves the classification rules iteratively using the principles of multi-neighborhood structure (i.e., mutation and crossover) procedures to overcome the local optima problem. The performance of the proposed classifier was tested against other existing hybrid ant-mining classification algorithms namely, ACO/SA and ACO/PSO2 using classification accuracy, the number of discovered rules and model complexity. For the experiment, the 10-fold cross-validation procedure was used on 12 benchmark datasets from the University California Irwine machine learning repository. Experimental results show that the proposed hybridization was able to produce impressive results in all evaluation criteria. © 2020 Hayder Naser Khraibet AL-Behadilil, Ku Ruhana Ku-Mahamud and Rafid Sagban.","Ant-Miner; Data mining; Machine learning; Rules-based classification; Swarm intelligence",
"Alberg D., Last M., Neuman R., Sharon A.","Induction of mean output prediction trees from continuous temporal meteorological data","10.1109/ICDMW.2009.30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951147643&doi=10.1109%2fICDMW.2009.30&partnerID=40&md5=4bc84916f5999270c9ee3197f2f6d6b8","In this paper, we present a novel method for fast data-driven construction of regression trees from temporal datasets including continuous data streams. The proposed Mean Output Prediction Tree (MOPT) algorithm transforms continuous temporal data into two statistical moments according to a user-specified time resolution and builds a regression tree for estimating the prediction interval of the output (dependent) variable. Results on two benchmark data sets show that the MOPT algorithm produces more accurate and easily interpretable prediction models than other state-of-the-art regression tree methods. © 2009 IEEE.","Inductive learning; Multivariate statistics; Multivariate time series; Regression trees; Split criteria; Temporal prediction; Time resolution","Inductive learning; Multivariate statistics; Multivariate time series; Temporal prediction; Time resolution; Data mining; Forecasting; Mathematical models; Multivariant analysis; Regression analysis; Technical presentations; Time series; Trees (mathematics)"
"Albers D.J., Elhadad N., Claassen J., Perotte R., Goldstein A., Hripcsak G.","Estimating summary statistics for electronic health record laboratory data for use in high-throughput phenotyping algorithms","10.1016/j.jbi.2018.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041422758&doi=10.1016%2fj.jbi.2018.01.004&partnerID=40&md5=4f1dccb973cb85f34287a1eb0037daa6","We study the question of how to represent or summarize raw laboratory data taken from an electronic health record (EHR) using parametric model selection to reduce or cope with biases induced through clinical care. It has been previously demonstrated that the health care process (Hripcsak and Albers, 2012, 2013), as defined by measurement context (Hripcsak and Albers, 2013; Albers et al., 2012) and measurement patterns (Albers and Hripcsak, 2010, 2012), can influence how EHR data are distributed statistically (Kohane and Weber, 2013; Pivovarov et al., 2014). We construct an algorithm, PopKLD, which is based on information criterion model selection (Burnham and Anderson, 2002; Claeskens and Hjort, 2008), is intended to reduce and cope with health care process biases and to produce an intuitively understandable continuous summary. The PopKLD algorithm can be automated and is designed to be applicable in high-throughput settings; for example, the output of the PopKLD algorithm can be used as input for phenotyping algorithms. Moreover, we develop the PopKLD-CAT algorithm that transforms the continuous PopKLD summary into a categorical summary useful for applications that require categorical data such as topic modeling. We evaluate our methodology in two ways. First, we apply the method to laboratory data collected in two different health care contexts, primary versus intensive care. We show that the PopKLD preserves known physiologic features in the data that are lost when summarizing the data using more common laboratory data summaries such as mean and standard deviation. Second, for three disease-laboratory measurement pairs, we perform a phenotyping task: we use the PopKLD and PopKLD-CAT algorithms to define high and low values of the laboratory variable that are used for defining a disease state. We then compare the relationship between the PopKLD-CAT summary disease predictions and the same predictions using empirically estimated mean and standard deviation to a gold standard generated by clinical review of patient records. We find that the PopKLD laboratory data summary is substantially better at predicting disease state. The PopKLD or PopKLD-CAT algorithms are not meant to be used as phenotyping algorithms, but we use the phenotyping task to show what information can be gained when using a more informative laboratory data summary. In the process of evaluation our method we show that the different clinical contexts and laboratory measurements necessitate different statistical summaries. Similarly, leveraging the principle of maximum entropy we argue that while some laboratory data only have sufficient information to estimate a mean and standard deviation, other laboratory data captured in an EHR contain substantially more information than can be captured in higher-parameter models. © 2018","Electronic health record; Kullback-Leibler divergence; Laboratory tests; phenotyping; Summary statistic","Forecasting; Records management; Research laboratories; Statistics; Throughput; Electronic health record; Kullback Leibler divergence; Laboratory test; Phenotyping; Summary statistic; eHealth; algorithm; Article; automation; electronic medical record; entropy; information processing; intensive care; prediction; primary medical care; priority journal; statistical analysis; algorithm; data mining; electronic health record; high throughput screening; human; laboratory technique; phenotype; procedures; statistical model; statistics and numerical data; Algorithms; Clinical Laboratory Techniques; Data Mining; Electronic Health Records; High-Throughput Screening Assays; Humans; Models, Statistical; Phenotype"
"Albert A., Maasoumy M.","Predictive segmentation of energy consumers","10.1016/j.apenergy.2016.05.128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971350835&doi=10.1016%2fj.apenergy.2016.05.128&partnerID=40&md5=a40cafc823a65253a28d383f0eaeda9f","This paper proposes a predictive segmentation technique for identifying sub-groups in a large population that are both homogeneous with respect to certain patterns in customer attributes, and predictive with respect to a desired outcome. Our motivation is creating a highly-interpretable and intuitive segmentation and targeting process for customers of energy utility companies that is also optimal in some sense. In this setting, the energy utility wants to design a small number of message types to be sent to appropriately-chosen customers who are most likely to respond to different types of communications. The proposed method uses consumption, demographics, and program enrollment data to extract basic predictive patterns using standard machine learning techniques. We next define a feasible potential assignment of patterns to a small number of segments described by expert guidelines and hypotheses about consumer characteristics, which are available from prior behavioral research. The algorithm then identifies an optimal allocation of patterns to segments that is feasible and maximizes predictive power. This is formulated as maximizing the minimum enrollment rate from across the segments, which is then expressed as solving a mixed-integer linear-fractional program. We propose a bisection-based method to quickly solve this program by means of identifying feasible sets. We exemplify the methodology on a large-scale dataset from a leading U.S. energy utility, and obtain segments of customers whose likelihood of enrollment is more than twice larger than that of the average population, and that are described by a small number of simple, intuitive rules. The segments designed this way achieve a 2-3× improvement in the probability of enrollment over the overall population. © 2016 Elsevier Ltd.","Energy efficiency; Segmentation; Targeting","Artificial intelligence; Behavioral research; Energy efficiency; Image segmentation; Integer programming; Learning systems; Energy utilities; Large population; Large-scale dataset; Mixed integer linear; Optimal allocation; Segmentation techniques; Standard machines; Targeting; Sales; algorithm; linear programing; machine learning; methodology; segmentation; United States"
"Albertetti F., Grossrieder L., Ribaux O., Stoffel K.","Change points detection in crime-related time series: An on-line fuzzy approach based on a shape space representation","10.1016/j.asoc.2015.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951996959&doi=10.1016%2fj.asoc.2015.12.004&partnerID=40&md5=b7c414fb4e96c5651cc89448bdb559e1","The extension of traditional data mining methods to time series has been effectively applied to a wide range of domains such as finance, econometrics, biology, security, and medicine. Many existing mining methods deal with the task of change points detection, but very few provide a flexible approach. Querying specific change points with linguistic variables is particularly useful in crime analysis, where intuitive, understandable, and appropriate detection of changes can significantly improve the allocation of resources for timely and concise operations. In this paper, we propose an on-line method for detecting and querying change points in crime-related time series with the use of a meaningful representation and a fuzzy inference system. Change points detection is based on a shape space representation, and linguistic terms describing geometric properties of the change points are used to express queries, offering the advantage of intuitiveness and flexibility. An empirical evaluation is first conducted on a crime data set to confirm the validity of the proposed method and then on a financial data set to test its general applicability. A comparison to a similar change-point detection algorithm and a sensitivity analysis are also conducted. Results show that the method is able to accurately detect change points at very low computational costs. More broadly, the detection of specific change points within time series of virtually any domain is made more intuitive and more understandable, even for experts not related to data mining. © 2015 Elsevier B.V. All rights reserved.","Change points detection; Crime analysis; Fuzzy logic; Qualitative description of data; Time series analysis","Computation theory; Computational linguistics; Crime; Data mining; Economics; Financial data processing; Fuzzy inference; Fuzzy logic; Linguistics; Sensitivity analysis; Statistical tests; Statistics; Change point detection; Change-points; Detection of changes; Empirical evaluations; Existing mining method; Fuzzy inference systems; Geometric properties; Qualitative description of data; Time series analysis"
"Albilani M., Bouzeghoub A.","Dynamic Adjustment of Reward Function for Proximal Policy Optimization with Imitation Learning: Application to Automated Parking Systems","10.1109/IV51971.2022.9827194","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135381724&doi=10.1109%2fIV51971.2022.9827194&partnerID=40&md5=8c07ca019c6211ab3209031e4903024f","Automated Parking Systems (APS) are responsible for performing a parking maneuver in a secure and time-efficient full autonomy.These systems include mainly three methods; parking spot exploration, path planning, and path tracking. In the literature, there are several path planning and tracking methods where the application of reinforcement learning is widespread. However, performance tuning and ensuring efficiency remains a significant open problem. Moreover, these methods suffer from a non-linearity issue of vehicle dynamics, that causes a deviation from the original route, and do not respect the BS ISO 16787-2017 standard that outlines the minimum requirements needed in APS. To overcome these limitations, our contribution in this paper, named DPPO-IL, is fourfold: (i) A new framework using the Proximal Policy optimization algorithm, allowing agent to explore an empty parking spot, plan then park a car in a random parking spot by avoiding static and dynamic obstacles; (ii) A dynamic adjustment of the reward function using intrinsic reward signals to induce the agent to explore more; (iii) An approach to learn policies from expert demonstrations using imitation learning combined with deep reinforcement learning to speed up the learning phase and reduce the training time; (iv) A task-specific curriculum learning to train the agent in a very complex environment. Experiments show promising results, especially that our approach managed to achieve a 90% success rate where 97% of them were aligned with the parking spot, with an inclination angle greater than ±0.2° and a deviation less than 0.1 meter. These results exceeded the state of the art while respecting the ISO 16787-2017 standard. © 2022 IEEE.",,"Deep learning; ISO Standards; Learning systems; Motion planning; Parks; Automated parking; Dynamic adjustment; Imitation learning; Parking manoeuvre; Parking spot; Parking systems; Policy optimization; Reinforcement learnings; Reward function; Time-efficient; Reinforcement learning"
"Albinati J., Oliveira S.E.L., Otero F.E.B., Pappa G.L.","An ant colony-based semi-supervised approach for learning classification rules","10.1007/s11721-015-0116-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948663078&doi=10.1007%2fs11721-015-0116-8&partnerID=40&md5=3d7d25b54c325d54bcb0e0cc616595f7","Semi-supervised learning methods create models from a few labeled instances and a great number of unlabeled instances. They appear as a good option in scenarios where there is a lot of unlabeled data and the process of labeling instances is expensive, such as those where most Web applications stand. This paper proposes a semi-supervised self-training algorithm called Ant-Labeler. Self-training algorithms take advantage of supervised learning algorithms to iteratively learn a model from the labeled instances and then use this model to classify unlabeled instances. The instances that receive labels with high confidence are moved from the unlabeled to the labeled set, and this process is repeated until a stopping criteria is met, such as labeling all unlabeled instances. Ant-Labeler uses an ACO algorithm as the supervised learning method in the self-training procedure to generate interpretable rule-based models—used as an ensemble to ensure accurate predictions. The pheromone matrix is reused across different executions of the ACO algorithm to avoid rebuilding the models from scratch every time the labeled set is updated. Results showed that the proposed algorithm obtains better predictive accuracy than three state-of-the-art algorithms in roughly half of the datasets on which it was tested, and the smaller the number of labeled instances, the better the Ant-Labeler performance. © 2015, Springer Science+Business Media New York.","Ant colony optimization; Classification rules; Self-training; Semi-supervised learning","Algorithms; Ant colony optimization; Artificial intelligence; Iterative methods; Learning systems; Optimization; Supervised learning; Accurate prediction; Classification rules; Predictive accuracy; Self training; Semi- supervised learning; Semi-supervised learning methods; State-of-the-art algorithms; Supervised learning methods; Learning algorithms"
"Albrecht S., Andreani T., Andrade-Navarro M.A., Fontaine J.F.","Single-cell specific and interpretable machine learning models for sparse scChIP-seq data imputation","10.1371/journal.pone.0270043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133393615&doi=10.1371%2fjournal.pone.0270043&partnerID=40&md5=aa8a4a1816b29a74e26735479025d5fa","Motivation Single-cell Chromatin ImmunoPrecipitation DNA-Sequencing (scChIP-seq) analysis is challenging due to data sparsity. High degree of sparsity in biological high-throughput single-cell data is generally handled with imputation methods that complete the data, but specific methods for scChIP-seq are lacking. We present SIMPA, a scChIP-seq data imputation method leveraging predictive information within bulk data from the ENCODE project to impute missing protein-DNA interacting regions of target histone marks or transcription factors. Results Imputations using machine learning models trained for each single cell, each ChIP protein target, and each genomic region accurately preserve cell type clustering and improve pathway- related gene identification on real human data. Results on bulk data simulating single cells show that the imputations are single-cell specific as the imputed profiles are closer to the simulated cell than to other cells related to the same ChIP protein target and the same cell type. Simulations also show that 100 input genomic regions are already enough to train single-cell specific models for the imputation of thousands of undetected regions. Furthermore, SIMPA enables the interpretation of machine learning models by revealing interaction sites of a given single cell that are most important for the imputation model trained for a specific genomic region. The corresponding feature importance values derived from promoterinteraction profiles of H3K4me3, an activating histone mark, highly correlate with co-expression of genes that are present within the cell-type specific pathways in 2 real human and mouse datasets. The SIMPA's interpretable imputation method allows users to gain a deep understanding of individual cells and, consequently, of sparse scChIP-seq datasets. Availability and implementation Our interpretable imputation algorithm was implemented in Python and is available at https://github.com/salbrec/SIMPA. © 2022 Albrecht et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"histone; transcription factor; DNA; algorithm; animal cell; Article; chromatin immunoprecipitation; gene expression; gene identification; human; human cell; machine learning; mouse; nonhuman; protein targeting; simulation; single cell analysis; animal; cluster analysis; DNA sequence; genomics; procedures; Animals; Cluster Analysis; DNA; Genomics; Machine Learning; Mice; Sequence Analysis, DNA"
"Albreiki B.","Framework for automatically suggesting remedial actions to help students at risk based on explainable ML and rule-based models","10.1186/s41239-022-00354-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138263009&doi=10.1186%2fs41239-022-00354-6&partnerID=40&md5=7799a3ffcba3955c4f1c3b9fd9913754","Higher education institutions often struggle with increased dropout rates, academic underachievement, and delayed graduations. One way in which these challenges can potentially be addressed is by better leveraging the student data stored in institutional databases and online learning platforms to predict students’ academic performance early using advanced computational techniques. Several research efforts have focused on developing systems that can predict student performance. However, there is a need for a solution that can predict student performance and identify the factors that directly influence it. This paper aims to develop a model that accurately identifies students who are at risk of low performance, while also delineating the factors that contribute to this phenomenon. The model employs explainable machine learning (ML) techniques to delineate the factors that are associated with low performance and integrates rule-based model risk flags with the developed prediction system to improve the accuracy of performance predictions. This helps low-performing students to improve their academic metrics by implementing remedial actions that address the factors of concern. The model suggests proper remedial actions by mapping the students’ performance in each identified checkpoint with the course learning outcomes (CLOs) and topics taught in the course. The list of possible actions is mapped to this checkpoint. The developed model can accurately distinguish students at risk (total grade < 70 %) from students with good performance. The Area under the ROC Curve (AUC ROC) of binary classification model fed with four checkpoints reached 1.0. Proposed framework may aid the student to perform better, increase the institution’s effectiveness and improve their reputations and rankings. © 2022, The Author(s).","Classification; Early prediction; Explainable ML; Rule-based system; Student performance; Students at risk",
"Albu A.","Logical inference modeled by Petri nets","10.1109/SACI.2016.7507358","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981328125&doi=10.1109%2fSACI.2016.7507358&partnerID=40&md5=1595b638ff53e883042f5ea312b40b2b","Decision-making is a fundamental step in a large variety of activities and the persons that are responsible for this should be able, in any circumstances, to find the right solutions. Some decisional systems have been developed in order to help humans, trying to provide intelligent behavior. Logical inference is one of the handiest mechanisms of artificial intelligence that is suitable for decision-making. In this paper it was chosen to prove that Petri nets can be used for modeling medical decisional systems. The example presented here is an application developed for medical predictions, an area where these mechanisms are successfully used. © 2016 IEEE.",,"Artificial intelligence; Information science; Petri nets; Decisional system; Intelligent behavior; Logical inference; Decision making"
"Albu A.-I., Czibula G.","Analysing protein dynamics using machine learning based generative models","10.1109/SACI49304.2020.9118834","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087416168&doi=10.1109%2fSACI49304.2020.9118834&partnerID=40&md5=a1f44e3cb563791d6b58bdf60771592a","The ability to understand and model proteins' dynamics is of great relevance in biology and medicine. A good comprehension of the way the proteins change their structure is important as these transitions give the function of the protein within the organism. In this paper we are introducing an unsupervised learning based approach using variational autoencoders, for uncovering protein motions and conformational transitions. The main goal of the research is to offer an interpretable method for proteins' trajectories visualisation by learning a low dimensional space that accurately represents the input data, as empirically confirmed through the performed experiments. An additional aim is to comparatively evaluate the impact of two protein representations on the learning process. © 2020 IEEE.","generative models; Protein dynamics; unsupervised learning; variational autoencoders","Learning algorithms; Machine learning; Biology and medicine; Conformational transitions; Generative model; Learning process; Learning-based approach; Low-dimensional spaces; Model proteins; Protein dynamics; Proteins"
"Albukhitan S., Alnazer A., Helmy T.","Framework of Semantic Annotation of Arabic Document using Deep Learning","10.1016/j.procs.2020.03.096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085543563&doi=10.1016%2fj.procs.2020.03.096&partnerID=40&md5=5e8c42ff4ba67b52daa9702b1793751c","Semantic Web vision is to have machines interpret and understand the content of Web documents. There is a need to convert the existing Web of documents into an understandable format, which could be done by automatic semantic annotation. Annotation could be performed using a set of tools provided with general and domain-specific ontologies. The aim of this paper is to present a generic semantic annotation framework of Arabic text using deep learning models. The framework produces annotations using different output formats for a given set of Arabic documents and ontologies. With a prototype of the framework, the initial evaluation shows a promising performance using different public Arabic word embedding models with different vectorization and matching techniques. © 2020 The Authors. Published by Elsevier B.V.All rights reserved.","Arabic Language; Deep Learning; Ontology; Semantic Annotation","Industry 4.0; Ontology; Semantic Web; Arabic document; Domain-specific ontologies; Learning models; Matching techniques; Output formats; Semantic annotations; Semantic web vision; Vectorization; Deep learning"
"Albuquerque T., Cruz R., Cardoso J.S.","Ordinal losses for classification of cervical cancer risk","10.7717/peerj-cs.457","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109860689&doi=10.7717%2fpeerj-cs.457&partnerID=40&md5=b4d1dd90a505989a507331cbfb998e73","Cervical cancer is the fourth leading cause of cancer-related deaths in women, especially in low to middle-income countries. Despite the outburst of recent scientific advances, there is no totally effective treatment, especially when diagnosed in an advanced stage. Screening tests, such as cytology or colposcopy, have been responsible for a substantial decrease in cervical cancer deaths. Cervical cancer automatic screening via Pap smear is a highly valuable cell imaging-based detection tool, where cells must be classified as being within one of a multitude of ordinal classes, ranging from abnormal to normal. Current approaches to ordinal inference for neural networks are found to not sufficiently take advantage of the ordinal problem or to be too uncompromising. A non-parametric ordinal loss for neuronal networks is proposed that promotes the output probabilities to follow a unimodal distribution. This is done by imposing a set of different constraints over all pairs of consecutive labels which allows for a more flexible decision boundary relative to approaches from the literature. Our proposed loss is contrasted against other methods from the literature by using a plethora of deep architectures. A first conclusion is the benefit of using non-parametric ordinal losses against parametric losses in cervical cancer risk prediction. Additionally, the proposed loss is found to be the top-performer in several cases. The best performing model scores an accuracy of 75.6% for seven classes and 81.3% for four classes. © 2021 Albuquerque et al. All Rights Reserved.","Cervical cytology; Convolutional Neural networks; Deep learning; Ordinal classification; Pap smear","Cytology; Neural networks; Neurons; Probability distributions; Automatic screening; Cervical cancers; Decision boundary; Deep architectures; Middle-income countries; Neuronal networks; Scientific advances; Unimodal distribution; Diseases"
"Albus J., Barbera T., Schlenoff C.","RCS: An intelligent agent architecture",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-32144463853&partnerID=40&md5=03458313d7126fbce440a4692b4e518f","RCS (Real-time Control System) is an intelligent agent architecture designed to enable any level of intelligent behavior, up to and including human levels of performance. RCS was inspired 30 years ago by a theoretical model of the cerebellum, the portion of the brain responsible for fine motor coordination and control of conscious motions. It was originally designed for sensory-interactive goal-directed control of laboratory manipulators. Over three decades, it has evolved into a real-time control architecture for intelligent machine tools, factory automation systems, and intelligent autonomous vehicles. In this paper, we describe the 4D/RCS architecture, how it relates to other popular intelligent agent architectures, how it addresses the three most significant theoretical arguments against intelligent agent architectures, and the its underlying engineering methodology.",,"Factory automation systems; Intelligent agent architectures; Intelligent behavior; Motor coordination; Automation; Cognitive systems; Machine tools; Mathematical models; Real time systems; Software agents; Artificial intelligence"
"Albusac J., Vallejo D., Castro-Schez J.J., Jimenez-Linares L.","OCULUS surveillance system: Fuzzy on-line speed analysis from 2D images","10.1016/j.eswa.2011.04.071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957999099&doi=10.1016%2fj.eswa.2011.04.071&partnerID=40&md5=c094d428d235de1451252b28f423decf","This paper presents an independent component integrated into a global surveillance system named as OCULUS. The aim of this component is to classify the speed of moving objects as normal or abnormal in order to detect anomalous events, taking into account the object class and spatio-temporal information such as locations and movements. The proposed component analyses the speed of the detected objects in real-time without needing several cameras, a 3D representation of the environment, or the estimation of precise values. Unlike other works, the proposed method does require knowing the camera parameters previously (e.g. height, angle, zoom level, etc.). The knowledge used by this component is automatically acquired by means of a learning algorithm that generates a set of highly interpretable fuzzy rules. The experimental results demonstrate that the proposed method is accurate, robust and provides a real-time analysis. © 2010 Elsevier Ltd. All rights reserved.","Anomaly detection; Behaviour analysis; Machine learning; Speed classification; Surveillance system","2D images; Anomalous events; Anomaly detection; Behaviour analysis; Camera parameter; Component analysis; Global surveillance; Independent components; Interpretable fuzzy rules; Machine learning; Moving objects; Object class; Real time analysis; Spatiotemporal information; Speed analysis; Surveillance system; Surveillance systems; Cameras; Learning algorithms; Monitoring; Security systems; Independent component analysis"
"Albusac J., Castro-Schez J.J., Lopez-Lopez L.M., Vallejo D., Jimenez-Linares L.","Learning and classification of events in monitored environments",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871852817&partnerID=40&md5=0044095719f75302c3c719c1ae96753c","This paper presents a prototype system to automatically carry out surveillance tasks in monitored environments. This system consists in a supervised machine learning algorithm that generates a set of highly interpretable rules in order to classify events as normal or anomalous from 2D images without needing to build a 3D model of the environment. Each security camera has an associated knowledge base which is updated when the environmental conditions change. To deal with uncertainty and vagueness inherent in video surveillance, we make use of Fuzzy Logic. The process of building the knowledge base and how to apply the generated sets of fuzzy rules is described in depth for a virtual environment.","Automated video surveillance; Fuzzy logic; Machine learning; Visual information analysis","2D images; 3D models; Automated video surveillance; Environmental conditions; Interpretable rules; Knowledge base; Prototype system; Security cameras; Supervised machine learning; Surveillance task; Video surveillance; Visual information analysis; Fuzzy logic; Fuzzy systems; Knowledge based systems; Learning algorithms; Learning systems; Monitoring; Virtual reality; Security systems"
"Albusac J., Castro-Schez J.J., Lopez-Lopez L.M., Vallejo D., Jimenez-Linares L.","A supervised learning approach to automate the acquisition of knowledge in surveillance systems","10.1016/j.sigpro.2009.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67651115658&doi=10.1016%2fj.sigpro.2009.04.008&partnerID=40&md5=abce5621b3892494ea81e618d923d051","This paper presents a machine learning-based method to build knowledge bases used to carry out surveillance tasks in environments monitored with video cameras. The method generates three sets of rules for each camera that allow to detect objects' anomalous behaviours depending on three parameters: object class, object position, and object speed. To deal with uncertainty and vagueness inherent in video surveillance we make use of fuzzy logic. Thanks to this approach we are able to generate a set of rules highly interpretable by security experts. Besides, the simplicity of the surveillance system offers high efficiency and short response time. The process of building the knowledge base and how to apply the generated sets of fuzzy rules is described in depth for a real environment. © 2009 Elsevier B.V. All rights reserved.","Anomalous behaviour detection; Automated video surveillance; Machine learning; MPEG video analysis; Soft computing","Anomalous behaviour detection; Automated video surveillance; High efficiency; Knowledge base; Knowledge basis; Machine learning; MPEG video analysis; Object class; Object positions; Real environments; Security experts; Set of rules; Short response time; Surveillance systems; Surveillance task; Three parameters; Video surveillance; Cameras; Computer science; Education; Fuzzy logic; Fuzzy sets; Knowledge based systems; Monitoring; Motion Picture Experts Group standards; Robot learning; Soft computing; Video amplifiers; Video cameras; Visual communication; Security systems"
"Alcalá R., Cordón O., Herrera F., Zwir I.","Insurance market risk modeling with hierarchical Fuzzy Rule Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909588696&partnerID=40&md5=5259e70a171e7121d1a3f3fc04aafa16","The continued development of large, sophisticated, repositories of knowledge and information has facilitated the accessibility to vast amounts of data about complex objects and their behavior. However, in spite of the recent renewed interest in knowledge-discovery techniques (or data mining), the usefulness of these databases is partially limited by the inability to understand the system-related characteristics of the data. Some applications from the financial or insurance market-such the ones concerned with risk analysis-require to meet solutions that emphasize precision while aiding to understand and validate their structure and relations. We present results about an ongoing project being carried out by the Argentinian State Insurance Agency for tracking the status of the insurance companies, i.e., for screening and analyzing their condition through time. Specifically in this paper, we will tackle with the modeling of the mathematical reserves of the premiums, or risk reserves, of the insurance companies in the local insurance market. To do so, we propose the use of Linguistic Modeling which is one of the most important applications of Fuzzy Rule-Based Systems. Particularly, we apply Hierarchical Linguistic Modeling with the aim of obtaining the desired trade-off between accuracy and interpretability of the system modeled, i.e., decomposing such nonlinear systems into a number of simpler linguistically interpretable subproblems. The achieved results will be also compared with global hierarchical methods and other system modeling techniques, such as classical regressions and neural networks.","Fuzzy Rule-Based Systems; Genetic algorithms; Hierarchical knowledge base; Hierarchical linguistic partitions; Insurance market; Linguistic Modeling","Complex networks; Fuzzy rules; Genetic algorithms; Information systems; Insurance; Knowledge based systems; Linguistics; Risk analysis; Risk assessment; Fuzzy rule-based systems; Hierarchical fuzzy; Hierarchical knowledge; Hierarchical method; Insurance companies; Insurance markets; Interpretability; Linguistic modeling; Hierarchical systems"
"Alcobaça E., Mastelini S.M., Botari T., Pimentel B.A., Cassar D.R., de Carvalho A.C.P.D.L.F., Zanotto E.D.","Explainable Machine Learning Algorithms For Predicting Glass Transition Temperatures","10.1016/j.actamat.2020.01.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079549586&doi=10.1016%2fj.actamat.2020.01.047&partnerID=40&md5=ad196cf8c0855c53f9ea32df29f39017","Modern technologies demand the development of new glasses with unusual properties. Most of the previous developments occurred by slow, expensive trial-and-error approaches, which have produced a considerable amount of data over the past 100 years. By finding patterns in such types of data, Machine Learning (ML) algorithms can extract useful knowledge, providing important insights into composition-property maps. A key step in glass composition design is to identify their physical-chemical properties, such as the glass transition temperature, Tg. In this paper, we investigate how different ML algorithms can be used to predict the Tg of glasses based on their chemical composition. For such, we used a dataset of 43,240 oxide glass compositions, each one with its assigned Tg. Besides, to assess the predictive performance obtained by ML algorithms, we investigated the possible gains by tuning the hyperparameters of these algorithms. The results show that the best ML algorithm for predicting Tg is the Random Forest (RF). One of the main challenges in this task is the prediction of extreme Tg values. To do this, we assessed the predictive performance of the investigated ML algorithms in three Tg intervals. For extreme Tg values ( ≤ 450 K and ≥ 1150 K), the top-performing algorithm was the k-Nearest Neighbours, closely followed by RF. The induced RF model predicted extreme values of Tg with a Relative Deviation (RD) of 3.5% for glasses with high Tg ( ≥ 1150 K), and RD of 7.5% for glasses with very low Tg ( ≤ 450 K). Finally, we propose a new visual approach to explain what our RF model learned, highlighting the importance of each chemical element to obtain glasses with extreme Tg. This study can be easily expanded to predict other composition–property combinations and can advantageously replace empirical approaches for developing novel glasses with relevant properties and applications. © 2020 Acta Materialia Inc.",,"Chemical elements; Decision trees; Forecasting; Glass; Glass transition; Nearest neighbor search; Random forests; Temperature; Chemical compositions; Empirical approach; K-nearest neighbours; Modern technologies; Physical chemical property; Predictive performance; Relative deviations; Trial-and-error approach; Machine learning"
"Aldeghi M., Coley C.W.","A focus on simulation and machine learning as complementary tools for chemical space navigation","10.1039/d2sc90130g","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134713648&doi=10.1039%2fd2sc90130g&partnerID=40&md5=50043a47c26d52cd90b320b9a970137b","Computer-aided molecular design benefits from the integration of two complementary approaches: machine learning and first-principles simulation. Mohr et al. (B. Mohr, K. Shmilovich, I. S. Kleinwächter, D. Schneider, A. L. Ferguson and T. Bereau, Chem. Sci., 2022, 13, 4498-4511, https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc00116k) demonstrated the discovery of a cardiolipin-selective molecule via the combination of coarse-grained molecular dynamics, alchemical free energy calculations, Bayesian optimization and interpretable regression to reveal design principles. © 2022 The Royal Society of Chemistry.",,"Calculations; Computer aided instruction; Free energy; Machine learning; Phospholipids; Cardiolipins; Chemical space; Coarse-grained; Complementary tools; Computer aided molecular design; Design benefits; First-principles simulations; Free-energy calculations; Machine-learning; Space navigation; Molecular dynamics"
"Aldeia G.S.I., de França F.O.","Interpretability in symbolic regression: a benchmark of explanatory methods using the Feynman data set","10.1007/s10710-022-09435-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131098263&doi=10.1007%2fs10710-022-09435-x&partnerID=40&md5=4db7e0ab644132ac6272022f68718b7e","In some situations, the interpretability of the machine learning models plays a role as important as the model accuracy. Interpretability comes from the need to trust the prediction model, verify some of its properties, or even enforce them to improve fairness. Many model-agnostic explanatory methods exists to provide explanations for black-box models. In the regression task, the practitioner can use white-boxes or gray-boxes models to achieve more interpretable results, which is the case of symbolic regression. When using an explanatory method, and since interpretability lacks a rigorous definition, there is a need to evaluate and compare the quality and different explainers. This paper proposes a benchmark scheme to evaluate explanatory methods to explain regression models, mainly symbolic regression models. Experiments were performed using 100 physics equations with different interpretable and non-interpretable regression methods and popular explanation methods, evaluating the performance of the explainers performance with several explanation measures. In addition, we further analyzed four benchmarks from the GP community. The results have shown that Symbolic Regression models can be an interesting alternative to white-box and black-box models that is capable of returning accurate models with appropriate explanations. Regarding the explainers, we observed that Partial Effects and SHAP were the most robust explanation models, with Integrated Gradients being unstable only with tree-based models. This benchmark is publicly available for further experiments. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Benchmark; Explanatory methods; Feature importance attribution; Symbolic regression","Quality control; Benchmark; Black box modelling; Data set; Explanatory method; Feature importance attribution; Interpretability; Performance; Regression modelling; Symbolic regression; White box; Regression analysis"
"Alderden J., Kennerly S.M., Wilson A., Dimas J., McFarland C., Yap D.Y., Zhao L., Yap T.L.","Explainable artificial intelligence for predicting hospital-acquired pressure injuries in covid-19–positive critical care patients","10.1097/CIN.0000000000000943","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139571726&doi=10.1097%2fCIN.0000000000000943&partnerID=40&md5=74475e8d71c6395528f036a32477cb68",[No abstract available],,"artificial intelligence; decubitus; hospital; human; intensive care; Artificial Intelligence; COVID-19; Critical Care; Hospitals; Humans; Pressure Ulcer"
"Al-Dhaen F., Hou J., Rana N.P., Weerakkody V.","Advancing the Understanding of the Role of Responsible AI in the Continued Use of IoMT in Healthcare","10.1007/s10796-021-10193-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114088396&doi=10.1007%2fs10796-021-10193-x&partnerID=40&md5=cda39a5fc56f179e0d30267d4582ab44","This paper examines the continuous intention by healthcare professionals to use the Internet of Medical Things (IoMT) in combination with responsible artificial intelligence (AI). Using the theory of Diffusion of Innovation (DOI), a model was developed to determine the continuous intention to use IoMT taking into account the risks and complexity involved in using AI. Data was gathered from 276 healthcare professionals through a survey questionnaire across hospitals in Bahrain. Empirical outcomes reveal nine significant relationships amongst the constructs. The findings show that despite contradictions associated with AI, continuous intention to use behaviour can be predicted during the diffusion of IoMT. This study advances the understanding of the role of responsible AI in the continued use of IoMT in healthcare and extends DOI to address the diffusion of two innovations concurrently. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Artificial intelligence; Awareness; Diffusion of innovation; Internet of medical things; Novelty seeking; Responsible AI","Health care; Surveys; Bahrain; Continued use; Diffusion of innovations; Health care professionals; Intention to use; Artificial intelligence"
"Al-Eisawi D.","A Framework for Responsible Research and Innovation in new Technological Trends towards MENA Region","10.1109/ICE/ITMC49519.2020.9198506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093089247&doi=10.1109%2fICE%2fITMC49519.2020.9198506&partnerID=40&md5=fa020e4bda9635cd1878245408127592","Responsible Research and Innovation (RRI) is a term used to explain ground-breaking tactics for ethically and socially governing technological research. Recent attention is being focused on the concept, particularly in Europe. However, the perception of RRI within MENA region is still required to be fuzzy and non-existent. This research aims to stretch the understating of RRI beyond the European countries to reach parts of the MENA region and its recent research and innovation trends. The research followed a qualitative Grounded Theory Methodology (GTM) to shape a proposed set of categories and themes presented as a final RRI framework towards some MENA countries engaged in the study (Jordan, UAE, and Bahrain). Data was collected via Semi-Structured interviews for tackling potential creation of synergies amongst entities involved in IS/technology research. The Practical implications delivered a pragmatic contribution to the ethical and societal build-up of IS research in MENA, aiming to achieve a transformative power of recent technological research. © 2020 IEEE.","Artificial Intelligence; Coronavirus; Grounded Theory Methodology; Information Communication Technologies; Information Systems; Responsible Research and Innovation","Engineering; Industrial engineering; European Countries; Grounded theory; Innovation trends; Is researches; Recent researches; Semi structured interviews; Technological researches; Technological trends; Engineering research"
"Aleixo R., Kon F., Rocha R., Camargo M.S., De Camargo R.Y.","Predicting Dengue Outbreaks with Explainable Machine Learning","10.1109/CCGrid54584.2022.00114","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135750144&doi=10.1109%2fCCGrid54584.2022.00114&partnerID=40&md5=d2f62e3eafe4e8235eeb0f8614705b4f","Seasonal infectious diseases, such as dengue, have been causing great losses in many countries around the world in terms of deaths, quality of life, and economic burden. In Brazil, this is relevant not only in large cities such as Rio de Janeiro and São Paulo but, according to the Ministry of Health, in another 500 cities throughout the country. Predicting the occurrence of diseases, such as dengue bursts, can be a valuable instrument for public health management as health officials can better prepare and redirect resources to the affected areas. In this paper, we present an explainable machine learning model to forecast the number of dengue occurrences in a large metropolis, Rio de Janeiro. We focus on explainable models, which provide health authorities with the reasons for outbreak predictions, allowing them to plan their actions accordingly. We trained a gradient boosting decision tree algorithm (CatBoost) with data from the National System of Information on Notifiable Diseases (SINAN), weather data, and socio-demographic data from The Brazilian Institute of Geoaraphy and Statistics (IBGE). © 2022 IEEE.","Dengue Fever; Epidemiology; Explainable AI; Machine Learning; Public Health","Adaptive boosting; Decision trees; Forecasting; Machine learning; Affected area; Dengue fevers; Economic burden; Explainable AI; Health management; Infectious disease; Large cities; Machine-learning; Quality of life; Sao Paulo; Public health"
"Aleksander I.","Neuroconsciousness: A theoretical framework","10.1016/0925-2312(96)00113-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030608076&doi=10.1016%2f0925-2312%2896%2900113-0&partnerID=40&md5=196bd84f848ea7182e0f2c539d4bec9b","The topic of artificial consciousness asks the following question. Were a manufactured object appear to be conscious what mechanisms could be responsible for such a state of affairs? First the paper defines the foundation of the theory as a postulate which requires the manufactured object to be neural, specifying that the firing of some neurons is responsible for conscious states. This leads to the title 'neuroconsciousness'. It follows this up by a series of 12 corollaries which is open-ended in the belief that a definition of consciousness is an open- ended series of properties. The method of theoretical expression is that of a neural state machine for which the state structure is the product of learning and generalisation. Central to this scheme is a novel concept of 'iconic' learning which creates states that are sampled versions of sensory experience. The paper ends with both a technical critique that is directed towards the construction of this type of machinery and a philosophical critique which asks how helpful is the concept of artificial consciousness in elucidating the real thing.The topic of artificial consciousness asks the following question. Were a manufactured object appear to be conscious what mechanisms could be responsible for such a state of affairs? First the paper defines the foundation of the theory as a postulate which requires the manufactured object to be neural, specifying that the firing of some neurons is responsible for conscious states. This leads to the title 'neuroconsciousness'. It follows this up by a series of 12 corollaries which is open-ended in the belief that a definition of consciousness is an open-ended series of properties. The method of theoretical expression is that of a neural state machine for which the state structure is the product of learning and generalisation. Central to this scheme is a novel concept of 'iconic' learning which creates states that are sampled versions of sensory experience. The paper ends with both a technical critique that is directed towards the construction of this type of machinery and a philosophical critique which asks how helpful is the concept of artificial consciousness in elucidating the real thing.","Digital ulceral nets; Iconic training; Mental image models; Models of planning; Models of self; Neural state machines","Artificial intelligence; Data structures; Learning systems; Mathematical models; Philosophical aspects; Planning; Sensory perception; Digital ulceral nets; Iconic training; Mental image models; Neural state machines; State structure; Neural networks; article; awareness; brain; consciousness; learning; mathematical analysis; memory; nerve cell stimulation; priority journal; sensory stimulation; theory"
"Alekseevsky D.","Conformal Model of Hypercolumns in V1 Cortex and the Möbius Group. Application to the Visual Stability Problem","10.1007/978-3-030-80209-7_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112544268&doi=10.1007%2f978-3-030-80209-7_8&partnerID=40&md5=861975d8e3cea4f8c3dec063e160d83b","A conformal spherical model of hypercolumns of primary visual cortex V1 is proposed. It is a modification of the Bressloff-Cowan Riemannian spherical model. The main assumption is that simple neurons of a hypercolumn, considered as Gabor filters, obtained for the mother Gabor filter by transformations from the Möbius group Sl(2, C). It is shown that in a small neighborhood of a pinwheel, which is responsible for detection of high (resp., low) frequency stimuli, it reduces to the Sarti-Citti-Petitot symplectic model of V1 cortex. Application to the visual stability problem is discussed. © 2021, Springer Nature Switzerland AG.","Conformal model of hypercolumn; Möbius group; Stability problem","Artificial intelligence; Computer science; Computers; Frequency stimuli; Hypercolumns; Primary visual cortex; Spherical models; Symplectic; Visual stabilities; Gabor filters"
"Alelyani S.","Detection and evaluation of machine learning bias","10.3390/app11146271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110577464&doi=10.3390%2fapp11146271&partnerID=40&md5=575c995b1f8c40f162b5269db6d5ea77","Machine learning models are built using training data, which is collected from human experience and is prone to bias. Humans demonstrate a cognitive bias in their thinking and behavior, which is ultimately reflected in the collected data. From Amazon’s hiring system, which was built using ten years of human hiring experience, to a judicial system that was trained using human judging practices, these systems all include some element of bias. The best machine learning models are said to mimic humans’ cognitive ability, and thus such models are also inclined towards bias. However, detecting and evaluating bias is a very important step for better explainable models. In this work, we aim to explain bias in learning models in relation to humans’ cognitive bias and propose a wrapper technique to detect and evaluate bias in machine learning models using an openly accessible dataset from UCI Machine Learning Repository. In the deployed dataset, the potentially biased attributes (PBAs) are gender and race. This study introduces the concept of alternation functions to swap the values of PBAs, and evaluates the impact on prediction using KL divergence. Results demonstrate females and Asians to be associated with low wages, placing some open research questions for the research community to ponder over. © 2021 by the author. Licensee MDPI, Basel, Switzerland.","Bias detection; Bias evaluation; Cognitive bias; Explainable models; KL divergence; Machine learning bias",
"Alenazy W.M.","Blockchain-enabled internet of things for unsupervised structural health monitoring in potential building structures","10.4018/978-1-7998-6870-5.ch011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118513945&doi=10.4018%2f978-1-7998-6870-5.ch011&partnerID=40&md5=e5d0e3417786d47581c1b361d856d0f9","The integration of internet of things, artificial intelligence, and blockchain enabled the monitoring of structural health with unattended and automated means. Remote monitoring mandates intelligent automated decision-making capability, which is still absent in present solutions. The proposed solution in this chapter contemplates the architecture of smart sensors, customized for individual structures, to regulate the monitoring of structural health through stress, strain, and bolted joints looseness. Long range sensors are deployed for transmitting the messages a longer distance than existing techniques. From the simulated results, different sensors record the monitoring information and transmit to the blockchain platform in terms of pressure points, temperature, pre-tension force, and the architecture deems the criticality of transactions. Blockchain platform will also be responsible for storage and accessibility of information from a decentralized medium, automation, and security. © 2021 by IGI Global.",,
"Alencar T.R., Leite P.T.","Development of a computational tool for application in the operation of hydrothermal power systems","10.1109/TDC.2010.5484342","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954768637&doi=10.1109%2fTDC.2010.5484342&partnerID=40&md5=81e96c439f7dec1c39f46a1ffde264b9","The article has the purpose to show a Smart Object Oriented Framework Tool that will bring a modern structure and easy use to the user, to be used in scheming and operation of the hydrothermal power systems. The main objective of this article is showing the developed tool, which can reproduce experiments with plants belonging to the Brazilian Southeast System. Aiming its application in concessionaire routine in training of agents responsible by operation and scheming of hydrothermal systems and in the university in the courses of post graduations, graduations and extensions courses oriented to energetic planning studies. It also believes that is an innovative tool to the agents of the energy sector, because make it a new alternative to analysis the decision-makings in your studies and actions. © 2010 IEEE.","Artificial Intelligence and genetic algorithms; Energy; Hydrothermal power systems; Operation planning; Optimization","Computational tools; Energy sector; Hydrothermal power systems; Hydrothermal system; Modern structures; Operation planning; Post graduation; Smart objects; Genetic algorithms; Optimization; Artificial intelligence"
"Aleo P.D., Lock S.J., Cox D.J., Levy S.A., Naiman J.P., Christensen A.J., Borkiewicz K., Patterson R.","Clustering-informed cinematic astrophysical data visualization with application to the Moon-forming terrestrial synestia","10.1016/j.ascom.2020.100424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090843240&doi=10.1016%2fj.ascom.2020.100424&partnerID=40&md5=d85fdf9ea933a6a647961f7186b3b2b7","Scientific visualization tools are currently not optimized to create cinematic, production-quality representations of numerical data for the purpose of science communication. In our pipeline Estra, we outline a step-by-step process from a raw simulation into a finished render as a way to teach non-experts in the field of visualization how to achieve production-quality outputs on their own. We demonstrate feasibility of using the visual effects software Houdini for cinematic astrophysical data visualization, informed by machine learning clustering algorithms. To demonstrate the capabilities of this pipeline, we used a post-impact, thermally-equilibrated Moon-forming synestia from Lock et al., (2018). Our approach aims to identify “physically interpretable” clusters, where clusters identified in an appropriate phase space (e.g. here we use a temperature–entropy phase–space) correspond to physically meaningful structures within the simulation data. Clustering results can then be used to highlight these structures by informing the color-mapping process in a simplified Houdini software shading network, where dissimilar phase–space clusters are mapped to different color values for easier visual identification. Cluster information can also be used in 3D position space, via Houdini's Scene View, to aid in physical cluster finding, simulation prototyping, and data exploration. Our clustering-based renders are compared to those created by the Advanced Visualization Lab (AVL) team for the full dome show “Imagine the Moon” as proof of concept. With Estra, scientists have a tool to create their own production-quality, data-driven visualizations. © 2020 Elsevier B.V.","Methods: Data analysis; Methods: Numerical","Astrophysics; Clustering algorithms; Learning algorithms; Machine learning; Moon; Object oriented programming; Phase space methods; Pipelines; Rendering (computer graphics); Visualization; Advanced visualizations; Clustering results; Data exploration; Physical clusters; Production quality; Proof of concept; Science communications; Visual identification; Data visualization"
"Aler Tubella A., Barsotti F., Koçer R.G., Mendez J.A.","Ethical implications of fairness interventions: what might be hidden behind engineering choices?","10.1007/s10676-022-09636-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125646943&doi=10.1007%2fs10676-022-09636-z&partnerID=40&md5=f93f05e1438471b3d090b73604fec9d5","The importance of fairness in machine learning models is widely acknowledged, and ongoing academic debate revolves around how to determine the appropriate fairness definition, and how to tackle the trade-off between fairness and model performance. In this paper we argue that besides these concerns, there can be ethical implications behind seemingly purely technical choices in fairness interventions in a typical model development pipeline. As an example we show that the technical choice between in-processing and post-processing is not necessarily value-free and may have serious implications in terms of who will be affected by the specific fairness intervention. The paper reveals how assessing the technical choices in terms of their ethical consequences can contribute to the design of fair models and to the related societal discussions. © 2022, The Author(s).","AI Ethics; Bias mitigation; Fairness; Responsible AI","Ethical technology; AI ethic; Bias mitigation; Ethical implications; Fairness; Fairness performance; Machine learning models; Modeling performance; Responsible AI; Trade off; Typical model; Economic and social effects"
"Alesiani F., Yu S., Shaker A., Yin W.","Towards Interpretable Multi-task Learning Using Bilevel Programming","10.1007/978-3-030-67661-2_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103297626&doi=10.1007%2f978-3-030-67661-2_35&partnerID=40&md5=d3fab382bf9ff48845efe462dbfad6d6","Interpretable Multi-Task Learning can be expressed as learning a sparse graph of the task relationship based on the prediction performance of the learned models. Since many natural phenomenon exhibit sparse structures, enforcing sparsity on learned models reveals the underlying task relationship. Moreover, different sparsification degrees from a fully connected graph uncover various types of structures, like cliques, trees, lines, clusters or fully disconnected graphs. In this paper, we propose a bilevel formulation of multi-task learning that induces sparse graphs, thus, revealing the underlying task relationships, and an efficient method for its computation. We show empirically how the induced sparse graph improves the interpretability of the learned models and their relationship on synthetic and real data, without sacrificing generalization performance. Code at https://bit.ly/GraphGuidedMTL. © 2021, Springer Nature Switzerland AG.","Interpretable machine learning; Multi-task learning; Sparse graph; Structure learning; Transfer learning","Data mining; Learning systems; Trees (mathematics); Bi-level programming; Connected graph; Disconnected graph; Generalization performance; Interpretability; Natural phenomena; Prediction performance; Synthetic and real data; Multi-task learning"
"Alessandri E., Gasparetto A., Garcia R.V., Béjar R.M.","An application of artificial intelligence to medical robotics","10.1007/s10846-005-3509-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-13944281700&doi=10.1007%2fs10846-005-3509-x&partnerID=40&md5=c6d6de0b67ca9cacb241dad6fdf42a4c","In this paper an application of Artificial Intelligence (AI) to Medical Robotics is described. Namely, a specific AI technique is employed to generate a sequence of operations understandable by the control system of a robot which is to perform a semi-automatic surgical task. According to this technique, a planner is implemented to translate the ""natural"" language of the surgeon into the robotic sequence that should be executed by the robot. A robotic simulator has been implemented in order to test the planned sequence in a virtual environment. The planned sequence is then to be input to the medical robotic system, which will execute the surgical operation. The work described in this paper features a high level of originality, since no similar applications of AI to medical robotics could be found in the scientific literature.","Artificial intelligence; Medical robotics; Neurosurgery; Ontologies; Planner; Robotic simulator; STRIPS","Algorithms; Control system analysis; Natural language processing systems; Neurosurgery; Robotics; Simulators; Virtual reality; Medical robotics; Ontologies; Planner; Specific linear planner (STRIPS); Artificial intelligence"
"Alevizopoulos A., Kritikos J., Alevizopoulos G.","Intelligent machines and mental health in the era of COVID-19","10.22365/jpsych.2021.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112127220&doi=10.22365%2fjpsych.2021.015&partnerID=40&md5=86ba90655ff8736bf6b3c8640070645e","The idea of a network of small devices that would be able to connect each other, appeared in the early 80s. In a prophetic article, Mark Weiser,1 described such a connection, that it is now known under the term of Internet of Things (IoT). In a broadest sense, the term IoT encompasses everything connected to the internet, but it is increasingly being used to define objects that ""talk"" to each other, creating a network from simple sensors to smartphones and wearables connected. During the recent years this network of communicating devices has been combined with other technological achievements, and particularly with the Virtual Reality (VR)2 and the Artificial Intelligence (AI).3 The emerge of COVID-19 pandemic in 2019, resulted to the poor response and healthcare failures of many countries globally.4 One of the main reasons for such a failure, was the inability of accurate data collection from different sources. Apparently, it was the first time, humanity realized the need for massive amounts of heterogeneous data to be collected, interpreted, and shared. Amid the ongoing COVID-19 pandemic, several innovators and public authorities are looking to leverage IoT tools to reduce the burden on the healthcare systems.5 Mental health is one of the areas that seems to benefit the most of such technologies. A significant decrease of the total amount of ER visits and a dramatic increase of internet access from the patients and care givers along to the development of applications for mental health issues, followed the outbreak of SARS-CoV-2.6 Such technologies proved to be efficient to help mentally ill patients and pioneer the path in the future. Probably the most obvious use of these emerged technologies is the improvement of the telehealth options. Patients who suffer from mental illness face significant problems towards the continuity of care during the crisis.7 Nonetheless, they usually have other health problems, that deprive them from an equitable health care provision. Improved telehealth platforms can give them a single point access to address all their problems. The use of electronic health records can reduce the fragmentary health services and improve the outcome.8 However, this is only the beginning. The COVID-19 crisis and the subsequent social isolation, to reduce both the contamination and the spread of the disease, highlighted the necessity for providing accurate and secure diagnoses and treatments from a safe distance. Virtual reality combined with IoT and AI technologies seem to be a reliable alternative to the classic physical and mental examination and treatment in many areas of mental and neurological diseases.2 These novel techniques can spot the early signs and detect mental illnesses with high accuracy. However, caution and more work are required to bridge the space between these recently thrived technologies and mental health care.7 It is worth mentioning, that internet-oriented health care procedures can also help to reduce the gaps caused by the stigma of mental illness. For example, the development of AI chatbots (an application used to chat directly with a human) can alleviate the fears of judgment of the help seeking persons and provide the professionals with a supplemental support toward improved services to their patients.9 A final remark for conclusion. Humanity is more and more depended to the ""intelligent"" machines. However, we must not forget that we humans are responsible to set the rules of such co-existence.",,"artificial intelligence; communicable disease control; epidemiology; health care delivery; human; medical information system; mental health; needs assessment; organization and management; prevention and control; procedures; social interaction; telemedicine; virtual reality; Artificial Intelligence; Communicable Disease Control; COVID-19; Health Information Systems; Health Services Accessibility; Humans; Internet of Things; Mental Health; Needs Assessment; SARS-CoV-2; Social Interaction; Telemedicine; Virtual Reality"
"Alexander D.H., Lange K.","Enhancements to the ADMIXTURE algorithm for individual ancestry estimation","10.1186/1471-2105-12-246","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958863696&doi=10.1186%2f1471-2105-12-246&partnerID=40&md5=01e1d5db1542dd1799e13ebd24123de3","Background: The estimation of individual ancestry from genetic data has become essential to applied population genetics and genetic epidemiology. Software programs for calculating ancestry estimates have become essential tools in the geneticist's analytic arsenal.Results: Here we describe four enhancements to ADMIXTURE, a high-performance tool for estimating individual ancestries and population allele frequencies from SNP (single nucleotide polymorphism) data. First, ADMIXTURE can be used to estimate the number of underlying populations through cross-validation. Second, individuals of known ancestry can be exploited in supervised learning to yield more precise ancestry estimates. Third, by penalizing small admixture coefficients for each individual, one can encourage model parsimony, often yielding more interpretable results for small datasets or datasets with large numbers of ancestral populations. Finally, by exploiting multiple processors, large datasets can be analyzed even more rapidly.Conclusions: The enhancements we have described make ADMIXTURE a more accurate, efficient, and versatile tool for ancestry estimation. © 2011 Alexander and Lange; licensee BioMed Central Ltd.",,"Cross validation; Genetic epidemiologies; High-performance tools; Multiple processors; Population genetics; Single nucleotide polymorphisms; Software program; Versatile tools; Population statistics; Tools; Estimation; algorithm; article; artificial intelligence; computer program; ethnic and racial groups; gene frequency; human; human genome; population genetics; single nucleotide polymorphism; statistical model; Algorithms; Artificial Intelligence; Gene Frequency; Genetics, Population; Genome, Human; Humans; Likelihood Functions; Polymorphism, Single Nucleotide; Population Groups; Software"
"Alexandre L., Costa R.S., Santos L.L., Henriques R.","Mining Pre-Surgical Patterns Able to Discriminate Post-Surgical Outcomes in the Oncological Domain","10.1109/JBHI.2021.3064786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102648999&doi=10.1109%2fJBHI.2021.3064786&partnerID=40&md5=cb174aff87322460fd797aa0ec24e527","Understanding the individualized risks of undertaking surgical procedures is essential to personalize preparatory, intervention and post-care protocols for minimizing post-surgical complications. This knowledge is key in oncology given the nature of interventions, the fragile profile of patients with comorbidities and cytotoxic drug exposure, and the possible cancer recurrence. Despite its relevance, the discovery of discriminative patterns of post-surgical risk is hampered by major challenges: i) the unique physiological and demographic profile of individuals, as well as their differentiated post-surgical care; ii) the high-dimensionality and heterogeneous nature of available biomedical data, combining non-identically distributed risk factors, clinical and molecular variables; iii) the need to generalize tumors have significant histopathological differences and individuals undertake unique surgical procedures; iv) the need to focus on non-trivial patterns of post-surgical risk, while guaranteeing their statistical significance and discriminative power; and v) the lack of interpretability and actionability of current approaches. Biclustering, the discovery of groups of individuals correlated on subsets of variables, has unique properties of interest, being positioned to satisfy the aforementioned challenges. In this context, this work proposes a structured view on why, when and how to apply biclustering to mine discriminative patterns of post-surgical risk with guarantees of usability, a subject remaining unexplored up to date. These patterns offer a comprehensive view on how the patient profile, cancer histopathology and entailed surgical procedures determine: i) post-surgical complications, ii) survival, and iii) hospitalization needs. The gathered results confirm the role of biclustering in comprehensively finding interpretable, actionable and statistically significant patterns of post-surgical risk. The found patterns are already assisting healthcare professionals at IPO-Porto to establish specialized pre-habilitation protocols and bedside care. © 2013 IEEE.","Biclustering; discriminative pattern mining; oncology; post-surgical complications; surgical risk","Diseases; Cancer recurrence; Demographic profile; Discriminative power; Health care professionals; High dimensionality; Significant patterns; Statistical significance; Surgical procedures; Surgery; cytotoxic agent; Article; biclustering; bioinformatics; cancer patient; cancer surgery; cancer survival; cohort analysis; computer model; drug exposure; endoscopy; histopathology; hospitalization; human; intensive care unit; learning algorithm; machine learning; malignant neoplasm; physical activity; postoperative complication; preoperative exercise; prospective study; surgical technique; usability; health care personnel; neoplasm; treatment outcome; Health Personnel; Humans; Neoplasms; Treatment Outcome"
"Alexandrov B., DeSantis D.F., Manzini G., Skau E.W.","Nonnegative canonical tensor decomposition with linear constraints: nnCANDELINC","10.1002/nla.2443","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128565681&doi=10.1002%2fnla.2443&partnerID=40&md5=87ea3719379b1f1810e9d8c65c4e3a62","There is an emerging interest for tensor factorization applications in big-data analytics and machine learning. To speed up the factorization of extra-large datasets, organized in multidimensional arrays (also known as tensors), easy to compute compression-based tensor representations, such as, Tucker and tensor train formats, are used to approximate the initial large-tensor. Further, tensor factorization is used to extract latent features that can facilitate discoveries of new mechanisms and signatures hidden in the data, where the explainability of the latent features is of principal importance. Nonnegative tensor factorization extracts latent features that are naturally sparse and parts of the data, which makes them easily interpretable. However, to take into account available domain knowledge and subject matter expertise, often additional constraints need to be imposed, which lead us to canonical decomposition with linear constraints (CANDELINC), a canonical polyadic decomposition with rank deficient factors. In CANDELINC, Tucker compression is used as a preprocessing step, which lead to a larger residual error but to more explainable latent features. Here, we propose a nonnegative CANDELINC (nnCANDELINC) accomplished via a specific nonnegative Tucker decomposition; we refer to as minimal or canonical nonnegative Tucker. We derive several results required to understand the specificity of nnCANDELINC, focusing on the difficulties of preserving the nonnegative rank of a tensor to its Tucker core and comparing the real valued to nonnegative case. Finally, we demonstrate nnCANDELINC performance on synthetic and real-world examples. Published 2022. This article is a U.S. Government work and is in the public domain in the USA. Numerical Linear Algebra with Applications published by John Wiley & Sons Ltd.","data compression; linear constraints; minimal cones; nonnegative CANDELINC; nonnegative multirank; nonnegative rank; nonnegative Tucker",
"Alexandrov B.S., Stanev V.G., Vesselinov V.V., Rasmussen K.Ø.","Nonnegative tensor decomposition with custom clustering for microphase separation of block copolymers","10.1002/sam.11407","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061839741&doi=10.1002%2fsam.11407&partnerID=40&md5=4800e92802b44e1c0792ae75b7ddcf11","High-dimensional datasets are becoming ubiquitous in many applications and therefore unsupervised tensor methods to interrogate them are needed. Here, we report a new unsupervised machine learning (ML) approach (NTFk) based on nonnegative tensor factorization integrated with a custom k-means clustering. We demonstrate the ability of NTFk to extracting temporal and spatial features of phase separation of copolymers as they are modeled by self-consistent field theory. Microphase separation of block copolymers has been extensively studied both experimentally and theoretically. However, the interpretation of computer simulations and/or experimental data, representing temporal and spatial changes of molecular species concentration is still a challenging task. Thus, extracting the phase diagram from simulations or experimental data as well as the interpretation of data requires discernment of the model/experimental parameters (such as, temperature, concentrations, the number of molecular species and the interaction between species) impact on the microphase separation process. An attractive and unique aspect of the introduced ML method is that it ensures the nonnegativity of the extracted latent features. Nonnegativity is an essential constraint needed to obtain interpretable and sparse latent features that are parts-based representation of the data. The custom clustering in NTFk serves to estimate the number of latent features in the data. © 2019 The Authors. Statistical Analysis and Data Mining: The ASA Data Science Journal published by Wiley Periodicals, Inc.","dimension reduction; feature extraction; nonnegative tensor factorization; phase separation; unsupervised learning","Block copolymers; Factorization; Feature extraction; K-means clustering; Machine learning; Mean field theory; Molecular orbitals; Molecular structure; Phase separation; Tensors; Unsupervised learning; Dimension reduction; High dimensional datasets; Interpretation of data; Non-negative tensor decompositions; Nonnegative tensor factorizations; Self consistent field theory; Temporal and spatial changes; Unsupervised machine learning; Microphase separation"
"Alexandrov N.M.","Explainable AI decisions for human-autonomy interactions","10.2514/6.2017-3991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085850591&doi=10.2514%2f6.2017-3991&partnerID=40&md5=cb81b95dbe5686a86b83c2f0c3a3b714","Autonomous systems governed by a variety of adaptive algorithms are making an appearance in safety-critical and time-critical domains, such as automobile traffic and aviation. Until autonomous systems are proven and perceived to be as or more adaptable than humans, and resilient in the face of unanticipated faults and variable conditions, humans will have to remain in ultimate control of decision-making, while supported by machine-based information and advice. Human-machine interaction in many domains has numerous well-known difficulties, including lack or excess of trust, both of which can lead to serious problems, especially when human decision makers are overwhelmed with information. Interactions between humans and autonomous systems-a subset of general human-machine interactions--will be more problematic still. One complication is that sophisticated machine learning systems produce outputs that may be difficult for a human user to interpret. The development of trust has been a major motivation for the area of explainable AI, or XAI. We offer initial observations about the relation between trust and explicability and propose candidate methods for formalizing these notions in technical domains. © 2017, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.",,"Adaptive algorithms; Decision making; Man machine systems; Safety engineering; Automobile traffic; Autonomous systems; Human decisions; Human machine interaction; Human users; Sophisticated machines; Time-critical domains; Variable conditions; Learning systems"
"Alexeyev A., Solianyk T.","Decision-Making Support System for Experts of Penal Law","10.1007/978-3-030-35649-1_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083420592&doi=10.1007%2f978-3-030-35649-1_8&partnerID=40&md5=bb3cf117942e3dc1a6fbe09f579c7d37","This section substantiates the relevance of the task of developing an expert decision-making support system for criminal law specialists. The goals of developing an expert system, the tasks solved by the expert system are formulated, and the experts responsible for filling the system and the types of users are defined. For the effective implementation of the expert system, a special data set structure was proposed, based on the basic properties of the Criminal Code article. The data sets organized in this way allow them to be used in data mining in the future. The decision tree method was chosen as a method for extracting new knowledge from fact bases. Analyzed its capabilities and limitations. Two main ways of its construction are considered—manual and machine learning. The possible storage options for the obtained tree in the system are described. The developed system is built based on client-server architecture. The description of the main parts of the system and their functional capabilities is given. Various technologies for the implementation of this system as a separate application for existing platforms, and for the implementation of the cross-platform version are considered. The main modes of operation of the dialogue part of the system, obtaining a solution and interpreting the results obtained are described. An example of decision making using the developed system is given. The final part contains a discussion of the possibilities and limitations of the developed system, the positive and negative aspects of the methods and technologies used. © 2020, Springer Nature Switzerland AG.","Decision tree method; Decision-making support system; Penal law","Client server computer systems; Crime; Data mining; Decision trees; Digital storage; Expert systems; Client-server architectures; Criminal laws; Cross-platform; Decision making support system; Decision tree method; Functional capabilities; Main mode; Various technologies; Decision making"
"ALfatih M., Li C., Saadalla N.E.","Prediction of groups responsible for terrorism attack using tree based models","10.1145/3349341.3349424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073062667&doi=10.1145%2f3349341.3349424&partnerID=40&md5=32e2e085b6307c5a78ac6d8701c58550","The most significant challenging for the humankind around the world is Terrorist attacks, which consider the whole attention. To anticipate the terrorist group which is accountable for results and activities utilizing historical information is a difficult task because of the shortage of detailed terrorist data. Data about terrorist attacks was analyzed, several machine learning algorithms were trained on the dataset of the Global Terrorism Database to learn to foretell the perpetrator of a terrorist attack, given data about the types of attack, target and weapon in addition to the location, year and other attributes of the event. Results shows that Random Forest technique gave accuracy higher than 84 % in predicting the perpetrators. This approach has disparity to aid investigating agencies and carries significant implications for national and international security. However, the outcome results of experiments showed which algorithm is more suitable for specific dataset(GTD). Tests are performed on real-life data and also the final analysis and conclusion based on F1-score, accuracy and precision performance which revealed that Random Forest, is more precise than Decision Tree, and Gradient Boosting. © 2019 Association for Computing Machinery.","Algorithms; Analysis; Dataset; GTD; Terrorism Data","Algorithms; Decision trees; Machine learning; National security; Random forests; Accuracy and precision; Analysis; Dataset; Global terrorism; Gradient boosting; Historical information; International security; Terrorist attacks; Terrorism"
"Alfeo A.L., Cimino M.G.C.A., Vaglini G.","Degradation stage classification via interpretable feature learning","10.1016/j.jmsy.2021.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105746772&doi=10.1016%2fj.jmsy.2021.05.003&partnerID=40&md5=90b9adba2c1b063aaccbd14ca77a0e15","Predictive maintenance (PdM) advocates for the usage of machine learning technologies to monitor asset's health conditions and plan maintenance activities accordingly. However, according to the specific degradation process, some health-related measures (e.g. temperature) may be not informative enough to reliably assess the health stage. Moreover, each measure needs to be properly treated to extract the information linked to the health stage. Those issues are usually addressed by performing a manual feature engineering, which results in high management cost and poor generalization capability of those approaches. In this work, we address this issue by coupling a health stage classifier with a feature learning mechanism. With feature learning, minimally processed data are automatically transformed into informative features. Many effective feature learning approaches are based on deep learning. With those, the features are obtained as a non-linear combination of the inputs, thus it is difficult to understand the input's contribution to the classification outcome and so the reasoning behind the model. Still, these insights are increasingly required to interpret the results and assess the reliability of the model. In this regard, we propose a feature learning approach able to (i) effectively extract high-quality features by processing different input signals, and (ii) provide useful insights about the most informative domain transformations (e.g. Fourier transform or probability density function) of the input signals (e.g. vibration or temperature). The effectiveness of the proposed approach is tested with publicly available real-world datasets about bearings' progressive deterioration and compared with the traditional feature engineering approach. © 2021 The Society of Manufacturing Engineers","Autoencoder; Deep learning; Explainable artificial intelligence; Feature learning; Interpretable machine learning; Predictive maintenance","Cost engineering; Deep learning; Degradation; Deterioration; Health; Probability density function; Degradation process; Domain transformation; Feature engineerings; Generalization capability; Machine learning technology; Maintenance activity; Minimally processed; Real-world datasets; Classification (of information)"
"Alfeo A.L., Cimino M.G.C.A., Manco G., Ritacco E., Vaglini G.","Using an autoencoder in the design of an anomaly detector for smart manufacturing","10.1016/j.patrec.2020.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086889361&doi=10.1016%2fj.patrec.2020.06.008&partnerID=40&md5=c892916aefec41f0ef58f3c15c2d9068","According to the smart manufacturing paradigm, the analysis of assets’ time series with a machine learning approach can effectively prevent unplanned production downtimes by detecting assets’ anomalous operational conditions. To support smart manufacturing operators with no data science background, we propose an anomaly detection approach based on deep learning and aimed at providing a manageable machine learning pipeline and easy to interpret outcome. To do so we combine (i) an autoencoder, a deep neural network able to produce an anomaly score for each provided time series, and (ii) a discriminator based on a general heuristics, to automatically discern anomalies from regular instances. We prove the convenience of the proposed approach by comparing its performances against isolation forest with different case studies addressing industrial laundry assets’ power consumption and bearing vibrations. © 2020","Anomaly detection; Anomaly discriminator; Autoencoder; Fault detection; Interpretable machine learning; Smart industry; Smart manufacturing","Anomaly detection; Deep neural networks; Flow control; Learning systems; Manufacture; Time series; Time series analysis; Anomaly detector; Auto encoders; Bearing vibrations; Detection approach; Machine learning approaches; Operational conditions; Science background; Smart manufacturing; Deep learning"
"Alfi I.A., Rahman M.M., Shorfuzzaman M., Nazir A.","A Non-Invasive Interpretable Diagnosis of Melanoma Skin Cancer Using Deep Learning and Ensemble Stacking of Machine Learning Models","10.3390/diagnostics12030726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127372282&doi=10.3390%2fdiagnostics12030726&partnerID=40&md5=1b5b548c206432744554526730db7d61","A skin lesion is a portion of skin that observes abnormal growth compared to other areas of the skin. The ISIC 2018 lesion dataset has seven classes. A miniature dataset version of it is also available with only two classes: malignant and benign. Malignant tumors are tumors that are cancerous, and benign tumors are non-cancerous. Malignant tumors have the ability to multiply and spread throughout the body at a much faster rate. The early detection of the cancerous skin lesion is crucial for the survival of the patient. Deep learning models and machine learning models play an essential role in the detection of skin lesions. Still, due to image occlusions and imbalanced datasets, the accuracies have been compromised so far. In this paper, we introduce an interpretable method for the non-invasive diagnosis of melanoma skin cancer using deep learning and ensemble stacking of machine learning models. The dataset used to train the classifier models contains balanced images of benign and malignant skin moles. Hand-crafted features are used to train the base models (logistic regression, SVM, random forest, KNN, and gradient boosting machine) of machine learning. The prediction of these base models was used to train level one model stacking using cross-validation on the training set. Deep learning models (MobileNet, Xception, ResNet50, ResNet50V2, and DenseNet121) were used for transfer learning, and were already pre-trained on ImageNet data. The classifier was evaluated for each model. The deep learning models were then ensembled with different combinations of models and assessed. Furthermore, shapely adaptive explanations are used to construct an interpretability approach that generates heatmaps to identify the parts of an image that are most suggestive of the illness. This allows dermatologists to understand the results of our model in a way that makes sense to them. For evaluation, we calculated the accuracy, F1-score, Cohen’s kappa, confusion matrix, and ROC curves and identified the best model for classifying skin lesions. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","deep learning; diagnosis; interpretability; machine learning; melanoma; skin cancer; stacking model","area under the curve; Article; cancer diagnosis; classifier; clinical feature; convolutional neural network; cross validation; cutaneous melanoma; deep learning; deep neural network; dermatologist; diagnostic accuracy; feature extraction; geometry; histogram; human; image processing; image segmentation; k nearest neighbor; logistic regression analysis; machine learning; nevus; non invasive measurement; performance indicator; prediction; random forest; residual neural network; sensitivity and specificity; support vector machine"
"Algahtani E., Kazakov D.","CONNER: A Concurrent ILP Learner in Description Logic","10.1007/978-3-030-49210-6_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086242191&doi=10.1007%2f978-3-030-49210-6_1&partnerID=40&md5=a5fb8ba2378d52d649c5ae536f93d6d5","Machine Learning (ML) approaches can achieve impressive results, but many lack transparency or have difficulties handling data of high structural complexity. The class of ML known as Inductive Logic Programming (ILP) draws on the expressivity and rigour of subsets of First Order Logic to represent both data and models. When Description Logics (DL) are used, the approach can be applied directly to knowledge represented as ontologies. ILP output is a prime candidate for explainable artificial intelligence; the expense being computational complexity. We have recently demonstrated how a critical component of ILP learners in DL, namely, cover set testing, can be speeded up through the use of concurrent processing. Here we describe the first prototype of an ILP learner in DL that benefits from this use of concurrency. The result is a fast, scalable tool that can be applied directly to large ontologies. © 2020, Springer Nature Switzerland AG.","Description logics; GPGPU; Inductive logic programming; Ontologies; Parallel computing","Artificial intelligence; Computer circuits; Data description; Data handling; Formal languages; Formal logic; Ontology; Concurrent processing; Critical component; Description logic; First order logic; Structural complexity; Inductive logic programming (ILP)"
"Alger M.J., Livingston J.D., McClure-Griffiths N.M., Nabaglo J.L., Wong O.I., Ong C.S.","Interpretable Faraday complexity classification","10.1017/pasa.2021.10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104854243&doi=10.1017%2fpasa.2021.10&partnerID=40&md5=c91f3bd394b1a47f259209fc7d8ac3da","Faraday complexity describes whether a spectropolarimetric observation has simple or complex magnetic structure. Quickly determining the Faraday complexity of a spectropolarimetric observation is important for processing large, polarised radio surveys. Finding simple sources lets us build rotation measure grids, and finding complex sources lets us follow these sources up with slower analysis techniques or further observations. We introduce five features that can be used to train simple, interpretable machine learning classifiers for estimating Faraday complexity. We train logistic regression and extreme gradient boosted tree classifiers on simulated polarised spectra using our features, analyse their behaviour, and demonstrate our features are effective for both simulated and real data. This is the first application of machine learning methods to real spectropolarimetry data. With 95% accuracy on simulated ASKAP data and 90% accuracy on simulated ATCA data, our method performs comparably to state-of-the-art convolutional neural networks while being simpler and easier to interpret. Logistic regression trained with our features behaves sensibly on real data and its outputs are useful for sorting polarised sources by apparent Faraday complexity. © The Author(s), 2021. Published by Cambridge University Press on behalf of the Astronomical Society of Australia.","astrostatistics; classification; Keywords:; radio astronomy; radio spectroscopy; spectropolarimetry",
"Alghamdi H.S.","Towards Explainable Deep Neural Networks for the Automatic Detection of Diabetic Retinopathy","10.3390/app12199435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139952075&doi=10.3390%2fapp12199435&partnerID=40&md5=50db8a8968b86bd20a7049ee0de3456c","Featured Application: The proposed approach can be applied to any of the Convolutional Neural Networks-based architecture to explain, evaluate and validate the model’s decisions. Diabetic Retinopathy (DR) is a common complication associated with diabetes, causing irreversible vision loss. Early detection of DR can be very helpful for clinical treatment. Ophthalmologists’ manual approach to DR diagnoses is expensive and time-consuming; thus, automatic detection of DR is becoming vital, especially with the increasing number of diabetes patients worldwide. Deep learning methods for analyzing medical images have recently become prevalent, achieving state-of-the-art results. Consequently, the need for interpretable deep learning has increased. Although it was demonstrated that the representation depth is beneficial for classification accuracy for DR diagnoses, model explainability is rarely analyzed. In this paper, we evaluated three state-of-the-art deep learning models to accelerate DR detection using the fundus images dataset. We have also proposed a novel explainability metric to leverage domain-based knowledge and validate the reasoning of a deep learning model’s decisions. We conducted two experiments to classify fundus images into normal and abnormal cases and to categorize the images according to the DR severity. The results show the superiority of the VGG-16 model in terms of accuracy, precision, and recall for both binary and DR five-stage classification. Although the achieved accuracy of all evaluated models demonstrates their capability to capture some lesion patterns in the relevant DR cases, the evaluation of the models in terms of their explainability using the Grad-CAM-based color visualization approach shows that the models are not necessarily able to detect DR related lesions to make the classification decision. Thus, more investigations are needed to improve the deep learning model’s explainability for medical diagnosis. © 2022 by the author.","convolutional neural networks; deep learning; DenseNet; diabetic retinopathy; explainable deep networks; Grad-CAM; ResNet",
"Alghamdi H.S., Amoudi G., Elhag S., Saeedi K., Nasser J.","Deep Learning Approaches for Detecting COVID-19 from Chest X-Ray Images: A Survey","10.1109/ACCESS.2021.3054484","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100454060&doi=10.1109%2fACCESS.2021.3054484&partnerID=40&md5=d6d5cdde33ed639b711a39cf49dae7b3","Chest X-ray (CXR) imaging is a standard and crucial examination method used for suspected cases of coronavirus disease (COVID-19). In profoundly affected or limited resource areas, CXR imaging is preferable owing to its availability, low cost, and rapid results. However, given the rapidly spreading nature of COVID-19, such tests could limit the efficiency of pandemic control and prevention. In response to this issue, artificial intelligence methods such as deep learning are promising options for automatic diagnosis because they have achieved state-of-the-art performance in the analysis of visual information and a wide range of medical images. This paper reviews and critically assesses the preprint and published reports between March and May 2020 for the diagnosis of COVID-19 via CXR images using convolutional neural networks and other deep learning architectures. Despite the encouraging results, there is an urgent need for public, comprehensive, and diverse datasets. Further investigations in terms of explainable and justifiable decisions are also required for more robust, transparent, and accurate predictions. © 2013 IEEE.","Chest x-ray; coronavirus; COVID-19; deep learning; radiological imaging","Convolutional neural networks; Diagnosis; Medical imaging; Accurate prediction; Artificial intelligence methods; Automatic diagnosis; Control and prevention; Examination methods; Learning architectures; State-of-the-art performance; Visual information; Deep learning"
"Algosaibi A., Albahli S.","Web documents structures as source for machine-understandable document","10.1145/3354142.3354145","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073244647&doi=10.1145%2f3354142.3354145&partnerID=40&md5=e01bfb79637e6e3dda982bd06fee1c25","The web documents are not understandable by computers. And, web documents cross the web hold nourmance amount of information. To gain the maximum precision of reaching precise results, understanding these web documents is vital task. Moreover, it has always been the case that the Semantic Web is an extension of the current web. The Semantic Web or Web 3.0 is a collection of components that work together so that a machine is able to process and understand information. In order for this vision to be realized, formal standards for representing and interpreting data must be formulated and imposed. These formal standards include machine understandable ontologies Ontology Web Language (OWL). The Semantic Web vision provides the current Web with necessary infrastructure that allows computer to process knowledge and one of the cornerstones of this infrastructure is the Ontology. As this survey focuses on ―web documents"" as source for machine-understable document, we will discuss it in details how the researchers dealt with these web documents in order to build ontology. © 2019 Association for Computing Machinery.","Ontology; Ontology learning; Semantic Web; Web Generations; Web mining; World Wide Web; WWW","Data mining; Ontology; World Wide Web; Amount of information; Ontology learning; Ontology web language; Process knowledge; Semantic web vision; Web document; Web Generations; Web Mining; Semantic Web"
"Algur S.P., Bhat P., Sunitha Hiremath P.G.","Application of data mining in the classification of historical monument places","10.5815/ijisa.2016.08.07","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000420333&doi=10.5815%2fijisa.2016.08.07&partnerID=40&md5=614b3bb199ce8120386c9f48af80251f","The economic development and promotion of a country or region is depends on several facts such as- tourism, industries, transport, technology, GDP etc. The Government of the country is responsible to facilitate the opportunities to develop tourism, technology, transport etc. In view of this, we look into the Department of Tourism to predict and classify the number of tourists visiting historical Indian monuments such as Taj- Mahal, Agra, and Ajanta etc.. The data set is obtained from the Indian Tourist Statistics which contains year wise statistics of visitors to historical monuments places. A survey undertaken every year by the government is preprocessed to fill out the possible missing values, and normalize inconsistent data. Various classification techniques under Decision Tree approach such as- Random Tree, REPTree, Random Forest and J48 algorithms are applied to classify the historical monuments places. Performance evaluation measures of the classification models are analyzed and compared as a step in the process of knowledge discovery. © 2016 MECS.",,
"Al-Habaibeh A., Sen A., Chilton J.","Evaluation tool for the thermal performance of retrofitted buildings using an integrated approach of deep learning artificial neural networks and infrared thermography","10.1016/j.enbenv.2020.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125199481&doi=10.1016%2fj.enbenv.2020.06.004&partnerID=40&md5=8d6ccd87c23725e1c2091b4fe78dc4e3","In most countries, buildings are responsible for significant energy consumption where space heating and air conditioning is responsible for the majority of this energy use. To reduce this massive consumption and decrease carbon emission, thermal insulation of buildings can play an important role. The estimation of energy savings following the improvement of a building's insulation remains a key area of research in order to calculate the cost savings and the payback period. In this paper, a case study has been presented where deep retrofitting has been introduced to an existing building to bring it closer to a Passivhaus standard with the introduction of insulation and solar photovoltaic panels. The thermal performance of the building with its improved insulation has been evaluated using infrared thermography. Artificial intelligence using deep learning neural networks is implemented to predict the thermal performance of the building and the expected energy savings. The prediction of neural networks is compared with the actual savings calculated using historical weather data. The results of the neural network show high accuracy of predicting the actual energy savings with success rate of about 82% when compared with the calculated values. The results show that this suggested approach can be used to rapidly predict energy savings from retrofitting of buildings with reasonable accuracy, hence providing a practical rapid tool for the building industry and communities to estimate energy savings. A mathematical model has been also developed which has indicated a life-long monitoring will be needed to precisely estimate the benefits of energy savings in retrofitting due to the change in weather conditions and people's behaviour. © 2020","Artificial intelligence; Building thermal performance; Deep retrofitting; Infrared thermography; Neural networks; Wall insulation","Air conditioning; Buildings; Construction industry; Deep learning; Energy conservation; Energy utilization; Estimation; Forecasting; Investments; Meteorology; Neural networks; Solar power generation; Thermal insulation; Thermography (imaging); Building thermal performance; Deep retrofitting; Energy savings; Energy-consumption; Energy-savings; Evaluation tool; Integrated approach; Neural-networks; Wall insulation; Retrofitting"
"Alhaddad M.J., Mohammed A., Kamel M., Hagras H.","A genetic interval type-2 fuzzy logic-based approach for generating interpretable linguistic models for the brain P300 phenomena recorded via brain–computer interfaces","10.1007/s00500-014-1312-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925291394&doi=10.1007%2fs00500-014-1312-y&partnerID=40&md5=c4716ff9de645171d8b0769b6534c9b2","One of the important areas of brain–computer interface (BCI) research is to identify event-related potentials (ERPs) which are spatial–temporal patterns of the brain activity that happen after presentation of a stimulus and before execution of a movement. One of the important ERPs is the P300 which is an endogenous component of ERPs with a latency of about 300 ms which is elicited by significant stimuli (visual, or auditory). Various machine learning-based classifiers have been used to predict the P300 events and relate them to the human intended activities. However, the vast majority of the employed techniques like Bayesian linear discriminant analysis (BLDA) and regularized fisher linear discriminant analysis (RFLDA) are black box models which are difficult to understand and analyse by a normal clinician. In addition, due to the inter- and intra-user uncertainties associated with the P300 events, most of the existing classifiers need to be trained for a specific user under specific circumstances and the classifier needs to be retrained for different users or change of circumstances. In this paper, we present an interval type-2 fuzzy logic-based classifier which is able to handle the users’ uncertainties to produce better prediction accuracies than other competing classifiers such as BLDA or RFLDA. In addition, the generated type-2 fuzzy classifier is learnt from data via genetic algorithms to produce a small number of rules with a rule length of only one antecedent to maximise the transparency and interpretability for the normal clinician. We also employ a feature selection system based on an ensemble neural networks recursive feature selection which is able to find the effective time instances within the effective sensors in relation to given P300 event. We will present various experiments which were performed on standard data sets and using real-data sets obtained from real subjects’ experiments performed in the BCI laboratory in King Abdulaziz University. It will be shown that the produced type-2 fuzzy logic-based classifier will learn simple rules which are easy to understand explaining the events in question. In addition, the produced type-2 fuzzy logic classifier will be able to give better accuracies when compared to BLDA or RFLDA on various human subjects on the standard and real-world data sets. © 2014, Springer-Verlag Berlin Heidelberg.","Brain–computer interfaces; Linguistic model generation; Modeling perceptions; Type-2 fuzzy logic systems","Brain; Brain computer interface; Computer circuits; Discriminant analysis; Fuzzy sets; Genetic algorithms; Linguistics; Change of circumstances; Ensemble neural network; Eventrelated potential (ERPs); Fisher linear discriminant analysis; Interval type-2 fuzzy logic; Linear discriminant analysis; Linguistic modeling; Type-2 fuzzy logic system; Fuzzy logic"
"Al-Hadithi B.M., Cena C.E.G., León R.C., Loor C.L.","Development of an artificial intelligent lighting system for protected crops [Desarrollo de un Sistema de Iluminación Artificial Inteligente para Cultivos Protegidos]","10.1016/j.riai.2016.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994817329&doi=10.1016%2fj.riai.2016.07.005&partnerID=40&md5=e36b2f34124cce52660a9f1745bbefab","This article describes the hardware and software development of an artificial lighting system for protected crops. The system consists of a set of lamps, power control circuits and light intensity sensor, which together with a software developed in LabVIEW® allow to carry out the control over the amount of energy radiated throughout the cultivation process taking into consideration how this energy is distributed in each photoperiod. All this monitoring process takes place in a computer which is connected to an Arduino MEGA device serves as data acquisition card. To program the operation of the control system an interface is designed that enables the user to input parameters and display the system operating information. It also allows selecting the appropriate control strategy with a choice of selecting a predictive control or PD control system. Both algorithms make use of a mathematical model of the lamps which is responsible for transforming the signals generated by the drivers in digital signals that govern the operation of the implemented electronic system. Finally, the experimental results are analysed in various tests using both control algorithms to show the benefits and disadvantages of each controller. © 2015 CEA. Publicado por Elsevier España, S.L.U.","computer interfaces; Modeling; PD control; predictive control; software tools","Artificial intelligence; Computer aided software engineering; Computer programming languages; Computer software; Control systems; Crops; Data acquisition; Interfaces (computer); Lighting; Lighting fixtures; Models; Power control; Reconfigurable hardware; Software design; Artificial intelligent; Artificial lighting; Cultivation process; Data acquisition cards; Hardware and software; PD control; Power control circuits; Predictive control; Computer control"
"Alhaidari F.A., Al-Dahasi E.M.","New approach to determine DDoS attack patterns on SCADA system using machine learning","10.1109/ICCISci.2019.8716432","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067001093&doi=10.1109%2fICCISci.2019.8716432&partnerID=40&md5=9d4fd37fdd84b6b5f942bba108dc9a65","Recently, the importance of Supervisory Control and Data Acquisition (SCADA) systems has grown for many industries around the world. These systems are controlling many vital infrastructures such as grids of power, plants, and water treatment systems. In fact, nowadays SCADA systems cannot be isolated from the public and thus being more vulnerable and exposed to many malicious attacks. Several studies have reviewed the latest developments in cyber-security risks for SCADA systems and found that many factors are responsible for increasing the amount and the level of risks on modern control systems. Among such factors are the network architecture and the reliance on standard technologies that have known vulnerabilities. In this paper, we attempt to improve a framework of SCADA system against Distributed Denial of Service (DDoS) attacks using three machine learning algorithms (i) J48; (ii) Naive Bayes; (iii) Random Forest to determine the attack patterns. These algorithms were trained and evaluated on KDDCup'99 dataset. The preprocessing phase of the dataset was conducted based on the goal of the paper and the obtained results showed that the best classification is obtained using Random Forest classifier (RF) with 99.99% accuracy rate, while Naïve Bayes classifier has the lowest accuracy rate of 97.74%. © 2019 IEEE.","Computer Network Attack; Cyberattack; Cybersecurity; DDoS; Denial of Service Attack; DoS; SCADA; Simulation","Classification (of information); Decision trees; DOS; Electric power system control; Learning algorithms; Machine learning; Network architecture; Network security; SCADA systems; Water treatment; Computer network attacks; Cyber security; Cyber-attacks; DDoS; SCADA; Simulation; Denial-of-service attack"
"Alhakami W.","Alerts clustering for intrusion detection systems: Overview and machine learning perspectives","10.14569/ijacsa.2019.0100574","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066754508&doi=10.14569%2fijacsa.2019.0100574&partnerID=40&md5=11601645908d9da5c6406903002549db","The tremendous amount of the security alerts due to the high-speed alert generation of high-speed networks make the management of intrusion detection computationally expensive. Evidently, the high-level rate of wrong alerts disproves the Intrusion Detection Systems (IDS) performances and decrease its capability to prevent cyber-attacks which lead to tedious alert analysis task. Thus, it is important to develop new tools to understand intrusion data and to represent them in a compact forms using, for example, an alert clustering process. This hot topic of research is studied here and an understandable taxonomy followed by a deep survey of main published works related to intrusion alert management is presented in this paper. The second part of this work exposes different useful steps for designing a unified IDS system on the basis of machine learning techniques which are considered one of the most powerful tools for solving certain problems related to alert management and outlier detection. © 2018 The Science and Information (SAI) Organization Limited.","Alert clustering; Intrusion detection systems; Machine learning; Survey; Taxonomy","Clustering algorithms; Computer crime; Cybersecurity; Intrusion detection; Machine learning; Network security; Taxonomies; Alert clustering; Alert management; Clustering process; High Speed; High-speed Networks; Intrusion data; Intrusion Detection Systems; Intrusion-Detection; Security alerts; Systems performance; Surveys"
"Alhammad B.E., Al-Abdallat A.M., Jighly A., Obeid N., Alnemer L.M.","Microsatellites based algorithm for cross flanking regions identification in grass species","10.1109/ICITECH.2017.8079970","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039980189&doi=10.1109%2fICITECH.2017.8079970&partnerID=40&md5=2053e4ee461e4025f50d54f51aeebd63","This research aims at exploring the evolutionary path of selected five grass species to understand how they evolve through time from a computer science perspective, by designing a new algorithm that employs raw data sequences to extract the information necessary for explaining the hypothesis that claims crossing over regions on genomes are responsible of pushing the evolutionary path of plant species. Basic Local Alignment Search Tool (BLAST) and Clustal Omega application were used to perform local alignment and global multiple alignments between species and define the relativity between them in order to approximately determine what species falls in the evolutionary path over time. © 2017 IEEE.","BLAST; Clustal Omega; Comparative Analysis; Data Mining; SciRoKo","Alignment; Blasting; Evolutionary algorithms; Basic local alignment search tools; Clustal Omega; Comparative analysis; Evolutionary path; Flanking regions; Local alignment; Multiple alignment; SciRoKo; Data mining"
"Alharbi B., Liang Z., Aljindan J.M., Agnia A.K., Zhang X.","Explainable and Interpretable Anomaly Detection Models for Production Data","10.2118/208586-PA","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125967027&doi=10.2118%2f208586-PA&partnerID=40&md5=59a0bd2e6944bcbe56bb0ad3be6d225d","Trusting a machine-learning model is a critical factor that will speed the spread of the fourth industrial revolution. Trust can be achieved by understanding how a model is making decisions. For white-box models, it is easy to “see” the model and examine its prediction. For black-box models, the explanation of the decision process is not straightforward. In this work, we compare the performance of several white- and black-box models on two production data sets in an anomaly detection task. The presence of anomalies in production data can significantly influence business decisions and misrepresent the results of the analysis, if not identified. Therefore, identifying anomalies is a crucial and necessary step to maintain safety and ensure that the wells perform at full capacity. To achieve this, we compare the performance of K-nearest neighbor (KNN), logistic regression (Logit), support vector machines (SVMs), decision tree (DT), random forest (RF), and rule fit classifier (RFC). F1 and complexity are the two main metrics used to compare the prediction performance and interpretability of these models. In one data set, RFC outperformed the remaining models in both F1 and complexity, where F1 = 0.92, and complexity = 0.5. In the second data set, RF outperformed the rest in prediction performance with F1 = 0.84, yet it had the lowest complexity metric (0.04). We further analyzed the best performing models by explaining their predictions using local interpretable model-agnostic explanations, which provide justification for decisions made for each instance. Additionally, we evaluated the global rules learned from white-box models. Local and global analysis enable decision makers to understand how and why models are making certain decisions, which in turn allows trusting the models. Copyright © 2022 Society of Petroleum Engineers.",,"Decision trees; Forecasting; Logistic regression; Nearest neighbor search; Random forests; Support vector regression; Anomaly detection models; Black box modelling; Critical factors; Data set; Machine learning models; Performance; Prediction performance; Production data; Random forests; White-box models; Anomaly detection"
"Alharbi B.","Back to Basics: An Interpretable Multi-Class Grade Prediction Framework","10.1007/s13369-021-06153-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114825107&doi=10.1007%2fs13369-021-06153-x&partnerID=40&md5=00f8c05880e8ce093f1a644343dbc2f1","Next-term grade prediction is a challenging problem. The objective of this problem is to predict students grades in new courses, given their grades in courses they have previously taken. Adopting various machine learning algorithms is a very common and straightforward approach to tackling this problem. However, such models are very difficult to interpret. That is, it is difficult to explain to a student (or a teacher) why the model predicted grade B for a given student for example. In this work, we shed light on the importance of building interpretable models for educational data mining tasks. Specifically, we propose a novel interpretable framework for multi-class grade prediction that is based on an optimal rule-list mining algorithm. Additionally, we evaluate our proposed framework on two private datasets and compare our results with baseline models. Our findings show that our proposed framework is capable of achieving higher prediction and interpretability values when compared to black-box models. © 2021, King Fahd University of Petroleum & Minerals.","Interpretable machine learning; Multi-class classification; Next-term grade prediction; Rule-list algorithms; Student performance prediction",
"Alharbi R., Vu M.N., Thai M.T.","Evaluating Fake News Detection Models from Explainable Machine Learning Perspectives","10.1109/ICC42927.2021.9500467","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115697705&doi=10.1109%2fICC42927.2021.9500467&partnerID=40&md5=751ab21dbf3d85bc2e41c15bbf300814","Many research efforts recently have aimed at understanding the phenomenon of fake news, including recognizing their common features and patterns, leading to several fake news detection models based on machine learning. Yet, the real distinct strength of those models remains uncertain: some perform well only with particular data, but others are more general. Most of the models classified the fake news as a black-box without giving any explanations to users. In this work, therefore, we conduct an exploratory investigation that evaluates and interprets fake new detection models, including looking into which important features that contribute to the models' prediction from the explainable machine learning perspective. This give us some insights on how the detection models work and their trustworthiness. © 2021 IEEE.","explainable machine learning; Fake news; social network","Machine learning; Black boxes; Classifieds; Common features; Detection models; Explainable machine learning; Fake news; Machine-learning; Model-based OPC; Research efforts; Social network; Feature extraction"
"Alharbi R., Vu M.N., Thai M.T.","Learning Interpretation with Explainable Knowledge Distillation","10.1109/BigData52589.2021.9671988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125356285&doi=10.1109%2fBigData52589.2021.9671988&partnerID=40&md5=3ab44c5808af1e9a0167144c9c8e6565","Knowledge Distillation (KD) has been considered as a key solution in model compression and acceleration in recent years. In KD, a small student model is generally trained from a large teacher model by minimizing the divergence between the probabilistic outputs of the two. However, as demonstrated in our experiments, existing KD methods might not transfer critical explainable knowledge of the teacher to the student, i.e. the explanations of predictions made by the two models are not consistent. In this paper, we propose a novel explainable knowledge distillation model, called XDistillation, through which both the performance the explanations' information are transferred from the teacher model to the student model. The XDistillation model leverages the idea of convolutional autoencoders to approximate the teacher explanations. Our experiments shows that models trained by XDistillation outperform those trained by conventional KD methods not only in term of predictive accuracy but also faithfulness to the teacher models. © 2021 IEEE.","Explainable Machine Learning; Knowledge Distillation; Knowledge Transfer; Neural Network Distillation","Distillation; Machine learning; Students; Distillation method; Explainable machine learning; Knowledge distillation; Knowledge transfer; Machine-learning; Neural network distillation; Neural-networks; Student Modeling; Teacher models; Teachers'; Knowledge management"
"Alharthi A.S., Casson A.J., Ozanyan K.B.","Spatiotemporal Analysis by Deep Learning of Gait Signatures from Floor Sensors","10.1109/JSEN.2021.3078336","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105877092&doi=10.1109%2fJSEN.2021.3078336&partnerID=40&md5=f571dede00465db5840ab86bde84e79c","The recognition of gait pattern variation is of high importance to various industrial and commercial applications, including security, sport, virtual reality, gaming, robotics, medical rehabilitation, mental illness diagnosis, space exploration, and others. The purpose of this paper is to study the nature of gait variability in more detail, by identifying gait intervals responsible for gait pattern variations in individuals, as well as between individuals, using cognitive demanding tasks. This work uses deep learning methods for sensor fusion of 116 plastic optical fiber (POF) distributed sensors for gait recognition. The floor sensor system captures spatiotemporal samples due to varying ground reaction force (GRF) in multiples of up to 4 uninterrupted steps on a continuous 2× 1 m area. We demonstrate classifications of gait signatures, achieving up to 100% F1-score with Convolutional Neural Networks (CNN), in the context of gait recognition of 21 subjects, with imposters and clients. Classifications under cognitive load, induced by 4 different dual tasks, manifested lower F1-scores. Layer-Wise Relevance Propagation (LRP) methods are employed to decompose a trained neural network prediction to relevant standard events in the gait cycle, by generating a 'heat map' over the input used for classification. This allows valuable insight into which parts of the gait spatiotemporal signal have the heaviest influence on the gait classification and consequently, which gait events, such as heel strike or toe-off, are mostly affected by cognitive load. © 2001-2012 IEEE.","cognitive load; Deep convolutional neural networks (CNN); ground reaction force (GRF); interpretable neural networks; sensors fusion","Backpropagation; Biophysics; Convolutional neural networks; Diagnosis; Diseases; Floors; Gait analysis; Industrial robots; Learning systems; Medical robotics; Multilayer neural networks; Network security; Pattern recognition; Plastic optical fibers; Space research; Commercial applications; Distributed sensor; Gait classification; Ground reaction forces; Plastic optical fiber (POF); Spatio-temporal signals; Spatiotemporal analysis; Trained neural networks; Deep learning"
"Alharthi A.S., Casson A.J., Ozanyan K.B.","Gait Spatiotemporal Signal Analysis for Parkinson's Disease Detection and Severity Rating","10.1109/JSEN.2020.3018262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098198116&doi=10.1109%2fJSEN.2020.3018262&partnerID=40&md5=21d0b9da446aaa3fa33f98b686b01da4","Deep learning models are used to process and fuse raw data of gait-induced ground reaction force (GRF) for Parkinson's disease (PD) patients and healthy subjects with the aim to categorize PD severity. This is achieved by learning automatically, end-to-end, the spatiotemporal GRF signals, resulting in an effective PD severity classification with mean performance F1-score of 95.5% and F1-score standard errors of 0.28%. Layer-wise relevance propagation (LRP) is used to interpret the models' output and provide insight into which features in the spatiotemporal gait GRF signals are most significant for the models' predictions. This allows their assignment to gait events, implying that while for the classification of healthy gait the heel strike and body balance are the most indicative gait elements, foot landing and body balance are those most affected in advanced stages of PD. The proposed models are resilient to noise and are computationally efficient for processing and classification of large longitudinal GRF signal datasets, therefore they can be useful for detecting deterioration in the postural balance and rating PD severity. © 2001-2012 IEEE.","Deep convolutional neural networks (DCNN); deep learning; gait; ground reaction forces (GRF); interpretable neural networks; Parkinson's disease; perturbation","Biophysics; Classification (of information); Deep learning; Deterioration; Large dataset; Neurodegenerative diseases; Computationally efficient; Ground reaction forces; Healthy subjects; Learning models; Parkinson's disease; Postural balance; Spatio-temporal signals; Standard errors; Signal analysis"
"Alhassan Z., Watson M., Budgen D., Alshammari R., Alessa A., Moubayed N.A.","Improving current glycated hemoglobin prediction in adults: Use of machine learning algorithms with electronic health records","10.2196/25237","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106937796&doi=10.2196%2f25237&partnerID=40&md5=f80c14a8bf9ea9de2384f311aeffd2d1","Background: Predicting the risk of glycated hemoglobin (HbA1c) elevation can help identify patients with the potential for developing serious chronic health problems, such as diabetes. Early preventive interventions based upon advanced predictive models using electronic health records data for identifying such patients can ultimately help provide better health outcomes. Objective: Our study investigated the performance of predictive models to forecast HbA1c elevation levels by employing several machine learning models. We also examined the use of patient electronic health record longitudinal data in the performance of the predictive models. Explainable methods were employed to interpret the decisions made by the black box models. Methods: This study employed multiple logistic regression, random forest, support vector machine, and logistic regression models, as well as a deep learning model (multilayer perceptron) to classify patients with normal (&lt;5.7%) and elevated (≥5.7%) levels of HbA1c. We also integrated current visit data with historical (longitudinal) data from previous visits. Explainable machine learning methods were used to interrogate the models and provide an understanding of the reasons behind the decisions made by the models. All models were trained and tested using a large data set from Saudi Arabia with 18,844 unique patient records. Results: The machine learning models achieved promising results for predicting current HbA1c elevation risk. When coupled with longitudinal data, the machine learning models outperformed the multiple logistic regression model used in the comparative study. The multilayer perceptron model achieved an accuracy of 83.22% for the area under receiver operating characteristic curve when used with historical data. All models showed a close level of agreement on the contribution of random blood sugar and age variables with and without longitudinal data. Conclusions: This study shows that machine learning models can provide promising results for the task of predicting current HbA1c levels (≥5.7% or less). Using patients’ longitudinal data improved the performance and affected the relative importance for the predictors used. The models showed results that are consistent with comparable studies. ©Zakhriya Alhassan, Matthew Watson, David Budgen, Riyad Alshammari, Ali Alessa, Noura Al Moubayed.","Deep learning; Diabetes; Electronic health records; Glycated hemoglobin HbA1c; Longitudinal data; Machine learning; Multilayer perceptron; Neural network; Prediction; Time series data",
"Al-Hegami A.","Pruning based interestingness of mined classification patterns",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954671730&partnerID=40&md5=aa1bad63a9955053f93002eb62b2c592","Classification is an important problem in data mining. Decision tree induction is one of the most common techniques that are applied to solve the classification problem. Many decision tree induction algorithms have been proposed based on different attribute selection and pruning strategies. Although the patterns induced by decision trees are easy to interpret and comprehend compare to the patterns induced by other classification algorithms, the constructed decision trees may contain hundreds or thousand of nodes which are difficult to comprehend and interpret by the user who examines the patterns. For this reasons, the question of an appropriate constructing and providing a good pruning criteria have long been a topic of considerable debate. The main objective of such criteria is to create a tree such that the classification accuracy, when used on unseen data, is maximized and the tree size is minimized. Usually, most of decision tree algorithms perform splitting criteria to construct a tree first, then, prune the tree to find an accurate, simple, and comprehensible tree. Even after pruning, the decision tree constructed may be extremely huge and may reflect patterns, which are not interesting from the user point of view. In many scenarios, users are only interested in obtaining patterns that are interesting; thus, users may require obtaining a simple, and interpretable, but only approximate decision tree much better than an accurate tree that involves a lot of details. In this paper, we proposed a pruning approach that captures the user subjectivity to discoverer interesting patterns. The approach computes the subjective interestingness and uses it as a pruning criterion to prune away uninteresting patterns. The proposed framework helps in reducing the size of the induced model and maintaining the model. One of the features of the proposed approach is to capture the user background knowledge, which is monotonically augmented. The experimental results are quite promising.","Data mining; Decision tree; Domain knowledge; Interestingness; Knowledge discovery in databases; Novelty measure",
"Alheyasat O.","A hybrid k-mean and graph metrics algorithm for node sleeping scheduling in wireless sensor network (Wsn)","10.15866/IRECAP.V11I3.20018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114117635&doi=10.15866%2fIRECAP.V11I3.20018&partnerID=40&md5=8edb27d9e53ed1d1c122c72378f63a10","Wireless Sensor Networks (WSN) has proliferated in the past decade. These networks consist of massive number of battery-powered nodes distrusted over a given area. The nodes are responsible for sensing the environment and delivering the sensed data to a central point, named sink node. In order to reduce the power consumption of these nodes, sleeping/waking scheduling strategy has been proposed. In this work, a new hybrid sleeping/waking scheduling algorithm is proposed based on graph theory metrics and unsupervised K-mean machine learning algorithm. In the proposed algorithm, the sink node is responsible for calculating the metrics and clustering the nodes into three main clusters; dense, mid and light. Subsequently, the algorithm attempts to reduce the load on the nodes in light cluster in order to prolong the network lifetime. The algorithm has been simulated in 3D WSN with a clustering routing protocol. The simulation results show that the algorithm reduces the number of working sensor network nodes without affecting the network diameter. Moreover, the scheduling strategy has prolonged the network lifetime and has reduced the number of disconnected components. © 2021 Praise Worthy Prize S.r.l.-All rights reserved.","Graph Metrics; K-Mean Algorithm; Machine Learning; Scheduling; Wireless Sensor Networks (WSN)",
"Al-Hmouz R., Pedrycz W., Balamash A., Morfeq A.","Logic-Oriented Autoencoders and Granular Logic Autoencoders: Developing Interpretable Data Representation","10.1109/TFUZZ.2020.3043659","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097924201&doi=10.1109%2fTFUZZ.2020.3043659&partnerID=40&md5=28d358ebb5c92eb30cfccf9bf4a9c2d4","The plethora of ways of data representation and their applications to system modeling is inherently associated with dimensionality reduction. In a nutshell, the result of dimensionality reduction should support efficient ways of constructing ensuing models (classifiers and predictors) as well as an interpretation of the data themselves. Furthermore, there should be a suitable measure quantifying the quality of data positioned in the reduced space. We advocate that what makes the reduced data interpretable goes hand in hand with revealing a logic fabric of the data, suppressing redundancy, and finally arriving at a logic description of data. The anticipation is that the reduced data can be described in the form of logic expressions formed over the original highly dimensional data. Evidently, having these above-stated points in mind, the aim of this article is twofold: 1) to develop a logic-oriented data representation with the aid of autoencoders; and 2) to quantify the quality of results of this dimensionality reduction by incorporating a facet of information granularity. In other words, we argue that the result of dimensionality reduction gives rise to information granules whose level of granularity associates with the quality of processing completed by the autoencoder. In light of the recent surge of architectures of deep learning, the study is focused on the construction and analysis of logic-oriented autoencoders. We propose a two-level architecture composed of the logic-oriented processing units organized in two layers of logic processing units. As data representation provided by the autoencoder is not ideal, we augment the original architecture by granular parameters, which give rise to granular logic-oriented autoencoders. A suite of experiments is also reported. © 1993-2012 IEEE.","Autoencoder; deep learning; granular computing; information granules; logic autoencoder","Architecture; Computer circuits; Deep learning; Dimensionality reduction; Information granules; Learning systems; Data representations; Information granularity; Logic expressions; Processing units; Quality of data; Quality of results; Reduced space; System modeling; Data reduction"
"Alhumam A.","Explainable Software Fault Localization Model: From Blackbox to Whitebox","10.32604/cmc.2022.029473","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130152678&doi=10.32604%2fcmc.2022.029473&partnerID=40&md5=495128bcfb1036c453273937b026c1fb","The most resource-intensive and laborious part of debugging is finding the exact location of the fault from the more significant number of code snippets. Plenty of machine intelligence models has offered the effective localization of defects. Some models can precisely locate the faulty with more than 95% accuracy, resulting in demand for trustworthy models in fault localization. Confidence and trustworthiness within machine intelligence-based software models can only be achieved via explainable artificial intelligence in Fault Localization (XFL). The current study presents a model for generating counterfactual interpretations for the fault localization model’s decisions. Neural system approximations and disseminated presentation of input information may be achieved by building a nonlinear neural network model. That demonstrates a high level of proficiency in transfer learning, even with minimal training data. The proposed XFL would make the decision-making transparent simultaneously without impacting the model’s performance. The proposed XFL ranks the software program statements based on the possible vulnerability score approximated from the training data. The model’s performance is further evaluated using various metrics like the number of assessed statements, confidence level of fault localization, and Top-N evaluation strategies. © 2022 Tech Science Press. All rights reserved.","explainable artificial intelligence; Software fault localization; statement ranking; vulnerability detection","Artificial intelligence; Computer software; Fault detection; Black boxes; Explainable artificial intelligence; Fault localization; Localization modeling; Machine intelligence; Performance; Software fault localization; Statement ranking; Training data; Vulnerability detection; Decision making"
"Al-Hussaini I., Mitchell C.S.","SERF: Interpretable Sleep Staging using Embeddings, Rules, and Features","10.1145/3511808.3557700","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140843435&doi=10.1145%2f3511808.3557700&partnerID=40&md5=a7c231088ff2e6c797d6db1084a62c0d","The accuracy of recent deep learning based clinical decision support systems is promising. However, lack of model interpretability remains an obstacle to widespread adoption of artificial intelligence in healthcare. Using sleep as a case study, we propose a generalizable method to combine clinical interpretability with high accuracy derived from black-box deep learning. Clinician-determined sleep stages from polysomnogram (PSG) remain the gold standard for evaluating sleep quality. However, PSG manual annotation by experts is expensive and time-prohibitive. We propose SERF, interpretable Sleep staging using Embeddings, Rules, and Features to read PSG. SERF provides interpretation of classified sleep stages through meaningful features derived from the AASM Manual for the Scoring of Sleep and Associated Events. In SERF, the embeddings obtained from a hybrid of convolutional and recurrent neural networks are transposed to the interpretable feature space. These representative interpretable features are used to train simple models like a shallow decision tree for classification. Model results are validated on two publicly available datasets. SERF surpasses the current state-of-the-art for interpretable sleep staging by 2%. Using Gradient Boosted Trees as the classifier, SERF obtains 0.766 κ and 0.870 AUC-ROC, within 2% of the current state-of-the-art black-box models. © 2022 Owner/Author.","cnn; eeg; embedding; interpretable; lstm; representation learning; rule learning; sleep stage classification","Decision support systems; Decision trees; Long short-term memory; Sleep research; Cnn; Eeg; Embeddings; Interpretable; Lstm; Polysomnograms; Representation learning; Rule learning; Sleep stages classifications; Sleep staging; Embeddings"
"Ali A., Aliyuda K., Elmitwally N., Muhammad Bello A.","Towards more accurate and explainable supervised learning-based prediction of deliverability for underground natural gas storage","10.1016/j.apenergy.2022.120098","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140272188&doi=10.1016%2fj.apenergy.2022.120098&partnerID=40&md5=722bc2c57061192cd22a79f1107b41bd","Numerous subsurface factors, including geology and fluid properties, can affect the connectivity of the storage spaces in depleted reservoirs; hence, fluid flow simulations become more complicated, and predicting their deliverability remains challenging. This paper applies Machine Learning (ML) techniques to predict the deliverability of underground natural gas storage (UNGS) in depleted reservoirs. First, three baseline models were developed based on Support Vector Regression (SVR), Artificial Neural Network (ANN), and Random Forest (RF) algorithms. To improve the accuracy of the RF model as the best-performing baseline model, a unified framework, referred to as SARF, was developed. SARF combines the capabilities of Sparse Autoencoder (SA) and that of Random Forest (RF). To achieve this, the internal representations of the SA, which constitute extracted features of the input variables, are used in RF to develop the proposed SARF framework. The predictive capabilities of the baseline models and the proposed SARF model were validated using 3744 real-world storage data samples of 52 active storage reservoirs in the United States. The experimental result of this study shows that the proposed SARF model achieved an average 5.7% increase in accuracy on four separate data partitions over the baseline RF model. Furthermore, a set of eXplainable Artificial Intelligence (XAI) methods were developed to provide an intuitive explanation of which factors influence the deliverability of reservoir storage. The visualizations developed using the XAI method provide an easy-to-understand interpretation of how the SARF model predicted the deliverability values for separate reservoirs. © 2022 The Author(s)","Artificial neural network; Data-driven modeling; Interpretable machine learning; Natural gas industry; Random forests; Support vector regression","Digital storage; Flow of fluids; Forecasting; Gas industry; Machine learning; Natural gas; Neural networks; Random forests; Baseline models; Data-driven model; Deliverability; Depleted reservoirs; Interpretable machine learning; Machine-learning; Natural Gas Industry; Random forests; Support vector regressions; Underground natural gas storages; Decision trees; algorithm; artificial neural network; fluid flow; gas storage; machine learning; modeling; natural gas; support vector machine; United States"
"Ali A., Ali A., Yousaf M.M.","Novel Three-Tier Intrusion Detection and Prevention System in Software Defined Network","10.1109/ACCESS.2020.3002333","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087278582&doi=10.1109%2fACCESS.2020.3002333&partnerID=40&md5=cc59742ed8c46bf6f1201de4750214f0","Software Defined Network (SDN) is a flexible paradigm that provides support for a variety of data-intensive applications with real-world smart Internet of Things (IoT) devices. This emerging architecture updates with the managing ability and network control. Still, the benefits are challenging to achieve due to the presence of intruder flow into the network. The research topic of intrusion detection and prevention system (IDPS) has grasped the attention to reduce the effect of intruders. Distributed Denial of Service (DDoS) is a targeted attack that develops malicious traffic is flooded into a particular network device. These intruders also involve even with legitimate network devices, the authenticated device will be compromised to inject malicious traffic. In this paper, we investigate the involvement of intruders in three-Tier IDPS with regard to user validation, packet validation and flow validation. Not all the authentication users can be legitimate, since they are compromised, so that the major contribution is to identify all the compromised devices by knee analysis of the packets. Routers are the edge devices employed in first tier which is responsible to validate the IoT user with RFID tag and encrypted signature. Then the authenticated user's packets are submitted into second tier with switches that validates the packets using type-II fuzzy filtering. Then the key features are extracted from packets and they are classified into normal, suspicious and malicious. The mismatched packets are analyzed in controllers which maintain two queues as suspicious and normal. Then suspicious queue packets are classified and predicted using deep learning method. The proposed work is experimented in OMNeT++ environment and the performances are evaluated in terms of intruder Detection Rate, Failure Rate, Delay, Throughput and Traffic Load. © 2013 IEEE.","intrusion prevention system; IoT; packet classification; RFID; SDN security","Advanced traffic management systems; Application programs; Authentication; Deep learning; Denial-of-service attack; Failure analysis; Internet of things; Intrusion detection; Learning systems; Network security; Software defined networking; Data-intensive application; Distributed denial of service; Emerging architectures; Encrypted signatures; Internet of Things (IOT); Intruder detection; Intrusion detection and prevention systems; Malicious traffic; Routers"
"Ali A.F., Taha M.M.R., Thornton G.M., Shrive N.G., Frank C.B.","Biomechanical study using fuzzy systems to quantify collagen fiber recruitment and predict creep of the rabbit medial collateral ligament","10.1115/1.1894372","https://www.scopus.com/inward/record.uri?eid=2-s2.0-21144458832&doi=10.1115%2f1.1894372&partnerID=40&md5=e21b88712e4e02e42b70fa7d6c1bc5d5","In normal daily activities, ligaments are subjected to repeated loads, and respond to this environment with creep and fatigue. While progressive recruitment of the collagen fibers is responsible for the toe region of the ligament stress-strain curve, recruitment also represents an elegant feature to help ligaments resist creep. The use of artificial intelligence techniques in computational modeling allows a large number of parameters and their interactions to be incorporated beyond the capacity of classical mathematical models. The objective of the work described here is to demonstrate a tool for modeling creep of the rabbit medial collateral ligament that can incorporate the different parameters while quantifying the effect of collagen fiber recruitment during creep. An intelligent algorithm was developed to predict ligament creep. The modeling is performed in two steps: first, the ill-defined fiber recruitment is quantified using the fuzzy logic. Second, this fiber recruitment is incorporated along with creep stress and creep time to model creep using an adaptive neurofuzzy inference system. The model was trained and tested using an experimental database including creep tests and crimp image analysis. The model confirms that quantification of fiber recruitment is important for accurate prediction of ligament creep behavior at physiological loads. Copyright © 2005 by ASME.",,"Collagen; Computer simulation; Creep; Fuzzy control; Fuzzy sets; Ligaments; Mathematical models; Neurology; Strain; Stress analysis; Collagen fibers; Fuzzy systems; Stress-strain curve; Toe region; Biomechanics; collagen fiber; algorithm; article; artificial intelligence; biology; biomechanics; data base; image analysis; knee ligament; ligament; mathematical analysis; mathematical model; model; nonhuman; rabbit; stress strain relationship; toe; Algorithms; Animals; Biomechanics; Computer Simulation; Elasticity; Fibrillar Collagens; Fuzzy Logic; Medial Collateral Ligament, Knee; Models, Biological; Neural Networks (Computer); Rabbits; Stress, Mechanical; Viscosity; Oryctolagus cuniculus"
"Ali A.R., Budka M.","An Automated Approach for Timely Diagnosis and Prognosis of Coronavirus Disease","10.1109/IJCNN52387.2021.9533786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116498133&doi=10.1109%2fIJCNN52387.2021.9533786&partnerID=40&md5=991973b2da79c8b20fb286cecd1b4118","Since the outbreak of Coronavirus Disease 2019 (COVID-19), most of the impacted patients have been diagnosed with high fever, dry cough, and soar throat leading to severe pneumonia. Hence, to date, the diagnosis of COVID-19 from lung imaging is proved to be a major evidence for early diagnosis of the disease. Although nucleic acid detection using real-time reverse-transcriptase polymerase chain reaction (rRT-PCR) remains a gold standard for the detection of COVID-19, the proposed approach focuses on the automated diagnosis and prognosis of the disease from a non-contrast chest computed tomography (CT) scan for timely diagnosis and triage of the patient. The prognosis covers the quantification and assessment of the disease to help hospitals with the management and planning of crucial resources, such as medical staff, ventilators and intensive care units (ICUs) capacity. The approach utilises deep learning techniques for automated quantification of severity of COVID-19 disease via measuring the area of multiple rounded ground-glass opacities (GGO) and consolidations in the periphery (CP) of the lungs and accumulating them to form a severity score. The severity of the disease can be correlated with the medicines prescribed during the triage to assess the effectiveness of the treatment. The proposed approach shows promising results where the classification model achieved 93% accuracy on hold-out data. © 2021 IEEE.","assessment of treatment; computed tomography; diagnostic imaging; explainable AI; quantification of disease; radiography","Automation; Computerized tomography; Deep learning; Diagnosis; Diseases; Human resource management; Intensive care units; Polymerase chain reaction; 'Dry' [; Assessment of treatment; Automated approach; Coronaviruses; Diagnosis and prognosis; Diagnostic imaging; Early diagnosis; Explainable AI; Lung imaging; Quantification of disease; Coronavirus"
"Ali D., Frimpong S.","Artificial intelligence models for predicting the performance of hydro-pneumatic suspension struts in large capacity dump trucks","10.1016/j.ergon.2018.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049336711&doi=10.1016%2fj.ergon.2018.06.005&partnerID=40&md5=b33d864c94d01485a7f4b1087ca4c797","Large dump trucks are being matched with large shovels to achieve bulk economic production in surface mining operations. This process results in high impact shovel loading operations (HISLO) and exposes operators to severe levels of whole-body vibrations (WBV). The performance of the hydro-pneumatic suspension struts, responsible for vibration attenuation in large dump trucks, decreases as a truck age. There is a need for a system for monitoring and predicting the performance of the suspension struts in real time. Artificial intelligence (AI) has been applied for modeling and predicting the suspension system performance for light/smaller vehicles. However, no work has been done to implement AI for modeling and predicting the performance of hydro-pneumatic struts in large dump trucks. This paper is a pioneering effort towards developing AI models for solving this problem. These AI models would incorporate the Artificial Neural Networks (ANN), Mamdani Fuzzy Logic (MFL) and a hybrid system, the Hybrid Neural Fuzzy Interference System (HyFIS), for achieving this goal. Experiments were conducted using a 3D virtual simulator for the CAT 793D in MSC.ADMAS. RMS accelerations in the vertical and horizontal directions at the operator seat were recorded as the two main outputs for the suspension system performance. Eighty percent (80%) of the total experimental data was used in training and developing the models and the remaining 20% for testing and validating the developed models. With an R2 and RMSE of 0.98168505 and 0.00852251 for the training phase, respectively, and 0.9660429 and 0.0195620 for the testing phase, HyFIS model showed the best accuracy for predicting the hydro-pneumatic suspension struts performance for dump trucks. This is the first time that AI models have been developed for dump truck suspension system performance prediction. With the implementation of these models in the dump truck, maintenance personnel can monitor the performance of the suspension system in real-time and schedule proper maintenance and/or replacement. Implementation of such a system will improve the workplace safety, operator's health and the overall system efficiency. © 2018 Elsevier B.V.","Artificial intelligence; Artificial neural network; Dump truck; HISLO; HyFIS; Mamdani fuzzy logic; Suspension system; Whole body vibrations","Artificial intelligence; Automobile bodies; Computer circuits; Forecasting; Fuzzy inference; Fuzzy logic; Fuzzy neural networks; Hybrid systems; Mine trucks; Mining; Neural networks; Pneumatic control; Shovels; Struts; Vibrations (mechanical); Dump trucks; HISLO; HyFIS; Mamdani fuzzy; Suspension system; Whole body vibration; Suspensions (components); acceleration; Article; artificial intelligence; artificial neural network; controlled study; fuzzy logic; Hybrid Neural Fuzzy Interference System; hydropneumatic suspension strut; limit of quantitation; Mamdani Fuzzy Logic; measurement accuracy; measurement precision; prediction; priority journal; process optimization; validation process; virtual reality"
"Ali F., Cai Q., Hu J., Zhang L., Hoare R., Monaghan S.J., Pang H.","In silico analysis of AhyI protein and AI-1 inhibition using N-cis-octadec-9z-enoyl-l-homoserine lactone inhibitor in Aeromonas hydrophila","10.1016/j.micpath.2021.105356","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121579948&doi=10.1016%2fj.micpath.2021.105356&partnerID=40&md5=f5dadf851aaee9530f1c2cf68318aa7d","AhyI is homologous to the protein LuxI and is conserved throughout bacterial species including Aeromonas hydrophila. A. hydrophila causes opportunistic infections in fish and other aquatic organisms. Furthermore, this pathogennot only poses a great risk for the aquaculture industry, but also for human public health. AhyI (expressing acylhomoserine lactone) is responsible for the biosynthesis of autoinducer-1 (AI-1), commonly referred to as a quorum sensing (QS) signaling molecule, which plays an essential role in bacterial communication. Studying protein structure is essential for understanding molecular mechanisms of pathogenicity in microbes. Here, we have deduced a predicted structure of AhyI protein and characterized its function using in silico methods to aid the development of new treatments for controlling A.hydrophila infections. In addition to modeling AhyI, an appropriate inhibitor molecule was identified via high throughput virtual screening (HTVS) using mcule drug-like databases.The AhyI-inhibitor N-cis-octadec-9Z-enoyl-L-Homoserine lactone was selected withthe best drug score. In order to understand the pocket sites (ligand binding sites) and their interaction with the selected inhibitor, docking (predicted protein binding complex) servers were used and the selected ligand was docked with the predicted AhyI protein model. Remarkably, N-cis-octadec-9Z-enoyl-L-Homoserine lactone established interfaces with the protein via16 residues (V24, R27, F28, R31, W34, V36, D45, M77, F82, T101, R102, L103, 104, V143, S145, and V168), which are involved with regulating mechanisms of inhibition. These proposed predictions suggest that this inhibitor molecule may be used as a novel drug candidate for the inhibition of auto-inducer-1 (AI-1) activity.The N-cis-octadec-9Z-enoyl-L-Homoserine lactone inhibitor molecule was studied on cultured bacteria to validate its potency against AI-1 production. At a concentration of 40 μM, optimal inhibition efficiency of AI-1 was observedin bacterial culture media.These results suggest that the inhibitor molecule N-cis-octadec-9Z-enoyl-L-Homoserine lactone is a competitive inhibitor of AI-1 biosynthesis. © 2021 Elsevier Ltd","Aeromonas hydrophila; AhyI; AI-1 biosynthesis; High throughput virtual screening; I-TASSER; LuxI; Molecular docking","AhyI protein; bacterial protein; n acylhomoserine lactone; n octadec 9z enoyl levo homoserine lactone inhibitor; protein inhibitor; unclassified drug; gamma butyrolactone; homoserine lactone; Aeromonas hydrophila; amino acid sequence; amino terminal sequence; Article; bacterium culture; binding site; carboxy terminal sequence; cellular distribution; concentration (parameter); controlled study; high throughput screening; ligand binding; nonhuman; prediction; protein analysis; protein function; protein localization; protein structure; quorum sensing; animal; artificial intelligence; human; 4-Butyrolactone; Aeromonas hydrophila; Animals; Artificial Intelligence; Bacterial Proteins; Humans; Quorum Sensing"
"Ali G., Al-Obeidat F., Tubaishat A., Zia T., Ilyas M., Rocha A.","Counterfactual explanation of Bayesian model uncertainty","10.1007/s00521-021-06528-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115614824&doi=10.1007%2fs00521-021-06528-z&partnerID=40&md5=73b1201e07f51d88d62274e762d5eca9","Artificial intelligence systems are becoming ubiquitous in everyday life as well as in high-risk environments, such as autonomous driving, medical treatment, and medicine. The opaque nature of the deep neural network raises concerns about its adoption in high-risk environments. It is important for researchers to explain how these models reach their decisions. Most of the existing methods rely on softmax to explain model decisions. However, softmax is shown to be often misleading, particularly giving unjustified high confidence even for samples far from the training data. To overcome this shortcoming, we propose Bayesian model uncertainty for producing counterfactual explanations. In this paper, we compare the counterfactual explanation of models based on Bayesian uncertainty and softmax score. This work predictively produces minimal important features, which maximally change classifier output to explain the decision-making process of the Bayesian model. We used MNIST and Caltech Bird 2011 datasets for experiments. The results show that the Bayesian model outperforms the softmax model and produces more concise and human-understandable counterfactuals. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Bayesian model uncertainty; Counterfactual explanation; Deep learning","Decision making; Deep neural networks; Uncertainty analysis; Artificial intelligence systems; Autonomous driving; Bayesian model uncertainty; Bayesian modelling; Counterfactual explanation; Counterfactuals; Deep learning; High risk environment; Medical treatment; Modeling uncertainties; Bayesian networks"
"Ali H., Khan M.S., Al-Fuqaha A., Qadir J.","Tamp-X: Attacking explainable natural language classifiers through tampered activations","10.1016/j.cose.2022.102791","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131824415&doi=10.1016%2fj.cose.2022.102791&partnerID=40&md5=e0c67fc6e408cfa583b8573244c6418a","While the technique of Deep Neural Networks (DNNs) has been instrumental in achieving state-of-the-art results for various Natural Language Processing (NLP) tasks, recent works have shown that the decisions made by DNNs cannot always be trusted. Recently Explainable Artificial Intelligence (XAI) methods have been proposed as a method for increasing DNN's reliability and trustworthiness. These XAI methods are however open to attack and can be manipulated in both white-box (gradient-based) and black-box (perturbation-based) scenarios. Exploring novel techniques to attack and robustify these XAI methods is crucial to fully understand these vulnerabilities. In this work, we propose Tamp-X—a novel attack which tampers the activations of robust NLP classifiers forcing the state-of-the-art white-box and black-box XAI methods to generate misrepresented explanations. To the best of our knowledge, in current NLP literature, we are the first to attack both the white-box and the black-box XAI methods simultaneously. We quantify the reliability of explanations based on three different metrics—the descriptive accuracy, the cosine similarity, and the Lp norms of the explanation vectors. Through extensive experimentation, we show that the explanations generated for the tampered classifiers are not reliable, and significantly disagree with those generated for the untampered classifiers despite that the output decisions of tampered and untampered classifiers are almost always the same. Additionally, we study the adversarial robustness of the tampered NLP classifiers, and find out that the tampered classifiers which are harder to explain for the XAI methods, are also harder to attack by the adversarial attackers. © 2022 The Author(s)","Adversarial attacks; Attacking XAI; Explainable artificial intelligence (XAI); Model tampering; Natural language processing","Chemical activation; Deep neural networks; Adversarial attack; Attacking XAI; Black boxes; Explainable artificial intelligence (XAI); Gradient based; Model tampering; Natural languages; Novel techniques; State of the art; White box; Natural language processing systems"
"Ali H., Maynard A.D.","Design the Future Activities (DFA): A Pedagogical Content Knowledge Framework in Engineering Design Education",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123843471&partnerID=40&md5=cece1170e0f8ac02ac7fa586ac78159b","We propose an effective, innovative framework for developing content for design activities that address the challenges of the future where emerging technologies play a central role. Although engineering education research is concerned with preparing future engineers, the integration of future trends in technology with the engineering curriculum has been limited. We propose the Design the Future Activities (DFA) as a framework for systematically identifying and integrating emerging areas of research and technologies, such as artificial intelligence, into the teaching of engineering design. The core of developing and delivering the DFA framework is the teaching of the technology of artificial intelligence (AI). Because these technologies will change the nature of the future, we seek to engage with the ongoing discourse on the relationship between content (for design education) and pedagogy, through a proposed pedagogical content knowledge conceptual framework. Through a scholarship of integration that breaks the boundaries between disciplines, we propose a three-level framework: (1) Understanding technology analysis and system integration (to allow students to identify appropriate solutions given new technologies); (2) Making a value chain (or how these are appropriate solutions); and (3) Developing responsible innovations (or why these are appropriate solutions). While engineers continue to be creators and influencers of such technologies, the lack of understanding of the impact of their own technologies continues to cause an imbalanced innovation landscape, in education and in the workplace. We conclude that a new design approach to the engineering curriculum should be attempted, assuming that educators will systematically anticipate the future and recalibrate the curriculum. © American Society for Engineering Education, 2021",,"Artificial intelligence; Curricula; Teaching; Design activity; Design Education; Emerging technologies; Engineering curriculum; Engineering design; Engineering design education; Engineering education research; Future trends; Knowledge frameworks; Pedagogical content knowledge; Engineering education"
"Ali H.","DESIGN THE FUTURE ACTIVITIES (DFA): FRAMEWORK TO DEVELOP CASE STUDIES TO INCORPORATE DEEP UNDERSTANDING OF THE COUPLING BETWEEN TECHNOLOGY, SOCIETY AND THE FUTURE","10.1115/IMECE2021-73344","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123872929&doi=10.1115%2fIMECE2021-73344&partnerID=40&md5=88f7efdd27a05d6fa3c5cf409226b101","Although engineering education research is concerned with preparing future engineers, the integration of future trends in technology with the engineering curriculum has been limited. This paper utilizes the Design the Future Activities (DFA) framework to provide guidelines for developing case studies in engineering design education to address the need for design for the future. The DFA as a framework systematically identifies and integrates emerging areas of research and technologies, such as artificial intelligence, into the teaching of engineering design. It links the contents of its three levels (Understanding technology analysis and system integration; Making a value chain, and Developing responsible innovations) with effective pedagogy for each level. Here, case studies are discussed in depth as an effective pedagogy for the second and third levels of the DFA framework. While engineers continue to be creators and influencers of such technologies, the lack of understanding of the impact of their own technologies continues to cause an imbalanced innovation landscape, in education and in the workplace. The new, revived design education approach should be attempted, assuming that educators will systematically anticipate the future and recalibrate the curriculum. Copyright © 2021 by ASME","Case studies; Design for the future; Design the future activities; Engineering design education","Curricula; Case-studies; Design for the future; Design the future activity; Engineering curriculum; Engineering design; Engineering design education; Engineering education research; Future trends; Technology analysis; Three-level; Engineering education"
"Ali H.","Design the Future Activities (DFA): Typology of Content for Case Studies","10.1109/FIE49875.2021.9637407","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123853409&doi=10.1109%2fFIE49875.2021.9637407&partnerID=40&md5=d48881d77636152bdcd2b2061faf529e","This work-in-progress, research-to-practice provides a typology for content for case studies that are developed as part of the Design the Future Activities (DF A) framework. This work is based on the understanding that the nature of engineering design education and practice is changing due to the emergence of intelligent, autonomous objects. Traditional ways of teaching design, which are based on teaching design as a process of creatively solving problems, are not sufficient. Complementary to previous teaching practices is the need to provide design content to address the need for designing for the future, where intelligent objects act as a stakeholder in the design problem. In this paper, a typology of content is proposed to develop case studies that address the need for developing and delivering the DFA framework; that is engineering design education with the core of the technology of artificial intelligence. © 2021 IEEE.","AI case studies; creating responsible innovations; Design for the future; making a value chain; technology analysis","Engineering education; AI case study; Case-studies; Creating responsible innovation; Design for the future; Engineering design education; In-progress research; Making a value chain; Teaching designs; Technology analysis; Value chains; Design"
"Ali M., Khattak A.M., Ali Z., Hayat B., Idrees M., Pervez Z., Rizwan K., Sung T.-E., Kim K.-I.","Estimation and interpretation of machine learning models with customized surrogate model","10.3390/electronics10233045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120685537&doi=10.3390%2felectronics10233045&partnerID=40&md5=d05bb07d33036f5f2e69745fc40d9a08","Machine learning has the potential to predict unseen data and thus improve the productivity and processes of daily life activities. Notwithstanding its adaptiveness, several sensitive applications based on such technology cannot compromise our trust in them; thus, highly accurate machine learning models require reason. Such models are black boxes for end-users. Therefore, the concept of interpretability plays the role if assisting users in a couple of ways. Interpretable models are models that possess the quality of explaining predictions. Different strategies have been proposed for the aforementioned concept but some of these require an excessive amount of effort, lack generalization, are not agnostic and are computationally expensive. Thus, in this work, we propose a strategy that can tackle the aforementioned issues. A surrogate model assisted us in building interpretable models. Moreover, it helped us achieve results with accuracy close to that of the black box model but with less processing time. Thus, the proposed technique is computationally cheaper than traditional methods. The significance of such a novel technique is that data science developers will not have to perform strenuous hands-on activities to undertake feature engineering tasks and end-users will have the graphical-based explanation of complex models in a comprehensive way—consequently building trust in a machine. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Data science; Interpretable model; Machine learning; Signal processing; Supervised learning; Surrogate models",
"Ali M.N.Y., Nimmy S.F.","Two-step verifications for multi-instance features selection: A machine learning approach","10.1007/978-3-319-65981-7_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034246984&doi=10.1007%2f978-3-319-65981-7_7&partnerID=40&md5=7e9155a3595a7eda61042c51b7ffbc3e","Multi-instance features measurement is an important step in identifying characteristics that are bound to various experimental events. In biological data processing, a set of critical factors is responsible for several diseases. Computational simulation will help to design an optimal tool for cost-effective drug design. In this regard, the processing of big data is valuable for efficient simulation. Recent experimental results generate huge amounts of related data. In the current work, noisy data have been treated with three filtering techniques: cross-validated committees filtering (CVCF), iterative partitioning filtering (IPF) and ensemble filtering (EF). A comparison was made of these three filtering approaches. The filtered datasets were normalized. The repeated application of three normalization techniques removed the irregularities and structured the datasets. Wide ranges of comparisons were made among these three normalization techniques. After being appropriately structured, these normalized datasets were transformed accordingly with three different transformation processes: rank transformation, nominal to binary transformation and Box-Cox transformation. To prevent false positive and false negative outcomes of the experiments, certain key aspects were considered: accuracy, sensitivity and F-measures. Accuracy of the experiments relates to the level of precise detection of certain factors; specificity allows the selection of the dominant factors; and sensitivity and F-measures are the ratio between the training and testing datasets. Detailed experimental analysis included a comparison study of the four classifiers for the deoxyribonucleic acid (DNA) dataset. © 2018, Springer International Publishing AG.","Box-Cox transformation; Filtering; Iterative filtering; Nominal to binary transformation; Rank transformation",
"Ali S., DiPaola D., Lee I., Sindato V., Kim G., Blumofe R., Breazeal C.","Children as creators, thinkers and citizens in an AI-driven future","10.1016/j.caeai.2021.100040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123988548&doi=10.1016%2fj.caeai.2021.100040&partnerID=40&md5=99c8f5e95b7db29854ad3f07223c9693","Generative Artificial Intelligence (AI) approaches open up new avenues of digital creation, and are simultaneously accompanied by societal and ethical implications such as the creation of Deepfakes and spread of misinformation, renewing our understanding of technical AI systems as socio-technical systems. Applications of, and media generated by generative AI techniques are abundantly present on social media platforms frequented by children, who are not yet aware of the existence of AI-manipulated media. Previous work has highlighted the importance of digital media literacy and AI literacy for children. In this work, we introduce middle school students to generative AI techniques as a tool for creation, while also focusing on critical discussion about their societal and ethical implications, and encouraging pro-activeness in being responsible consumers, creators and stakeholders of technology. We present learning activities that introduce 38 middle-school students to generative modeling, how it is used to generate Deepfakes, cues that help to recognize Deepfakes, and the spread and effects of misinformation. Students demonstrated an understanding that generative media may be believable, but not necessarily true, and can contribute to the spread of misinformation. They were also able to identify why misinformation may be harmful or lasting, drawing specific examples to social settings that indicate human-centered implications. Finally, students expressed opinions about policies surrounding the presence of Deepfakes on social media. This approach can be adopted to introduce students to other technical systems that constitute both productive applications and potential negative implications of technology. CCS concepts: ⋅Applied computing → Interactive learning environments; ⋅Human-centered computing → Social media; Social networks; ⋅Social and professional topics → Computing literacy; K-12 education; Additional key words and phrases: Misinformation, Deepfakes, digital literacy, media literacy, social media. © 2021 The Authors","Deepfakes; Digital literacy; Generative AI; Media literacy; Misinformation; Social media",
"Ali S.D., Tayara H., Chong K.T.","Interpretable machine learning identification of arginine methylation sites","10.1016/j.compbiomed.2022.105767","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132932977&doi=10.1016%2fj.compbiomed.2022.105767&partnerID=40&md5=9864b3fe04eaa4cb512fc853f011b01f","Protein methylation is one of the most prominent posttranslation modifications that essentially regulates several biological processes in eukaryotes. Therefore, identification of the arginine methylation site is crucial in deciphering its characteristics and functions in cell biology, disease mechanisms, and guided drug development. The computation methods address the long-term bottleneck together with the cost, time, and labor required in experimental methods for large-scale identification of protein arginine methylation sites. In this study, we proposed a robust machine learning-based computational tool known as iIRMethyl, employing the primary sequence and physicochemical properties of protein along with a two-step feature selection method for optimal selection of feature descriptors. Moreover, the performance of iIRMethyl was comprehensively evaluated via k-fold cross-validation on a benchmark dataset and independent test dataset. iIRMethyl demonstrated a remarkably greater performance than the state-of-the-art method and achieved an average area under the curve value of 0.99 for both k-fold cross-validation and an independent test set in the identification of protein arginine methylation sites. Furthermore, the outcomes reveal that iIRMethyl is a robust and accurate computational tool for large-scale identification of arginine methylation sites and would facilitate the understanding of their functional mechanisms and accelerating their application in drug development and clinical therapy. Additionally, the prediction mechanism of the proposed model iIRMethyl is interpreted using the SHapley Additive exPlanation algorithm. © 2022","Arginine methylation; Cross-validation; Machine learning; Two-step feature selection","Alkylation; Benchmarking; Computational methods; Cytology; Feature Selection; Methylation; Physicochemical properties; Proteins; Statistical tests; Arginine methylation; Computational tools; Cross validation; Drug development; Features selection; Identification of proteins; Large-scales; Machine-learning; Performance; Two-step feature selection; Arginine; arginine; lysine; protein; amino acid composition; area under the curve; Article; Bayes theorem; carboxy terminal sequence; comparative study; controlled study; convolutional neural network; cross validation; cytology; dimensionality reduction; feature selection; human; hydrophobicity; k fold cross validation; machine learning; molecular size; physical chemistry; prediction error; protein methylation; random forest; short term memory; support vector machine; algorithm; biology; chemistry; machine learning; metabolism; methylation; Algorithms; Arginine; Computational Biology; Lysine; Machine Learning; Methylation; Proteins"
"Ali S.D., Alam W., Tayara H., Chong K.T.","Identification of Functional piRNAs Using a Convolutional Neural Network","10.1109/TCBB.2020.3034313","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131686133&doi=10.1109%2fTCBB.2020.3034313&partnerID=40&md5=5beea8e054546090b8df129080d76597","Piwi-interacting RNAs (piRNAs) are a distinct sub-class of small non-coding RNAs that are mainly responsible for germline stem cell maintenance, gene stability, and maintaining genome integrity by repression of transposable elements. piRNAs are also expressed aberrantly and associated with various kinds of cancers. To identify piRNAs and their role in guiding target mRNA deadenylation, the currently available computational methods require urgent improvements in performance. To facilitate this, we propose a robust predictor based on a lightweight and simplified deep learning architecture using a convolutional neural network (CNN) to extract significant features from raw RNA sequences without the need for more customized features. The proposed model's performance is comprehensively evaluated using k-fold cross-validation on a benchmark dataset. The proposed model significantly outperforms existing computational methods in the prediction of piRNAs and their role in target mRNA deadenylation. In addition, a user-friendly and publicly-accessible web server is available at http://nsclbio.jbnu.ac.kr/tools/2S-piRCNN/. © 2004-2012 IEEE.","convolutional neural network; deep learning; piRNAs; sequence analysis; small non-coding RNAs","Computational methods; Convolution; Deep neural networks; Network coding; Stem cells; Convolutional neural network; Deadenylation; Deep learning; Germline stem cells; Performance; Piwi-interacting RNA; Sequence analysis; Small non-coding RNA; Sub class; Transposable elements; Benchmarking; messenger RNA; small interfering RNA; genetics; human; neoplasm; transposon; DNA Transposable Elements; Humans; Neoplasms; Neural Networks, Computer; RNA, Messenger; RNA, Small Interfering"
"Ali S.H.","Novel approach for generating the key of stream cipher system using random forest data mining algorithm","10.1109/DeSE.2013.54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946689350&doi=10.1109%2fDeSE.2013.54&partnerID=40&md5=6fc225e04ad7a37fb5b0f46d87efeebd","Key generator is part of the stream cipher system that is responsible for generating a long random sequence of binary bits key that used in ciphering and deciphering processes. Therefore, key generator is the heart of the stream cipher system. A system with traditional key generating techniques such as using the feedback shift register is vulnerable to cypher attacks which render it to be ineffective and insecure. A suggested novel method to overcome this problem is to use a Random Forest-Data Mining (RF-DM) algorithm. It incorporates using new types of linear and nonlinear functions to mix the plain text with the key in the encryption process and the cipher text with the reverse key in the decryption process. The method is successful in satisfying the secrecy goal of the stream cipher system because it uses two types of dual randomization to baffle the attacker or cryptanalysist. By mathematical logic and prove by experiments, we generate the key successfully by encrypting messages of different sizes. This proves the ablity to generated the unique key for each message based on its length without repeating the key in most cases. In addition, by combining the use of the random forest data mining technique as the randomizating principle in the ciperhing process makes it exteremely difficult for any attacker because the attacker does not have access or understanding of the not only the technique used but also the process to decipher. The result from our experiment shows that the longer the number of the message size, the longer is the length of the generated key, which determines its strength and hence, its complexity to break by brute force. © 2013 IEEE.","Attacks; Cryptanalysis; Data mining; Key generator; Linear and nonlinear functions; Random forest algorithm; Security; Stream cipher","Algorithms; Cryptography; Decision trees; Functions; Shift registers; Attacks; Cryptanalysis; Key generator; Nonlinear functions; Random forest algorithm; Security; Stream Ciphers; Data mining"
"Ali S.H.","Miner for OACCR: Case of medical data analysis in knowledge discovery","10.1109/SETIT.2012.6482043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875699631&doi=10.1109%2fSETIT.2012.6482043&partnerID=40&md5=ab7735d9d942565e50263fd1236de8b6","Modern scientific data consist of huge datasets which gathered by a very large number of techniques and stored in much diversified and often incompatible data repositories as data of bioinformatics, geoinformatics, astroinformatics and Scientific World Wide Web. At the other hand, lack of reference data is very often responsible for poor performance of learning where one of the key problems in supervised learning is due to the insufficient size of the training dataset. Therefore, we try to suggest a new development a theoretically and practically valid tool for analyzing small of sample data remains a critical and challenging issue for researches. This paper presents a methodology for Obtaining Accurate and Comprehensible Classification Rules (OACCR) of both small and huge datasets with the use of hybrid techniques represented by knowledge discovering. In this article the searching capability of a Genetic Programming Data Construction Method (GPDCM) has been exploited for automatically creating more visual samples from the original small dataset. Add to that, this paper attempts to developing Random Forest data mining algorithm to handle missing value problem. Then database which describes depending on their components were built by Principle Component Analysis (PCA), after that, association rule algorithm to the FP-Growth algorithm (FP-Tree) was used. At the last, TreeNet classifier determines the class under which each association rules belongs to was used. The proposed methodology provides fast, Accurate and comprehensible classification rules. Also, this methodology can be use to compression dataset in two dimensions (number of features, number of records). © 2012 IEEE.","Adboosting; FP-Growth; GPDCM; PCA; Random Forest","Adboosting; FP growths; GPDCM; PCA; Random forests; Algorithms; Association rules; Bioinformatics; Classification (of information); Data mining; Decision trees; Forestry; Genetic programming; Medical education; Principal component analysis; World Wide Web; Trees (mathematics); Algorithms; Classification; Decision Making; Forestry; Information Retrieval; Mathematics; Random Processes; Trees"
"Ali S.I., Lee S.","Ensemble based Cost-Sensitive Feature Selection for Consolidated Knowledge Base Creation","10.1109/IMCOM48794.2020.9001751","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081140933&doi=10.1109%2fIMCOM48794.2020.9001751&partnerID=40&md5=20907b22905661a5bec721037d7ad3dd","This is paper proposes a knowledge construction system. The key objective of the system is to extract knowledge from structured data which is generally available in the form of electronic medical records (EMR). In this regard, the main focus of the research is to design and develop a domain-independent system that is capable of assisting the domain expert(s) in gaining non-trivial insights from the underlying EMR data. It is important to note that most of the research in the domain of cost-sensitive feature selection relies on black-box models which only provide a prediction of a final class label. Whereas, the goal of this research is to acquire insights for domain experts such as chronic kidney disease classification. This goal is achieved by designing and developing a knowledge construction system that is based on a two-stage methodology. Stage one deals with identifying salient cost-sensitive features in the EMR data, whereas, stage-two deals with consolidating knowledge (i.e. in the form of production rules) from a set of interpretable machine learning models. Finally, in order to demonstrate the efficacy of the system a chronic kidney disease case study is adopted. © 2020 IEEE.","Data driven system; Decision Tree Models; Ensemble Models; Feature Selection; Interpretable Machine Learning models","Decision trees; Information management; Knowledge based systems; Machine learning; Medical computing; Trees (mathematics); Chronic kidney disease; Data driven; Decision tree models; Domain independents; Electronic medical record; Ensemble models; Knowledge construction; Machine learning models; Feature extraction"
"Ali S.I., Lee S.","The comparative analysis of single-objective and multi-objective evolutionary feature selection methods","10.1007/978-3-030-19063-7_76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066825279&doi=10.1007%2f978-3-030-19063-7_76&partnerID=40&md5=3147b750488d4ddbe9e7f2161e9bb56d","This research presents a comparative analysis of single-objective and multi-objective evolutionary feature selection methods over interpretable models. The question taken in this research is to investigate the role of aforementioned techniques for feature selection on classification model’s interpretability as well as accuracy. Since, feature selection is a non-deterministic polynomial-time hardness (NP-hard) problem therefore exhaustively searching for all the possible feature sets is not computationally feasible. Evolutionary algorithms provide a very powerful searching mechanism that is utilized for candidate feature generation in a reasonable time frame. Single-objective (SO) algorithms are generally geared towards finding a subset of candidate feature set which achieves highest evaluation score e.g. classification accuracy. On the other hand, multi-objective (MO) methods are relatively more comprehensive than their counterparts. MO feature selection algorithms can simultaneously optimize two or more objectives such as classification accuracy of a final feature set along with the cardinality of the feature set. In this research, we have selected two representative feature selection algorithms from both the groups. Decision tree and a rule-based classifiers are used for the performance evaluation in terms of interpretability i.e. model size, and predictive accuracy. This research is undertaken to investigate application of SO and MO feature selection methods on small to medium sized classification datasets. The experimental results on 3 interpretable classification models indicate that the relative differences between the two set of models on small datasets may not be much pronounced, yet for the medium-sized datasets MO models provide a promising alternative. Although multi-objective techniques resulted in smaller feature subsets in general, but overall the difference between both the single-objective and the multi-objective feature subset selection techniques, investigated in this study, is not statistical significant. © Springer Nature Switzerland AG 2019.","Decision tree; Model complexity; Multi-objective feature selection; Rule-based classifier; Single-objective feature selection","Data mining; Decision trees; Evolutionary algorithms; Feature extraction; Information management; Polynomial approximation; Feature selection algorithm; Feature selection methods; Feature subset selection; Model complexity; Multi objective; Multi-objective evolutionary; Rule-based classifier; Single objective; Classification (of information)"
"Ali S.S., Kaur R., Persis D.J., Saha R., Pattusamy M., Sreedharan V.R.","Developing a hybrid evaluation approach for the low carbon performance on sustainable manufacturing environment","10.1007/s10479-020-03877-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097046339&doi=10.1007%2fs10479-020-03877-1&partnerID=40&md5=4bbcfc6606d7e8a6b84bbc9e0d1c5453","Societal emergence and sustainability are results of human actions and practices which can either imbalance it with maximum exploitation or retain it through responsible utilization of resources. Based on theories on institutional and resource-based views, the study explores the enablers of green sustainable practices of procurement, logistics, product and process design and regulatory frameworks for low carbon performance.The study employs a Hybrid approach of step-by-step empirical process to examine the impact of sustainable practices on low carbon performance which further affects the sustainable manufacturing and societies. A theoretical model developed based on hypothesis is tested first using modified Dillman’s approach. Then it is tested in in the PLS-SEM package using 380 data responses collected from the various manufacturers. Further robustness of proposed model is validated using different ML (machine learning) followed by post hoc analysis using Item Response Theory to validate the scale and efficacy of the measurement model. The study validates the positive relationships of sustainable practices on the low carbon performance which eventually is responsible for sustainable societies. The area of sustainable manufacturing is found relatively lacking and requires further attention of leadership for better societal establishments. The study hopes to further enrich the literature with its unique Hybrid approach of SEM/PLS Machine Learning and IRT which is used to presents carbon performance as a central entity deriving from green practices and driving sustainable manufacturing and societies. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Item response theory; Low carbon performance; Machine learning; Regulatory framework; Sustainable manufacturing; Sustainable society",
"Ali Y., Hussain F., Bliemer M.C.J., Zheng Z., Haque M.M.","Predicting and explaining lane-changing behaviour using machine learning: A comparative study","10.1016/j.trc.2022.103931","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140805272&doi=10.1016%2fj.trc.2022.103931&partnerID=40&md5=c63faceeaea4b90ecc1135e428a82ede","Predicting lane-changing behaviour is an integral part of lane-changing decision models and has a significant impact on both traffic flow characteristics and traffic safety. A variety of lane-changing decision models have been developed for this purpose, with most of them focussing only on explaining lane-changing behaviour, while assessing the predictive capability of these models has comparatively received less attention. Meanwhile, machine learning techniques are often preferred for prediction purposes, but their application to predicting lane-changing behaviour is limited. However, the lack of interpretability of machine learning techniques is often criticised and needs a solution. Motivated by these research needs, this study explains and predicts driver's mandatory and discretionary lane-changing behaviours using a set of suitable machine learning techniques. Input features are objectively selected using the technique of Recursive Feature Elimination, and standard classification metrics are employed to select the best model. By accounting for class imbalance, this study finds that the Extra Trees classifier outperforms other machine learning techniques as well as a conventional utility theory-based model in predicting lane-changing behaviour. Furthermore, by keeping the model hyperparameters unchanged, this classifier shows good transferability in predicting lane-changing behaviour when tested on a completely new dataset. Finally, through explainable artificial intelligence, the output of the Extra Trees classifier is interpreted. The findings of this study advocate the use of machine learning techniques in future studies for explaining and predicting lane-changing behaviour. © 2022 Elsevier Ltd","Discretionary lane-changing; Interpretability; Machine learning; Mandatory lane-changing; NGSIM; Prediction; Transferability","Classification (of information); Forestry; Learning algorithms; Machine learning; Discretionary lane-changing; Interpretability; Lane changing; Lane-changing behaviors; Machine learning techniques; Machine-learning; Mandatory lane-changing; NGSIM; Transferability; Forecasting"
"Ali Zaidi S.S., Fraz M.M., Shahzad M., Khan S.","A multiapproach generalized framework for automated solution suggestion of support tickets","10.1002/int.22701","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116096256&doi=10.1002%2fint.22701&partnerID=40&md5=72160ca594a3b78e88e746f051dab698","Nowadays, customer support systems are one of the key factors in maintaining any big company's reputation and success. These systems are capable of handling a large number of tickets systemically and provides a mechanism to track/logs the communication between customer and support agents. Companies invest huge amounts of money in training support agents and deploying customer care services for their products and services. Support agents are responsible for handling different customer queries and implementing required actions to solve a particular issue or problem raised by the service/product user. In a bigger picture, customer support systems could receive a large amount of ticket raised depending upon the number of users and services being offered. Customer care service gets directly affected due to the high volume of tickets and a limited number of support agents. Therefore, providing support agents with the recommendations about the possible resolution actions for a new ticket would be helpful and can save a lot of time. This study is focused on the development of an end-to-end framework for suggesting resolution actions rather than recommending free form resolution text against a newly raised ticket. To develop such a system, the pipeline is broadly divided into four components that are data preprocessing, actions extractor, resolution predictor, and evaluation. In actions extractor module, we have proposed a technique to identify and extract actionable phrases from resolution text. For resolution predictor, we have proposed two different pipelines that are referred as “Similarity Search Model” and “End-to-End Model.” The similarity search method is based on a ticket similarity search to find the most relevant historical tickets which then leads to corresponding resolution actions. On the other hand, end-to-end model make use of actions extractor module directly and implemented in a way to directly predict resolution actions. To compare and evaluate the mentioned methods on the same ground, we also proposed an actions evaluation criterion which uses BertScore and METEOR score jointly to compute the score against actual and predicted actions for a particular test ticket. The analysis and experiments are performed on the real-world IBM ticket data set. Overall, we observed that end-to-end model outperformed similarity search-based methods and achieved better performance and scores comparatively. The trained models and code are available at https://bit.ly/2GbUBVk. © 2021 Wiley Periodicals LLC.","action extraction; artificial intelligence; automated resolution; natural language processing; solution suggestion; ticket resolution recommendation","Pipelines; Sales; Action extraction; Automated resolution; Automated solutions; Customer care; Customer support; End-to-end models; Similarity search; Solution suggestion; Support systems; Ticket resolution recommendation; Natural language processing systems"
"Alibabaei K., Gaspar P.D., Assunção E., Alirezazadeh S., Lima T.M.","Irrigation optimization with a deep reinforcement learning model: Case study on a site in Portugal","10.1016/j.agwat.2022.107480","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123028578&doi=10.1016%2fj.agwat.2022.107480&partnerID=40&md5=ea75f42da58a65ebef2534d6b32f2095","In the field of agriculture, the water used for irrigation should be given special treatment, as it is responsible for a large proportion of total water consumption. Irrigation scheduling is critical to food production because it guarantees producers a consistent harvest and minimizes the risk of losses due to water shortages. Therefore, the creation of an automatic irrigation method using new technologies is essential. New methods such as deep learning algorithms have attracted a lot of attention in agriculture and are already being used successfully. In this work, a Deep Q-Network was trained for irrigation scheduling. The agent was trained to schedule irrigation for a tomato field in Portugal. Two Long Short Term Memory models were used as the agent environment. One predicts the total water in the soil profile on the next day. The other one was employed to estimate the yield based on the environmental condition during a season and then measure the net return. The agent uses this information to decide the following irrigation amount. An Artificial Neural Network, a Long Short Term Memory, and a Convolutional Neural Network were used to estimating the Q-table during training. Unlike the Long-Short Terms Memory model, the Artificial Neural Network and the Convolutional Neural Network could not estimate the Q-table, and the agent's reward decreased during training. The comparison of the performance of the model was done with fixed base irrigation and threshold based irrigation. The trained model increased productivity by 11% and decreased water consumption by 20–30% compared to the fixed method. © 2022 Elsevier B.V.","Agriculture; Deep reinforcement learning; Irrigation scheduling; LSTM","Brain; Convolution; Convolutional neural networks; Learning algorithms; Long short-term memory; Reinforcement learning; Scheduling; Soils; Case-studies; Convolutional neural network; Irrigation optimization; Irrigation scheduling; LSTM; Memory modeling; Portugal; Reinforcement learning models; Special treatments; Water consumption; Irrigation; artificial neural network; crop plant; food production; optimization; Portugal"
"Alicioglu G., Sun B.","A survey of visual analytics for Explainable Artificial Intelligence methods","10.1016/j.cag.2021.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115649136&doi=10.1016%2fj.cag.2021.09.002&partnerID=40&md5=df70c01c437764147a6a61bf1589d4ea","Deep learning (DL) models have achieved impressive performance in various domains such as medicine, finance, and autonomous vehicle systems with advances in computing power and technologies. However, due to the black-box structure of DL models, the decisions of these learning models often need to be explained to end-users. Explainable Artificial Intelligence (XAI) provides explanations of black-box models to reveal the behavior and underlying decision-making mechanisms of the models through tools, techniques, and algorithms. Visualization techniques help to present model and prediction explanations in a more understandable, explainable, and interpretable way. This survey paper aims to review current trends and challenges of visual analytics in interpreting DL models by adopting XAI methods and present future research directions in this area. We reviewed literature based on two different aspects, model usage and visual approaches. We addressed several research questions based on our findings and then discussed missing points, research gaps, and potential future research directions. This survey provides guidelines to develop a better interpretation of neural networks through XAI methods in the field of visual analytics. © 2021 Elsevier Ltd","Black-box models; Explainable Artificial Intelligence; Interpretable neural networks; Visual analytics","Decision making; Deep learning; Surveys; Artificial intelligence methods; Autonomous Vehicles; Black box modelling; Explainable artificial intelligence; Future research directions; Interpretable neural network; Learning models; Neural-networks; Performance; Visual analytics; Visualization"
"Alighanbari S., Azad N.L.","Safe Adaptive Deep Reinforcement Learning for Autonomous Driving in Urban Environments. Additional Filter? How and Where?","10.1109/ACCESS.2021.3119915","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117808810&doi=10.1109%2fACCESS.2021.3119915&partnerID=40&md5=53628872b620901dffd351a0865a0d55","Autonomous driving (AD) provides a reliable solution for safe driving by replacing human drivers responsible for the majority of accidents. The emergence of Machine Learning, specifically Deep Reinforcement Learning (DRL), and its ability to solve complex games proved its potential to address AD challenges. However, model-free methods still suffer from safety-related issues that can be resolved using safe-DRL approaches. The addition of model-based safety filters to the learning-based algorithms provides safety bounds on their performance and constraint satisfaction. In this paper, we investigate the addition of a safety filter based on Model Predictive Control and show an increase in mean testing episode reward by 110% from -75 mean episode reward during testing for 50 episodes for Deep Deterministic Policy Gradient ( $DDPG$ ) to 7.758. We study the impacts of safety filters (7.758 mean reward), heuristic rules, bounded additive noises (0.49% performance increase comparing to noise-free case), and exploration (3.425 mean reward) on the learning algorithm. We compare the effects of filters in the context of simulated exploration and bounded exploration and prove that bounded exploration results in 9.86% increase in mean reward and 12.95% decrease in std comparing to the other method. Additionally, inspired by Deep Internal Learning and biological mechanisms like brain plasticity, we investigate the idea of using each sample for training only once instead of utilizing stochastic batches which increases the mean testing accumulated reward by 1.87% and leads to the best performance (7.942 mean reward and 0.048 std). Finally, the results demonstrate better automotive results for our proposed method than DDPG. Our proposed method, DDPG with safety filter in bounded exploration and adaptive learning under noisy input conditions, has a success rate of 100% under different traffic densities for the simulation environment used in this paper and our assumptions. The proposed method's automotive results are shown for a braking scenario to avoid collision with other road users. © 2013 IEEE.","adaptive learning; autonomous vehicles; Deep reinforcement learning; safe exploration; safety filters","Additive noise; Autonomous vehicles; Bandpass filters; Learning algorithms; Model predictive control; Reinforcement learning; Stochastic systems; Adaptive learning; Autonomous driving; Autonomous Vehicles; Deterministics; Performance; Policy gradient; Safe exploration; Safety filter; Deep learning"
"Al-Imam A., Ali H., Saad A.","Face recognition abilities in Iraqi medical students: An inferential, cross sectional analysis","10.5812/ijpbs.9801","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046281534&doi=10.5812%2fijpbs.9801&partnerID=40&md5=306b93c65611e075049bcffb33205462","Background: The ability of humans to recognise faces of countless individuals is unique and has an evolutionary basis. The cortical surface, responsible for this task, is significantly large in humans. The aim of this study was to analyze the face recognition abilities of a selected population of Iraqi students and to determine the correlation of these abilities with gender, handedness, and ethnicity. Objectives: To identify potential super-recognizers in a population of Iraqi medical students. Methods: This cross sectional study started in October 2016. The participants included medical students (n, 309), aged 17 - 25 years, form 4 ethnic groups: Arabs (288), Kurds (12), Turks (7), and Christian ethnicities (2). The face recognition ability was quantitatively scored (0 - 14), using a face recognition test. The test was distributed electronically via bit-encrypted Intranet systems. Nonparametric and inferential statistics were measured to determine the correlation between the scores and gender, handedness, and ethnicity. Results: More than half of the participants (51.5%) were found to be potential super-recognizers. There was a significant difference between males and females (10.72 vs. 10.05; P = 0.027). However, there was no significant difference between right- and left-handed individuals (10.29 vs. 10.09; P = 0.394). On the other hand, there was a significant interethnic difference between Arabs and Kurds (10.19 vs. 11.5; P = 0.022). Conclusions: Face recognition abilities had not been investigated in Iraqi populations before the present study. This study indicated the correlation of face recognition abilities with gender and ethnicity. Individuals with high scores on face recognition tests were known as super-recognizers. These individuals can be valuable to law-enforcement and intelligence agencies worldwide. Nonetheless, practical applications of this study are not limited to artificial intelligence, biometrics, or anthropometrics. ©2018, Iranian Journal of Psychiatry and Behavioral Sciences.","Biometric identification; Cerebral dominance; Facial recognition; Law enforcement; Prosopagnosia","adolescent; adult; Arab; Article; Benton facial recognition test; Christian; controlled study; cross-sectional study; ethnic difference; ethnicity; facial recognition; female; gender; human; intranet; Iraqi; Kurd (people); left handedness; male; medical student; right handedness; Turk (people)"
"Aliman N.-M., Kester L.","Epistemic defenses against scientific and empirical adversarial AI attacks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112595045&partnerID=40&md5=dba21a744bbce64e462f41ec57dfa0d0","In this paper, we introduce “scientific and empirical adversarial AI attacks” (SEA AI attacks) as umbrella term for not yet prevalent but technically feasible deliberate malicious acts of specifically crafting AI-generated samples to achieve an epistemic distortion in (applied) science or engineering contexts. In view of possible socio-psycho-technological impacts, it seems responsible to ponder countermeasures from the onset on and not in hindsight. In this vein, we consider two illustrative use cases: the example of AI-produced data to mislead security engineering practices and the conceivable prospect of AI-generated contents to manipulate scientific writing processes. Firstly, we contextualize the epistemic challenges that such future SEA AI attacks could pose to society in the light of broader i.a. AI safety, AI ethics and cybersecurity-relevant efforts. Secondly, we set forth a corresponding supportive generic epistemic defense approach. Thirdly, we effect a threat modelling for the two use cases and propose tailor-made defenses based on the foregoing generic deliberations. Strikingly, our transdisciplinary analysis suggests that employing distinct explanation-anchored, trust-disentangled and adversarial strategies is one possible principled complementary epistemic defense against SEA AI attacks - albeit with caveats yielding incentives for future work. ∗Copyright © 2021 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",,"Network security; Safety engineering; Contextualize; Cyber security; Scientific writing; Security engineering; Artificial intelligence"
"Aliman N.-M., Kester L.","Malicious Design in AIVR, Falsehood and Cybersecurity-oriented Immersive Defenses","10.1109/AIVR50618.2020.00031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099829481&doi=10.1109%2fAIVR50618.2020.00031&partnerID=40&md5=2f3a9ae6ba45f705b316e39829068dcb","Advancements in the AI field unfold tremendous opportunities for society. Simultaneously, it becomes increasingly important to address emerging ramifications. Thereby, the focus is often set on ethical and safe design forestalling unintentional failures. However, cybersecurity-oriented approaches to AI safety additionally consider instantiations of intentional malice - including unethical malevolent AI design. Recently, an analogous emphasis on malicious actors has been expressed regarding security and safety for virtual reality (VR). In this vein, while the intersection of AI and VR (AIVR) offers a wide array of beneficial cross-fertilization possibilities, it is responsible to anticipate future malicious AIVR design from the onset on given the potential socio-psycho-technological impacts. For a simplified illustration, this paper analyzes the conceivable use case of Generative AI (here deepfake techniques) utilized for disinformation in immersive journalism. In our view, defenses against such future AIVR safety risks related to falsehood in immersive settings should be transdisciplinarily conceived from an immersive co-creation stance. As a first step, we motivate a cybersecurity-oriented procedure to generate defenses via immersive design fictions. Overall, there may be no panacea but updatable transdisciplinary tools including AIVR itself could be used to incrementally defend against malicious actors in AIVR. © 2020 IEEE.","AI; AI Safety; Cybersecurity; Design Fiction; Disinformation; HCI; Immersive Journalism; Psychology; VR","Security of data; Virtual reality; Co-creation; Cross fertilization; Cyber security; Immersive; Immersive designs; Safe designs; Safety risks; Artificial intelligence"
"Al-Imran M., Rahaman K.J., Rasel M., Ripon S.H.","An Analytical Evaluation of a Deep Learning Model to Detect Network Intrusion","10.1007/978-3-030-80253-0_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112019638&doi=10.1007%2f978-3-030-80253-0_12&partnerID=40&md5=62fc148fb6de7167bc22b7d1e2632c73","Widespread use of internet connected devices results in a flow of data among the connected devices. This has led to creating these network systems prone to different type of cyber attacks among which network intrusion is one of a kind. To detect network intrusion, an intrusion detection system (IDS) is placed in the system. The previous works in this field classify the attack mainly with traditional machine learning algorithm based models. This experiment proposes Long Short Term Memory (LSTM) based neural network model along with two other machine learning models i.e. Support Vector Machine (SVM), K-Nearest Neighbor (KNN), which shows significant improved performance in intrusion detection. The performance of the model is proved to be effective comparing to the previous works and also deployable in real life environments which is validated by using explainable AI. For this experiment, Kyoto University Honeypot log dataset has been used to build and evaluate the performances of the IDS model. © 2021, Springer Nature Switzerland AG.","Class imbalance; Intrusion detection; LIME; LSTM; SVM","Computer crime; Deep learning; Intrusion detection; Learning algorithms; Learning systems; Nearest neighbor search; Network security; Support vector machines; Analytical evaluation; Intrusion Detection Systems; K nearest neighbor (KNN); Kyoto University; Learning models; Machine learning models; Network intrusions; Neural network model; Long short-term memory"
"Alipour K., Ray A., Lin X., Schulze J.P., Yao Y., Burachas G.T.","The Impact of Explanations on AI Competency Prediction in VQA","10.1109/HCCAI49649.2020.00010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096609339&doi=10.1109%2fHCCAI49649.2020.00010&partnerID=40&md5=769be3a398706a1bf88b5d5c8ce93942","Explainability is one of the key elements for building trust in AI systems. Among numerous attempts to make AI explainable, quantifying the effect of explanations remains a challenge in conducting human-AI collaborative tasks. Aside from the ability to predict the overall behavior of AI, in many applications, users need to understand an AI agent's competency in different aspects of the task domain. In this paper, we evaluate the impact of explanations on the user's mental model of AI agent competency within the task of visual question answering (VQA). We quantify users' understanding of competency, based on the correlation between the actual system performance and user rankings. We introduce an explainable VQA system that uses spatial and object features and is powered by the BERT language model. Each group of users sees only one kind of explanation to rank the competencies of the VQA model. The proposed model is evaluated through between-subject experiments to probe explanations' impact on the user's perception of competency. The comparison between two VQA models shows BERT based explanations and the use of object features improve the user's prediction of the model's competencies. © 2020 IEEE.","Explainable AI; Interpretability; Visual Question Answering","Forecasting; Predictive analytics; Actual system; Collaborative tasks; Language model; Mental model; Question Answering; Subject experiment; User rankings; User's perceptions; Artificial intelligence"
"Alipour K., Schulze J.P., Yao Y., Ziskind A., Burachas G.","A study on multimodal and interactive explanations for visual question answering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081563716&partnerID=40&md5=a82e66360b861482db89bb890f2b3c9a","Explainability and interpretability of AI models is an essential factor affecting the safety of AI. While various explainable AI (XAI) approaches aim at mitigating the lack of transparency in deep networks, the evidence of the effectiveness of these approaches in improving usability, trust, and understanding of AI systems are still missing. We evaluate multimodal explanations in the setting of a Visual Question Answering (VQA) task, by asking users to predict the response accuracy of a VQA agent with and without explanations. We use between-subjects and within-subjects experiments to probe explanation effectiveness in terms of improving user prediction accuracy, confidence, and reliance, among other factors. The results indicate that the explanations help improve human prediction accuracy, especially in trials when the VQA system's answer is inaccurate. Furthermore, we introduce active attention, a novel method for evaluating causal attentional effects through intervention by editing attention maps. User explanation ratings are strongly correlated with human prediction accuracy and suggest the efficacy of these explanations in human-machine AI collaboration tasks. © 2020 for this paper by its authors.",,"Artificial intelligence; Forecasting; AI systems; Collaboration task; Human-machine; Interpretability; Multi-modal; Prediction accuracy; Question Answering; Still missing; Safety factor"
"Alipour M., Harris D.K., Barnes L.E., Ozbulut O.E., Carroll J.","Load-Capacity Rating of Bridge Populations through Machine Learning: Application of Decision Trees and Random Forests","10.1061/(ASCE)BE.1943-5592.0001103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027266886&doi=10.1061%2f%28ASCE%29BE.1943-5592.0001103&partnerID=40&md5=e591c0e9641e9b252d05d26258b1005c","The functionality of the U.S. transportation infrastructure system is dependent upon the health of an aging network of over 600,000 bridges, and agencies responsible for maintaining these bridges rely on the process of load rating to assess the adequacy of individual structures. This paper presents a new approach for safety screening and load-capacity evaluation of large bridge populations that seeks to uncover heretofore unseen patterns within the National Bridge Inventory database and establish relationships between select bridge attributes and their load-capacity status. Decision-tree and random-forest classification models were trained on the national concrete slab bridge data set of over 40,000 structures. The resulting models were validated on an independent data set and then compared with a number of existing judgment-based schemes found in an extensive survey of the current state of practice in the United States. The proposed approach offers a method that provides guidance for improved allocation of resources by informing maintenance decisions through rapid identification of candidate bridges that require further scrutiny for either possible load restriction or restriction removal. © 2017 American Society of Civil Engineers.","Data-driven; Decision trees; Load posting; Load rating; National Bridge Inventory (NBI); Random forests","Air navigation; Classification (of information); Concrete slabs; Learning systems; Rating; Data driven; Load capacity evaluation; Load ratings; National Bridge Inventory (NBI); National Bridge Inventory database; Random forest classification; Random forests; Transportation infrastructures; Decision trees"
"Alison Paprica P., Sutherland E., Smith A., Brudno M., Cartagena R.G., Crichlow M., Courtney B.K., Loken C., McGrail K.M., Ryan A., Schull M.J., Thorogood A., Virtanen C., Yang K.","Essential requirements for establishing and operating data trusts: Practical guidance co-developed by representatives from fifteen canadian organizations and initiatives","10.23889/IJPDS.V5I1.1353","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091652927&doi=10.23889%2fIJPDS.V5I1.1353&partnerID=40&md5=27dca1b263f5d377342a0c585ba4e051","Introduction Increasingly, the label “data trust” is being applied to repeatable mechanisms or approaches to sharing data in a timely, fair, safe, and equitable way. However, there is an absence of practical guidance regarding how to establish and operate a data trust. Aim and approach In December 2019, the Canadian Institute for Health Information and the Vector Institute for Artificial Intelligence convened a working meeting of 19 people representing 15 Canadian organizations/initiatives involved in data sharing, most of which focus on public sector health data. The objective was to identify essential requirements for the establishment and operation of data trusts in the Canadian context. Preliminary requirements were discussed during the meeting and then refined as authors contributed to this manuscript. Results Twelve minimum specification requirements (“min specs”) for data trusts were identified. The foundational min spec is that data trusts must meet all legal requirements, including legal authority to collect, hold or share data. In addition, there was agreement that data trusts must have (i) an accountable governing body to ensure that the data trust achieves its stated purpose and is transparent, (ii) comprehensive data management including clear processes and qualified individuals responsible for the collection, storage, access, disclosure and use of data, (iii) training and accountability requirements for all data users and (iv) ongoing public and stakeholder engagement. Conclusions Practical guidance for the establishment and operation of data trusts was articulated in the form of 12 min specs requirements. The 12 min specs are a starting point. Future work to refine and strengthen them with members of the public, companies, and additional research data stakeholders from within and outside of Canada, is recommended. August 2020 © The Authors.","Data governance; Data infrastructure; Data protection; Data trust; Public engagement","adult; article; Canada; data protection; human; organization; stakeholder engagement; trust"
"Aliyev E., Gaziyev Z.","Weighted Assessment of the Microcredit Borrower Solvency Using a Fuzzy Analysis of Personal Data","10.1007/978-3-030-64058-3_66","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124432641&doi=10.1007%2f978-3-030-64058-3_66&partnerID=40&md5=f28eb1e86d87cf104322ef37eb5cfdeb","In practice, the opinions of different analysts or those responsible for making credit decisions often differ, especially if controversial situations are considered that have many acceptable alternative solutions. As a result, in assessing the solvency of potential microloan borrowers, the subjective opinion of the expert and the incompetent or deliberate interpretation of the information resulting in the adoption of decisions that are detrimental to the microfinance organization are overweight. To increase the degree of objectivity, the paper discusses an approach to assessing the responsibility and solvency of microloan borrowers, based on the use of the fuzzy maxmin convolution method of multi-criteria choice of alternatives under uncertainty. Taking into account the weakly structured personal data of applicants this approach allows them to be flexibly and promptly assessed for microcredits. The applied qualitative criteria of assessment are weighed on the base of agreed expert opinions relative to priority of each of them. An important advantage of the applied model is that it is simple, convenient to use and able to adapt to the requirements of various commercial banks and micro-financial organizations. The model has been tested on the example of ten hypothetical microloan borrowers. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Borrower solvency; Credit scoring; Fuzzy set; Maxmin convolution; Membership function; Microcredit","Artificial intelligence; Computation theory; Convolution; Fuzzy systems; Soft computing; Alternative solutions; Commercial bank; Convolution methods; Expert opinion; Financial organizations; Fuzzy analysis; Multi-criteria; Qualitative criteria; Uncertainty analysis"
"Aliyev S., Almisreb A.A., Turaev S.","Azerbaijani sign language recognition using machine learning approach","10.1088/1742-6596/2251/1/012007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131209555&doi=10.1088%2f1742-6596%2f2251%2f1%2f012007&partnerID=40&md5=9270338a450cb63968f07e28dd2f7dec","Sign Language recognition is one of the essential and focal areas for researchers in terms of improving the integration of speech and hearing-impaired people into common society. The main idea is to detect the hand gestures of impaired people and convert them to understandable formats, such as text by leveraging advanced approaches. In this paper, we present our contribution to the improvement of Azerbaijani Sign Language (AzSL). We worked on AzSL Alphabet static signs real-time recognition. The method applied in this work is Object Classification and Recognition by leveraging pre-trained lightweight Convolutional Neural Networks models. At first, a dataset containing near to 1000 images has been collected, then interesting objects on images have been labeled with bounding boxing option. To build, train, evaluate and deploy the relevant model, TensorFlow Object Detection API with Python has been employed. MobileNet v2 pre-trained model has been leveraged for this task. In the trial experiment with four sign classes (A, B, C, E) and 5000 step numbers 15.2% training loss and 83% evaluation mean average precision results have been obtained. In the next step of model deployment experiments with all 24 static signs of AzSL, 49700 and 27700 steps (180 and 100 epochs, respectively) 6.4% and 18.2% training losses, 66.5% and 71.6% mAP outcomes gained, respectively. © Published under licence by IOP Publishing Ltd.","Azerbaijan Sign Language; Convolutional Neural Networks; MobileNet; TensorFlow","Convolution; Machine learning; Neural networks; Object detection; Python; Speech recognition; Azerbaijan; Azerbaijan sign language; Convolutional neural network; Impaired people; Machine learning approaches; Mobilenet; Sign language; Sign Language recognition; Static signs; Tensorflow; Audition"
"Alizadeh F., Esau M., Stevens G., Cassens L.","eXplainable AI: Take one Step Back, Move two Steps forward: Investigating Folk Theories and Users' Perception of Artificial Intelligence","10.18420/muc2020-ws111-369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133922018&doi=10.18420%2fmuc2020-ws111-369&partnerID=40&md5=07efc04f2df589f5349e35867e468a5f","In 1991 the researchers at the center for the Learning Sciences of Carnegie Mellon University were confronted with the confusing question of “where is AI” from the users, who were interacting with AI but did not realize it. Three decades of research and we are still facing the same issue with the AI-technology users. In the lack of users' awareness and mutual understanding of AI-enabled systems between designers and users, informal theories of the users about how a system works (“Folk theories”) become inevitable but can lead to misconceptions and ineffective interactions. To shape appropriate mental models of AI-based systems, explainable AI has been suggested by AI practitioners. However, a profound understanding of the current users' perception of AI is still missing. In this study, we introduce the term “Perceived AI” as “AI defined from the perspective of its users”. We then present our preliminary results from deep-interviews with 50 AI-technology users, which provide a framework for our future research approach towards a better understanding of PAI and users' folk theories. © Proceedings of the Mensch und Computer 2020 Workshop on «Workshop on User-Centered Artificial Intelligence (UCAI 2020)». Copyright held by the owner/author(s)","ArtiSicial intelligence; Folk theories; Mental models; Misconception; Perceived AI","Cognitive systems; AI Technologies; Artisicial intelligence; Carnegie Mellon University; Folk theory; Learning science; Mental model; Misconception; Mutual understanding; Perceived AI; User perceptions; Artificial intelligence"
"Alizadeh M., Shapiro J.N., Buntain C., Tucker J.A.","Content-based features predict social media influence operations","10.1126/sciadv.abb5824","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090076325&doi=10.1126%2fsciadv.abb5824&partnerID=40&md5=fac18675e3430b7047f8d87b711d2849","We study how easy it is to distinguish influence operations from organic social media activity by assessing the performance of a platform-agnostic machine learning approach. Our method uses public activity to detect content that is part of coordinated influence operations based on human-interpretable features derived solely from content. We test this method on publicly available Twitter data on Chinese, Russian, and Venezuelan troll activity targeting the United States, as well as the Reddit dataset of Russian influence efforts. To assess how well content-based features distinguish these influence operations from random samples of general and political American users, we train and test classifiers on a monthly basis for each campaign across five prediction tasks. Content-based features perform well across period, country, platform, and prediction task. Industrialized production of influence campaign content leaves a distinctive signal in user-generated content that allows tracking of campaigns from month to month and across different accounts. © 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).",,"Forecasting; Statistical tests; Content-based features; Industrialized production; Machine learning approaches; Prediction tasks; Random sample; Social media; User-generated content; Social networking (online); Agnostic; article; classifier; human; plant leaf; prediction; random sample; social media; United States"
"Alizadeh R., Mohebbi Najm Abad J., Fattahi A., Alhajri E., Karimi N.","Application of machine learning to investigation of heat and mass transfer over a cylinder surrounded by porous media - The radial basic function network","10.1115/1.4047402","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090172352&doi=10.1115%2f1.4047402&partnerID=40&md5=9ce1a1f7ddce0d67058fcaccbe0683ba","This paper investigates heat and mass transport around a cylinder featuring non-isothermal homogenous and heterogeneous chemical reactions in a surrounding porous medium. The system is subject to an impinging flow, while local thermal non-equilibrium, non-linear thermal radiation within the porous region, and the temperature dependency of the reaction rates are considered. Further, non-equilibrium thermodynamics, including Soret and Dufour effects are taken into account. The governing equations are numerically solved using a finite-difference method after reducing them to a system of non-linear ordinary differential equations. Since the current problem contains a large number of parameters with complex interconnections, low-cost models such as those based on artificial intelligence are desirable for the conduction of extensive parametric studies. Therefore, the simulations are used to train an artificial neural network. Comparing various algorithms of the artificial neural network, the radial basic function network is selected. The results show that variations in radiative heat transfer as well as those in Soret and Dufour effects can significantly change the heat and mass transfer responses. Within the investigated parametric range, it is found that the diffusion mechanism is dominantly responsible for heat and mass transfer. Importantly, it is noted that the developed predictor algorithm offers a considerable saving of the computational burden. © 2020 American Society of Mechanical Engineers (ASME). All rights reserved.","artificial neural network; forced convection; heat energy generation/storage/transfer; homogenous and heterogeneous reactions; machine learning; non-linear thermal radiation; porous media","Cylinders (shapes); Finite difference method; Heat transfer; Machine learning; Ordinary differential equations; Porous materials; Radial basis function networks; Reaction rates; Heat and mass transports; Heterogeneous chemical reaction; Local thermal non-equilibrium; Non equilibrium thermodynamics; Nonlinear ordinary differential equation; Radial basic function network; Soret and Dufour effects; Temperature dependencies; Mass transfer"
"Aljaddouh B., Malathi D.","Trends of using machine learning for detection and classification of respiratory diseases: Investigation and analysis","10.1016/j.matpr.2022.03.120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127549915&doi=10.1016%2fj.matpr.2022.03.120&partnerID=40&md5=1a001369d880faa67770d8c76283c342","The risk of respiratory diseases is significantly increased due to its easy spread, which makes many researchers in various fields responsible for dealing with the spread of these diseases. The most prominent of these diseases, and a recent challenge, is Covid-19. In this research, a comprehensive review of research interested in the detection and classification of respiratory diseases using machine learning was conducted. The research includes the fifty most important papers published between 2018 and the end of 2021. The results of the research provided an analysis of research trends in this field in terms of the techniques and data used. These results enable researchers in the future to choose the best way to plan their research and make a good contribution before entering this field of research. © 2022","Covid-19; Deep learning; Lung disease detection; Machine learning; Respiratory diseases","Pulmonary diseases; Covid-19; Deep learning; Disease detection; Investigation and analysis; Lung disease detection; Machine-learning; Research trends; Deep learning"
"Al-Jallad N.T., Ning X., Khairalla M.A., Al-Qaness M.A.A.","Rule mining models for predicting dropout/ stopout and switcher at college using satisfaction and SES features","10.1504/IJMIE.2019.098182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062782461&doi=10.1504%2fIJMIE.2019.098182&partnerID=40&md5=337f98c274a7fbdb030e6e719df4f953","Predicting students' dropout/stop-out and switch registration aspects at college is one of the important managerial issues that concern the academic institutions. This issue presents a specific challenge due to a large number of factors that can affect the student's decision and the imbalanced nature of the educational data. In this paper, a novel feature extraction method is applied to student satisfaction and socio-economic features during the pre-processing stage to reduce the high dimensionality of the data. Thus, different interpretable data mining approaches, including decision trees and rule induction methods, were examined using actual data of students at the Technical University of Palestine. After resolving imbalanced problem of the students' data, the results showed that the student satisfaction and socio-economic status predictors are important to distinguish different registration aspects. Moreover, the results revealed that J4.8 algorithm achieved best results due to the ability to apply an appropriate trade-off regarding accuracy versus interpretability. © 2019 Inderscience Enterprises Ltd.","decision tree; dropout; educational data mining; registration aspects; rule mining; satisfaction; SES; socio-economic status; switch major",
"Aljameel S.S.","A Proactive Explainable Artificial Neural Network Model for the Early Diagnosis of Thyroid Cancer","10.3390/computation10100183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140652714&doi=10.3390%2fcomputation10100183&partnerID=40&md5=3188e8bcde1a1a3d33657dd6d40dfbf0","Early diagnosis of thyroid cancer can reduce mortality, and can decrease the risk of recurrence, side effects, or the need for lengthy surgery. In this study, an explainable artificial neural network (EANN) model was developed to distinguish between malignant and benign nodules and to understand the factors that are predictive of malignancy. The study was conducted using the records of 724 patients who were admitted to Shengjing Hospital of China Medical University. The dataset contained the patients’ demographic information, nodule characteristics, blood test findings, and thyroid characteristics. The performance of the model was evaluated using the metrics of accuracy, sensitivity, specificity, F1 score, and area under the curve (AUC). The SMOTEENN combined sampling method was used to correct for a significant imbalance between malignant and benign nodules in the dataset. The proposed model outperformed a baseline study, with an accuracy of 0.99 and an AUC of 0.99. The proposed EANN model can assist health care professionals by enabling them to make effective early cancer diagnoses. © 2022 by the author.","artificial neural network; explainable artificial intelligence; thyroid cancer",
"Al-Janabi S.","Overcoming the Main Challenges of Knowledge Discovery through Tendency to the Intelligent Data Analysis","10.1109/ICDABI53623.2021.9655916","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124662443&doi=10.1109%2fICDABI53623.2021.9655916&partnerID=40&md5=e72504e8f045857cdd12ff3ece840dc3","Intelligent Data Analysis (IDA) approach proves its ability to deal with small and huge data sets; this ability can be done by generating and developing new methodologies. While; Knowledge Discovery in Database (KDD) aiming to automatic interpretation of large data sets. As a result, we can conclude a strong relationship between two concepts. This study discusses five challenges related to data analytical to making right decision. These challenges include: Missing values; Data scarcity, Data dimensionality reduction, Black box; and Mathematical model. First: One of the important trends in KDD will be the growing importance of data processing. But this point faces problems similar to those of data mining (High dimensional data, missing values imputation and data integration). As explained before, one of the open still problems in estimation missing values methods are how to select the optimal number of nearest neighbors of those values. Second: Data scarcity (insufficient size of dataset) problem is one of the challenges in KDD. Insufficient size of data is very often responsible for the poor performances in learning. Third: The goal of dimension reduction methods is using the correlation structure among the predicator variables to reduction the three main dimensions (features, samples and value of features). But, how we can combination among these three dimensions without lose any important inform is still as one of the challenges in KDD system. Fourth: As we know data mining algorithm are 'black box character' for many reasons explained in chapter two. But how we can convert it to system will behaviors and conclusion that can be explained and analyzed in a comprehensible manner (i.e., white box) is remained one of the main challenges of KDD system. © 2021 IEEE.","Black box; Data scarcity; Intelligent Data Analysis; Knowledge Discovery in Database; Mathematical model; Missing values; Reduction Three Dimension","Clustering algorithms; Data integration; Data reduction; % reductions; Analysis approach; Black boxes; Data scarcity; Data set; Intelligent data analysis; Knowledge discoveries in database; Missing values; Reduction three dimension; Three dimensions; Data mining"
"AlJarullah A.A.","Decision tree discovery for the diagnosis of type II diabetes","10.1109/INNOVATIONS.2011.5893838","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959965420&doi=10.1109%2fINNOVATIONS.2011.5893838&partnerID=40&md5=443fbfa11bd4b472be3caee139b5aa6e","The discovery of knowledge from medical databases is important in order to make effective medical diagnosis. The aim of data mining is to extract knowledge from information stored in database and generate clear and understandable description of patterns. In this study, decision tree method was used to predict patients with developing diabetes. The dataset used is the Pima Indians Diabetes Data Set, which collects the information of patients with and without developing diabetes. The study goes through two phases. The first phase is data preprocessing including attribute identification and selection, handling missing values, and numerical discretization. The second phase is a diabetes prediction model construction using the decision tree method. Weka software was used throughout all the phases of this study. © 2011 IEEE.","data mining; Decision Tree; diabetes; diagnosis","Data preprocessing; Data sets; Decision tree method; Handling missing values; Medical database; Numerical discretization; Prediction model; Second phase; Tree discovery; Type II; Decision trees; Diagnosis; Information technology; Innovation; Mathematical models; Medical computing; Medical education; Plant extracts; Data mining"
"Alkhalili M., Qutqut M.H., Almasalha F.","Investigation of Applying Machine Learning for Watch-List Filtering in Anti-Money Laundering","10.1109/ACCESS.2021.3052313","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099724162&doi=10.1109%2fACCESS.2021.3052313&partnerID=40&md5=b354da42de42aec75eee803d2f3761e3","Financial institutions must meet international regulations to ensure not to provide services to criminals and terrorists. They also need to continuously monitor financial transactions to detect suspicious activities. Businesses have many operations that monitor and validate their customer's information against sources that either confirm their identities or disprove. Failing to detect unclean transaction(s) will result in harmful consequences on the financial institution responsible for that such as warnings or fines depending on the transaction severity level. The financial institutions use Anti-money laundering (AML) software sanctions screening and Watch-list filtering to monitor every transaction within the financial network to verify that none of the transactions can be used to do business with forbidden people. Lately, the financial industry and academia have agreed that machine learning (ML) may have a significant impact on monitoring money transaction tools to fight money laundering. Several research work and implementations have been done on Know Your Customer (KYC) systems, but there is no work on the watch-list filtering systems because of the compliance risk. Thus, we propose an innovative model to automate the process of checking blocked transactions in the watch-list filtering systems. To the best of our knowledge, this paper is the first research work on automating the watch-list filtering systems. We develop a Machine Learning-Component (ML-Component) that will be integrated with the current watch-list filtering systems. Our proposed ML-Component consists of three phases; monitoring, advising, and take action. Our model will handle a known critical issue, which is the false-positives (i.e., transactions that are blocked by a false alarm). Also, it will minimize the compliance officers' effort, and provide faster processing time. We performed several experiments using different ML algorithms (SVM, DT, and NB) and found that the SVM outperforms other algorithms. Because our dataset is nonlinear, we used the polynomial kernel and achieved higher accuracy for predicting the transactionś decision, and the correlation matrix to show the relationship between the numeric features. © 2013 IEEE.","Anti-money laundering; financial transactions monitoring; machine learning (ML); sanctions screening; watch-list filtering","Finance; Laundering; Support vector machines; Turing machines; Watches; Anti-money laundering; Compliance officers; Correlation matrix; Financial institution; Financial transactions; International regulations; Money transactions; Polynomial kernels; Learning systems"
"Alkhammash H., Polyvyanyy A., Moffat A., García-Bañuelos L.","Entropic relevance: A mechanism for measuring stochastic process models discovered from event data","10.1016/j.is.2021.101922","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121134341&doi=10.1016%2fj.is.2021.101922&partnerID=40&md5=8998952bf510f27976d29219e73ce9a5","There are many fields of computing in which having access to large volumes of data allows very precise models to be developed. For example, machine learning employs a range of algorithms that deliver important insights based on analysis of data resources. Similarly, process mining develops algorithms that use event data induced by real-world processes to support the modeling of – and hence understanding and long-term improvement of – those processes. In process mining, the quality of the learned process models is assessed using conformance checking techniques, which measure how well the models represent and generalize the data. This article presents the entropic relevance measure for conformance checking of stochastic process models, which are models that also provide information in regard to the likelihood of observing each sequence of observed events. Accurate stochastic conformance measurement allows identification of models that describe the data better, including the captured sequences of process events and their frequencies, with information about the likelihood of the described processes being an essential step toward simulating and forecasting future processes. Entropic relevance represents a blend between the traditional precision and recall quality criteria in conformance checking, in that it both penalizes observed processes that the model does not describe, and also penalizes processes that are permitted by the model yet were not observed. Entropic relevance can be computed in time linear in the size of the input data; and measures a fundamentally different phenomenon than other existing measures. Our evaluation over industrial datasets confirms the feasibility of using the measure in practice. © 2021 Elsevier Ltd","Conformance checking; Explainable AI; Model inference; Model quality; Process learning; Process mining; Stochastic conformance checking","Data mining; Machine learning; Random processes; Stochastic models; Conformance checking; Explainable AI; Model inference; Modeling quality; Process learning; Process mining; Stochastic conformance checking; Stochastic process model; Stochastics; Stochastic systems"
"Al-Khatib K., Hou Y., Wachsmuth H., Jochim C., Bonin F., Stein B.","End-to-end argumentation knowledge graph construction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104696065&partnerID=40&md5=85845123d747ab07f22da568382c5f65","This paper studies the end-to-end construction of an argumentation knowledge graph that is intended to support argument synthesis, argumentative question answering, or fake news detection, among others. The study is motivated by the proven effectiveness of knowledge graphs for interpretable and controllable text generation and exploratory search. Original in our work is that we propose a model of the knowledge encapsulated in arguments. Based on this model, we build a new corpus that comprises about 16k manual annotations of 4740 claims with instances of the model’s elements, and we develop an end-to-end framework that automatically identifies all modeled types of instances. The results of experiments show the potential of the framework for building a web-based argumentation graph that is of high quality and large scale. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Effectiveness of knowledge; End to end; Exploratory search; High quality; Knowledge graphs; Manual annotation; Question Answering; Text generations; Knowledge representation"
"Allab K., Labiod L., Nadif M.","Simultaneous semi-NMF and PCA for clustering","10.1109/ICDM.2015.66","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963565222&doi=10.1109%2fICDM.2015.66&partnerID=40&md5=c2897de1964de0cb4376680cb00b2a5c","Cluster analysis is often carried out in combination with dimension reduction. The Semi-Non-negative Matrix Factorization (Semi-NMF) that learns a low-dimensional representation of a data set lends itself to a clustering interpretation. In this work we propose a novel approach to finding an optimal subspace of multi-dimensional variables for identifying a partition of the set of objects. The use of a low-dimensional representation can be of help in providing simpler and more interpretable solutions. We show that by doing so, our model is able to learn low-dimensional representations that are better suited for clustering, outperforming not only Semi-NMF, but also other NMF variants. © 2015 IEEE.","Clustering; Dimension Reduction; Semi-NMF","Cluster analysis; Data mining; Factorization; Matrix algebra; Clustering; Data set; Dimension reduction; Low-dimensional representation; Multi dimensional; Nonnegative matrix factorization; Optimal subspace; Semi-NMF; Reduction"
"Allab K., Labiod L., Nadif M.","Power simultaneous spectral data embedding and clustering","10.1137/1.9781611974348.31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991735503&doi=10.1137%2f1.9781611974348.31&partnerID=40&md5=4bf49cbf617c3b41b168ba92a3059c26","Spectral clustering methods use the Laplacian eigenvalues and eigenvectors to obtain a low-dimensional embedding that can be trivially clustered. For that purpose, spectral clustering is often based on a tandem approach where the two steps: affinity matrix eigendecomposition and k-means clustering, are performed separately. The potential flaw of such common practice is that the obtained relaxed continuous spectral solution can severely deviate from the true discrete clustering solution. Given the high computational cost of such spectral clustering methods, this paper provides the so-called PSDEC framework. PSDEC performs simultaneously the eigendecomposition of the affinity matrix and clustering tasks, and uses the Power method to speed up the unified process convergence. In PSDEC, the selected top eigenvectors of the Laplacian matrix can be of help in detecting a cluster structure of objects and providing simpler and more interprétable solutions. We show that by doing so, our method can learn low-dimensional representations that are better suited to clustering, outperforming not only spectral clustering algorithms but also some NMF variants. Copyright © 2016 Kais Allab, Lazhar Labiod, Mohamed Nadif.",,"Cluster analysis; Data mining; Eigenvalues and eigenfunctions; Embeddings; Laplace transforms; Matrix algebra; Clustering solutions; Eigen decomposition; Laplacian eigenvalues; Low dimensional embedding; Low-dimensional representation; Spectral clustering; Spectral clustering algorithms; Spectral clustering methods; K-means clustering"
"Allah Bukhsh Z., Saeed A., Stipanovic I., Doree A.G.","Predictive maintenance using tree-based classification techniques: A case of railway switches","10.1016/j.trc.2019.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061435024&doi=10.1016%2fj.trc.2019.02.001&partnerID=40&md5=0d8224540ef1a56b3cc1bae4c099a575","With growing service demands, rapid deterioration due to extensive usage, and limited maintenance due to budget cuts, the railway infrastructure is in a critical state and require continuous maintenance. The infrastructure managers have to come up with smart maintenance decisions in order to improve the assets’ condition, spend optimal cost and keep the network available. Currently, the infrastructure managers lack the tools and decision support models that could assist them in taking (un) planned maintenance decisions effectively and efficiently. Recently, many literature studies have proposed to employ the machine learning techniques to estimate the performance state of an asset, predict the maintenance need, possible failure modes, and such similar aspects in advance. Most of these studies have utilised additional data collection measures to record the assets’ behaviour. Though useful for experimentation, it is expensive and impractical to mount monitoring devices on multiple assets across the network. Therefore, the objective of this study is to develop predictive models that utilise existing data from a railway agency and yield interpretable results. We propose to leverage the tree-based classification techniques of machine learning in order to predict maintenance need, activity type and trigger's status of railway switches. Using the data from an in-use business process, predictive models based on the decision tree, random forest, and gradient boosted trees are developed. Moreover, to facilitate in models interpretability, we provided a detail explanation of models’ predictions by features importance analysis and instance level details. Our solution approach of predictive models development and their results explanation have wider applicability and can be used for other asset types and different (maintenance) planning scenarios. © 2019 Elsevier Ltd","Classification; Data-driven; Decision support; LIME; Machine learning; Predictive maintenance; Railway infrastructure; Switches and crossings","Budget control; Classification (of information); Critical current density (superconductivity); Decision support systems; Decision trees; Deterioration; Forecasting; Learning systems; Lime; Machine learning; Managers; Predictive analytics; Railroad transportation; Railroads; Classification technique; Data driven; Decision support models; Decision supports; Infrastructure managers; Machine learning techniques; Railway infrastructure; Switches and crossings; Predictive maintenance; decision support system; machine learning; maintenance; railway transport; transportation infrastructure"
"Allahham M.S., Al-Sa'd M.F., Al-Ali A., Mohamed A., Khattab T., Erbad A.","DroneRF dataset: A dataset of drones for RF-based detection, classification and identification","10.1016/j.dib.2019.104313","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071552598&doi=10.1016%2fj.dib.2019.104313&partnerID=40&md5=83f1c10ebfa9bea05862eb7dcf4334b7","Modern technology has pushed us into the information age, making it easier to generate and record vast quantities of new data. Datasets can help in analyzing the situation to give a better understanding, and more importantly, decision making. Consequently, datasets, and uses to which they can be put, have become increasingly valuable commodities. This article describes the DroneRF dataset: a radio frequency (RF) based dataset of drones functioning in different modes, including off, on and connected, hovering, flying, and video recording. The dataset contains recordings of RF activities, composed of 227 recorded segments collected from 3 different drones, as well as recordings of background RF activities with no drones. The data has been collected by RF receivers that intercepts the drone's communications with the flight control module. The receivers are connected to two laptops, via PCIe cables, that runs a program responsible for fetching, processing and storing the sensed RF data in a database. An example of how this dataset can be interpreted and used can be found in the related research article “RF-based drone detection and identification using deep learning approaches: an initiative towards a large open source drone database” (Al-Sa'd et al., 2019). © 2019 The Author(s)","Anti-drone systems; Classification; Drone identification; UAV detection","Aircraft detection; Decision making; Deep learning; Drones; Large dataset; Video recording; Anti-drone system; Classification and identifications; Decisions makings; Drone identification; Drone system; Information age; Modern technologies; Radio frequency receivers; Radiofrequencies; UAV detection; Classification (of information)"
"Allahyari H., Lavesson N.","User-oriented assessment of classification model understandability","10.3233/978-1-60750-754-3-11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956080091&doi=10.3233%2f978-1-60750-754-3-11&partnerID=40&md5=07216adbaef3268750890e5950bae100","This paper reviews methods for evaluating and analyzing the understandability of classification models in the context of data mining. The motivation for this study is the fact that the majority of previous work on evaluation and optimization of classification models has focused on assessing or increasing the accuracy of the models and thus user-oriented properties such as comprehensibility and understandability have been largely overlooked. We conduct a quantitative survey to examine the concept of understandability from the user's point of view. The survey results are analyzed using the analytic hierarchy process (AHP) to rank models according to their understandability. The results indicate that decision tree models are perceived as more understandable than rule-based models. Using the survey results regarding understandability of a number of models in conjunction with quantitative measurements of the complexity of the models, we are able to establish a negative correlation between the complexity and understandability of the classification models, at least for one of the two studied data sets. © 2011 The authors and IOS Press. All rights reserved.","Classification; evaluation; understandability","Analytic hierarchy process; Artificial intelligence; Data mining; Decision trees; Surveys; Analytic hierarchy process (ahp); Classification models; Decision tree models; evaluation; Negative correlation; Quantitative measurement; Rule-based models; Understandability; Classification (of information)"
"Allard T., Alvino P., Shing L., Wollaber A., Yuen J.","A dataset to facilitate automated workflow analysis","10.1371/journal.pone.0211486","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061174786&doi=10.1371%2fjournal.pone.0211486&partnerID=40&md5=23d47deebcf4e651eac89311dd071c80","Data sets that provide a ground truth to quantify the efficacy of automated algorithms are rare due to the time consuming and expensive, although highly valuable, task of manually annotating observations. These datasets exist for niche problems in developed fields such as Natural Language Processing (NLP) and Business Process Mining (BPM), however it is difficult to find a suitable dataset for use cases that span across multiple fields, such as the one described in this study. The lack of established ground truth maps between cyberspace and the human-interpretable, persona-driven tasks that occur therein, is one of the principal barriers preventing reliable, automated situation awareness of dynamically evolving events and the consequences of loss due to cybersecurity breaches. Automated workflow analysis —the machine-learning assisted identification of templates of repeated tasks—is the likely missing link between semantic descriptions of mission goals and observable events in cyberspace. We summarize our efforts to establish a ground truth for an email dataset pertaining to the operation of an open source software project. The ground truth defines semantic labels for each email and the arrangement of emails within a sequence that describe actions observed in the dataset. Identified sequences are then used to define template workflows that describe the possible tasks undertaken for a project and their business process model. We present the overall purpose of the dataset, the methodology for establishing a ground truth, and lessons learned from the effort. Finally, we report on the proposed use of the dataset for the workflow discovery problem, and its effect on system accuracy. © 2019 Allard et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"article; awareness; computer security; e-mail; human; machine learning; mining; natural language processing; process model; software; workflow; automation; data mining; Automation; Data Mining; Natural Language Processing; Software; Workflow"
"Alle S., Karthikeyan A., Kanakan A., Siddiqui S., Garg A., Mehta P., Mishra N., Chattopadhyay P., Devi P., Waghdhare S., Tyagi A., Tarai B., Hazarik P.P., Das P., Budhiraja S., Nangia V., Dewan A., Sethuraman R., Subramanian C., Srivastava M., Chakravarthi A., Jacob J., Namagiri M., Konala V., Dash D., Sethi T., Jha S., Agrawal A., Pandey R., Vinod P.K., Priyakumar U.D.","COVID-19 Risk Stratification and Mortality Prediction in Hospitalized Indian Patients: Harnessing clinical data for public health benefits","10.1371/journal.pone.0264785","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126658383&doi=10.1371%2fjournal.pone.0264785&partnerID=40&md5=8943325a3fa8e46cf9760fcb026e8d0d","The variability of clinical course and prognosis of COVID-19 highlights the necessity of patient sub-group risk stratification based on clinical data. In this study, clinical data from a cohort of Indian COVID-19 hospitalized patients is used to develop risk stratification and mortality prediction models. We analyzed a set of 70 clinical parameters including physiological and hematological for developing machine learning models to identify biomarkers. We also compared the Indian and Wuhan cohort, and analyzed the role of steroids. A bootstrap averaged ensemble of Bayesian networks was also learned to construct an explainable model for discovering actionable influences on mortality and days to outcome. We discovered blood parameters, diabetes, co-morbidity and SpO2 levels as important risk stratification features, whereas mortality prediction is dependent only on blood parameters. XGboost and logistic regression model yielded the best performance on risk stratification and mortality prediction, respectively (AUC score 0.83, AUC score 0.92). Blood coagulation parameters (ferritin, D-Dimer and INR), immune and inflammation parameters IL6, LDH and Neutrophil (%) are common features for both risk and mortality prediction. Compared with Wuhan patients, Indian patients with extreme blood parameters indicated higher survival rate. Analyses of medications suggest that a higher proportion of survivors and mild patients who were administered steroids had extreme neutrophil and lymphocyte percentages. The ensemble averaged Bayesian network structure revealed serum ferritin to be the most important predictor for mortality and Vitamin D to influence severity independent of days to outcome. The findings are important for effective triage during strains on healthcare infrastructure. Copyright: © 2022 Alle et al.",,"biological marker; D dimer; ferritin; interleukin 6; lactate dehydrogenase; steroid; adolescent; adult; aged; Article; blood clotting; blood oxygen tension; child; comorbidity; coronavirus disease 2019; diabetes mellitus; female; ferritin blood level; hospital patient; hospitalization; human; Indian; infection risk; inflammation; international normalized ratio; machine learning; major clinical study; male; mortality; neutrophil; prediction; public health; risk assessment; survival rate; Bayes theorem; China; epidemiology; etiology; India; middle aged; procedures; risk assessment; risk factor; statistical model; very elderly; young adult; Adolescent; Adult; Aged; Aged, 80 and over; Bayes Theorem; Child; China; COVID-19; Female; Hospitalization; Humans; India; Machine Learning; Male; Middle Aged; Models, Statistical; Risk Assessment; Risk Factors; Young Adult"
"Allegrini F., Olivieri A.C.","Linear or non-linear multivariate calibration models? That is the question","10.1016/j.aca.2022.340248","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137166790&doi=10.1016%2fj.aca.2022.340248&partnerID=40&md5=2aa7601a6c111f67e682783960056a03","Concepts from data science, machine learning, deep learning and artificial neural networks are spreading in many disciplines. The general idea is to exploit the power of statistical tools to interpret complex and, in many cases, non-linear data. Specifically in analytical chemistry, many chemometrics tools are being developed. However, they tend to get more complex without necessarily improving the prediction ability, which conspires against parsimony. In this report, we show how non-linear analytical data sets can be solved with equal or better efficiency by easily interpretable modified linear models, based on the concept of local sample selection before model building. The latter activity is conducted by choosing a sub-set of samples located in the neighborhood of each unknown sample in the space spanned by the latent variables. Two experimental examples related to the use of near infrared spectroscopy for the analysis of target properties in food samples are examined. The comparison with seemingly more complex chemometric models reveals that local regression is able to achieve similar analytical performance, with considerably less computational burden. © 2022 Elsevier B.V.","Artificial neural networks; Local partial least-squares; Near infrared spectroscopy; Non-linear systems","Chemical analysis; Complex networks; Deep learning; Infrared devices; Least squares approximations; Near infrared spectroscopy; Neural networks; Principal component analysis; Statistical mechanics; Calibration model; Chemometric tools; Local partial least-square; Machine-learning; Multivariate calibration; Non linear; Non linear system; Partial least-squares; Power; Statistical tools; Linear systems; article; artificial neural network; calibration; local regression; machine learning; near infrared spectroscopy; neighborhood; nonlinear system; partial least squares regression; prediction; calibration; least square analysis; procedures; statistical model; Calibration; Least-Squares Analysis; Linear Models; Neural Networks, Computer; Spectroscopy, Near-Infrared"
"Allen A.E.A., Tkatchenko A.","Machine learning of material properties: Predictive and interpretable multilinear models","10.1126/sciadv.abm7185","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129949903&doi=10.1126%2fsciadv.abm7185&partnerID=40&md5=90461891867fe0333c15f85ba3544e5b","Machine learning models can provide fast and accurate predictions of material properties but often lack transparency. Interpretability techniques can be used with black box solutions, or alternatively, models can be created that are directly interpretable. We revisit material datasets used in several works and demonstrate that simple linear combinations of nonlinear basis functions can be created, which have comparable accuracy to the kernel and neural network approaches originally used. Linear solutions can accurately predict the bandgap and formation energy of transparent conducting oxides, the spin states for transition metal complexes, and the formation energy for elpasolite structures. We demonstrate how linear solutions can provide interpretable predictive models and highlight the new insights that can be found when a model can be directly understood from its coefficients and functional form. Furthermore, we discuss how to recognize when intrinsically interpretable solutions may be the best route to interpretability. Copyright © 2022 The Authors.",,"Machine learning; Transition metals; Transparent conducting oxides; Accurate prediction; Black Box solution; Formation energies; Interpretability; Linear combinations; Linear solution; Machine learning models; Multilinear models; Nonlinear basis functions; Simplest linear; Metal complexes"
"Allen B., Lane M., Steeves E.A., Raynor H.","Using Explainable Artificial Intelligence to Discover Interactions in an Ecological Model for Obesity","10.3390/ijerph19159447","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136342022&doi=10.3390%2fijerph19159447&partnerID=40&md5=34aac9d48d7766c189287c76eb95d2fe","Ecological theories suggest that environmental, social, and individual factors interact to cause obesity. Yet, many analytic techniques, such as multilevel modeling, require manual specification of interacting factors, making them inept in their ability to search for interactions. This paper shows evidence that an explainable artificial intelligence approach, commonly employed in genomics research, can address this problem. The method entails using random intersection trees to decode interactions learned by random forest models. Here, this approach is used to extract interactions between features of a multi-level environment from random forest models of waist-to-height ratios using 11,112 participants from the Adolescent Brain Cognitive Development study. This study shows that methods used to discover interactions between genes can also discover interacting features of the environment that impact obesity. This new approach to modeling ecosystems may help shine a spotlight on combinations of environmental features that are important to obesity, as well as other health outcomes. © 2022 by the authors.","adolescent obesity; ecological theory; explainable artificial intelligence; household income; machine learning; neighborhood education; neighborhood poverty; parent education","adolescence; artificial intelligence; computer simulation; household income; machine learning; neighborhood; obesity; poverty alleviation; Article; artificial intelligence; child; controlled study; dietary intake; educational status; environmental factor; ethnicity; feature extraction; female; follow up; Hispanic; household income; human; major clinical study; male; neighborhood; obesity; physical activity; pollution; poverty; predictive model; puberty; race; random forest; residence characteristics; school child; United States; waist to height ratio; adolescent; ecosystem; obesity; Adolescent; Artificial Intelligence; Ecosystem; Humans; Obesity; Waist-Height Ratio"
"Allen C., Hu J., Kumar V., Ahmad M.A., Teredesai A.","Interpretable Phenotyping for Electronic Health Records","10.1109/ICHI52183.2021.00034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118108564&doi=10.1109%2fICHI52183.2021.00034&partnerID=40&md5=883cc984745053330b83883953ff721f","Datasets from Electronic Health Records (EHRs) are increasingly large and complex, creating challenges in their use for predictive modeling. The two major challenges are large-scale and high-dimensionality. One of the common way to address the large-scale challenge is through use of data phenotypes: clinically relevant characteristic groupings that can be expressed as logical queries (e.g., 'senior patients with diabetes'). With the increasing use of machine learning across the continuum of care, phenotypes play an important role in modeling for population management, clinical trials, observational and interventional research, and quality measures. Yet, phenotype interpretation can often be difficult and require post-hoc clarifications from experienced clinicians. For example, detailed analysis may be needed to find that all patients in a a phenotype are diabetic seniors with complications from previous surgery. Moreover, the high-dimensionality problem is often addressed either separately or simultaneously with phenotyping by dimension reduction methods that may further hinder interpretability. In this paper, we introduce the notion of interpretable data phenotypes generated by an unsupervised learning technique. Methods are designed to disambiguate relative feature memberships, thus facilitating general clinical validation, and alleviating the problem of high-dimensionality. The empirical study applies the proposed unsupervised interpretable phenotyping method to a real world healthcare dataset (MIMIC), then uses hospital length of stay as a reference prediction task. The results demonstrate that the proposed method produces phenotypes with improved interpretability and without diminishing the quality of prediction results. © 2021 IEEE.","Data Phenotyping; EHRs; High-Dimensionality; Interpretable Phenotyping; Unsupervised Learning","Clinical research; Large dataset; Unsupervised learning; Data phenotyping; High dimensionality; Interpretability; Interpretable phenotyping; Large-scales; Phenotyping; Population management; Predictive models; Unsupervised learning; Records management"
"Allen J., Moussa A., Liu X.","Human-in-the-loop learning of qualitative preference models",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068868146&partnerID=40&md5=e64978f2dedc2ddf40bde46df15c891a","In this work, we present a novel human-in-the-loop framework to help the human user understand the decision making process that involves choosing preferred options. We focus on qualitative preference models over alternatives from combinatorial domains. This framework is interactive: The user provides her behavioral data to the framework, and the framework explains the learned model to the user. It is iterative: The framework collects feedback on the learned model from the user and tries to improve it accordingly till the user terminates the iteration. In order to communicate the learned preference model to the user, we develop visualization of intuitive and explainable graphic models, such as lexicographic preference trees and forests, and conditional preference networks. To this end, we discuss key aspects of our framework for lexicographic preference models. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Decision making; Behavioral data; Conditional preferences; Decision making process; Graphic models; Human-in-the-loop; Lexicographic preferences; Preference modeling; Preference models; Behavioral research"
"Allen R., Desmoulins C., Trilling L.","Intelligent tutors and artificial intelligence: Problems in the construction of geometric figures [Tuteurs intelligents et intelligence artificielle: Problèmes posés en construction de figures géométriques]","10.1007/3-540-55606-0_40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029493455&doi=10.1007%2f3-540-55606-0_40&partnerID=40&md5=e02215391cc9d332f46d5278395fd69f","Our purpose is to illustrate, through the conception and realization of an ITS for the construction of geometric figures, an approach to the expression of the pedagogical contract based on first order logic. It is critical for the contract to be very precise as well as understandable and explanable throughout. This requires the teacher to define the specification of the goal to be attained and the context using tools with a precise semantics. The means of expression available to the student for constructing a solution must also possess a clear semantics. We show that a methodology associating a formula in a logic language which is common to the specification and to the solution makes it possible to give a first concrete definition of a given contract. We can then better grasp both the requirements for the contract not accounted for in a first stage and the constraints of implementation and efficiency. Certain points which still require improvement—e.g., the exact meaning of negation and the non particularity of constructions—are brought to light. Finally, we present the results of experiments with exercises typically found in geometry textbooks. © 1992, Springer Verlag. All rights reserved.",,"Computer aided instruction; Computer circuits; Formal logic; Geometry; Semantics; Specifications; First order logic; Intelligent tutors; Logic languages; Intelligent vehicle highway systems"
"Alley E.C., Turpin M., Liu A.B., Kulp-McDowall T., Swett J., Edison R., Von Stetina S.E., Church G.M., Esvelt K.M.","A machine learning toolkit for genetic engineering attribution to facilitate biosecurity","10.1038/s41467-020-19612-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097301040&doi=10.1038%2fs41467-020-19612-0&partnerID=40&md5=ce51c16f80c61c764affea4c30f07a6f","The promise of biotechnology is tempered by its potential for accidental or deliberate misuse. Reliably identifying telltale signatures characteristic to different genetic designers, termed ‘genetic engineering attribution’, would deter misuse, yet is still considered unsolved. Here, we show that recurrent neural networks trained on DNA motifs and basic phenotype data can reach 70% attribution accuracy in distinguishing between over 1,300 labs. To make these models usable in practice, we introduce a framework for weighing predictions against other investigative evidence using calibration, and bring our model to within 1.6% of perfect calibration. Additionally, we demonstrate that simple models can accurately predict both the nation-state-of-origin and ancestor labs, forming the foundation of an integrated attribution toolkit which should promote responsible innovation and international security alike. © 2020, The Author(s).",,"biotechnology; calibration; DNA; genetic engineering; innovation; machine learning; phenotype; article; calibration; genetic engineering; phenotype; prediction; recurrent neural network; biotechnology; bioterrorism; data analysis; factual database; forensic genetics; genetic engineering; information processing; organization and management; prevention and control; procedures; DNA; Biotechnology; Bioterrorism; Data Analysis; Databases, Factual; Datasets as Topic; DNA; Forensic Genetics; Genetic Engineering; Neural Networks, Computer; Security Measures"
"Allgaier J., Schlee W., Probst T., Pryss R.","Prediction of Tinnitus Perception Based on Daily Life MHealth Data Using Country Origin and Season","10.3390/jcm11154270","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136457877&doi=10.3390%2fjcm11154270&partnerID=40&md5=7325ef3e3437cd4fca1568528617aced","Tinnitus is an auditory phantom perception without external sound stimuli. This chronic perception can severely affect quality of life. Because tinnitus symptoms are highly heterogeneous, multimodal data analyses are increasingly used to gain new insights. MHealth data sources, with their particular focus on country- and season-specific differences, can provide a promising avenue for new insights. Therefore, we examined data from the TrackYourTinnitus (TYT) mHealth platform to create symptom profiles of TYT users. We used gradient boosting engines to classify momentary tinnitus and regress tinnitus loudness, using country of origin and season as features. At the daily assessment level, tinnitus loudness can be regressed with a mean absolute error rate of 7.9% points. In turn, momentary tinnitus can be classified with an F1 score of 93.79%. Both results indicate differences in the tinnitus of TYT users with respect to season and country of origin. The significance of the features was evaluated using statistical and explainable machine learning methods. It was further shown that tinnitus varies with temperature in certain countries. The results presented show that season and country of origin appear to be valuable features when combined with longitudinal mHealth data at the level of daily assessment. © 2022 by the authors.","explainable machine learning; gradient boosting machine; machine learning; mobile health; multimodal data; tinnitus",
"Allodi L.","The heavy tails of vulnerability exploitation","10.1007/978-3-319-15618-7_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924049184&doi=10.1007%2f978-3-319-15618-7_11&partnerID=40&md5=685833dc479fb9fa32e34d36a66928ee","In this paper we analyse the frequency at which vulnerabilities are exploited in the wild by relying on data collected worldwide by Symantec’s sensors. Our analysis comprises 374 exploited vulnerabilities for a total of 75.7 Million recorded attacks spanning three years (2009-2012). We find that for some software as little as 5% of exploited vulnerabilities is responsible for about 95% of the attacks against that platform. This strongly skewed distribution is consistent for all considered software categories, for which a general take-away is that less than 10% of vulnerabilities account for more than 90% of the attacks (with the exception of pre-2009 Java vulnerabilities). Following these findings, we hypothesise vulnerability exploitation may follow a Power Law distribution. Rigorous hypothesis testing results in neither accepting nor rejecting the Power Law Hypothesis, for which further data collection from the security community may be needed. Finally, we present and discuss the Law of the Work-Averse Attacker as a possible explanation for the heavy-tailed distributions we find in the data, and present examples of its effects for Apple Quicktime and Microsoft Internet Explorer vulnerabilities. © Springer International Publishing Switzerland 2015.",,"Artificial intelligence; Computer science; Computers; Data collection; Heavy-tailed distribution; Heavy-tails; Hypothesis testing; Internet explorers; Power law distribution; Security community; Skewed distribution; Computer software"
"Allouche M.-K., Sayettat C., Boissier O.","Towards a multi-agent system for the supervision of dynamic systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030718128&partnerID=40&md5=dbec80a397d38887ca798a6218eff59a","The supervision of dynamic systems is essential in industrial applications. The decentralization of such systems make their supervision more difficult. In this article, we tackle the supervision problem by using a temporal scenario recognition approach. The supervision is performed by a society of agents where an agent is considered as a watching process responsible for a subset of possible scenarios of the functioning of the system. The cooperation among agents is based on interaction protocols as well as dependence networks.",,"Adaptive systems; Artificial intelligence; Decision support systems; Interactive computer systems; Network protocols; Operations research; Dynamic systems supervision; Multiagent systems; Pattern recognition systems"
"Alloulah M., Isopoussu A., Min C., Kawsar F.","On Tracking the Physicality of Wi-Fi: A Subspace Approach","10.1109/ACCESS.2019.2897840","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062469478&doi=10.1109%2fACCESS.2019.2897840&partnerID=40&md5=f676a95ae2f0899748d9fbdefa0adf72","Wi-Fi channel state information (CSI) has emerged as a plausible modality for sensing different human activities as a function of modulations in the wireless signal that travels between wireless devices. Until now, most research has taken a statistical approach and/or purpose-built inference pipeline. Although interesting, these approaches struggle to sustain sensing performances beyond experimental conditions. As such, the full potential of CSI as a general-purpose sensing modality is yet to be realized. We argue that a universal approach with the well-grounded formalization is necessary to characterize the relationship between the wireless channel modulations (spatial and temporal) and human movement. To this end, we present a formalism for quantifying the changing part of the wireless signal modulated by human motion. Grounded in this formalization, we then present a new subspace tracking technique to describe the channel statistics in an interpretable way, which succinctly contains the human modulated part of the channel. We characterize the signal and noise subspaces for the case of uncontrolled human movement and show that these subspaces are dynamic. Our results demonstrate that the proposed channel statistics alone can robustly reproduce the state-of-the-art application-specific feature engineering baseline, however, across multiple usage scenarios. We expect that our universal channel statistics will yield an effective general-purpose featurization of wireless channel measurements and will uncover opportunities for applying CSI for a variety of human sensing applications in a robust way. © 2019 IEEE.","Channel sensing; interpretable dimensionality reduction; machine learning; multiple-input multiple-output (MIMO)","Communication channels (information theory); Learning systems; MIMO systems; Modulation; Wi-Fi; Wireless local area networks (WLAN); Application specific; Channel sensing; Dimensionality reduction; Experimental conditions; Feature engineerings; Sensing performance; Statistical approach; Universal approach; Channel state information"
"Alluri B.K.S.P.K.R.","Machine learning for health care",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114569565&partnerID=40&md5=2395501680f14acd938758cac0a6268b","Machine learning (ML) is predominately being used to solve various technical and nontechnical challenges around the world. Taking the context of health care, ML took over the control and helping many practitioners for effective decision-making. Practically, this is once again proved by the researchers as they are using the ML algorithms for fast detection of COVID-19 and steps are initiated for the drug discovery of the same. In this chapter, we initially discuss the work happening in COVID-19 using ML algorithms. Then, we summarize the role of ML for analyzing and assessing various chronic diseases. Even though ML algorithms are predicting multidimensional aspects of the target disease, the experts in the field are still hesitating to use those outcomes as they lack in justification. To address this, a separate concept called explainable AI (XAI) is discussed. Data scientists are using ML algorithms to address chronic health issues with less cost and in a more accurate way. The question is, how ML could achieve this? This will be the main motivation for the entire chapter. In a recent report of a Harvard Medical Survey, it said that nearly 5, 000, 000 Indians die every year due to the medical errors (Harvard, 2020). These errors are getting reduced when the same disease is analyzed with ML. There is a famous proverb, “Prevention is better than cure.” According to World Health Organization, in most of the cases, chronic diseases cannot be prevented. But, the impact of them can be reduced with proper data collection and effective analysis using advanced ML algorithms. For instance, to detect lung cancer, it would cost hundreds of dollars for undergoing diagnosis and medication. Above that, if the disease is detected in the advanced stages then the probability of the cure is also very less. All these negative consequences can be reduced drastically when the usage of ML for health care is further exploited. © The Institution of Engineering and Technology 2021.",,
"Allys E., Marchand T., Cardoso J.-F., Villaescusa-Navarro F., Ho S., Mallat S.","New interpretable statistics for large-scale structure analysis and generation","10.1103/PhysRevD.102.103506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096100649&doi=10.1103%2fPhysRevD.102.103506&partnerID=40&md5=095148d4025050bdc13c9ee665a18d14","We introduce wavelet phase harmonics (WPH) statistics: interpretable low-dimensional statistics that describe 2D non-Gaussian fields. These statistics are built from WPH moments, which were recently introduced in the data science and machine learning community. We apply WPH statistics to projected 2D matter density fields from the Quijote N-body simulations of the large-scale structure of the Universe. By computing Fisher information matrices, we find that the WPH statistics place more stringent constraints on four of five cosmological parameters when compared to statistics based on the combination of the power spectrum and bispectrum. We also use the WPH statistics with a maximum entropy model to statistically generate new 2D density fields that accurately reproduce the probability density function, the mean and standard deviation of the power spectrum, the bispectrum, and Minkowski functionals of the input density fields. Although other methods are efficient for either parameter estimates or statistical syntheses of the large-scale structure, WPH statistics are the first statistics that achieve state-of-the-art results for both tasks as well as being interpretable. © 2020 American Physical Society.",,
"Alm C.O., Hedges A.","Visualizing NLP in Undergraduate Students' Learning about Natural Language",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130046264&partnerID=40&md5=060565c5d97fbd854fd5f29e0769ea47","We report on the use of open-source natural language processing capabilities in a web-based interface to allow undergraduate students to apply what they have learned about formal natural language structures. The learning activities encourage students to interpret data in new ways, think originally about natural language, and critique the back-end NLP models and algorithms visualized on the user front end. This work is of relevance to AI resources developed for education by focusing on inclusivity of students from many disciplinary backgrounds. Specifically, we comprehensively extended a web-based system with new resources. To test the students' reactions to NLP analyses that offer insights into both the strengths and limitations of AI systems, we incorporated a range of automated analyses focused on language-independent processing or meaning representations which still represent challenges for NLP. We conducted a survey-based evaluation with students in open-ended case-based assignments in undergraduate coursework. Responses indicated that the students reinforced their knowledge, applied critical thinking about language and NLP applications, and used the application not to solve the assignment for them, but as a tool in their own effort to address the task. We further discuss how using interpretable visualizations of system decisions is an opportunity to learn about ethical issues in NLP, and how making AI systems interpretable may broaden multidisciplinary interest in AI in early educational experiences. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved",,"Artificial intelligence; Education computing; Multimedia systems; Students; Websites; AI systems; Language structure; Learning Activity; Model and algorithms; Natural languages; Open-source; Processing capability; Student learning; Undergraduate students; Web-based interface; Natural language processing systems"
"Almaghrabi F., Xu D.-L., Yang J.-B.","A new machine learning technique for predicting traumatic injuries outcomes based on the vital signs","10.23919/IConAC.2019.8895012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075775171&doi=10.23919%2fIConAC.2019.8895012&partnerID=40&md5=4316be5020187c1ada51e7fe4ab838e9","Traditional vital signs are an essential part of triage assessment in emergency departments (ED), and have been widely used in trauma prediction models. Previous researchers have studied the effect of vital signs scores on predicting traumatic injury outcomes and have found it to be significant. Based on the vital signs' scores, an Interpretable Machine Learning (IML) method is proposed to predict patient outcomes and is compared with various ML algorithms. Results indicate that the IML method has a comparable performance with a mean AUC of 0.683, and its interpretability would help in the early identification of trauma patients at risk of mortality. © 2019 Chinese Automation and Computing Society in the UK-CACSUK.","Belief rule-based inference; Interpretable machine learning technique; Maximum likelihood evidential reasoning; Trauma outcome prediction; Vital signs","Forecasting; Learning algorithms; Maximum likelihood; Belief rules; Evidential reasoning; Machine learning techniques; Outcome prediction; Vital sign; Machine learning"
"Almagrabi A.O., Bashir A.K.","A classification-based privacy-preserving decision-making for secure data sharing in Internet of Things assisted applications","10.1016/j.dcan.2021.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119970663&doi=10.1016%2fj.dcan.2021.09.003&partnerID=40&md5=8699cb9a396169413fb5c3d80250b5e1","The introduction of the Internet of Things (IoT) paradigm serves as pervasive resource access and sharing platform for different real-time applications. Decentralized resource availability, access, and allocation provide a better quality of user experience regardless of the application type and scenario. However, privacy remains an open issue in this ubiquitous sharing platform due to massive and replicated data availability. In this paper, privacy-preserving decision-making for the data-sharing scheme is introduced. This scheme is responsible for improving the security in data sharing without the impact of replicated resources on communicating users. In this scheme, classification learning is used for identifying replicas and accessing granted resources independently. Based on the trust score of the available resources, this classification is recurrently performed to improve the reliability of information sharing. The user-level decisions for information sharing and access are made using the classification of the resources at the time of availability. This proposed scheme is verified using the metrics access delay, success ratio, computation complexity, and sharing loss. © 2021 Chongqing University of Posts and Telecommunications","Classification learning; Data mining; IoT; Privacy-preserving; Resource replication",
"Alman A.","Hybrid process modeling and mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117201955&partnerID=40&md5=3afacec6d5daff3ded82b3f68cb5a947","One of the cornerstones of business process management and process mining is the ability to describe any business process systematically as a process model. The notations used for these descriptions can, in general, be divided into two paradigms: procedural and declarative. The procedural paradigm describes processes in terms of explicit process control flow, whereas the declarative paradigm describes processes in terms of process rules while leaving the process control flow implicit. This difference has significant implications for describing knowledge-intensive processes. A knowledge-intensive process may have thousands of case variants, all of which may correspond to valid process executions. While procedural models are generally considered more understandable, describing a knowledge-intensive process using the procedural paradigm often results in an unreadable spaghetti model. The declarative paradigm avoids this issue by leaving the control flow of the process implicit. However, this has a tendency to hide the overall structure of the process, and therefore again leading to understandability issues. During this project we plan to address these issues by first analysing the existing notations suitable for process modeling, and then using the results of this analysis for developing a novel hybrid process modeling approach and a set of corresponding process mining techniques. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Hybrid modeling; Knowledge-intensive processes; Process mining; Process modeling","Enterprise resource management; Process control; Process engineering; Business Process; Control-flow; Hybrid model; Hybrid process; Knowledge intensive process; Procedural models; Process execution; Process mining; Process-models; Understandability; Data mining"
"Almansour N.M.","Triple-Negative Breast Cancer: A Brief Review About Epidemiology, Risk Factors, Signaling Pathways, Treatment and Role of Artificial Intelligence","10.3389/fmolb.2022.836417","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124287831&doi=10.3389%2ffmolb.2022.836417&partnerID=40&md5=436a4d41b32a0cc9ca3773d3457c9dd8","Triple-negative breast cancer (TNBC) is a kind of breast cancer that lacks estrogen, progesterone, and human epidermal growth factor receptor 2. This cancer is responsible for more than 15–20% of all breast cancers and is of particular research interest as it is therapeutically challenging mainly because of its low response to therapeutics and highly invasive nature. The non-availability of specific treatment options for TNBC is usually managed by conventional therapy, which often leads to relapse. The focus of this review is to provide up-to-date information related to TNBC epidemiology, risk factors, metastasis, different signaling pathways, and the pathways that can be blocked, immune suppressive cells of the TNBC microenvironment, current and investigation therapies, prognosis, and the role of artificial intelligence in TNBC diagnosis. The data presented in this paper may be helpful for researchers working in the field to obtain general and particular information to advance the understanding of TNBC and provide suitable disease management in the future. Copyright © 2022 Almansour.","artificial intelligence; prognosis; risk factor; signaling pathways; triple negative breast cancer",
"Almars A.M., Almaliki M., Noor T.H., Alwateer M.M., Atlam E.","HANN: Hybrid Attention Neural Network for Detecting Covid-19 Related Rumors","10.1109/ACCESS.2022.3146712","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123712297&doi=10.1109%2fACCESS.2022.3146712&partnerID=40&md5=16a87b0b537a01793b0c676edfe3e709","In the age of social media, the spread of rumors is becoming easier due to the proliferation of communication and information dissemination platforms. Detecting rumors is a major problem with significant consequences for the economy, democracy, and public safety. Deep learning approaches were used to classify rumors and have yielded state-of-the-art results. Nevertheless, the majority of techniques do not attempt to explain why or how decisions are made. This paper introduces a hybrid attention neural network (HANN) to identify rumors from social media. The advantage of HANN is that it will allow the main user to capture the relative and important features between different classes as well as provide an explanation of the model's decisions. Two deep neural networks are included in the proposal: CNNs and Bidirectional Long Short Term Memory (Bi-LSTM) networks with attention modules. In this paper, the model is trained using a benchmark dataset containing 3612 distinct tweets crawled from Twitter including several types of rumors related to COVID-19. Each subset of data has a balanced label distribution with 1480 rumors tweets (46.87%) and 1677 non-rumors tweets (53.12%). The experimental results demonstrate that the new approach (HANN model) performs better results in terms of performance and accuracy (about 0.915%) than many contemporary models (AraBERT, MARBEART, PCNN, LSTM, LSTM-PCNN and Attention LSTM). Moreover, a number of software engineering features such as followers, friends, and registration age are used to enhance the model's accuracy. © 2013 IEEE.","accuracy; attention Bi-LSTM; explainable rumors detection; F-Score; HANN; LSTM; Rumors","Deep neural networks; Information dissemination; Long short-term memory; Social networking (online); Software engineering; Support vector machines; Accuracy; Attention bidirectional long short term memory; Convolutional neural network; COVID-19; Deep learning; Explainable rumor detection; F-score; Features extraction; Hybrid attention neural network; LSTM; Neural-networks; Rumor; Social networking (online); Support vectors machine; Feature extraction"
"Almashan M., Narusue Y., Morikawa H.","Decision tree regressions for estimating liquid holdup in two-phase gas-liquid flows",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097561458&partnerID=40&md5=8b592fe979988a749cb31a9d48ce4b15","In producing oil and gas wells, two-phase flow of gas and liquid is inevitable. Gas density is lighter than liquid density, as a result, gas travels faster than liquid, leaving the liquid phase to build up in pipe segments. The amount of liquid occupying each pipe segment varies. This phenomenon is called liquid holdup, which is defined as the ratio of the liquid volume in a pipe element to the volume of the pipe element. Liquid holdup presents challenges in calculating mixture physical properties and two-phase flow pressure drop. Estimating liquid holdup is the first step to investigate production wells problems. Early detection and identification of liquid holdup (HL) in oil and gas wells is needed to reduce the maintenance downtime and thus increase production. This is significantly crucial in designing oil and gas facilities. Consequently, many studies on predicting liquid holdup have been carried out in the past, considering different flow conditions, resulting in a large number of empirical correlations with various degrees of accuracy. Machine learning approaches in predicting liquid holdup in multiphase flows have been recently studied to improve the prediction accuracy compared to the existing empirical correlations. However, these approaches ignored the heuristic feature importance of the input parameters to the predicted HL values. In our study, a machine learning predictive model, boosted decision tree regression (BDTR), is trained, tested and evaluated in predicating HL in multiphase flows in oil and gas wells. Decision trees are considered as non-parametric machine learning models. The datasets used in training and testing the predictive model are experimental and they were collected from literature (111 data-points). Air-kerosene and air-water mixtures were used in obtaining the 111 experimental data-points. Results show that, the proposed BDTR model outperforms the best empirical correlations and the fuzzy logic model used in estimating HL in gas-liquid multiphase flows. For the built model, the most important input feature in estimating HL is the superficial gas velocity (Vsg). The empirical correlations developed in the past for identifying HL in the multiphase flow phenomenon can only be applied under certain flow conditions by which they were originally developed but this machine learning model does not suffer from this limitation. To the best of our knowledge, the present study is the first work that shows how the decision forest regression predictive models can accurately predict HL. Using the BDTR model with its interpretable representation, one can clearly determine the heuristic feature importance of the input features used in building the model. Heuristic feature importance can help in having a better insight of the issues associated with the HL studies, such as the liquid loading phenomenon. © 2020, Society of Petroleum Engineers",,"Air; Decision trees; Density of gases; Forecasting; Fuzzy logic; Gases; Gasoline; Liquids; Machine learning; Mixtures; Natural gas well production; Natural gas wells; Oil well production; Predictive analytics; Turing machines; Boosted decision trees; Decision tree regression; Detection and identifications; Interpretable representation; Machine learning approaches; Machine learning models; Superficial gas velocities; Two-phase gas-liquid flow; Two phase flow"
"Almashan M., Narusue Y., Morikawa H.","Estimating PVT properties of crude oil systems based on a boosted decision tree regression modelling scheme with k-means clustering","10.2118/196453-ms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088407742&doi=10.2118%2f196453-ms&partnerID=40&md5=fc5fdb9cde9ca4853334ccc32df255e2","Machine learning has been successfully implemented for the past 20 years in estimating reservoir fluid properties competing with the empirical correlations. One of the most commonly utilized modelling schemes is the artificial neural network which is known for its black-box problem that does not show the steps taken to reach the final estimation of the fluid properties. This study offers a different modeling approach that overcomes the limitations of the current implemented modeling scheme providing users with better predications and deeper understanding of the key input parameters in modeling. The proposed model predicts the bubble point pressure (Pb) and the oil formation volume factor at bubble point pressure (Bob) as a function of oil and gas specific gravity, solution gas-oil ratio, and reservoir temperature by a boosted decision tree regression (BDTR) predictive modeling scheme. The K-means clustering algorithm is performed as a preprocessing step based on the Pressure-Volume-Temperature (PVT) input features to increase the prediction accuracy. In addition, the predictive power of the built K-means clustered BDTR model implemented in this study is compared against the most commonly used empirical correlations, the ANNs, and the standalone BDTR model. Moreover, the feature importance of predicting Pb and Bob is discussed. The universal dataset used in building the predictive model consists of 5200 experimentally derived data points representing worldwide crude oils covering a wide range of geographical regions. The built BDTR model is more accurate and it outperforms the most commonly used empirical correlations and the previous machine learning models in predicting Pb and Bob in terms of the average absolute percent relative error. Furthermore, the proposed model can be integrated into simulators and it can also be applied towards predicating other oil and gas properties, in the gas-liquid two-phase flow pattern identification, and in predicting rock properties. As an interpretable approach in predicting the PVT properties of crude oils, the proposed model can be used as an alternative modeling scheme in PVT characterization where the importance of the input features can heuristically and accurately be determined. This can be applied towards preventive maintenance and anomaly detection studies where prediction decisions can be further investigated by the interpretable representation of the decision trees. In addition, it is the most accurate model to date in predicting the bubble point pressure and the oil formation volume factor at bubble point pressure of crude oils. Copyright 2019, Society of Petroleum Engineers.","Oil and gas; Predictive model; PVT; Reservoir characterization","Anomaly detection; Bottom hole pressure; Crude oil; Decision trees; Flow patterns; Forecasting; Gases; Geographical regions; Machine learning; Neural networks; Petroleum reservoir engineering; Preventive maintenance; Two phase flow; Gas - liquid two-phase flows; Interpretable representation; Machine learning models; Oil and gas; Oil formation volume factors; Predictive modeling; Pressure-volume-temperatures; Reservoir characterization; K-means clustering"
"Al-Mashhadany A.K., Hamood D.N., Sadiq Al-Obaidi A.T., Al-Mashhadany W.K.","Extracting numerical data from unstructured Arabic texts (ENAT)","10.11591/ijeecs.v21.i3.pp1759-1770","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102559278&doi=10.11591%2fijeecs.v21.i3.pp1759-1770&partnerID=40&md5=7f9039614f9f6b84e62f10f08782bf8c","Unstructured data becomes challenges because in recent years have observed the ability to gather a massive amount of data from annotated documents. This paper interested with Arabic unstructured text analysis. Manipulating unstructured text and converting it into a form understandable by computer is a high-level aim. An important step to achieve this aim is to understand numerical phrases. This paper aims to extract numerical data from Arabic unstructured text in general. This work attempts to recognize numerical characters phrases, analyze them and then convert them into integer values. The inference engine is based on the Arabic linguistic and morphological rules. The applied method encompasses rules of numerical nouns with Arabic morphological rules, in order to achieve high accurate extraction method. Arithmetic operations are applied to convert the numerical phrase into integer value. The proper operation is determined depending on linguistic and morphological rules. It will be shown that applying Arabic linguistic rules together with arithmetic operations succeeded in extracting numerical data from Arabic unstructured text with high accuracy reaches to 100%. © 2021 Institute of Advanced Engineering and Science. All rights reserved.","Arabic linguistic rules; Numerical dictionary; Related words; Text data mining; Unstructured data",
"Almasoud S.K., Mathkour H.I.","Instant adaptation enrichment technique to improve web accessibility for blind users","10.1145/3325917.3325931","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068708718&doi=10.1145%2f3325917.3325931&partnerID=40&md5=02528f86556be244dc2206fbbaa61460","The Internet in general and the web are considered the main medium of remote communication. They are important sources of information and services to all the users. Assistive technologies and tools facilitate web accessibility to help users with special needs by delivering the content of the webpages in a form that is understandable by these users. However, these assistive technologies and tools often face certain obstacles in delivering the information because of the confusion of web design elements caused by poor webpage design or carelessness in following the web accessibility guidelines. In this paper, we list the minimum required web accessibility guidelines formulated on the basis of a comprehensive review of WCAG 2.1 guidelines to be followed while developing webpages for visually impaired users. We then introduce a tool intended to enhance web accessibility for such users. We tested the tool on a few websites and observed that it significantly improved the webpage accessibility of these websites. © 2019 Association for Computing Machinery.","Blind; Chrome Extension; WCAG; Web Accessibility; Web Content Accessibility Guidelines; Webpage","Information systems; Information use; Web Design; Websites; Blind; Chrome extensions; WCAG; Web accessibility; Web content accessibility guidelines; Data mining"
"Almasri A., Alsaraireh J., Salman D., Aburagaga I.","Explainable Artificial Intelligence Models using Students' Academic Record Data, Tree Family Classifiers, and K-means Clustering to Predict Students' Performance","10.1109/icSmartGrid55722.2022.9848541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137843058&doi=10.1109%2ficSmartGrid55722.2022.9848541&partnerID=40&md5=1f42428c9d4caa36525c86109a05db4c","Academic systems work in a complex environment and face problems analysing students' performance using their current systems. Therefore, they use data mining to analyze an enormous set of data, get hidden and useful knowledge, and extract meaningful. Accordingly, the research aimed to analyze the performance of academic students by comparing the accuracies of each algorithm. Five classifiers of the tree family used for the experiments are J48, BF Tree, NB Tree, Random Forest, and REP Tree. The results indicated that J48 outperformed all the other tree family classifiers in terms of accuracy, precision and recall. Hence, it is a superior classification technique among the classifiers used for the educational datasets. Also, it has used labelled features to visualize an interpretable decision tree to indicate students' performance. Also, it has developed an interpretable model using the K-means clustering technique and the J48 tree. © 2022 IEEE.","K-Means; Student Performance; Tree Family; XAI","Classification (of information); Data mining; Decision trees; K-means clustering; Academic system; Complex environments; Current system; Data tree; Intelligence models; K-means; K-means++ clustering; Student performance; Tree family; XAI; Students"
"Almasri A., Alkhawaldeh R.S., Çelebi E.","Clustering-Based EMT Model for Predicting Student Performance","10.1007/s13369-020-04578-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085316040&doi=10.1007%2fs13369-020-04578-4&partnerID=40&md5=425b23931f3d9ea0e41886f8ccd47f26","Predicting students’ performance has emerged as an attractive task among researchers. They use supervised and unsupervised educational data mining (EDM) techniques to build an understandable and effective model. This helps decision makers enhance the performance of the students. The challenge of finding an optimal model leads to appearance of many techniques from both EDM techniques. Hence, we propose a unified framework to build a novel supervised cluster-based (CB) classifier model. The unified framework uses clustering technique to group historical records of students into a set of homogeneous clusters. Then, classifier model for each cluster is built and the final unified classifiers along with the centroids at each cluster are used as CB classifier model. The experimental results show that the CB model gains a high accuracy performance reached 96.25%. In addition, we use feature selection techniques for selecting the relevant features from a space of features. The model obtains a high accuracy performance using relevant features reached to 96.96% where the percentage of relevant features on average is 57.4% of overall features. © 2020, King Fahd University of Petroleum & Minerals.","Clustering-based EMT; Educational data mining; Machine learning; Wrapper-based feature selection",
"Almasri A., Celebi E., Alkhawaldeh R.S.","EMT: Ensemble meta-based tree model for predicting student performance","10.1155/2019/3610248","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062862231&doi=10.1155%2f2019%2f3610248&partnerID=40&md5=937266215fe6fc6dd0df74bf39b6a958","In recent decades, predicting the performance of students in the academic field has revealed the attention by researchers for enhancing the weaknesses and provides support for future students. In order to facilitate the task, educational data mining (EDM) techniques are utilized for constructing prediction models built from student academic historical records. (ese models present the embedded knowledge that is more readable and interpretable by humans. Hence, in this paper, the contributions are presented in three folds that include the following: (i) providing a thorough analysis about the selected features and their effects on the performance value using statistical analysis techniques, (ii) building and studying the performance of several classifiers from different families of machine learning (ML) techniques, (iii) proposing an ensemble meta-based tree model (EMT) classifier technique for predicting the student performance.(e experimental results show that the EMTas the ensemble technique gained a high accuracy performance reaching 98.5% (or 0.985). In addition, the proposed EMT technique obtains a high performance, which is a superior result compared to the other techniques. © 2019 Hindawi Limited. All rights reserved.",,"Data mining; Forecasting; Forestry; Learning systems; Academic fields; Educational data minings (EDM); Ensemble techniques; Historical records; Performance value; Prediction model; Student performance; Tree modeling; Students"
"Almeida A., Azevedo A.","A performance estimation framework for complex manufacturing systems","10.14809/faim.2014.0849","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960896772&doi=10.14809%2ffaim.2014.0849&partnerID=40&md5=76323f82efbfdc8b9298603ea46e7b63","To cope with today market challenges and guarantee adequate competitive performances, companies have been decreasing their products life cycles, as well as increasing the number of product varieties and respective services available on their portfolio. Consequently, it has been observed an increasing in complexity in all domains, from product and process development, factory and production planning to factory operation and management. This reality implies that organizations should be able to compile and analyze, in a more agile way, the immense quantity of data generated, as well as apply the suitable tools that, based on this knowledge, will supports stakeholders to take decision envisioning future performance scenarios. Aiming to pursuing this vision was developed a proactive performance management framework, composed by a performance thinking methodology and a performance estimation engine. While the methodology developed is an extension of the Systems Dynamics approach for complex systems' performance management, on the other hand, the performance estimation engine is an innovative IT solution responsible by capturing lagging indicators, as well as estimate future performance behaviors. As main outcome of this research work, it was demonstrated that following a systematic and formal approach, it is possible to identify the feedback loops and respective endogenous and exogenous variables responsible by hindering the systems behavior, in terms of a specific KPI. Moreover, based on this enhanced understanding about manufacturing systems behavior, it was proved to be possible to estimate with high levels of confidence not only the present but also future performance behavior. From the combination of both qualitative and quantitative approaches, it was explored an enhanced learning machine algorithm capable to specify the curve of behavior, characteristic from a specific manufacturing system, and thus estimate future behaviors based on a set of leading indicators. In order to achieve these objectives, both Neural Networks and Unscented Kalman Filter for nonlinear estimation were applied. Important results and conclusions were extracted from an application case performed within a real automotive plant, which demonstrated the feasibility of this research towards a more proactive management approach. © Copyright 2014 by DEStech Publications, Inc. All rights reserved.",,"Competition; Complex networks; Engineering education; Engines; Industrial plants; Life cycle; Machine learning; Production control; Competitive performance; Complex manufacturing systems; Non-linear estimation; Operation and management; Performance estimation; Performance management; Qualitative and quantitative approaches; Unscented Kalman Filter; Manufacture"
"Almeida E., Kosina P., Gama J.","Random rules from data streams","10.1145/2480362.2480518","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877997557&doi=10.1145%2f2480362.2480518&partnerID=40&md5=67e798bb44679d716a3f27c8f2cc23fe","Existing works suggest that random inputs and random features produce good results in classification. In this paper we study the problem of generating random rule sets from data streams. One of the most interpretable and flexible models for data stream mining prediction tasks is the Very Fast Decision Rules learner (VFDR). In this work we extend the VFDR algorithm using random rules from data streams. The proposed algorithm generates several sets of rules. Each rule set is associated with a set of Natt attributes. The proposed algorithm maintains all properties required when learning from stationary data streams: online and any-time classification, processing each example once. Copyright 2013 ACM.","Classification; Data streams; Random rules; Rule learning","Data stream; Data stream mining; Decision rules; Flexible model; Prediction tasks; Random features; Random rules; Rule learning; Algorithms; Classification (of information); Data communication systems; Data mining"
"Almeida J.L.S., Pires A.C., Cid K.F.V., Nogueira A.C.","Non-intrusive operator inference for chaotic systems","10.1109/TAI.2022.3207449","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139388062&doi=10.1109%2fTAI.2022.3207449&partnerID=40&md5=5a13d985c8d9e880ee706a0df5c783b4","This work explores the physics-driven machine learning technique Operator Inference (OpInf) for predicting the state of chaotic dynamical systems. OpInf provides a non-intrusive approach to infer approximations of polynomial operators in reduced space without having access to the full order operators appearing in discretized models. Datasets for the physics systems are generated using conventional numerical solvers and then projected to a low-dimensional space via Principal Component Analysis (PCA). In latent space, a least-squares problem is set to fit a quadratic polynomial operator, which is subsequently employed in a time-integration scheme in order to produce extrapolations in the same space. Once solved, the inverse PCA operation is applied to reconstruct the extrapolations in the original space. The quality of the OpInf predictions is assessed via the Normalized Root Mean Squared Error (NRMSE) metric from which the Valid Prediction Time (VPT) is computed. Numerical experiments considering the chaotic systems Lorenz 96 and the Kuramoto-Sivashinsky equation show promising forecasting capabilities of the OpInf reduced order models with VPT ranges that outperform state-of-the-art machine learning (ML) methods such as backpropagation and reservoir computing recurrent neural networks [1], as well as Markov neural operators [2]. IEEE","Artificial intelligence in industrial engineering; Computational modeling; Dynamical systems; Interpretable machine learning; Mathematical models; Numerical models; Predictive models; Read only memory; Simulation; Training","Backpropagation; Chaotic systems; Extrapolation; Forecasting; Inverse problems; Mean square error; Numerical methods; Partial differential equations; Personnel training; Principal component analysis; Recurrent neural networks; Artificial intelligence in industrial engineering; Computational modelling; Interpretable machine learning; Machine-learning; Polynomial operators; Prediction time; Predictive models; Principal-component analysis; Read-only memory; Simulation; Numerical models"
"Almeida R.N.D., Greenberg M., Bundalovic-Torma C., Martel A., Wang P.W., Middleton M.A., Chatterton S., Desveaux D., Guttman D.S.","Predictive modeling of Pseudomonas syringae virulence on bean using gradient boosted decision trees","10.1371/journal.ppat.1010716","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135598030&doi=10.1371%2fjournal.ppat.1010716&partnerID=40&md5=6f2d3e1463bc3801a2047ee8ad9356d6","Pseudomonas syringae is a genetically diverse bacterial species complex responsible for numerous agronomically important crop diseases. Individual P. syringae isolates are assigned pathovar designations based on their host of isolation and the associated disease symptoms, and these pathovar designations are often assumed to reflect host specificity although this assumption has rarely been rigorously tested. Here we developed a rapid seed infection assay to measure the virulence of 121 diverse P. syringae isolates on common bean (Phaseolus vulgaris). This collection includes P. syringae phylogroup 2 (PG2) bean isolates (pathovar syringae) that cause bacterial spot disease and P. syringae phylogroup 3 (PG3) bean isolates (pathovar phaseolicola) that cause the more serious halo blight disease. We found that bean isolates in general were significantly more virulent on bean than non-bean isolates and observed no significant virulence difference between the PG2 and PG3 bean isolates. However, when we compared virulence within PGs we found that PG3 bean isolates were significantly more virulent than PG3 non-bean isolates, while there was no significant difference in virulence between PG2 bean and non-bean isolates. These results indicate that PG3 strains have a higher level of host specificity than PG2 strains. We then used gradient boosting machine learning to predict each strain’s virulence on bean based on whole genome k-mers, type III secreted effector k-mers, and the presence/absence of type III effectors and phytotoxins. Our model performed best using whole genome data and was able to predict virulence with high accuracy (mean absolute error = 0.05). Finally, we functionally validated the model by predicting virulence for 16 strains and found that 15 (94%) had virulence levels within the bounds of estimated predictions. This study strengthens the hypothesis that P. syringae PG2 strains have evolved a different lifestyle than other P. syringae strains as reflected in their lower level of host specificity. It also acts as a proof-of-principle to demonstrate the power of machine learning for predicting host specific adaptation. © 2022 Almeida et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"plant toxin; Article; bacterial virulence; bacterium isolation; decision tree; DNA extraction; gene cluster; gene sequence; genetic variability; genome analysis; host range; machine learning; nonhuman; pangenome; prediction; Pseudomonas syringae; decision tree; microbiology; Phaseolus; plant disease; virulence; Decision Trees; Host Specificity; Phaseolus; Plant Diseases; Pseudomonas syringae; Virulence"
"Almentero B.K., Li J., Besse C.","Forecasting pharmacy purchases orders",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123404102&partnerID=40&md5=a9753d589acfc57fd53ee8752fc354ad","Inventory represents the largest asset in pharmacy products distribution. Forecasting pharmacy purchases is essential to keep an effective stock balancing supply and demand besides minimizing costs. In this work, we investigate how to forecast product purchases for a pharmaceutical distributor. The data contains inventory sale histories for more than 10 thousand active products during the past 15 years. We discuss challenges on data preprocessing of the pharmacy data including cleaning, feature constructions and selections, as well as processing data during the COVID period. We experimented on different machine learning and deep learning neural network models to predict future purchases for each product, including classical Seasonal Autoregressive Integrated Moving Average (SARIMA), Prophet from Facebook, linear regression, Random Forest, XGBoost and Long Short-Term Memory (LSTM). We demonstrate that a carefully designed SARIMA model outperformed the others on the task, and weekly prediction models perform better than daily predictions. © 2021 International Society of Information Fusion (ISIF).","COVID; Explainable AI; LSTM; Machine learning (ML); Regression; SARIMA; SHAP value; Time series forecasting","Balancing; Data handling; Decision trees; Forecasting; Sales; COVID; Explainable AI; Forecast products; Machine learning; Minimizing costs; Product distributions; Purchase orders; Seasonal autoregressive integrated moving averages; SHAP value; Time series forecasting; Long short-term memory"
"Al-Merri M., Miled Z.B.","Global Translation of Classification Models","10.3390/info13050246","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130514048&doi=10.3390%2finfo13050246&partnerID=40&md5=28f81c10f1b56374ada250d457c7a230","The widespread and growing usage of machine learning models, particularly for critical areas such as law, predicate the need for global interpretability. Models that cannot be audited are vulnerable to biases inherited from the datasets that were used to develop them. Moreover, locally interpretable models are vulnerable to adversarial attacks. To address this issue, the present paper proposes a new methodology that can translate any existing machine learning model into a globally interpretable one. MTRE-PAN is a hybrid SVM-decision tree architecture that leverages the interpretability of linear hyperplanes by creating a set of polygons that delimit the decision boundaries of the target model. Moreover, the present paper introduces two new metrics: certain and boundary model parities. These metrics can be used to accurately evaluate the performance of the interpretable model near the decision boundaries. These metrics are used to compare MTRE-PAN to a previously proposed interpretable architecture called TRE-PAN. As in the case of TRE-PAN, MTRE-PAN aims at providing global interpretability. The comparisons are performed over target models developed using three benchmark datasets: Abalone, Census and Diabetes data. The results show that MTRE-PAN generates interpretable models that have a lower number of leaves and a higher agreement with the target models, especially around the most important regions in the feature space, namely the decision boundaries. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","explainability; explainable AI; global interpretation; machine learning; translation; TRE-PAN","Support vector machines; Classification models; Decision boundary; Explainability; Explainable AI; Global interpretation; Interpretability; Machine learning models; Target model; Translation; TRE-PAN; Decision trees"
"Almiñana M., Escudero L.F., Pérez-Martín A., Rabasa A., Santamaría L.","A classification rule reduction algorithm based on significance domains","10.1007/s11750-012-0264-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898538721&doi=10.1007%2fs11750-012-0264-6&partnerID=40&md5=b72152de8f347c1018d1edc4a4eae483","Many rule systems generated from decision trees (like CART, ID3, C4.5, etc.) or from direct counting frequency methods (like Apriori) are usually non-significant or even contradictory. Nevertheless, most papers on this subject demonstrate that important reductions can be made to generate rule sets by searching and removing redundancies and conflicts and simplifying the similarities between them. The objective of this paper is to present an algorithm (RBS: Reduction Based on Significance) for allocating a significance value to each rule in the system so that experts may select the rules that should be considered as preferable and understand the exact degree of correlation between the different rule attributes. Significance is calculated from the antecedent frequency and rule frequency parameters for each rule; if the first one is above the minimal level and rule frequency is in a critical interval, its significance ratio is computed by the algorithm. These critical boundaries are calculated by an incremental method and the rule space is divided according to them. The significance function is defined for these intervals. As with other methods of rule reduction, our approach can also be applied to rule sets generated from decision trees or frequency counting algorithms, in an independent way and after the rule set has been created. Three simulated data sets are used to carry out a computational experiment. Other standard data sets from UCI repository (UCI Machine Learning Repository) and two particular data sets with expert interpretation are used too, in order to obtain a greater consistency. The proposed method offers a more reduced and more easily understandable rule set than the original sets, and highlights the most significant attribute correlations quantifying their influence on consequent attribute. © 2012 Sociedad de Estadística e Investigación Operativa.","Classification rules; Reduction; Significance measures",
"Almobarek M., Alrshdan A.","Monitoring water consumption using machine learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114226128&partnerID=40&md5=c32545cd9f4ffb9305bbb082806c924f","This paper concerns one of the most important elements in economic life, which is water. Normally, the user, who is responsible to manage water services, is not aware about the consumption situation prior to issuing the bill, so, this research proposes innovative managerial techniques to monitor water consumption across the network. Machine learning is used through clustering techniques to achieve the monitoring goal, which contributes towards decision making, fault prediction, and data management processes.in facility management. Artificial Neural Network is the specific part of the chosen technique and Python is used as a tool to implement the method. A case study is applied in one of the universities and a network including six locations is studied at a specific time. The method shows a significant result where the consumption at three locations were found high and accordingly, the user made further inspection and continuous monitoring about the network in discussion. © IEOM Society International.","Artificial Neural Network; Machine Learning; Monitoring and Clustering; Water Consumption",
"Almon R.R., DuBois D.C., Jin J.Y., Jusko W.J.","Temporal profiling of the transcriptional basis for the development of corticosteroid-induced insulin resistance in rat muscle","10.1677/joe.1.05953","https://www.scopus.com/inward/record.uri?eid=2-s2.0-13444250955&doi=10.1677%2fjoe.1.05953&partnerID=40&md5=6c6cefb5bbd5f9bf01288a7db051a9c6","Elevated systemic levels of glucocorticoids are causally related to peripheral insulin resistance. The pharmacological use of synthetic glucocorticoids (corticosteroids) often results in insulin resistance/type II diabetes. Skeletal muscle is responsible for close to 80% of the insulin-induced systemic disposal of glucose and is a major target for glucocorticoid-induced insulin resistance. We used Affymetrix gene chips to profile the dynamic changes in mRNA expression in rat skeletal muscle in response to a single bolus dose of the synthetic glucocorticoid methylprednisolone. Temporal expression profiles (analyzed on individual chips) were obtained from tissues of 48 drug-treated animals encompassing 16 time points over 72 h following drug administration along with four vehicle-treated controls. Data mining identified 653 regulated probe sets out of 8799 present on the chip. Of these 653 probe sets we identified 29, which represented 22 gene transcripts, that were associated with the development of insulin resistance. These 29 probe sets were regulated in three fundamental temporal patterns. 16 probe sets coding for 12 different genes had a profile of enhanced expression. 10 probe sets coding for eight different genes showed decreased expression and three probe sets coding for two genes showed biphasic temporal signatures. These transcripts were grouped into four general functional categories: signal transduction, transcription regulation, carbohydrate/fat metabolism, and regulation of blood flow to the muscle. The results demonstrate the polygenic nature of transcriptional changes associated with insulin resistance that can provide a temporal scaffolding for translational and post-translational data as they become available. © 2005 Society for Endocrinology.",,"carbohydrate; insulin; lipid; messenger RNA; methylprednisolone; animal experiment; animal model; animal tissue; article; carbohydrate metabolism; controlled study; DNA microarray; DNA probe; drug induced disease; gene expression profiling; genetic transcription; insulin resistance; lipid metabolism; male; muscle blood flow; nonhuman; nucleotide sequence; pathogenesis; priority journal; protein processing; rat; signal transduction; skeletal muscle; transcription regulation; Animals; Gene Expression; Glucocorticoids; Insulin Resistance; Male; Methylprednisolone; Muscle, Skeletal; Oligonucleotide Array Sequence Analysis; Rats; Rats, Wistar; Time Factors; Transcription, Genetic"
"Almseidin M., Al-Sawwa J., Alkasassbeh M., Alweshah M.","On detecting distributed denial of service attacks using fuzzy inference system","10.1007/s10586-022-03657-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135344004&doi=10.1007%2fs10586-022-03657-5&partnerID=40&md5=fb9d40609844d37ea9dc1313b943e7ba","Nowadays, attackers are constantly targeting the modern aspects of technology and attempting to abuse these technologies using different attacks types such as the distributed denial of service attack (DDoS). Therefore, protecting web services is not an easy task. There is a critical demand to detect and prevent DDoS attacks. This paper introduces a fuzzy inference-based anomaly-based intrusion detection (IDS) system to detect DDoS attacks. The aim of using the fuzzy inference system is to avoid binary decisions and, meanwhile, to avoid the issues associated with the deficiencies of IDS alert system awareness. This benefit could improve the IDS alert system’s robustness and effectively produce more readable and understandable IDS alerts. The proposed detection model was applied to a recent open-source DDoS dataset. At the early stage of designing the proposed detection model, the DDoS dataset was preprocessed using the Info-gain features selection algorithm to deal with the relevant features only and reduce the complexity of the fuzzy inference system. The proposed detection model was tested, evaluated, and obtained a 96.25% accuracy rate and a false-positive rate of 0.006%. Moreover, it effectively smoothes the boundaries between normal and DDoS traffic. In addition, the results obtained from the proposed detection model were compared with other literature results. The results indicated that the detection accuracy of this work is competitive with other methods. In addition to this, this work offers more elements of trust in DDoS attack detection by following the strategy to avoid the binary decision and offering the required extension of the binary decision to the continuous space; hence, the attack level could be easily measured. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Distributed denial of service attack (DDoS); Fuzzy inference system; Intrusion datasets; Intrusion detection system (IDS); Machine learning","Fuzzy inference; Fuzzy systems; Intrusion detection; Machine learning; Network security; Web services; Binary decision; Denialof- service attacks; Detection models; Distributed denial of service; Distributed denial of service attack; Fuzzy inference systems; Intrusion dataset; Intrusion detection system; Intrusion Detection Systems; Machine-learning; Denial-of-service attack"
"Al-Mudimigh A.S., Ullah Z., Saleem F.","Data mining strategies and techniques for CRM systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-74949128538&partnerID=40&md5=096c7de1c1fc39ae9390b1cfc1a7110a","Whenever millions of data is being stored in database regularly, data mining is responsible to discover the hidden knowledge, rules and patterns from it. Data mining is going to be involved in every organization for extracting extra information which are not visible for everyone. Organizations always planning to get useful information from it. Though, study on customer relationship management (CRM) is reaching more practical and attractive factor for the growth of every organization in the same way, discovery the hidden gold is also supporting to achieve the goal and for the success of organization. The main critical success factor for any (CRM) includes, Marketing Management, Customer Support Management, Sales Management and Facilities Management, etc. In this paper we proposed, analyzed and validated that data mining is also a major success factor in the success of CRM. We first presented the CRM model and then explained the main role of each feature, then we add data mining feature in the CRM model. Further more, we applied data mining strategies and techniques for the generation of new rules and patterns. We talk about that within the boundaries of CRM strategies the data mining tool also play an affective and valuable role for the establishment and growth of the organization. © 2009 IEEE.","Critical success factor of CRM; Customer relationship management; Data mining techniques","Critical success factor; CRM model; CRM systems; Customer relationship management; Customer support; Data-mining tools; Facilities management; Hidden knowledge; Major success factors; Marketing management; Sales management; Customer satisfaction; Management; Public relations; Sales; Strategic planning; Systems engineering; Data mining"
"Almutairi M., Stahl F., Bramer M.","ReG-Rules: An Explainable Rule-Based Ensemble Learner for Classification","10.1109/ACCESS.2021.3062763","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101842589&doi=10.1109%2fACCESS.2021.3062763&partnerID=40&md5=5b1e0a9e1f93d28a510e20622ff093cc","The learning of classification models to predict class labels of new and previously unseen data instances is one of the most essential tasks in data mining. A popular approach to classification is ensemble learning, where a combination of several diverse and independent classification models is used to predict class labels. Ensemble models are important as they tend to improve the average classification accuracy over any member of the ensemble. However, classification models are also often required to be explainable to reduce the risk of irreversible wrong classification. Explainability of classification models is needed in many critical applications such as stock market analysis, credit risk evaluation, intrusion detection, etc. Unfortunately, ensemble learning decreases the level of explainability of the classification, as the analyst would have to examine many decision models to gain insights about the causality of the prediction. The aim of the research presented in this paper is to create an ensemble method that is explainable in the sense that it presents the human analyst with a conditioned view of the most relevant model aspects involved in the prediction. To achieve this aim the authors developed a rule-based explainable ensemble classifier termed Ranked ensemble G-Rules (ReG-Rules) which gives the analyst an extract of the most relevant classification rules for each individual prediction. During the evaluation process ReG-Rules was evaluated in terms of its theoretical computational complexity, empirically on benchmark datasets and qualitatively with respect to the complexity and readability of the induced rule sets. The results show that ReG-Rules scales linearly, delivers a high accuracy and at the same time delivers a compact and manageable set of rules describing the predictions made. © 2013 IEEE.","Data mining; ensemble learning; explainable algorithms; rule-based classification","Data mining; Forecasting; Intrusion detection; Risk assessment; Classification accuracy; Classification models; Classification rules; Credit risk evaluation; Critical applications; Ensemble classifiers; Individual prediction; Stock market analysis; Learning systems"
"Al-Najjar H.A.H., Pradhan B., Beydoun G., Sarkar R., Park H.-J., Alamri A.","A novel method using explainable artificial intelligence (XAI)-based Shapley Additive Explanations for spatial landslide prediction using Time-Series SAR dataset","10.1016/j.gr.2022.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136507569&doi=10.1016%2fj.gr.2022.08.004&partnerID=40&md5=f616c7c36b6ffed10c987d9568e17d0e","As artificial intelligence (AI) techniques are becoming more popular in landslide modeling, it is important to understand how decisions are made. Fairness, and transparency becomes ever more vital due to ethical concerns and trust. Despite the popularity of machine learning (ML) algorithms in landslide modeling, the explainability of these methods are often considered as black box. This paper aims to propose an explainable artificial intelligence (XAI) for landslide prediction using synthetic-aperture radar (SAR) time-series data, NDVI (normalized difference vegetation index) time-series data and other geo-environmental factors such as DEM (digital elevation model) derivatives. We employed a Shapley Additive Explanations (SHAP) approach to understand how and what decisions ML-based models are making. 37 features were extracted from various sources such as ALOS-PALSAR (ALOS Phased Array type L-band Synthetic Aperture Radar), ALOS-2 (SAR), Landsat-8, topographic maps, and DEM for landslide susceptibility mapping in a landslide prone area in Chukha, Bhutan as a test site. The result was then compared using two standard ML methods: random forest (RF) and support vector machine (SVM). As per results, the RF model outperformed (0.914) the SVM. Moreover, the higher reliability of the RF model was proved by the area under the curve (AUC) of 0.941. XAI results revealed, features like altitude, aspect, NDVI-2014, NDVI-2017, and NDVI-2018 were the most effective features for landslide prediction by both models. Interestingly, among those features, NDVI-2014, aspect, and NDVI-2017 negatively correlated with the landslide prediction; whereas positively correlated when SVM was utilized. This interpretation ability indicates the advantages of XAI over the conventional methods as it measures the impact, interaction and correlation of conditioning factors within a model. The current research finding can provide more transparency and explainability when working with MLs in landslide studies. This could help to build trust among the geoscientists and decision-makers while making geohazard prediction. © 2022 International Association for Gondwana Research","Bhutan; Explainable artificial intelligence; Landslide susceptibility; SAR; Shapley Additive Explanations; XAI",
"Al-Nashashibi M., Hadi W., El-Khalili N., Issa G., Albanna A.A.","A new two-step ensemble learning model for improving stress prediction of automobile drivers","10.34028/iajit/18/6/9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120831779&doi=10.34028%2fiajit%2f18%2f6%2f9&partnerID=40&md5=f015c8342e7d461a423847976be5fb55","Commuting when there is a significant volume of traffic congestion has been acknowledged as one of the key factors causing stress. Significant levels of stress whilst driving are seen to have a profoundly negative effect on the actions and ability of a driver; this has the capacity to result in risks, hazards and accidents. As such, there is a recognized need to determine drivers’ levels of stress and accordingly predict the key causes responsible for high levels of stress. In this work, the objective is centred on providing an ensemble machine learning framework in order to determine the stress levels of drivers. Moreover, the study also provides a fresh set of data, as gathered from 14 different drivers, with data collection having taken place during driving in Amman, Jordan. Data was gathered via the implementation of a wearable biomedical instrument that was attached to the driver on a continuous basis in order to gather physiological data. The data gathered was accordingly categorised into two different groups: ‘Yes’, which represents the presence of stress, whilst ‘No’ represents the absence of stress. Importantly, in an effort to circumvent the negative impact of driver instances with a minority class on stress predictions, oversampling technique was applied. A two-step ensemble classifier was developed through bringing together the findings from random forest, decision tree, and Repeated Incremental Pruning to Produce Error Reduction (RIPPER) classifiers, which was then inputted into a Multi-Layer Perceptron neural network. The experimental findings highlight that the suggested framework is far more precise and has a more scalable capacity when compared with all classifiers in relation to accuracy, g-mean measures and sensitivity. © 2021, Zarka Private University. All rights reserved.","Data mining algorithms; Ensemble learning; Oversampling; Stress prediction",
"Alobaidi M., Malik K.M., Hussain M.","Automated ontology generation framework powered by linked biomedical ontologies for disease-drug domain","10.1016/j.cmpb.2018.08.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052192591&doi=10.1016%2fj.cmpb.2018.08.010&partnerID=40&md5=5998c92511671c0e5a9cc9573e374c39","Objective and background: The exponential growth of the unstructured data available in biomedical literature, and Electronic Health Record (EHR), requires powerful novel technologies and architectures to unlock the information hidden in the unstructured data. The success of smart healthcare applications such as clinical decision support systems, disease diagnosis systems, and healthcare management systems depends on knowledge that is understandable by machines to interpret and infer new knowledge from it. In this regard, ontological data models are expected to play a vital role to organize, integrate, and make informative inferences with the knowledge implicit in that unstructured data and represent the resultant knowledge in a form that machines can understand. However, constructing such models is challenging because they demand intensive labor, domain experts, and ontology engineers. Such requirements impose a limit on the scale or scope of ontological data models. We present a framework that will allow mitigating the time-intensity to build ontologies and achieve machine interoperability. Methods: Empowered by linked biomedical ontologies, our proposed novel Automated Ontology Generation Framework consists of five major modules: a) Text Processing using compute on demand approach. b) Medical Semantic Annotation using N-Gram, ontology linking and classification algorithms, c) Relation Extraction using graph method and Syntactic Patterns, d), Semantic Enrichment using RDF mining, e) Domain Inference Engine to build the formal ontology. Results: Quantitative evaluations show 84.78% recall, 53.35% precision, and 67.70% F-measure in terms of disease-drug concepts identification; 85.51% recall, 69.61% precision, and F-measure 76.74% with respect to taxonomic relation extraction; and 77.20% recall, 40.10% precision, and F-measure 52.78% with respect to biomedical non-taxonomic relation extraction. Conclusion: We present an automated ontology generation framework that is empowered by Linked Biomedical Ontologies. This framework integrates various natural language processing, semantic enrichment, syntactic pattern, and graph algorithm based techniques. Moreover, it shows that using Linked Biomedical Ontologies enables a promising solution to the problem of automating the process of disease-drug ontology generation. © 2018 Elsevier B.V.","Linked biomedical ontologies; Ontology generation; Semantic web","Artificial intelligence; Automation; Decision support systems; Diagnosis; Extraction; Health care; Inference engines; Natural language processing systems; Semantic Web; Syntactics; Text processing; Biomedical ontologies; Classification algorithm; Clinical decision support systems; Electronic health record; Health care application; Healthcare management systems; Ontology generation; Quantitative evaluation; Ontology; drug; Article; automation; clinical decision support system; diseases; electronic health record; health care management; machine learning; medical ontology; ontology development; semantics; word processing; algorithm; biological ontology; data mining; diseases; drug therapy; human; knowledge base; statistics and numerical data; Algorithms; Biological Ontologies; Data Mining; Decision Support Systems, Clinical; Disease; Drug Therapy; Humans; Knowledge Bases; Machine Learning; Semantics"
"Aloimonos Y., Fermüller C.","Analyzing action representations","10.1007/10722492_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937429260&doi=10.1007%2f10722492_1&partnerID=40&md5=77f8780d7f685218e1c978588fdd094e","We argue that actions represent the basic seed of intelligence underlying perception of the environment, and the representations encoding actions should be the starting point upon which further studies of cognition are built. In this paper we make a first effort in characterizing these action representations. In particular, from the study of simple actions related to 3D rigid motion interpretation, we deduce a number of principles for the possible computations responsible for the interpretation of space-time geometry. Using these principles, we then discuss possible avenues on how to proceed in analyzing the representations of more complex human actions. © Springer-Verlag Berlin Heidelberg 2000.",,"Artificial intelligence; Computer science; Computers; 3D rigid motion; Action representations; Human actions; Space time geometry; Algebra"
"Alokaili A., Aletras N., Stevenson M.","Re-ranking words to improve interpretability of automatically generated topics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083989478&partnerID=40&md5=21dd47e6e5818e6646b70cf1b1970b3c","Topics models, such as LDA, are widely used in Natural Language Processing. Making their output interpretable is an important area of research with applications to areas such as the enhancement of exploratory search interfaces and the development of interpretable machine learning models. Conventionally, topics are represented by their n most probable words, however, these representations are often difficult for humans to interpret. This paper explores the re-ranking of topic words to generate more interpretable topic representations. A range of approaches are compared and evaluated in two experiments. The first uses crowdworkers to associate topics represented by different word rankings with related documents. The second experiment is an automatic approach based on a document retrieval task applied on multiple domains. Results in both experiments demonstrate that re-ranking words improves topic interpretability and that the most effective re-ranking schemes were those which combine information about the importance of words both within topics and their relative frequency in the entire corpus. In addition, close correlation between the results of the two evaluation approaches suggests that the automatic method proposed here could be used to evaluate re-ranking methods without the need for human judgements. © 2019 IWCS 2019 - Proceedings of the 13th International Conference on Computational Semantics - Long Papers. All rights reserved.",,"Learning algorithms; Semantics; Automatic approaches; Automatically generated; Document Retrieval; Exploratory search; Interpretability; Machine learning models; Re-ranking; Search interfaces; Topic Modeling; Topic words; Natural language processing systems"
"Alomari M., Duckworth P., Bore N., Hawasly M., Hogg D.C., Cohn A.G.","Grounding of human environments and activities for autonomous robots","10.24963/ijcai.2017/193","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031897942&doi=10.24963%2fijcai.2017%2f193&partnerID=40&md5=71dfa93e4142244931b04467e73e8c00","With the recent proliferation of human-oriented robotic applications in domestic and industrial scenarios, it is vital for robots to continually learn about their environments and about the humans they share their environments with. In this paper, we present a novel, online, incremental framework for unsupervised symbol grounding in real-world, human environments for autonomous robots. We demonstrate the flexibility of the framework by learning about colours, people names, usable objects and simple human activities, integrating stateofthe-art object segmentation, pose estimation, activity analysis along with a number of sensory input encodings into a continual learning framework. Natural language is grounded to the learned concepts, enabling the robot to communicate in a human-understandable way. We show, using a challenging real-world dataset of human activities as perceived by a mobile robot, that our framework is able to extract useful concepts, ground natural language descriptions to them, and, as a proof-ofconcept, generate simple sentences from templates to describe people and the activities they are engaged in.",,"Artificial intelligence; Sensory analysis; Activity analysis; Continual learning; Human environment; Industrial scenarios; Natural languages; Object segmentation; Robotic applications; Symbol grounding; Robots"
"Alonso Á., Villar J.R., Benavides C., García I., Rodríguez F.","An approach to document engineering at Fundación Hullera Vasco-Leonesa","10.3182/20020721-6-es-1901.01454","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945542822&doi=10.3182%2f20020721-6-es-1901.01454&partnerID=40&md5=6cba479659fe158a1b701664ee58e83d","Fundación Hullera Vasco-Leonesa is a company with a documental department responsible for managing the bibliographic information the company uses. That department manually elaborates and distributes periodic documents (press bulletins, environmental dossiers, etc). This paper describes an intelligent multiagent system as a way to solve the work handled by this department. The objective was to design and implement a digital library with all the tasks needed, like query management, automatic design and generation of electronic documents, selective information distribution, etc. Copyright © 2002 IFAC.","Agents; Computer applications; Distributed artificial intelligence; Intelligent knowledge based systems; Learning systems; Reasoning","Agents; Automation; Computer applications; Digital libraries; Knowledge based systems; Learning systems; Multi agent systems; Bibliographic information; Design and implements; Distributed Artificial Intelligence; Document engineering; Electronic document; Information distributions; Intelligent multi agent systems; Reasoning; Distributed computer systems"
"Alonso Gonzalez C.J., Acosta Lazo G.G., De Prada Moraga C., Mira Mira J.","Knowledge based approach to fault detection and diagnosis in industrial processes: A case study",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028736886&partnerID=40&md5=63e84a0060a5a1411d1f9382384156ef","The article proposes a scheme for on-line fault detection and diagnosis using an expert system. This approach is been testing on a beet-sugar factory at Spain. A great deal of critical situations may arise in the normal operation of such a process. They are now managed by plant operators and cover both cases: faulty equipment diagnosis, which requires module substitution, as well as the detection of major improper control actions that may cause plant problems in the long run. This prototype not only comprises aspects related with fault detection and diagnosis but also is responsible of supervisory tasks to improve the whole process performance. The paper includes a description of the system architecture, a detailed presentation of the fault detection and diagnosis modules, some evaluation of the results obtained by the system operation at the factory and our conclusions.",,"Artificial intelligence; Computer architecture; Control system analysis; Diagnosis; Distributed parameter control systems; Failure (mechanical); Interfaces (computer); Monitoring; Process control; Processing; Sugar beets; Faulty equipment diagnosis; On line fault detection; Expert systems"
"Alonso J.M., Barro S., Bugarín A., van Deemter K., Gardent C., Gatt A., Reiter E., Sierra C., Theune M., Tintarev N., Yano H., Budzynska K.","Interactive Natural Language Technology for Explainable Artificial Intelligence","10.1007/978-3-030-73959-1_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105935557&doi=10.1007%2f978-3-030-73959-1_5&partnerID=40&md5=1a7ea35680115cb172f5ebaa31f7ddb9","We have defined an interdisciplinary program for training a new generation of researchers who will be ready to leverage the use of Artificial Intelligence (AI)-based models and techniques even by non-expert users. The final goal is to make AI self-explaining and thus contribute to translating knowledge into products and services for economic and social benefit, with the support of Explainable AI systems. Moreover, our focus is on the automatic generation of interactive explanations in natural language, the preferred modality among humans, with visualization as a complementary modality. © 2021, Springer Nature Switzerland AG.","Argumentative conversational agents; Explainable Artificial Intelligence; Human-centered modeling; Human-machine persuasive interaction; Multi-modal explanations; Trustworthiness","Natural language processing systems; AI systems; Automatic Generation; Economic and social benefits; Expert users; Interdisciplinary programs; Natural languages; Products and services; Artificial intelligence"
"Alonso J.M., Toja-Alamancos J., Bugarin A.","Experimental study on generating multi-modal explanations of black-box classifiers in terms of gray-box classifiers","10.1109/FUZZ48607.2020.9177770","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090496626&doi=10.1109%2fFUZZ48607.2020.9177770&partnerID=40&md5=b44c964ec9c573b82dbd62334e857367","Artificial Intelligence (AI) is a first class citizen in the cities of the 21st century. In addition, trust, fairness, accountability, transparency and ethical issues are considered as hot topics regarding AI-based systems under the umbrella of Explainable AI (XAI). In this paper we have conducted an experimental study with 15 datasets to validate the feasibility of using a pool of gray-box classifiers (i.e., decision trees and fuzzy rule-based classifiers) to automatically explain a black-box classifier (i.e., Random Forest). Reported results validate our approach. They confirm the complementarity and diversity among the gray-box classifiers under study, which are able to provide users with plausible multi-modal explanations of the considered black-box classifier for all given datasets. © 2020 IEEE.","Classification; Explainable Artificial Intelligence; Interpretable Machine Learning; Open Source Software","Artificial intelligence; Decision trees; Fuzzy inference; Fuzzy systems; Black boxes; Ethical issues; Fuzzy rule-based classifier; Hot topics; Multi-modal; Classification (of information)"
"Alonso J.M., Ducange P., Pecori R., Vilas R.","Building explanations for fuzzy decision trees with the expliclas software","10.1109/FUZZ48607.2020.9177725","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089584790&doi=10.1109%2fFUZZ48607.2020.9177725&partnerID=40&md5=3dc6590d1f6254f99645900a2f2c656b","Fairness, Accountability, Transparency and Explainability have become strong requirements in most practical applications of Artificial Intelligence (AI). Fuzzy sets and systems are recognized world-wide because of their outstanding contribution to model AI systems with a good interpretability-accuracy trade-off. Accordingly, fuzzy sets and systems are at the core of the so-called Explainable AI. ExpliClas is a software as a service which paves the way for interpretable and self-explainable intelligent systems. Namely, this software provides users with both graphical visualizations and textual explanations associated with intelligent classifiers automatically learned from data. This paper presents the new functionality of ExpliClas regarding the generation, evaluation and explanation of fuzzy decision trees along with fuzzy inference-grams. This new functionality is validated with two well-known classification datasets (i.e., Wine and Pima), but also with a real-world beer-style classifier. © 2020 IEEE.","Explainable AI; Fuzzy Rule-based Systems; Fuzzy Systems Software; Open Source Software; Software as a Service","Beer; Classification (of information); Decision trees; Economic and social effects; Forestry; Fuzzy sets; Intelligent systems; Software as a service (SaaS); AI systems; Classification datasets; Fuzzy decision trees; Fuzzy sets and systems; Graphical visualization; Intelligent classifiers; Interpretability; Real-world; Fuzzy inference"
"Alonso J.M.","Explainable artificial intelligence for kids",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089592709&partnerID=40&md5=7d8fb2cb0e358cca570616c919a35e80","Artificial Intelligence (AI) is part of our everyday life and has become one of the most outstanding and strategic technologies of the 21st century. Explainable AI (XAI in short) is expected to endow AI systems with explanation ability when interacting with humans. This paper describes how to provide kids with natural explanations, i.e., explanations verbalized in Natural Language, in the context of identifying/recognizing roles of basketball players. Semantic grounding is achieved through fuzzy concepts such as tall or short. Selected players are automatically classified by an ensemble of three different decision trees and one fuzzy rule-based classifier. All the single classifiers were first trained with the open source Weka software and then natural explanations were generated by the open source web service ExpliClas. The Human-Computer Interaction interface is implemented in Scratch, that is a visual programming language adapted to kids. The developed Scratch program is used for dissemination purposes when high-school teenagers visit the Research Center in Intelligent Technologies of the University of Santiago de Compostela. Copyright © 2019, the Authors. Published by Atlantis Press. This is an open access article under the CC BY-NC license (http://creativecommons.org/licenses/by-nc/4.0/).","Decision Trees; Explainable AI; Fuzzy rule-based Classifiers; Human-Computer Interaction; Natural Language Generation","Basketball; Computer circuits; Decision trees; Fuzzy inference; Fuzzy logic; Human computer interaction; Open source software; Open systems; Semantics; Visual languages; Web services; Fuzzy concept; Fuzzy rule-based classifier; Human computer interaction interface; Intelligent technology; Natural languages; Research center; Strategic technologies; Visual programming languages; Artificial intelligence"
"Alonso J.M.","Teaching explainable artificial intelligence to high school students","10.2991/ijcis.d.200715.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089591774&doi=10.2991%2fijcis.d.200715.003&partnerID=40&md5=39a35f4cf486d4fa68be4f74056ce78a","Artificial Intelligence (AI) is part of our everyday life and has become one of the most outstanding and strategic technologies. Explainable AI (XAI) is expected to endow intelligent systems with fairness, accountability, transparency and explanation ability when interacting with humans. This paper describes how to teach fundamentals of XAI to high school students who take part in interactive workshop activities at CiTIUS-USC. These workshop activities are carried out in the context of a strategic plan for promoting careers on Science, Technology, Engineering and Mathematics. Students learn (1) how to build datasets free of bias, (2) how to build interpretable classifiers and (3) how to build multi-modal explanations. © 2020 The Authors. Published by Atlantis Press B.V.","Decision trees; Educational AI resources; Explainable artificial intelligence; Fuzzy rule-based classifiers; Interpretable computational intelligence","Classification (of information); Intelligent systems; Professional aspects; High school students; Multi-modal; Science , Technology , Engineering and Mathematics; Strategic plan; Strategic technologies; Students"
"Alonso J.M., Bugarin A.","ExpliClas: Automatic Generation of Explanations in Natural Language for Weka Classifiers","10.1109/FUZZ-IEEE.2019.8859018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073783029&doi=10.1109%2fFUZZ-IEEE.2019.8859018&partnerID=40&md5=e3dc5bb0c8616ef0776e0ccf115e0b84","ExpliClas is a web service aimed at providing users with multimodal (textual + graphical) explanations related to Weka classifiers. In ExpliClas, two types of explanations are automatically generated. On the one hand, global explanations pay attention to the behavior of the classifier as a whole, i.e., they refer to a list of structural properties (number of classes, features, etc.) along with quality indicators such as accuracy or confusion matrix. On the other hand, local explanations go in depth with how the classifier deals with single instances. Current version of ExpliClas already explains classifications made by three different decision tree Weka implementations (J48, RepTree, and RandomTree) and one fuzzy algorithm (FURIA). In this paper, we describe ExpliClas in detail and illustrate its use with the Bank telemarketing dataset. © 2019 IEEE.","Decision Trees; Explainable Artificial Intelligence; Fuzzy Unordered Rule Induction Algorithm; Natural Language Generation; Open Source Software; Weka","Decision trees; Fuzzy sets; Natural language processing systems; Open systems; Trees (mathematics); Web services; Automatic Generation; Automatically generated; Confusion matrices; Natural language generation; Natural languages; Quality indicators; Rule induction algorithms; Weka; Open source software"
"Alonso J.M., Casalino G.","Explainable Artificial Intelligence for Human-Centric Data Analysis in Virtual Learning Environments","10.1007/978-3-030-31284-8_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075219702&doi=10.1007%2f978-3-030-31284-8_10&partnerID=40&md5=67ee23cb823f6d4afff73f4e99ed0b07","The amount of data to analyze in virtual learning environments (VLEs) grows exponentially everyday. The daily interaction of students with VLE platforms represents a digital foot print of the students’ engagement with the learning materials and activities. This big and worth source of information needs to be managed and processed to be useful. Educational Data Mining and Learning Analytics are two research branches that have been recently emerged to analyze educational data. Artificial Intelligence techniques are commonly used to extract hidden knowledge from data and to construct models that could be used, for example, to predict students’ outcomes. However, in the educational field, where the interaction between humans and AI systems is a main concern, there is a need of developing new Explainable AI (XAI) systems, that are able to communicate, in a human understandable way, the data analysis results. In this paper, we use an XAI tool, called ExpliClas, with the aim of facilitating data analysis in the context of the decision-making processes to be carried out by all the stakeholders involved in the educational process. The Open University Learning Analytics Dataset (OULAD) has been used to predict students’ outcome, and both graphical and textual explanations of the predictions have shown the need and the effectiveness of using XAI in the educational field. © 2019, Springer Nature Switzerland AG.","Data science; Educational Data Mining; Explainable AI; Trustworthy AI; Virtual learning environments","Artificial intelligence; Behavioral research; Computer aided instruction; Data handling; Data Science; Decision making; E-learning; Education computing; Forecasting; Learning systems; Students; Artificial intelligence techniques; Decision making process; Educational data mining; Educational process; Learning materials; Open universities; Virtual learning environments; Virtual learning environments (VLEs); Data mining"
"Alonso J.M.","From Zadeh’s Computing with Words Towards eXplainable Artificial Intelligence","10.1007/978-3-030-12544-8_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062781907&doi=10.1007%2f978-3-030-12544-8_21&partnerID=40&md5=cfba95438fd35c3357d09f1e520cc916","The European Commission has identified Artificial Intelligence (AI) as the “most strategic technology of the 21st century” [7]. © 2019, Springer Nature Switzerland AG.","Cointension; Computing with perceptions; Computing with words; Explainable AI; Fuzzy Logic; Interpretable fuzzy systems","Artificial intelligence; Computer circuits; Soft computing; Cointension; Computing with word (CWW); European Commission; Strategic technologies; Fuzzy logic"
"Alonso J.M., Ramos-Soto A., Castiello C., Mencar C.","Explainable AI beer style classifier",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054937050&partnerID=40&md5=c18b0d58e81acd280c484f2c0b78d8f2","This paper describes how to build an eXplainable Artificial Intelligent (XAI) classifier for a real use case related to beer style classification. It combines an opaque machine learning algorithm (Random Forest) with an interpretable machine learning algorithm (Decision Tree). The result is a XAI classifier which provides users with a good interpretability-accuracy trade-off but also with explanation capabilities. First, the opaque algorithm acts as an “oracle” which finds out the most plausible output. Then, we generate a textual explanation of the given output which emerges as an automatic interpretation of the inference process carried out by the related decision tree, if the outputs from both classifiers coincide. We apply a Natural Language Generation Approach to generate the textual explanations. © 2018 CEUR-WS. All Rights Reserved.","Classification Task; Explainable Artificial Intelligence; Natural Language Generation","Artificial intelligence; Beer; Data mining; Decision trees; Economic and social effects; Learning systems; Natural language processing systems; Artificial intelligent; Classification tasks; Inference process; Interpretability; Natural language generation; Random forests; Trade off; Use-case; Learning algorithms"
"Alonso J.M., Castiello C., Mencar C.","A bibliometric analysis of the explainable artificial intelligence research field","10.1007/978-3-319-91473-2_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048238356&doi=10.1007%2f978-3-319-91473-2_1&partnerID=40&md5=829ac0a3bbefef969d90f7eefc58b100","This paper presents the results of a bibliometric study of the recent research on eXplainable Artificial Intelligence (XAI) systems. We took a global look at the contributions of scholars in XAI as well as in the subfields of AI that are mostly involved in the development of XAI systems. It is worthy to remark that we found out that about one third of contributions in XAI come from the fuzzy logic community. Accordingly, we went in depth with the actual connections of fuzzy logic contributions with AI to promote and improve XAI systems in the broad sense. Finally, we outlined new research directions aimed at strengthening the integration of different fields of AI, including fuzzy logic, toward the common objective of making AI accessible to people. © Springer International Publishing AG, part of Springer Nature 2018.","Comprehensibility; Explainable AI; Interpretability; Interpretable fuzzy systems; Understandability","Artificial intelligence; Computer circuits; Fuzzy logic; Knowledge based systems; Actual connections; Artificial intelligence research; Bibliometric; Bibliometric analysis; Comprehensibility; Interpretability; Recent researches; Understandability; Information management"
"Alonso J.M., Mencar C.","Building cognitive cities with explainable artificial intelligent systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045445275&partnerID=40&md5=ec1776466627dfa2539cc7639338cccd","In the era of the Internet of Things and Big Data, data scientists are required to extract valuable knowledge from the given data. This challenging task is not straightforward. Data scientists first analyze, cure and pre-process data. Then, they apply Artificial Intelligence (AI) techniques to automatically extract knowledge from data. However, nowadays the focus is set on knowledge representation and how to enhance the human-machine interaction. Non-expert users, i.e., users without a strong background on AI, require a new generation of explainable AI systems. They are expected to naturally interact with humans, thus providing comprehensible explanations of decisions automatically made. In this paper, we sketch how certain computational intelligence techniques, namely interpretable fuzzy systems, are ready to play a key role in the development of explainable AI systems. Interpretable fuzzy systems have already successfully contributed to build explainable AI systems for cognitive cities. Copyright © 2018 for this paper by its authors. Copying permitted for private and academic purposes.","Cognitive cities; Explainable computational intelligence; Interpretable fuzzy systems; Natural language generation","Big data; Data mining; Fuzzy systems; Intelligent systems; Knowledge representation; Natural language processing systems; AI systems; Artificial intelligent; Cognitive cities; Computational intelligence techniques; Human machine interaction; Natural language generation; Non-experts; Process data; Cognitive systems"
"Alonso Moral J.M., Castiello C., Magdalena L., Mencar C.","Remarks and Prospects on Explainable Fuzzy Systems","10.1007/978-3-030-71098-9_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104016278&doi=10.1007%2f978-3-030-71098-9_7&partnerID=40&md5=56d97d0240e8d155a4a7c7a7e993fd1f","Since 2016, there is an increasing interest on research topics such as fairness, accountability and interpretability, in the entire community of researchers in Artificial Intelligence. However, there were researchers working hard on these research topics much earlier. For example, Michalski published his comprehensibility postulate in the 1980s while Zadeh introduced the notions of fuzzy sets and systems in the 1960s and the paradigm of computing with words in the 1990s. Moreover, Zadeh was talking about precisiated natural language in 2004. All these pioneer contributions to explainable Artificial Intelligence should be recognized properly. Explainable fuzzy systems go a step ahead of interpretable fuzzy systems and are ready to act as a cornerstone in the context of explainable Artificial Intelligence where interdisciplinary research fields (e.g., Computational Linguistics, Argumentation Technology, Human-machine Interaction, Philosophy, Math, Computer Science, Neuroscience, and so on) meet. Explainable fuzzy systems provide users with interactive explanations in natural language, the preferred modality among humans, with visualization as a complementary modality. Fuzzy Logic endows explainable fuzzy systems with a solid mathematical background to model properly the inherent uncertainty, imprecision and vagueness of human language. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,
"Alonso Moral J.M., Castiello C., Magdalena L., Mencar C.","Designing Interpretable Fuzzy Systems","10.1007/978-3-030-71098-9_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104014356&doi=10.1007%2f978-3-030-71098-9_5&partnerID=40&md5=b44dbbae3b6919a80766bb8b0936cddf","Fuzzy sets and fuzzy logic are powerful tools widely used to represent human knowledge and mimic human reasoning capabilities, being the main constituents of fuzzy systems. Among the different approaches to fuzzy systems, fuzzy rule-based systems represent the one offering a better framework for interpretability considerations. Their applications range from classification to modeling and control. Independently on its purpose, the behavior of a fuzzy rule-based system can be evaluated in terms of its reliability and comprehensibility, two concepts usually represented by accuracy and interpretability in the context of fuzzy systems. Indeed, achieving the highest possible levels of accuracy and interpretability is one of the central aspects of designing fuzzy systems. The present chapter will go through the design process considering its different steps and analyzing the multiple options allowing us to improve interpretability or to achieve a better interpretability-accuracy balance, in the search for interpretable fuzzy systems which represent an interesting approach in the framework of explainable Artificial Intelligence. We will consider questions related to the knowledge extraction and refinement process; some examples are complexity reduction and semantic improvement. We will also analyze other questions linked to the design of the processing structure, such as the effects of applying different aggregation and implication operators. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,
"Alonso Moral J.M., Castiello C., Magdalena L., Mencar C.","Toward Explainable Artificial Intelligence Through Fuzzy Systems","10.1007/978-3-030-71098-9_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104003533&doi=10.1007%2f978-3-030-71098-9_1&partnerID=40&md5=af6bfa2cd4409b8c1966ca2b398e476c","Explainable Artificial Intelligence is a novel paradigm conjugating the effectiveness of machine learning with the new requirements coming from the integration of intelligent systems in the human society. Explainable Artificial Intelligence can find successful application in a plethora of contexts, endowing classical intelligent systems with a crucial added value: the possibility for users to interact with machines, validate their results and ultimately trust their behavior. Fuzzy Set Theory provides a mathematical framework which is especially suitable to model concepts and perceptions of physical reality, thus injecting a kind of common-sense reasoning into machine learning algorithms and realizing a human-centric information processing which is the core of the Granular Computing paradigm. In particular, the exploitation of natural language enables Fuzzy Set Theory to act as a key-element for designing explainable models which can be able to provide explanations useful for human beings. We argue on those topics in order to show how the development of explainable fuzzy systems is a promising direction for paving the way from interpretable fuzzy systems to explainable intelligent systems. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,
"Alonso R.S.","Deep symbolic learning and semantics for an explainable and ethical artificial intelligence","10.1007/978-3-030-58356-9_30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091480696&doi=10.1007%2f978-3-030-58356-9_30&partnerID=40&md5=672234b1e7c2ef4a70311d0e0ac8dff6","The main objective of this research is to investigate new hybrid neuro-symbolic algorithms for the construction of an open-source Deep Symbolic Learning framework that allows the training and application of explainable and ethical Deep Learning models. This framework will be supported by an ontology and a layer model in which it is taken into account which user is responsible for interpreting each of the output results according to his or her role, considering, also, the ethical implications of those results. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2021.","Deep Learning; Deep Symbolic Learning; Ethical artificial intelligence; Explainable artificial intelligence; Interpretable machine learning","Application programs; Deep learning; Open source software; Philosophical aspects; Semantics; Ethical implications; Layer model; Learning models; Open sources; Symbolic algorithms; Symbolic learning; Ambient intelligence"
"Alonso-Moral J.M., Mencar C., Ishibuchi H.","Explainable and Trustworthy Artificial Intelligence [Guest Editorial]","10.1109/MCI.2021.3129953","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123185378&doi=10.1109%2fMCI.2021.3129953&partnerID=40&md5=0605cfbf1d3597a50e5a21df4b5aa77b",[No abstract available],,
"Alonso-Sarría F., Martínez-Hernández C., Romero-Díaz A., Cánovas-García F., Gomariz-Castillo F.","Main Environmental Features Leading to Recent Land Abandonment in Murcia Region (Southeast Spain)","10.1002/ldr.2447","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945207912&doi=10.1002%2fldr.2447&partnerID=40&md5=b4e542b25a167e56b2fd98fa3deb5a57","Land abandonment is a global phenomenon whose environmental consequences are difficult to assess. The Murcia region is one of the most arid regions in southern Europe and also one of the most prone to land abandonment. This study researches which environmental features are more relevant to explain abandonment at the agricultural plot scale. Geomorphometric features were measured at different scales to investigate which scales could be more relevant. Two different models have been used: logistic regression, a statistical model that allows the interpretation of the involved features, and Random Forest, a machine learning model with a higher predictive power but lower interpretability. The combined use of both such models allows a set of predictors to be selected, which, when used with Random Forest, produce a map that is highly accurate for predicting abandonment and, when used with logistic regression, produce an interpretable model. The main conclusion is that climate is the most relevant factor to explain land abandonment. © 2016 John Wiley & Sons, Ltd.","Data analysis; Feature selection; Land abandonment; Logistic regression; Random forest","Agricultural machinery; Artificial intelligence; Data reduction; Decision trees; Feature extraction; Learning systems; Agricultural plots; Environmental consequences; Environmental features; Land abandonments; Logistic regressions; Machine learning models; Random forests; Statistical modeling; Regression analysis; arid region; land degradation; regression analysis; Murcia [Spain]; Spain"
"Alop A.","The main challenges and barriers to the successful “smart shipping”","10.12716/1001.13.03.05","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073373757&doi=10.12716%2f1001.13.03.05&partnerID=40&md5=b7423061e0f355bc00c0fbdafe42427b","As with the powerful digitalization of the world in the 21st century, maritime affairs, like all other areas, are facing not only new opportunities, but also new big challenges and problems. From the point of view of the development of new technologies, it seems that everything is possible, for example the bringing of so‐ called”intelligent ships” and “smart ports” into one global system on base of internet of things and big data applications. However, if to look at the matter further, a number of factors and obstacles may appear which could be major threats to the normal functioning of such a system. While it is clear that systems with such high degree of complexity are even technically vulnerable, it seems to the author of this paper that questions that are no less difficult are in the field of human relations. For example, when ships and ports are becoming more and more “smarter” and need less and less people to intervene in their interactions, who at the end will be responsible for everything that can and definitely will happened at sea or in the port? What about liability of cargo carrier if “carrier” is an autonomous ship without any person on‐board during the entire journey? How to ensure cyber security? How to be secured against the risks of so‐called artificial intelligence systemic errors? It is possible that only new non‐trivial approaches can lead to acceptable results in this area, but what they may be and whether these approaches are possible at all ‐ these questions are still waiting for answers. © 2019, Faculty of Navigation, Gdynia Maritime University. All rights reserved.",,
"Alos A., Dahrouj Z.","Decision tree matrix algorithm for detecting contextual faults in unmanned aerial vehicles","10.3233/JIFS-191575","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088045973&doi=10.3233%2fJIFS-191575&partnerID=40&md5=c8106be42ba334d07bd85a8c3f4479a8","The importance of detecting faults in Unmanned Aerial Vehicles motivated researchers to work in this area over recent years. Complex relationships among UAV attributes (Sensor readings, and Commands) make the task a bit challenging. Many known algorithms consider detecting the faults by spotting data anomalies in the values of each attribute without concern for their context, which leaves an opportunity for potential improvement. The contextual faults occur when a defected sensor shows an invalid value concerning other attributes. Our contribution is a novel matrix platform for detecting the potential contextual faults. This platform consists of multiple small Decision Trees, instead of using one huge single Decision Tree, which could be difficult and time-consuming to produce, particularly in the case of a large dataset with too many attributes. We propose to use the C4.5 decision tree algorithm to build each decision tree. The Decision Tree is a machine learning technique, which is an effective supervised method used for classification. It is computationally inexpensive and capable of dealing with noisy data. Besides, our approach uses a sliding window technique during training and testing phases, which brings into consideration the effect of the previous state of the system on the process of detecting the contextual faults. The algorithm starts by collecting the attributes of the UAV into a table of pairs, where each pair consists of two attributes; then, it defines the Decision Tree matrix by assigning one Decision Tree for each pair of attributes. The Training step includes constructing training sub-datasets using the values of sliding windows. The C4.5 algorithm uses each constructed training sub-dataset to induce one Decision Tree in the matrix. Finally, the testing step is responsible for reading the values of the sliding windows and using the concerned Decision Tree to detect the contextual faults. We evaluated our approach using Detection Rate, False Alarm Rate, Precision, and F1-score indicators. Moreover, we made a comparison with other broadly used algorithms, such as K-Means and One-Class SVM. Our approach showed superior results in detecting different types of faults (sensor-offset, sensor-stuck, sensor-drift, and sensor-cut). The DT-Matrix performance was neither affected by the small values of the outliers, nor by the number of the outliers, and this caused the DT-Matrix to work better in most of the experiments compared to the other algorithms. © 2020 - IOS Press and the authors. All rights reserved","Abnormal; Anomaly detection; Classification; Contextual faults; Decision tree; Sensor faults; Supervised algorithm; System failure; UAV","Aircraft detection; Antennas; Decision trees; Decoding; K-means clustering; Large dataset; Learning systems; Matrix algebra; Statistics; Support vector machines; Trees (mathematics); Unmanned aerial vehicles (UAV); C4.5 decision tree algorithm; Complex relationships; Machine learning techniques; Matrix algorithms; Matrix performance; Sliding window techniques; Supervised methods; Training and testing; Fault detection"
"Aloud M., Tsang E., Dupuis A., Olsen R.","Minimal agent-based model for the origin of trading activity in foreign exchange market","10.1109/CIFER.2011.5953567","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961147419&doi=10.1109%2fCIFER.2011.5953567&partnerID=40&md5=2bb7be1dd76378e148fe49325a7d58c0","In this paper, we show that a minimal agent-based model for the Foreign Exchange (FX) market is capable of reproducing, to a certain extent, FX market trading activity. The model is minimal in that it has the advantage of having the minimum set of elements necessary for modelling the FX market in order to reproduce the FX market trading activity. The key elements are zero-Intelligence directional-change events traders, historical prices, actual FX traders' behaviour, limit orders, FX market trading sessions, market holidays, and the activation of the initial condition. All of these play a fundamental role. Most importantly, the simulation output is evaluated by contrast against the microscopic behavioural analysis of high-frequency data of individual traders' transactions on an account level provided by OANDA LTD. The results of this comparison prove that the trading agents' behaviour reproduces the FX market trading activity. Overall, the model leads to the identification of the key elements that may be responsible for the emergence of FX market trading activity in an agent-based model. © 2011 IEEE.","Agent-based modelling; Foreign exchange market; FX market trading activity; High-frequency data; In-traday FX market seasonality","Agent-based modelling; Foreign exchange markets; FX market trading activity; High frequency data; Seasonality; Artificial intelligence; Computational methods; Economics; Commerce"
"Alperin K.B., Wollaber A.B., Gomez S.R.","Improving Interpretability for Cyber Vulnerability Assessment Using Focus and Context Visualizations","10.1109/VizSec51108.2020.00011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101338474&doi=10.1109%2fVizSec51108.2020.00011&partnerID=40&md5=cd1dd90c1749edd6a02b407e018f643f","Risk scoring provides a simple and quantifiable metric for decision support in cyber security operations, including prioritizing how to address discovered software vulnerabilities. However, scoring systems are often opaque to operators, which makes scores difficult to interpret in the context of their own networks, each other, or in a broader threat landscape. This interpretability challenge is exacerbated by recent applications of artificial intelligence (AI) and machine learning (ML) for vulnerability assessment, where opaque machine reasoning can hinder domain experts' trust in the decision-support toolkit or the actionability of its outputs. In this paper, we address this challenge through a combination of visualizations and analytics that complement existing techniques for vulnerability assessment. We present a study toward designing more interpretable visual encodings for decision support for vulnerability assessment. In particular, we consider the problem of making datasets of known vulnerabilities more interpretable at multiple scales, inspired by focus and context principles from the information visualization design community. The first scale considers individually scored vulnerabilities by using an explainable AI (XAI) toolkit for an ML risk-scoring model and by developing new visualizations of CVSS score features. The second scale uses an embedding for vulnerability descriptions to cluster potentially similar vulnerabilities. We outline use cases for these tools and discuss opportunities for applying XAI concepts to cyber risk and vulnerability management. © 2020 IEEE.","Human-centered computing - Visualization - Visualization application domains - Information Visualization; Human-centered computing - Visualization - Visualization systems and tools - Visualization toolkits; Security and privacy - Systems security - Vulnerability Management","Decision support systems; Information systems; Network security; Visualization; Context visualization; Cyber vulnerabilities; Decision supports; Information visualization; Software vulnerabilities; Vulnerability assessments; Vulnerability description; Vulnerability management; Artificial intelligence"
"Alperin Lori B., Kedzierski Beverly I.","AI-BASED SOFTWARE MAINTENANCE.",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023209048&partnerID=40&md5=862bc3b3fc27f56a986398e266a66e53","The PM problem manager is one of the expert managers of Carnegie Group's knowledge-based software development environment. This environment is a framework for building integrated tools to support software managers, designers, programmers, and maintainers. PM deals with software problems, including bugs, changes, and enhancements to a system. It helps a user report, track, and process problems using its own knowledge as well as knowledge from the environment's other expert managers. In reporting mode, PM helps an experienced user locate a problem by calling on configuration knowledge. Using planning knowledge, it infers the names of the people responsible for analyzing the problem. They can then graphically view all of their problems and choose to fix them or take other action. PM automatically creates reports of problems organized by status, date, people and modules. It also helps to keep the project schedule up to date by automatically creating activities to account for the time involved in dealing with problems. The authors present an example of user interaction with PM, describe some of the AI technology used to build it, and give an account of its actual use.",,"ARTIFICIAL INTELLIGENCE - Expert Systems; CONFIGURATION KNOWLEDGE; KNOWLEDGE-BASED SYSTEMS; SOFTWARE MANAGERS; COMPUTER SOFTWARE"
"Alpizar-Chacon I., van der Hart M., Wiersma Z.S., Theunissen L.S.J., Sosnovsky S.","Transformation of PDF textbooks into intelligent educational resources",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093114479&partnerID=40&md5=c42c36f97fa03a6576ec72feb2e12fa3","The paper presents Intextbooks - the system for automated conversion of PDF-based textbooks into intelligent educational Web resources. The papers focuses on the new component of Intextbooks responsible for transformation of PDF-based content into semantically-annotated HTML/CSS. The architecture of the system, the design of the client application rendering resulting textbooks and a short validation experiment demonstrating the quality of the transformation work-flow are presented. 1 Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Adaptive textbooks; Digital textbooks; Educational resources; Intelligent textbooks; Interactive textbooks; Modeling textbooks; PDF to HTML","Artificial intelligence; Textbooks; Client applications; Educational resource; New components; Web resources; Work-flows; Electronic document exchange"
"Alqahtani A., Silaghi M.","Classification of debate threading models for representing decentralized debates",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017305032&partnerID=40&md5=2fde6c58d36b2b34d089d47f5f2f7f90","We analyze Debate Threading Models for representing Peer-to-Peer Decentralized Debates. The challenge, we address, is how to structure arguments in debate on a clear topic such that they are informative to the users trying to make up their mind, and such as to foster reciprocal understanding of each of the views in the community. Moreover, to enable the participation of average skilled users, the technique for inputting the arguments may have to be simple (i.e., close to the common practice, which is a subjective criteria). Knowledge representation for serving human users has been an important area of study in Computer Science and Artificial Intelligence. The area has grown significantly with conferences on topics such as ontologies, collaborative filtering, semantic web, and argument maps. We study and compare the various existing structures for representing complex knowledge in understandable forms, focusing on their applicability to decentralized debates with arguments from general users. We believe that this is the first kind of work that deals with the issue above. © 2019 ICAI 2015 - WORLDCOMP 2015. All rights reserved.","Comparison between TED; DDP2P; Debate Threading Model; Knowledge Representation; Related Work; Threading Model Classification; Threading Models for Arguments in Electronic Debates; YourView","Knowledge representation; Comparison between TED; DDP2P; Related works; Threading model; YourView; Collaborative filtering"
"Alqaraawi A., Schuessler M., Weiß P., Costanza E., Berthouze N.","Evaluating saliency map explanations for convolutional neural networks","10.1145/3377325.3377519","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082502614&doi=10.1145%2f3377325.3377519&partnerID=40&md5=9b576a01a7b322f7ca4ff62009108993","Convolutional neural networks (CNNs) offer great machine learning performance over a range of applications, but their operation is hard to interpret, even for experts. Various explanation algorithms have been proposed to address this issue, yet limited research effort has been reported concerning their user evaluation. In this paper, we report on an online between-group user study designed to evaluate the performance of ""saliency maps"" - a popular explanation algorithm for image classification applications of CNNs. Our results indicate that saliency maps produced by the LRP algorithm helped participants to learn about some specific image features the system is sensitive to. However, the maps seem to provide very limited help for participants to anticipate the network's output for new images. Drawing on our findings, we highlight implications for design and further research on explainable AL In particular, we argue the HCI and AI communities should look beyond instance-level explanations. © ACM.","explainable AI; heatmap; human-AI interaction; saliency-maps; user studies","Convolution; Image segmentation; Learning systems; User interfaces; Group users; heatmap; human-AI interaction; Image features; Research efforts; Saliency map; User evaluations; User study; Convolutional neural networks"
"Alqaralleh B.A.Y., Aldhaban F., AlQarallehs E.A., Al-Omari A.H.","Optimal Machine Learning Enabled Intrusion Detection in Cyber-Physical System Environment","10.32604/cmc.2022.026556","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128645149&doi=10.32604%2fcmc.2022.026556&partnerID=40&md5=dd73b01bfa973f307561dcb8c0c74542","Cyber-attacks on cyber-physical systems (CPSs) resulted to sensing and actuation misbehavior, severe damage to physical object, and safety risk. Machine learning (ML) models have been presented to hinder cyberattacks on the CPS environment; however, the non-existence of labelled data from new attacks makes their detection quite interesting. Intrusion Detection System (IDS) is a commonly utilized to detect and classify the existence of intrusions in the CPS environment, which acts as an important part in secure CPS environment. Latest developments in deep learning (DL) and explainable artificial intelligence (XAI) stimulate new IDSs to manage cyberattacks with minimum complexity and high sophistication. In this aspect, this paper presents an XAI based IDS using feature selection with Dirichlet Variational Autoencoder (XAIIDS-FSDVAE) model for CPS. The proposed model encompasses the design of coyote optimization algorithm (COA) based feature selection (FS) model is derived to select an optimal subset of features. Next, an intelligent Dirichlet Variational Autoencoder (DVAE) technique is employed for the anomaly detection process in the CPS environment. Finally, the parameter optimization of the DVAE takes place using a manta ray foraging optimization (MRFO) model to tune the parameter of the DVAE. In order to determine the enhanced intrusion detection efficiency of the XAIIDS-FSDVAE technique, a wide range of simulations take place using the benchmark datasets. The experimental results reported the better performance of the XAIIDS-FSDVAE technique over the recent methods in terms of several evaluation parameters. © 2022 Tech Science Press. All rights reserved.","Cyber-physical systems; deep learning; explainable artificial intelligence; intrusion detection; metaheuristics; security","Anomaly detection; Computer crime; Cyber Physical System; Cybersecurity; Deep learning; Embedded systems; Feature extraction; Network security; Optimization; Auto encoders; Cyber-attacks; Deep learning; Dirichlet; Explainable artificial intelligence; Features selection; Intrusion Detection Systems; Intrusion-Detection; Metaheuristic; Security; Intrusion detection"
"Alqudah A., Alqudah A.M.","Sliding Window Based Support Vector Machine System for Classification of Breast Cancer Using Histopathological Microscopic Images","10.1080/03772063.2019.1583610","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062782662&doi=10.1080%2f03772063.2019.1583610&partnerID=40&md5=37430515bf50b1e718309135c8e4e95a","Breast cancer is one of the most common cancers in women's community, which is responsible for millions of death cases. The early diagnosis of breast cancer in its early stages increases the chances of healing; the most common current diagnosis methods are based on digital mammogram images and histopathological images. Automatic diagnosis of breast cancer and classifying the type of cancer is recommended to decrease the error that can be caused by humans. Recently, many systems have been developed to diagnose breast cancer by extracting textural and non-textural features from digital mammograms or histopathological images. This paper proposes a new sliding window technique for feature extraction where the local binary pattern (LBP) features are used. In this technique, each image produces 25 sliding windows. Features extracted from each window are saved and used to build a Support Vector Machine (SVM) classifier. The SVM classifier is used to classify each image into benign and malignant based on its most common windows classes. The system can be used to localize the cancerous tissues from the whole histopathological image. The proposed method achieved an overall accuracy of 91.12%, sensitivity of 85.22%, and specificity of 94.01%. Which is considered high when compared with other systems in the literature. The system can extend to extract more features and a comparison between different machine learning algorithms can performed. © 2022 IETE.","Breast cancer; Histopathological images; Local binary pattern; Sliding window features; Support vector machine","Decoding; Diseases; Image classification; Learning algorithms; Mammography; Support vector machines; X ray screens; Automatic diagnosis; Breast Cancer; Histopathological images; Local binary patterns; Overall accuracies; Sliding Window; Sliding window techniques; Sliding window-based; Medical imaging"
"Alqudah A.M., Qazan S., Masad I.S.","Artificial Intelligence Framework for Efficient Detection and Classification of Pneumonia Using Chest Radiography Images","10.1007/s40846-021-00631-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107853221&doi=10.1007%2fs40846-021-00631-1&partnerID=40&md5=b2fbfe95b4c7ef545ae2af32032b407b","Background: Chest diseases are serious health problems that threaten the lives of people. The early and accurate diagnosis of such diseases is very crucial in the success of their treatment and cure. Pneumonia is one of the most widely occurred chest diseases responsible for a high percentage of deaths especially among children. So, detection and classification of pneumonia using the non-invasive chest x-ray imaging would have a great advantage of reducing the mortality rates. Results: The results showed that the best input image size in this framework was 64 × 64 based on comparison between different sizes. Using CNN as a deep features extractor and utilizing the tenfold methodology the propose artificial intelligence framework achieved an accuracy of 94% for SVM and 93.9% for KNN, a sensitivity of 93.33% for SVM and 93.19% for KNN and a specificity of 96.68% for SVM and 96.60% for KNN. Conclusions: In this study, an artificial intelligence framework has been proposed for the detection and classification of pneumonia based on chest x-ray imaging with different sizes of input images. The proposed methodology used CNN for features extraction that were fed to two different types of classifiers, namely, SVM and KNN; in addition to the SoftMax classifier which is the default CNN classifier. The proposed CNN has been trained, validated, and tested using a large dataset of chest x-ray images contains in total 5852 images. © 2021, Taiwanese Society of Biomedical Engineering.","Artificial intelligence; Deep learning; Framework; Machine learning; Pneumonia; X-ray","Diagnosis; Image classification; Large dataset; X ray radiography; Chest radiography; Chest X-ray image; Chest x-rays; Different sizes; Efficient detection; Features extraction; Input image; Mortality rate; Support vector machines; Article; artificial intelligence; bacterial pneumonia; controlled study; feature extraction; human; k nearest neighbor; pneumonia; sensitivity and specificity; support vector machine; thorax radiography; virus pneumonia"
"Alrabbaa C., Borgwardt S., Knieriemen N., Kovtunova A., Rothermel A.M., Wiehr F.","In the Hand of the Beholder: Comparing Interactive Proof Visualizations",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116137553&partnerID=40&md5=542ea77b8325a83c3c772f37386c66c1","Although logical inferences are interpretable, actually explaining them to a user is still a challenging task. While sometimes it may be enough to point out the axioms from the ontology that lead to the consequence of interest, more complex inferences require proofs with intermediate steps that the user can follow. Our main hypothesis is that different users need different representations of proofs for optimal understanding. To this end, we undertook some user experiments related to logical proofs. We explored how a user's cognitive abilities influence the performance in and preference for certain proof representations. In particular, we compared tree-shaped representations with linear, textbased ones, and for each we offered an interactive and a static version. After each proof, participants had to solve some tasks measuring their level of understanding and rated each proof according to the perceived comprehensibility. At the end of the questionnaire, subjects ranked the proofs by comprehensibility. We found no differences between the general performance or the subjective ratings of the proof representations. However, in the final ranking participants preferred the conditions with tree-shaped proofs over the textual ones, with significant differences in the rankings of the higher cognitive ability group and across both groups but not in the low cognitive ability group. © Author(s) 2021.",,"Artificial intelligence; Visualization; Cognitive ability; Condition; Interactive proofs; Logical inference; Ontology's; Performance; Proof representation; Subjective rating; User experiments; User need; Forestry"
"Alrabbaa C., Hieke W., Turhan A.-Y.","Counter model transformation for explaining non-subsumption in EL",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116888555&partnerID=40&md5=2cfaae285f7aaad4d83b960e2f4fa3b7","When subsumption relationships unexpectedly fail to be detected by a description logic reasoner, the cause for this “non-entailment” need not be evident. In this case, succinct automatically generated explanations would be helpful. Reasoners for the description logic EL compute the canonical model of a TBox in order to perform subsumption tests. We devise parts of such models as relevant parts for explanation and propose an approach based on graph transductions to extract such relevant parts from canonical models. Copyright © 2021 for this paper by its authors.","Description Logic; Explainable AI; Model Transformation","Artificial intelligence; Computer circuits; Formal languages; Automatically generated; Canonical models; Counter-models; Description logic; Explainable AI; Model transformation; Reasoners; Data description"
"Alrajeh D., Russo A.","Logic-based learning: Theory and application","10.1007/978-3-319-96562-8_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051139633&doi=10.1007%2f978-3-319-96562-8_9&partnerID=40&md5=da5e29373b38a6407bb0a747c9c4ff17","In recent years, research efforts have been directed towards the use of Machine Learning (ML) techniques to support and automate activities such as specification mining, risk assessment, program analysis, and program repair. The focus has largely been on the use of machine learning black box methods whose inference mechanisms are not easily interpretable and whose outputs are not declarative and guaranteed to be correct. Hence, they cannot readily be used to inform the elaboration and revision of declarative software models identified to be incorrect or incomplete. On the other hand, recent advances in ML have witnessed the emergence of new logic-based machine learning approaches that overcome such limitations and which have been proven to be well-suited for many software engineering tasks. In this chapter, we present a survey of the state-of-the-art of logic-based machine learning techniques, highlight their expressivity, define their different underlying semantics, and discuss their efficiency and the heuristics they adopt to guide the search for solutions. We then demonstrate the application of this type of machine learning to (declarative) specification refinement and revision as a complementary task to program analysis. © Springer International Publishing AG, part of Springer Nature 2018.",,"Application programs; Artificial intelligence; Computer circuits; Risk assessment; Semantics; Specifications; Black box method; Inference mechanism; Machine learning approaches; Machine learning techniques; Research efforts; Specification mining; Specification refinements; State of the art; Learning systems"
"Alrajeh D., Russo A., Uchitel S., Kramer J.","Logic-based learning in software engineering","10.1145/2889160.2891050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026652690&doi=10.1145%2f2889160.2891050&partnerID=40&md5=b04b59d297d3331ee04161779218cf10","In recent years, research efforts have been directed towards the use of Machine Learning (ML) techniques to support and automate activities such as program repair, specification mining and risk assessment. The focus has largely been on techniques for classification, clustering and regression. Although beneficial, these do not produce a declarative, interpretable representation of the learned information. Hence, they cannot readily be used to inform, revise and elaborate software models. On the other hand, recent advances in ML have witnessed the emergence of new logic-based learning approaches that differ from traditional ML in that their output is represented in a declarative, rule-based manner, making them well-suited for many software engineering tasks. In this technical briefing, we will introduce the audience to the latest advances in logic-based learning, give an overview of how logic-based learning systems can successfully provide automated support to a variety of software engineering tasks, demonstrate the application to two real case studies from the domain of requirements engineering and software design and highlight future challenges and directions. © 2016 Authors.",,"Application programs; Computer circuits; Learning systems; Risk assessment; Software design; Software engineering; Technical presentations; Automated support; Future challenges; Interpretable representation; Learning approach; Research efforts; Rule based; Software model; Specification mining; Engineering education"
"Alrajhi L., Pereira F.D., Cristea A.I., Aljohani T.","A Good Classifier is Not Enough: A XAI Approach for Urgent Instructor-Intervention Models in MOOCs","10.1007/978-3-031-11647-6_84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135878371&doi=10.1007%2f978-3-031-11647-6_84&partnerID=40&md5=d97d9deeaadf320499cdedec6f84e6a4","Deciding upon instructor intervention based on learners’ comments that need an urgent response in MOOC environments is a known challenge. The best solutions proposed used automatic machine learning (ML) models to predict the urgency. These are ‘black-box’-es, with results opaque to humans. EXplainable artificial intelligence (XAI) is aiming to understand these, to enhance trust in artificial intelligence (AI)-based decision-making. We propose to apply XAI techniques to interpret a MOOC intervention model, by analysing learner comments. We show how pairing a good predictor with XAI results and especially colour-coded visualisation could be used to support instructors making decisions on urgent intervention. © 2022, Springer Nature Switzerland AG.","Comments; MOOCs; NLP; Urgent intervention; XAI","Artificial intelligence; Automatic machines; Black boxes; Color-coded visualization; Comment; Decisions makings; Intervention models; Machine learning models; MOOC; Urgent intervention; XAI; Decision making"
"al-Rifaie M.M., Leymarie F.F., Latham W., Bishop M.","Swarmic autopoiesis and computational creativity","10.1080/09540091.2016.1274960","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009461042&doi=10.1080%2f09540091.2016.1274960&partnerID=40&md5=af61995e0bbc4c64eafd4ce4c707192a","In this paper two swarm intelligence algorithms are used, the first leading the “attention” of the swarm and the latter responsible for the tracing mechanism. The attention mechanism is coordinated by agents of Stochastic Diffusion Search where they selectively attend to areas of a digital canvas (with line drawings) which contains (sharper) corners. Once the swarm's attention is drawn to the line of interest with a sharp corner, the corresponding line segment is fed into the tracing algorithm, Dispersive Flies Optimisation which “consumes” the input in order to generate a “swarmic sketch” of the input line. The sketching process is the result of the “flies” leaving traces of their movements on the digital canvas which are then revisited repeatedly in an attempt to re-sketch the traces they left. This cyclic process is then introduced in the context of autopoiesis, where the philosophical aspects of the autopoietic artist are discussed. The autopoetic artist is described in two modalities: gluttonous and contented. In the Gluttonous Autopoietic Artist mode, by iteratively focussing on areas-of-rich-complexity, as the decoding process of the input sketch unfolds, it leads to a less complex structure which ultimately results in an empty canvas; therein reifying the artwork's “death”. In the Contented Autopoietic Artist mode, by refocussing the autopoietic artist's reflections on “meaning” onto different constitutive elements, and modifying her reconstitution, different behaviours of autopoietic creativity can be induced and therefore, the autopoietic processes become less likely to fade away and more open-ended in their creative endeavour. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","autopoiesis; Creativity; dispersive flies optimisation; generative art; stochastic diffusion search","Artificial intelligence; Iterative decoding; Philosophical aspects; Stochastic systems; Auto-poiesis; Creativity; Generative art; Optimisations; Stochastic diffusion searches; Drawing (graphics)"
"AlSaad R., Malluhi Q., Boughorbel S.","PredictPTB: an interpretable preterm birth prediction model using attention-based recurrent neural networks","10.1186/s13040-022-00289-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124996412&doi=10.1186%2fs13040-022-00289-8&partnerID=40&md5=f4377745ba62a7a2c9c99546726c5e31","Background: Early identification of pregnant women at risk for preterm birth (PTB), a major cause of infant mortality and morbidity, has a significant potential to improve prenatal care. However, we lack effective predictive models which can accurately forecast PTB and complement these predictions with appropriate interpretations for clinicians. In this work, we introduce a clinical prediction model (PredictPTB) which combines variables (medical codes) readily accessible through electronic health record (EHR) to accurately predict the risk of preterm birth at 1, 3, 6, and 9 months prior to delivery. Methods: The architecture of PredictPTB employs recurrent neural networks (RNNs) to model the longitudinal patient’s EHR visits and exploits a single code-level attention mechanism to improve the predictive performance, while providing temporal code-level and visit-level explanations for the prediction results. We compare the performance of different combinations of prediction time-points, data modalities, and data windows. We also present a case-study of our model’s interpretability illustrating how clinicians can gain some transparency into the predictions. Results: Leveraging a large cohort of 222,436 deliveries, comprising a total of 27,100 unique clinical concepts, our model was able to predict preterm birth with an ROC-AUC of 0.82, 0.79, 0.78, and PR-AUC of 0.40, 0.31, 0.24, at 1, 3, and 6 months prior to delivery, respectively. Results also confirm that observational data modalities (such as diagnoses) are more predictive for preterm birth than interventional data modalities (e.g., medications and procedures). Conclusions: Our results demonstrate that PredictPTB can be utilized to achieve accurate and scalable predictions for preterm birth, complemented by explanations that directly highlight evidence in the patient’s EHR timeline. © 2022, The Author(s).","Attention mechanism; Deep learning; Electronic health record; Predictive models; Pregnancy; Preterm birth","area under the curve; Article; clinical feature; electronic health record; female; human; prediction; premature labor; receiver operating characteristic; recurrent neural network; risk factor"
"Al-Saadi M., Khan A., Kelefouras V., Walker D.J., Al-Saadi B.","Unsupervised Machine Learning-Based Elephant and Mice Flow Identification","10.1007/978-3-030-80126-7_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130685100&doi=10.1007%2f978-3-030-80126-7_27&partnerID=40&md5=d3fc369933c51b72b58b51999bc17b69","Internet today holds traffic from a wide range of applications, which have different requirements and constraints on the resources of a network. Hence, it is normal to find a variety of flows with dissimilar features that contend for the network resources. Consequently, the problem that appears clearly is an unfair use of these resources by particular flows. This problem exposed to the so-called elephant and mice flows through real analysis of network traffic. Therefore, this problem might lead to degrading network performance. In this paper, we proposed a framework to optimize the network performance through characterising elephant and mice flows based on network performance metrics. The framework has three parts. Principal component analysis (PCA) is used in the first part to reduce the dimensionality. The next part was responsible for partitioning the traffic into distinct groups based on performance metrics such as packet loss, round trip time (RTT), and throughput by using an unsupervised clustering method with k-means. Finally, for each cluster, flows have been identified as huge (elephant) and small (mice) based on threshold values for the predefined parameters. Our results show that there is a potential in using network performance features to cluster the network traffic and to identify mice and elephant flows based on the number of packets, flow size, and duration of flow. We analyzed a (2 GB pcap file) to build our dataset. Finally, our proposed framework is capable of characterizing mice and elephant flows based on network performance metrics for each cluster. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2021","Elephant; k-Means; Machine learning; mice flows; Network performance; PCA","K-means clustering; Machine learning; Mammals; Network performance; Packet loss; Elephant; Elephant flow; Flow based; K-means; Mouse flow; Network performance metrics; Network resource; Network traffic; Principal-component analysis; Unsupervised machine learning; Principal component analysis"
"Al-Saadi M., Khan A., Kelefouras V., Walker D.J., Al-Saadi B.","Unsupervised Machine Learning-Based Elephant and Mice Flow Identification","10.1007/978-3-030-80126-7_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112709077&doi=10.1007%2f978-3-030-80126-7_27&partnerID=40&md5=b5ddd3e943963f3e7b74296b55fba1aa","Internet today holds traffic from a wide range of applications, which have different requirements and constraints on the resources of a network. Hence, it is normal to find a variety of flows with dissimilar features that contend for the network resources. Consequently, the problem that appears clearly is an unfair use of these resources by particular flows. This problem exposed to the so-called elephant and mice flows through real analysis of network traffic. Therefore, this problem might lead to degrading network performance. In this paper, we proposed a framework to optimize the network performance through characterising elephant and mice flows based on network performance metrics. The framework has three parts. Principal component analysis (PCA) is used in the first part to reduce the dimensionality. The next part was responsible for partitioning the traffic into distinct groups based on performance metrics such as packet loss, round trip time (RTT), and throughput by using an unsupervised clustering method with k-means. Finally, for each cluster, flows have been identified as huge (elephant) and small (mice) based on threshold values for the predefined parameters. Our results show that there is a potential in using network performance features to cluster the network traffic and to identify mice and elephant flows based on the number of packets, flow size, and duration of flow. We analyzed a (2 GB pcap file) to build our dataset. Finally, our proposed framework is capable of characterizing mice and elephant flows based on network performance metrics for each cluster. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Elephant and mice flows; k-Means; Machine learning; Network performance; PCA",
"Alsadhan N., Skillicorn D.B.","Comparing SVD and SDAE for Analysis of Islamist Forum Postings","10.1109/ICDMW.2015.108","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964734805&doi=10.1109%2fICDMW.2015.108&partnerID=40&md5=892c8dcec6009983458fe8c4d45a6694","We analyze postings in the Turn to Islam forum using techniques based on singular value decomposition (SVD) and the deep learning technique of stacked denoising autoencoders (SDAE). Models based on frequent words and jihadist language intensity are used, and the results compared. Our main conclusion is that SDAE approaches, while clearly discovering structure in document-word matrices, do not yet provide a natural interpretation strategy, limiting their practical usefulness. In contrast, SVD approaches provide interpretable models, primarily because of the coupling between document and word variation patterns. © 2015 IEEE.",,"Data mining; Learning systems; Autoencoders; De-noising; Deep learning; Variation pattern; Singular value decomposition"
"Alsagri H., Ykhlef M.","Quantifying feature importance for detecting depression using random forest","10.14569/IJACSA.2020.0110577","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085743594&doi=10.14569%2fIJACSA.2020.0110577&partnerID=40&md5=24629d391c0db0bef62f68b7b3d6958a","Feature selection based on importance is a fundamental step in machine learning models because it serves as a vital technique to orient the use of variables to what is most efficient and effective for a given machine learning model. In this study, an explainable machine learning model based on Random forest, is built to address the problem of identification of depression level for Twitter users. This model reflects its transparency through calculating its feature importance. There are several techniques to quantify the importance of features. However, in this study, random forest is used as both a classifier, which has over-performing aspects over many classifiers such as decision trees, and a method for weighting the input features as their importance imply. In this study, the importance of features is measured using different techniques including random forest, and the results of these techniques are compared. Furthermore, feature importance uses the concept of weighting the input variables inside a complete system for recommending a solution for depressed persons. The experimental results confirm the superiority of random forest over other classifiers using three different methods for measuring the features importance. The accuracy of random forest classification reached 84.7%, and the importance of features increased the classifier accuracy to 84.9%. © 2020 Science and Information Organization.","Depression; Emotions; Feature importance; Feature selection; Machine learning; Random forest; Twitter","Classification (of information); Decision trees; Machine learning; Random forests; Social networking (online); Depression; Emotion; Feature importance; Features selection; Input features; Input variables; Machine learning models; Model-based OPC; Random forests; Selection based; Feature extraction"
"Alsaleh N., Farooq B.","Interpretable data-driven demand modelling for on-demand transit services","10.1016/j.tra.2021.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116576692&doi=10.1016%2fj.tra.2021.10.001&partnerID=40&md5=93aee1479a2a57dd4485d305becc69e3","In recent years, with the advancements in information and communication technology, different emerging on-demand shared mobility services have been introduced as innovative solutions in the low-density areas, including on-demand transit (ODT), mobility on-demand (MOD) transit, and crowdsourced mobility services. However, due to their infancy, there is a strong need to understand and model the demand for these services. In this study, we developed trip production and distribution models for ODT services at Dissemination areas (DA) level using four machine learning algorithms: Random Forest (RF), Bagging, Artificial Neural Network (ANN) and Deep Neural Network (DNN). The data used in the modelling process were acquired from Belleville's ODT operational data and 2016 census data. Bayesian optimalization approach was used to find the optimal architecture of the adopted algorithms. Moreover, post-hoc model was employed to interpret the predictions and examine the importance of the explanatory variables. The results showed that the land-use type was the most important variable in the trip production model. On the other hand, the demographic characteristics of the trip destination were the most important variables in the trip distribution model. Moreover, the results revealed that higher trip distribution levels are expected between dissemination areas with commercial/industrial land-use type and dissemination areas with high-density residential land-use. Our findings suggest that the performance of ODT services can be further enhanced by (a) locating idle vehicles in the neighbourhoods with commercial/industrial land-use and (b) using the spatio-temporal demand models obtained in this work to continuously update the operating fleet size. © 2021 Elsevier Ltd","Demand modelling; Machine learning; Model interpretability; On-demand service (ODT); Shared mobility; Trip generation and distribution","Commercial vehicles; Decision trees; Deep neural networks; Fleet operations; Learning algorithms; Population statistics; Demand modelling; Interpretability; Model interpretability; On demands; On-demand service (on-demand transit); On-demand services; Services on demand; Shared mobility; Trip distribution; Trip generations; Land use; algorithm; artificial neural network; Bayesian analysis; data interpretation; service sector; spatial distribution; spatiotemporal analysis; travel demand"
"Al-Sammarraie L.H.A., Ibrahim A.A.","Predicting Breast Cancer in Fine Needle Aspiration Images Using Machine Learning","10.1109/ISMSIT50672.2020.9254891","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097662949&doi=10.1109%2fISMSIT50672.2020.9254891&partnerID=40&md5=e789778f8393036478df3890b62053a1","Breast cancer is the second leading cause of cancer death in women in the world. Statistics show that 1, 152, 161 new cases of breast cancer are found worldwide; and with 411, 093 deaths It has been shown that early diagnosis of breast cancer increases the probability of a complete recovery and reduces the mortality of patients suffering from this cancer [1] Cancer is the mutation of genes responsible for cell replication and the regulation of cell growth. These genes are found in the nucleus of cells and act as a control to turn different cells on or off so that old cells die while new ones take over. When a mutation occurs, these cells do not die and begin to divide uncontrollably, creating tumors in this work we have designed and implemented a system whose main purpose is to detect the existence of breast cancer lumps in fine needle aspiration images. We use a clustering and a feature extraction method such as CNN and then we use a classification method to detect the cancer. Early detection of breast cancer is essential to increase patient survival © 2020 IEEE.","Breast cancer; CNN; Fine needle aspiration; Machine learning",
"Alsawwaf M., Chaczko Z., Kulbacki M., Sarathy N.","In Your Face: Person Identification Through Ratios and Distances Between Facial Features","10.1142/S2196888822500105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120447773&doi=10.1142%2fS2196888822500105&partnerID=40&md5=fd4da9d79ac75f73857e65b90781a6b4","These days identification of a person is an integral part of many computer-based solutions. It is a key characteristic for access control, customized services, and a proof of identity. Over the last couple of decades, many new techniques were introduced for how to identify human faces. This approach investigates the human face identification based on frontal images by producing ratios from distances between the different features and their locations. Moreover, this extended version includes an investigation of identification based on side profile by extracting and diagnosing the feature sets with geometric ratio expressions which are calculated into feature vectors. The last stage involves using weighted means to calculate the resemblance. The approach considers an explainable Artificial Intelligence (XAI) approach. Findings, based on a small dataset, achieve that the used approach offers promising results. Further research could have a great influence on how faces and face-profiles can be identified. Performance of the proposed system is validated using metrics such as Precision, False Acceptance Rate, False Rejection Rate, and True Positive Rate. Multiple simulations indicate an Equal Error Rate of 0.89. This work is an extended version of the paper submitted in ACIIDS 2020. © 2022 The Author(s).","Biometrics; DNN; Feature extraction; Feature vector; Geometric ratios; Human recognition; Identification; Pixel segmentation; XAI",
"Alsaydia O.M., Saadallah N.R., Malallah F.L., AL-Adwany M.A.S.","Limiting Covid-19 Infection By Automatic Remote Face Mask Monitoring And Detection Using Deep Learning With Iot","10.15587/1729-4061.2021.238359","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119681326&doi=10.15587%2f1729-4061.2021.238359&partnerID=40&md5=1f98d3f2cd10a423319a7d719b3433e7","During the current outbreak of the COVID-19 pandemic, controlling and decreasing the possibilities of infections are massively required. One of the most important solutions is to use Artificial Intelligence (AI), which combines both fields of deep learning (DL) and the Internet of Things (IoT). The former one is responsible for detecting any face, which is not wearing a mask. Whereas, the latter is exploited to manage the control for the entire building or a public area such as bus, train station, or airport by connecting a Closed-Circuit Television (CCTV) camera to the room of management. The work is implemented using a Core-i5 CPU workstation attached with a Webcam. Then, MATLAB software is programmed to instruct both Arduino and NodeMCU (Micro-Controller Unit) for remote control as IoT. In terms of deep learning, a 15-layer convolutional neural network is exploited to train 1,376 image samples to generate a reference model to use for comparison. Before deep learning, preprocessing operations for both image enhancement and scaling are applied to each image sample. For the training and testing of the proposed system, the Simulated Masked Face Recognition Dataset (SMFRD) has been exploited. This dataset is published online. Then, the proposed deep learning system has an average accuracy of up to 98.98 %, where 80 % of the dataset was used for training and 20 % of the samples are dedicated to testing the proposed intelligent system. The IoT system is implemented using Arduino and NodeMCU_TX (for transmitter) and RX (for receiver) for the signal transferring through long distances. Several experiments have been conducted and showed that the results are reasonable and thus the model can be commercially applied © 2021, Authors. This is an open access article under the Creative Commons CC BY license","Big Data; Computer Vision; COVID-19; Deep Learning; Embedded System; IoT; Machine Learning; Remote Control",
"Alsayed A.O., Rahim M.S.M., Albidewi I., Hussain M., Jabeen S.H., Alromema N., Hussain S., Jibril M.L.","Selection of the right undergraduate major by students using supervised learning techniques","10.3390/app112210639","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119285013&doi=10.3390%2fapp112210639&partnerID=40&md5=8cccc0eba5c388367576a2ae0843bb3e","University education has become an integral and basic part of most people preparing for working life. However, placement of students into the appropriate university, college, or discipline is of paramount importance for university education to perform its role. In this study, various explainable machine learning approaches (Decision Tree [DT], Extra tree classifiers [ETC], Random forest [RF] classifiers, Gradient boosting classifiers [GBC], and Support Vector Machine [SVM]) were tested to predict students’ right undergraduate major (field of specialization) before admission at the undergraduate level based on the current job markets and experience. The DT classifier predicts the target class based on simple decision rules. ETC is an ensemble learning technique that builds prediction models by using unpruned decision trees. RF is also an ensemble technique that uses many individual DTs to solve complex problems. GBC classifiers and produce strong prediction models. SVM predicts the target class with a high margin, as compared to other classifiers. The imbalanced dataset includes secondary school marks, higher secondary school marks, experience, and salary to select specialization for students in undergraduate programs. The results showed that the performances of RF and GBC predict the student field of specialization (undergraduate major) before admission, as well as the fact that these measures are as good as DT and ETC. Statistical analysis (Spearman correlation) is also applied to evaluate the relationship between a student’s major and other input variables. The statistical results show that higher student marks in higher secondary (hsc_p), university degree (Degree_p), and entry test (etest_p) play an important role in the student’s area of specialization, and we can recommend study fields according to these features. Based on these results, RF and GBC can easily be integrated into intelligent recommender systems to suggest a good field of specialization to university students, according to the current job market. This study also demonstrates that marks in higher secondary and university and entry tests are useful criteria to suggest the right undergraduate major because these input features most accurately predict the student field of specialization. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Explainable machine learning; Imbalanced datasets; Intelligent tutoring system; Learning analytics; Machine learning; Student field forecasting",
"Alsayegh F., Alkhamis M.A., Ali F., Attur S., Fountain-Jones N.M., Zubaid M.","Anemia or other comorbidities? using machine learning to reveal deeper insights into the drivers of acute coronary syndromes in hospital admitted patients","10.1371/journal.pone.0262997","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123417163&doi=10.1371%2fjournal.pone.0262997&partnerID=40&md5=d1d2d15a7f322670efc9a1639acb562a","Acute coronary syndromes (ACS) are a leading cause of deaths worldwide, yet the diagnosis and treatment of this group of diseases represent a significant challenge for clinicians. The epidemiology of ACS is extremely complex and the relationship between ACS and patient risk factors is typically non-linear and highly variable across patient lifespan. Here, we aim to uncover deeper insights into the factors that shape ACS outcomes in hospitals across four Arabian Gulf countries. Further, because anemia is one of the most observed comorbidities, we explored its role in the prognosis of most prevalent ACS in-hospital outcomes (mortality, heart failure, and bleeding) in the region. We used a robust multi-algorithm interpretable machine learning (ML) pipeline, and 20 relevant risk factors to fit predictive models to 4,044 patients presenting with ACS between 2012 and 2013. We found that inhospital heart failure followed by anemia was the most important predictor of mortality. However, anemia was the first most important predictor for both in-hospital heart failure, and bleeding. For all in-hospital outcome, anemia had remarkably non-linear relationships with both ACS outcomes and patients' baseline characteristics. With minimal statistical assumptions, our ML models had reasonable predictive performance (AUCs > 0.75) and substantially outperformed commonly used statistical and risk stratification methods. Moreover, our pipeline was able to elucidate ACS risk of individual patients based on their unique risk factors. Fully interpretable ML approaches are rarely used in clinical settings, particularly in the Middle East, but have the potential to improve clinicians' prognostic efforts and guide policymakers in reducing the health and economic burdens of ACS worldwide. © 2022 Alsayegh et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"acute coronary syndrome; adult; anemia; Article; comorbidity; controlled study; coronary angiography; diagnostic error; disease burden; disease registry; female; health care policy; health economics; heart output; hospital admission; human; machine learning; major clinical study; male; percutaneous coronary intervention; prognosis; risk assessment; risk factor; support vector machine; acute coronary syndrome; aged; anemia; biological model; comorbidity; epidemiology; hospital mortality; middle aged; Middle East; mortality; register; Acute Coronary Syndrome; Aged; Anemia; Comorbidity; Female; Hospital Mortality; Humans; Machine Learning; Male; Middle Aged; Middle East; Models, Cardiovascular; Patient Admission; Registries; Risk Assessment"
"Al-Shaer R., Spring J.M., Christou E.","Learning the Associations of MITRE ATT CK Adversarial Techniques","10.1109/CNS48642.2020.9162207","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090163451&doi=10.1109%2fCNS48642.2020.9162207&partnerID=40&md5=08590e11c227d5dba703a5d02a9924a9","The MITRE ATTCK Framework provides a rich and actionable repository of adversarial tactics, techniques, and procedures (TTP). However, this information would be highly useful for attack diagnosis (i.e., forensics) and mitigation (i.e., intrusion response) if we can reliably construct technique associations that will enable predicting unobserved attack techniques based on observed ones. In this paper, we present our statistical machine learning analysis on APT and Software attack data reported by MITRE ATTCK to infer the technique clustering that represents the significant correlation that can be used for technique prediction. Due to the complex multidimensional relationships between techniques, many of the traditional clustering methods could not obtain usable associations. Our approach, using hierarchical clustering for inferring attack technique associations with 95% confidence, provides statistically significant and explainable technique correlations. Our analysis discovers 98 different technique associations (i.e., clusters) for both APT and Software attacks. Our evaluation results show that 78% of the techniques associated by our algorithm exhibit significant mutual information that indicates reasonably high predictability. © 2020 IEEE.",,"Hierarchical clustering; Attack diagnosis; Evaluation results; Intrusion response; Mutual informations; Software attacks; Statistical machine learning; Traditional clustering; Network security"
"Al-Shalabi L.","A Data mining model for students' choice of college major based on rough set theory","10.3844/jcssp.2019.1150.1160","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074232496&doi=10.3844%2fjcssp.2019.1150.1160&partnerID=40&md5=a9d32a902b4b2f82e7ed31cf0d42b04b","Literature is focusing on identifying factors that influence students' initial choice of major and few have studied students' involvements after registration in a selected major and this study is one of the few. This study aims to determine the important factors that influence high school students' choice of major based on data mining techniques. A questionnaire was designed to collect data from students in different universities in Kuwait and in different faculties such as science, literature, medicine and engineering. Rough set theory for feature selection was used to highlight and explain the significant factors related to students' skills and preferences awareness as well as their experience reflection that are responsible for the development of their satisfaction with the choice of their university majors. The findings of the study revealed that the calculated reducts have a significant influence on the students' choice of the university and collage major. This research contributes to literature by identifying the relationship between the conditional factors of the reduct (also known as the independent variables) and the classification attribute (also known as the dependent variable). The results of the study give valuable information to the high school students so they know the best majors which suite their skills, preference and experiences. This research also help students not to continually change their major because of the wrong choice of major they made which accordingly lead them to dissatisfaction of their major. © 2019 Luai Al-Shalabi.","Classification; Reduct; Rough set; University and college major",
"Alshamaila Y., Habib M., Aljarah I., Alsawalqah H., Faris H., Alsoud A.","An Intelligent Approach for the Effect of Social Media on Undergraduate Students Performance: A Case Study in the University of Jordan","10.1145/3397125.3397146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086181860&doi=10.1145%2f3397125.3397146&partnerID=40&md5=d0b1240f7afb40270353d9b8bb4b3b00","Promoting the learning environment within Jordanian universities and maximizing the students' academic gain are essential national-level problems. Since learning and teaching systems are main building bricks to grow individuals who are responsible for developing and flourishing culture and civilization for Jordanian society. Educational data mining focuses on developing new smart algorithms devoted to analyzing the resulted data from educational systems; in order to better understand students and the learning environments. In this paper, we are analyzing major factors affecting university students' performance and the effect of social media usage on them. Furthermore, predicting the students' performance by adopting different rule-based data mining algorithms like rule learner based on Repeated Incremental Pruning to Produce Error Reduction (JRIP) and a type of decision tree called (PART). We have conducted a research survey within the University of Jordan students that is covering all faculties and cover a vast range of different students. Using both JRIP and PART we have concluded fundamental remarks; mainly, we have noticed that using YouTube as a learning resource has positive impacts on students' performance especially within scientific faculties. Moreover, we have interpreted the impact of other factors, such as having an Internet connection, having several social media applications and others. Certainly, upon our findings, we recommend the importance of integrating YouTube as a learning resource within universities learning environments. © 2020 ACM.","Educational Data Mining; Machine Learning; Social Media; University of Jordan","Computer aided instruction; Data mining; Decision trees; Economic and social effects; Learning systems; Social networking (online); Trees (mathematics); Data mining algorithm; Educational data mining; Educational systems; Learning and teachings; Learning environments; National-level problem; Undergraduate students; University students; Students"
"Alshammari M., Nasraoui O., Sanders S.","Mining Semantic Knowledge Graphs to Add Explainability to Black Box Recommender Systems","10.1109/ACCESS.2019.2934633","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079639757&doi=10.1109%2fACCESS.2019.2934633&partnerID=40&md5=11d0c29f3d6a70fe8a95c79b12006977","Recommender systems are being increasingly used to predict the preferences of users on online platforms and recommend relevant options that help them cope with information overload. In particular, modern model-based collaborative filtering algorithms, such as latent factor models, are considered state-of-the-art in recommendation systems. Unfortunately, these black box systems lack transparency, as they provide little information about the reasoning behind their predictions. White box systems, in contrast, can, by nature, easily generate explanations. However, their predictions are less accurate than sophisticated black box models. Recent research has demonstrated that explanations are an essential component in bringing the powerful predictions of big data and machine learning methods to a mass audience without compromising trust. Explanations can take a variety of formats, depending on the recommendation domain and the machine learning model used to make predictions. The objective of this work is to build a recommender system that can generate both accurate predictions and semantically rich explanations that justify the predictions. We propose a novel approach to build an explanation generation mechanism into a latent factor-based black box recommendation model. The designed model is trained to learn to make predictions that are accompanied by explanations that are automatically mined from the semantic web. Our evaluation experiments, which carefully study the trade-offs between the quality of predictions and explanations, show that our proposed approach succeeds in producing explainable predictions without a significant sacrifice in prediction accuracy. © 2013 IEEE.","Artificial intelligence; collaborative filtering; explanations; matrix factorization; recommender systems; semantic web","Collaborative filtering; Economic and social effects; Forecasting; Knowledge representation; Machine learning; Online systems; Collaborative filtering algorithms; Evaluation experiments; Generation mechanism; Information overloads; Latent factor models; Machine learning methods; Machine learning models; Quality of predictions; Recommender systems"
"Alshammari O.G., Isaac O.S., Clarke S.D., Rigby S.E.","Mitigation of blast loading through blast–obstacle interaction","10.1177/20414196221115869","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138248125&doi=10.1177%2f20414196221115869&partnerID=40&md5=f96379e54e5d03147f81935341c0d4a2","Obstructing the passage of blast waves is an effective method of mitigating blast pressures downstream of the obstacle. To this end, the interaction between a blast wave and a simplified structural shape, such as a cylinder, has been widely investigated to understand the complex flow pattern that ensues around the obstacle. The patterns include the interference zones of the incident wave, the diffracted wave, and other secondary waves in the downstream region. Such zones are responsible for causing significant modifications to the blast wave parameters. This research aims to identify and study the factors that serve to mitigate the resulting blast loads downstream of a cylindrical obstacle – both on the ground, and on a rigid wall target that the obstacle is aiming to protect. Inputs from this numerical study are also used to develop a fast-running predictive method based on an artificial neural network (ANN) model. It was found that the size of the cylinder, the strength of the blast wave, the position of the cylindrical obstruction, and the target length, all have remarkable effects on the development of the complex flow-field downstream, and on the impulse mitigation on a reflective target. A number of key mitigation mechanisms are identified, namely shadowing and interference, and their origins and significance are discussed. An ANN model trained using scaled input parameters could successfully predict impulse values on such a reflective target. Using this model to predict the response of previously unseen configurations (for the ANN) gave excellent correlation, thereby demonstrating the high fidelity of this fast-running tool, and its ability to predict the effectiveness of various wave-cylinder interactions in mitigating blast loading. © The Author(s) 2022.","artificial neural network; Blast wave interaction; blast wave interference; blast wave mitigation; equivalent energy impulse; machine learning",
"Alshazly H., Linse C., Barth E., Martinetz T.","Explainable COVID-19 detection using chest CT scans and deep learning","10.3390/s21020455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099214827&doi=10.3390%2fs21020455&partnerID=40&md5=00c837c96f9a3425cf8b0a0ccb270dcd","This paper explores how well deep learning models trained on chest CT images can diagnose COVID-19 infected people in a fast and automated process. To this end, we adopted advanced deep network architectures and proposed a transfer learning strategy using custom-sized input tailored for each deep architecture to achieve the best performance. We conducted extensive sets of experiments on two CT image datasets, namely, the SARS-CoV-2 CT-scan and the COVID19-CT. The results show superior performances for our models compared with previous studies. Our best models achieved average accuracy, precision, sensitivity, specificity, and F1-score values of 99.4%, 99.6%, 99.8%, 99.6%, and 99.4% on the SARS-CoV-2 dataset, and 92.9%, 91.3%, 93.7%, 92.2%, and 92.5% on the COVID19-CT dataset, respectively. For better interpretability of the results, we applied visualization techniques to provide visual explanations for the models’ predictions. Feature visualizations of the learned features show well-separated clusters representing CT images of COVID-19 and non-COVID-19 cases. Moreover, the visualizations indicate that our models are not only capable of identifying COVID-19 cases but also provide accurate localization of the COVID-19-associated regions, as indicated by well-trained radiologists. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Coronavirus; COVID-19 detection; Explainable deep learning; Feature visualization; SARS-CoV-2","Automation; Deep learning; Learning systems; Network architecture; Transfer learning; Visualization; Automated process; Best model; Chest CT scans; Deep architectures; F1 scores; Interpretability; Learning models; Visualization technique; Computerized tomography; algorithm; computer assisted diagnosis; diagnosis; diagnostic imaging; factual database; human; pathogenicity; pathology; thorax; virology; x-ray computed tomography; Algorithms; COVID-19; Databases, Factual; Deep Learning; Humans; Neural Networks, Computer; Radiographic Image Interpretation, Computer-Assisted; SARS-CoV-2; Thorax; Tomography, X-Ray Computed"
"Alshazly H., Linse C., Barth E., Martinetz T.","Deep convolutional neural networks for unconstrained ear recognition","10.1109/ACCESS.2020.3024116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097845872&doi=10.1109%2fACCESS.2020.3024116&partnerID=40&md5=3c6329f630ef882be97a4ed318386e9c","This paper employs state-of-the-art Deep Convolutional Neural Networks (CNNs), namely AlexNet, VGGNet, Inception, ResNet and ResNeXt in a first experimental study of ear recognition on the unconstrained EarVN1.0 dataset. As the dataset size is still insufficient to train deep CNNs from scratch, we utilize transfer learning and propose different domain adaptation strategies. The experiments show that our networks, which are fine-tuned using custom-sized inputs determined specifically for each CNN architecture, obtain state-of-the-art recognition performance where a single ResNeXt101 model achieves a rank-1 recognition accuracy of 93.45%. Moreover, we achieve the best rank-1 recognition accuracy of 95.85% using an ensemble of fine-tuned ResNeXt101 models. In order to explain the performance differences between models and make our results more interpretable, we employ the t-SNE algorithm to explore and visualize the learned features. Feature visualizations show well-separated clusters representing ear images of the different subjects. This indicates that discriminative and ear-specific features are learned when applying our proposed learning strategies. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Biometrics; Convolutional neural networks; Deep learning; Ear recognition; Feature visualization; Transfer learning","Convolution; Deep neural networks; Learning to rank; Transfer learning; Data set size; Different domains; Ear recognition; Learning strategy; Recognition accuracy; State of the art; Convolutional neural networks"
"Alsheakh H., Bhattacharjee S.","Towards a unified trust framework for detecting IoT device attacks in smart homes","10.1109/MASS50613.2020.00080","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102201008&doi=10.1109%2fMASS50613.2020.00080&partnerID=40&md5=c59b73df0660abf7bec7dfe490ada1a9","Trust in Smart Home (SH) Internet of Things (IoT) technologies is a primary concern for consumers, which is preventing the widespread adoption of smart home services. Additionally, the variety of IoT devices and cyber attacks make it hard to build a generic attack detection framework for smart home IoT devices. In this paper, we present a roadmap towards building a unified approach towards establishing trust scores as an indicator of the security status of an IoT device in a smart home that works across multiple attacks and device types/protocols. Specifically, we first introduce artificial reasoning inspired evidence collection approach by introducing a small set of factors that are affected significantly if a smart home IoT device is under attack. Thereafter, we propose an explainable trust scoring model that maps the device level evidence into trust scores in a way that produces lower trust scores when devices are under attack. Specifically, the trust model involves an Augmented Bayesian Belief based Model embedded with novel non-linear weighing functions; explicitly designed to account for the severity of the attack, probabilistic discounting of parts of the evidence caused by benign changes, thus explaining our success. For evaluation of the framework, we use two real datasets that contain a variety of actual cyber-attacks and benign traffic from seven different smart home IoT devices. Our evaluation seeks to investigate the generality of our framework across multiple datasets, with various classes of IoT devices and cyber attacks. © 2020 IEEE.","Artificial Intelligence based Security; Internet-of-Things; Machine Learning; Security; Smart Home; Trust","Ambient intelligence; Automation; Computer crime; Crime; Intelligent buildings; Network security; Artificial reasoning; Attack detection; Evidence collection; Internet of Things (IOT); Multiple data sets; Smart home services; Trust modeling; Unified approach; Internet of things"
"Alshehri M.","Fuzzy Logic Based Explainable AI Approach for the Easy Calibration of AI Models in IoT Environments","10.1007/978-3-030-98404-5_57","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127090328&doi=10.1007%2f978-3-030-98404-5_57&partnerID=40&md5=4fa6ce1fef9a6e5ed63bba2287053faa","The Internet of Things (IoT) permeates all aspects of human existence shortly. As a result of the IoT, it can now construct a smart world. For this to happen, however, extracting meaningful information from raw sensory input functioning in loud and complicated settings must be addressed to achieve it. For example, bandwidth, processing power, and power consumption must be addressed while building a possible IoT system. Due to the current epidemic, the need for contactless solutions has risen. Possible solutions include a gesture-based control system that protects user privacy and can operate several different appliances simultaneously. When implementing such gesture-based control systems, opaque box artificial intelligence (AI) models are used. This opaque box AI model has shown good performance metrics on in-distribution data when tested in a lab. However, their complexity and opaqueness make them prone to failure when exposed to real-world out-of-distribution input. In contrast to opaque box models, explainable AI models based on fuzzy logic (EAI-FL) demonstrate comparable performance on lab data distributions. The type-2 fuzzy models, on the other hand, are readily calibrated and modified to offer equivalent performance to those attained on the lab in-distribution data in the real world. © 2022, Springer Nature Switzerland AG.","AI; Calibration; Fuzzy logic; IoT","Artificial intelligence; Computer circuits; Control systems; Fuzzy logic; Internet of things; Laboratories; 'current; Contact less; Fuzzy-Logic; Intelligence models; Performance; Processing power; Real-world; Sensory input; Smart world; User privacy; Calibration"
"Alsheref F.K., Gomaa W.H.","Blood diseases detection using classical machine learning algorithms","10.14569/ijacsa.2019.0100712","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070096350&doi=10.14569%2fijacsa.2019.0100712&partnerID=40&md5=1c719533e85cb9e81be04ffdcad2d057","Blood analysis is an essential indicator for many diseases; it contains several parameters which are a sign for specific blood diseases. For predicting the disease according to the blood analysis, patterns that lead to identifying the disease precisely should be recognized. Machine learning is the field responsible for building models for predicting the output based on previous data. The accuracy of machine learning algorithms is based on the quality of collected data for the learning process; this research presents a novel benchmark data set that contains 668 records. The data set is collected and verified by expert physicians from highly trusted sources. Several classical machine learning algorithms are tested and achieved promising results. © 2018 The Science and Information (SAI) Organization Limited.","Blood disease; Classification algorithms; Decision trees; K-means; KNN; Machine learning","Benchmarking; Blood; K-means clustering; Learning algorithms; Machine learning; Blood analysis; Blood disease; Building model; Classification algorithm; Data set; Disease detection; K-means; KNN; Learning process; Machine learning algorithms; Decision trees"
"Alshra'A A.S., Farhat A., Seitz J.","Deep Learning Algorithms for Detecting Denial of Service Attacks in Software-Defined Networks","10.1016/j.procs.2021.07.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115217401&doi=10.1016%2fj.procs.2021.07.032&partnerID=40&md5=70ccae045e504b351e9321fbbad6d2f7","In Software-Defined Networking (SDN) the controller is the only entity that has the complete view on the network, and it acts as the brain, which is responsible for traffic management based on its global knowledge of the network. Therefore, an attacker attempts to direct malicious traffic towards the controller, which could lead to paralyze the entire network. In this work, Deep Learning algorithms are used to protect the controller by applying high-security measures, which is essential for the continuous availability and connectivity in the network. Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) are proposed to recognize and prevent the intrusion attacks. We evaluate our models using a recently released dataset (InSDN dataset). Finally, our experiments manifest that our models achieve very high accuracy for the detection of Denial of Service (DoS) attacks. Thus, a significant improvement in attack detection can be shown compared to one of the benchmarking state of the art approaches. © 2021 Elsevier B.V.. All rights reserved.","Deep Learning (DL); Denial of Service (DoS); Gated Recurrent Unit (GRU); Long Short-Term Memory (LSTM); Recurrent Neural Networks (RNN); Software-Defined Networking (SDN)","Brain; Controllers; Denial-of-service attack; Learning algorithms; Network security; Software defined networking; Deep learning; Denial of Service; Denialof- service attacks; Gated recurrent unit; Long short-term memory; Recurrent neural network; Software-defined networking; Software-defined networkings; Software-defined networks; Long short-term memory"
"Alshraideh H., Castillo E.D., Gil Del Val A.","Process control via random forest classification of profile signals: An application to a tapping process","10.1016/j.jmapro.2020.08.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090265827&doi=10.1016%2fj.jmapro.2020.08.043&partnerID=40&md5=a9d38445d0e509ae7a190f6670b28208","Due to technological advancements, many manufacturing processes are now real-time monitored through sensors that provide continuous signals of the process parameters rather than providing simpler point observations of the process response. Signals (profiles) obtained through these sensors can reveal important information about the quality of the process being monitored. In this work, we propose a general predictive control framework for on-line process quality monitoring where data is available in the form of a profile. The proposed framework is an integration of ideas from classical on-line process control and advanced machine learning techniques, namely, Random Forests. The proposed framework has the advantages of being more interpretable compared to other methods found in the literature, and has the flexibility to include several commonly used transformations of the signal as features. In addition, abnormal out of control signal characteristics of the process known from experience by operators can be easily incorporated in the random forest technique. An illustration of the proposed framework applied to the case of a tapping manufacturing process is provided. Model comparison results show a superior performance of the proposed framework over previously proposed monitoring methods for the considered tapping process. From a receiver operating characteristic curve analysis, it was found that an area under the curve (AUC) of 0.923 was achieved by the proposed model compared to an AUC of 0.867 for the Generalized Variance model proposed in the literature. © 2020 The Society of Manufacturing Engineers","Predictive control; Profile monitoring; Random Forest; Tapping process","Decision trees; Learning systems; Predictive control systems; Random forests; Tapping (threads); Area under the curves; General predictive control; Machine learning techniques; On-line process controls; Out-of-control signals; Random forest classification; Receiver operating characteristic curve analysis; Technological advancement; Process control"
"Alsinglawi B., Alshari O., Alorjani M., Mubin O., Alnajjar F., Novoa M., Darwish O.","An explainable machine learning framework for lung cancer hospital length of stay prediction","10.1038/s41598-021-04608-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122813466&doi=10.1038%2fs41598-021-04608-7&partnerID=40&md5=84ade389fad3c8c3ef3db9e7450c92f4","This work introduces a predictive Length of Stay (LOS) framework for lung cancer patients using machine learning (ML) models. The framework proposed to deal with imbalanced datasets for classification-based approaches using electronic healthcare records (EHR). We have utilized supervised ML methods to predict lung cancer inpatients LOS during ICU hospitalization using the MIMIC-III dataset. Random Forest (RF) Model outperformed other models and achieved predicted results during the three framework phases. With clinical significance features selection, over-sampling methods (SMOTE and ADASYN) achieved the highest AUC results (98% with CI 95%: 95.3–100%, and 100% respectively). The combination of Over-sampling and under-sampling achieved the second-highest AUC results (98%, with CI 95%: 95.3–100%, and 97%, CI 95%: 93.7–100% SMOTE-Tomek, and SMOTE-ENN respectively). Under-sampling methods reported the least important AUC results (50%, with CI 95%: 40.2–59.8%) for both (ENN and Tomek- Links). Using ML explainable technique called SHAP, we explained the outcome of the predictive model (RF) with SMOTE class balancing technique to understand the most significant clinical features that contributed to predicting lung cancer LOS with the RF model. Our promising framework allows us to employ ML techniques in-hospital clinical information systems to predict lung cancer admissions into ICU. © 2022, The Author(s).",,"comparative study; human; length of stay; lung tumor; machine learning; Humans; Length of Stay; Lung Neoplasms; Machine Learning"
"Alsrehin N.O., Klaib A.F., Magableh A.","Intelligent Transportation and Control Systems Using Data Mining and Machine Learning Techniques: A Comprehensive Study","10.1109/ACCESS.2019.2909114","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065066857&doi=10.1109%2fACCESS.2019.2909114&partnerID=40&md5=988d79a09d27e93495c62498b6d8000f","Traffic congestion is becoming the issues of the entire globe. This study aims to explore and review the data mining and machine learning technologies adopted in research and industry to attempt to overcome the direct and indirect traffic issues on humanity and societies. The study's methodology is to comprehensively review around 165 studies, criticize, and categorize all these studies into a chronological and understandable category. The study is focusing on the traffic management approaches that were depended on data mining and machine learning technologies to detect and predict the traffic only. This study has found that there is no standard traffic management approach that the community of traffic management has agreed on. This study is important to the traffic research communities, traffic software companies, and traffic government officials. It has a direct impact on drawing a clear path for new traffic management propositions. This study is one of the largest studies with respect to the size of its reviewed articles that were focused on data mining and machine learning. Additionally, this study will draw general attention to a new traffic management proposition approach. © 2013 IEEE.","Artificial intelligent; data mining; intelligent transportation; machine learning","Data mining; Industrial research; Learning systems; Machine learning; Artificial intelligent; Government officials; Intelligent transportation; Machine learning techniques; Machine learning technology; Research communities; Software company; Traffic management; Traffic congestion"
"Alstad A., Mevassvik O.M., Nielsen M.N., Løvlid R.A., Henderson H.C., Jansen R.E.J., De Reus N.M.","Low-level battle management language",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877619663&partnerID=40&md5=ae0895d28439b261de27163369ff5db8","TNO (The Netherlands) and FFI (Norway) are cooperating in extending a COTS Computer Generated Forces (CGF) tool with a Coalition Battle Management Language (C-BML) interface for executing C-BML orders and issuing reports. Due to the lack of satisfactory models for command and control (C2)/combat management in existing CGF tools, TNO and FFI have investigated the use of external agent frameworks. Two different modelling paradigms have been used: Belief-desire-intention (BDI) and Context-based Reasoning (CxBR). As part of this work a Low-level Battle Management Language (Low-level BML) has been createdfor communication between the C2/combat management agents and the CGF tool over High Level Architecture (HLA). The hierarchy of combat management agents decompose a C-BML order into Low-level BML commands and tasks understandable by a CGF tool. The agents also receive Low-level BML events reported by the CGF tool and make use of these for agent behaviour and C-BML reports. This paper presents the structure of Low-level BML, how it is used and the rationale behind it.","Agent modelling; Artificial intelligence; Battle Management Language; Belief-desire-intention; Computer Generated Forces; Context-based reasoning","Agent modelling; Battle management languages; Belief-desire-intentions; Computer generated forces; Context-based reasoning; Artificial intelligence; Command and control systems; Intelligent agents; Interoperability; Management; C (programming language)"
"AlSulaiti F.","The national commitment towards conserving the heritage (documentation of historical and cultural sites in GCC Countries)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924282982&partnerID=40&md5=c851c7e7348232d8c769f77df32d49f6","The five Arab Gulf countries of Kuwait, Bahrain, Qatar, the United Arab Emirates and Oman possess many shared characteristics and historical ties across their common peninsula. The prime factor uniting them is the historical nature of their entwined involvement with peoples and nations beyond the region. That the Gulf has been an important water passageway since ancient times suggests that the inhabitants of its shores met early on with other civilizations. The knowledge of one's roots, history, and traditional arts supports awareness of inherited culture and can help contextualize and illuminate community reflection and identification. The intricacy of the recording and understanding processes of documentation requires skilled professionals, with knowledge and awareness for the associated tasks. Responsible of cultural heritage should provide the adequate documentations, recording and updating of the records. Collaboration of different individuals such as specialist heritage, archaeologists, surveyors, conservators, researchers, architectural historians, and many other expert personnel is the golden key of successful documentation. The purpose of this document is to show the authorities of Gulf Arab countries and their planning measures, management and sharing effect of recording the cultural heritage. This essay identifies key points in the approach to contextualizing and developing cultural identity in a way that respects organic qualities. Through highlighting a number of archeological ruins and outlining management plans, the essay explores frameworks that can be applied to promote and preserve integral identity of important sites and their greater surrounding communities.","Al-zubarah-qatar; Cultural heritage management plan; Documentation; Gulf countries","Artificial intelligence; Earth sciences; Remote sensing; Software engineering; System program documentation; Cultural heritages; Cultural identity; Gulf countries; Historical and cultural sites; Integral identities; Management plans; Planning measures; United Arab Emirates; History"
"Alsuradi H., Eid M.","Trial-based Classification of Haptic Tasks Based on EEG Data","10.1109/WHC49131.2021.9517230","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115152434&doi=10.1109%2fWHC49131.2021.9517230&partnerID=40&md5=fefa6a33ea8952b1ef60fc777608b0ac","With the increasing popularity of neural imaging techniques such as electroencephalography (EEG), developing quantitative measures to characterize haptic interactions is becoming a reality. Meanwhile, machine learning is a promising approach for trial-based EEG data analysis. This work presents a model that can distinguish between passive and active kinesthetic interactions based on a single trial EEG data. An interactive task that involves hitting a ball using a racket is developed under passive and active kinesthetic settings using a haptic device and a computer screen. Temporal and frequency domain features are extracted from the motor and somatosensory cortices, and a proposed 2-D CNN model is trained on data extracted from 19 participants. The model achieves a mean accuracy of 84.56%, 93.96%, and 95.89% across 5-fold validation when using one, four, or six electrodes, respectively. The model mechanism is assessed using an explainable machine learning algorithm, LIME, which shows that the model utilizes sensible features from a neuroscience perspective towards its prediction. This work paves the way for a better understanding of the neural mechanisms associated with kinesthetic haptic interaction, which proves helpful in many applications such as motor rehabilitation and brain-computer interactions, in addition to modeling the haptic quality of experience objectively. © 2021 IEEE.",,"Computational neuroscience; Electroencephalography; Electrophysiology; Frequency domain analysis; Imaging techniques; Lime; Machine learning; Quality of service; Brain computer interactions; Frequency domains; Haptic interactions; Motor rehabilitation; Neural mechanisms; Quality of experience (QoE); Quantitative measures; Somatosensory cortex; Learning algorithms"
"Alsuradi H., Park W., Eid M.","Explainable Classification of EEG Data for an Active Touch Task Using Shapley Values","10.1007/978-3-030-60117-1_30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094129176&doi=10.1007%2f978-3-030-60117-1_30&partnerID=40&md5=edcbdce0be4570b1515695f9196bfc58","Machine learning has been used in the last decade to solve many problems in the haptics field. In particular, EEG data that is recorded during haptic interactions was used to train machine learning (ML) models to answer questions that are of interest to the neurohaptics community. However, the behavior of machine learning models in taking out their decisions is treated as black box hindering the interpretability of these decisions. In this paper, we used Shapley values, a concept from game theory, to explain the behavior of a tree-based classifier model in classifying electroencephalography data that was collected during an interaction with a surface haptic device under two conditions: with and without tactile feedback. We trained a tree-based ML model to classify data based on the presence or absence of tactile feedback. Using Shapley values, we identified the features (across and within channels) that contribute the most to the classification decision. Results showed channel AF3 and neural activity after 700 ms from the onset contributed the most in recognizing tactile feedback in the interaction. This study demonstrates the use of explainable machine learning in the field of Neurohaptics. © 2020, Springer Nature Switzerland AG.","EEG; Explainable machine learning; Haptics; Neurohaptics","Electroencephalography; Electrophysiology; Game theory; Human computer interaction; Machine learning; Neurons; Trees (mathematics); Classification decision; Classifier models; Haptic interactions; Interpretability; Machine learning models; Neural activity; Surface haptic; Tactile feedback; Classification (of information)"
"Alt T., Weickert J.","Learning a generic adaptive wavelet shrinkage function for denoising","10.1109/ICASSP40776.2020.9054644","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091277497&doi=10.1109%2fICASSP40776.2020.9054644&partnerID=40&md5=f445da68d76bc7730cb85feab4f84632","The rise of machine learning in image processing has created a gap between trainable data-driven and classical modeldriven approaches: While learning-based models often show superior performance, classical ones are often more transparent. To reduce this gap, we introduce a generic wavelet shrinkage function for denoising which is adaptive to both the wavelet scales as well as the noise standard deviation. It is inferred from trained results of a tightly parametrised function which is inherited from nonlinear diffusion. Our proposed shrinkage function is smooth and compact while only using two parameters. In contrast to many existing shrinkage functions, it is able to enhance image structures by amplifying wavelet coefficients. Experiments show that it outperforms classical shrinkage functions by a significant margin. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Adaptive Thresholding; Denoising; Interpretable Learning; Wavelet Shrinkage","Audio signal processing; Image enhancement; Speech communication; Wavelet analysis; Adaptive wavelets; Learning Based Models; Model driven approach; Nonlinear diffusion; Shrinkage functions; Standard deviation; Wavelet coefficients; Wavelet shrinkage; Shrinkage"
"Altaheri H., Muhammad G., Alsulaiman M.","Physics-inform attention temporal convolutional network for EEG-based motor imagery classification","10.1109/TII.2022.3197419","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136127820&doi=10.1109%2fTII.2022.3197419&partnerID=40&md5=35edbee359d2beed028bdce16746bc35","The brain-computer interface (BCI) is a cutting-edge technology that has the potential to change the world. Electroencephalogram (EEG) motor imagery (MI) signal has been used extensively in many BCI applications to assist disabled people, control devices or environments, and even augment human capabilities. However, the limited performance of brain signal decoding is restricting the broad growth of the BCI industry. In this paper, we propose an attention-based temporal convolutional network (ATCNet) for EEG-based motor imagery classification. The ATCNet model utilizes multiple techniques to boost the performance of MI classification with a relatively small number of parameters. ATCNet employs scientific machine learning to design a domain-specific DL model with interpretable and explainable features, multi-head self-attention to highlight the most valuable features in MI-EEG data, temporal convolutional network (TCN) to extract high-level temporal features, and convolutional-based sliding window to augment the MI-EEG data efficiently. The proposed model outperforms the current state-of-the-art techniques in the BCI Competition IV-2a dataset with an accuracy of 85.38&#x0025; and 70.97&#x0025; for the subject-dependent and subject-independent modes, respectively. IEEE","attention; Brain modeling; classification; Convolution; convolution neural network (CNN); Convolutional neural networks; Data models; Deep learning; EEG; Electroencephalography; Feature extraction; motor imagery; scientific machine learning; Task analysis; temporal convolution networks (TCN)","Brain; Brain computer interface; Brain mapping; Convolution; Data mining; Deep learning; Electrophysiology; Image classification; Job analysis; Neural networks; Attention; Brain modeling; Convolution neural network; Convolutional neural network; Deep learning; Features extraction; Machine-learning; Motor imagery; Scientific machine learning; Task analysis; Temporal convolution network; Electroencephalography"
"Al-Tahmeesschi A., Umebayashi K., Iwata H., Lehtomäki J., Lopez-Benitez M.","Feature-Based Deep Neural Networks for Short-Term Prediction of WiFi Channel Occupancy Rate","10.1109/ACCESS.2021.3088423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112378721&doi=10.1109%2fACCESS.2021.3088423&partnerID=40&md5=98c5f377fd1c3c8534c36b1b5353940f","Spectrum occupancy prediction is a key enabling technology to facilitate a proactive resource allocation for dynamic spectrum management systems. This work focuses on the prediction of duty cycle (DC) metric that reflects spectrum usage (in the time domain). The spectrum usage is typically measured on a shorter time scale than needed for prediction. Hence, data thinning is required and we apply block averaging. However, averaging operation results in flattening the DC data and losing essential features to assist deep neural network (DNN) to predict the spectrum usage. To improve DC prediction after block averaging, a feature-based deep learning framework is proposed. Namely, long short-term memory (LSTM) and gated recurrent unit (GRU) are selected and enhanced by using features of the data, such as the variance of DC data in addition to DC data themself. The proposed model is capable of proactively predicting the spectrum usage by capturing complex relationships among various input features for the measured spectrum, thus providing higher prediction accuracy with an average improvement of 5% in RMSE compared with traditional models. Moreover, to have a better understanding of the proposed model, we quantify the effect of input features on the predicted spectrum usage values. Based on the most significant input features, a simpler and more efficient model is proposed to estimate DC with similar accuracy to when using all features. © 2013 IEEE.","5G; deep neural networks; explainable AI; GRU; LSTM; occupancy rate; SHAP; short-term prediction; spectrum awareness; WiFi","Deep learning; Deep neural networks; Forecasting; Wi-Fi; Wireless local area networks (WLAN); Complex relationships; Dynamic spectrum management; Enabling technologies; Essential features; Learning frameworks; Prediction accuracy; Short term prediction; Spectrum occupancies; Long short-term memory"
"Al-Taie Z., Hannink M., Mitchem J., Papageorgiou C., Shyu C.-R.","Drug repositioning and subgroup discovery for precision medicine implementation in triple negative breast cancer","10.3390/cancers13246278","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121433353&doi=10.3390%2fcancers13246278&partnerID=40&md5=259c68042d6d23d132154ceb3d221b24","Breast cancer (BC) is the leading cause of death among female patients with cancer. Patients with triple-negative breast cancer (TNBC) have the lowest survival rate. TNBC has substantial heterogeneity within the BC population. This study utilized our novel patient stratification and drug repositioning method to find subgroups of BC patients that share common genetic profiles and that may respond similarly to the recommended drugs. After further examination of the discovered patient subgroups, we identified five homogeneous druggable TNBC subgroups. A drug repositioning algorithm was then applied to find the drugs with a high potential for each subgroup. Most of the top drugs for these subgroups were chemotherapy used for various types of cancer, including BC. After analyzing the biological mechanisms targeted by these drugs, ferroptosis was the common cell death mechanism induced by the top drugs in the subgroups with neoplasm subdivision and race as clinical variables. In contrast, the antioxidative effect on cancer cells was the common targeted mechanism in the subgroup of patients with an age less than 50. Literature reviews were used to validate our findings, which could provide invaluable insights to streamline the drug repositioning process and could be further studied in a wet lab setting and in clinical trials. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Antioxidant; Data mining; Drug repositioning; Drug repurposing; Explainable artificial intelligence; Ferroptosis; Network analysis; Patient stratification; Subgrouping; Triple negative breast cancer","afatinib; bosutinib; cerulenin; dasatinib; dexamethasone; erlotinib; fluindostatin; fulvestrant; galantamine; gefitinib; imatinib; lapatinib; mevinolin; nilotinib; pazopanib; ponatinib; regorafenib; rifampicin; sorafenib; sunitinib; thioctic acid; vorinostat; adult; aging; antioxidant activity; Article; cancer cell; cancer chemotherapy; cell death; controlled study; drug design; drug efficacy; drug megadose; drug potency; drug repositioning; drug targeting; female; ferroptosis; genetic profile; human; human tissue; major clinical study; middle aged; personalized medicine; publication; triple negative breast cancer"
"Al-Taie Z., Liu D., Mitchem J.B., Papageorgiou C., Kaifi J.T., Warren W.C., Shyu C.-R.","Explainable artificial intelligence in high-throughput drug repositioning for subgroup stratifications with interventionable potential","10.1016/j.jbi.2021.103792","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107949586&doi=10.1016%2fj.jbi.2021.103792&partnerID=40&md5=4363de29a8df9b7e0b3a2509ce200890","Enabling precision medicine requires developing robust patient stratification methods as well as drugs tailored to homogeneous subgroups of patients from a heterogeneous population. Developing de novo drugs is expensive and time consuming with an ultimately low FDA approval rate. These limitations make developing new drugs for a small portion of a disease population unfeasible. Therefore, drug repositioning is an essential alternative for developing new drugs for a disease subpopulation. This shows the importance of developing data-driven approaches that find druggable homogeneous subgroups within the disease population and reposition the drugs for these subgroups. In this study, we developed an explainable AI approach for patient stratification and drug repositioning. Contrast pattern mining and network analysis were used to discover homogeneous subgroups within a disease population. For each subgroup, a biomedical network analysis was done to find the drugs that are most relevant to a given subgroup of patients. The set of candidate drugs for each subgroup was ranked using an aggregated drug score assigned to each drug. The proposed method represents a human-in-the-loop framework, where medical experts use the data-driven results to generate hypotheses and obtain insights into potential therapeutic candidates for patients who belong to a subgroup. Colorectal cancer (CRC) was used as a case study. Patients' phenotypic and genotypic data was utilized with a heterogeneous knowledge base because it gives a multi-view perspective for finding new indications for drugs outside of their original use. Our analysis of the top candidate drugs for the subgroups identified by medical experts showed that most of these drugs are cancer-related, and most of them have the potential to be a CRC regimen based on studies in the literature. © 2021 The Author(s)","Data mining; Drug repositioning; Explainable AI; Network analysis; Subgroup stratifications","Diseases; Knowledge based systems; Population statistics; Colorectal cancers (CRC); Contrast patterns; Data-driven approach; Drug repositioning; Heterogeneous Knowledge; Heterogeneous populations; Human-in-the-loop; Medical experts; Artificial intelligence; afatinib; cerulenin; crizotinib; dabrafenib; dactinomycin; dasatinib; digitoxin; digoxin; doxorubicin; gefitinib; idarubicin; menadione; niclosamide; perhexiline; varenicline; vinblastine; adult; aged; Article; artificial intelligence; attitude to health; clinical feature; cohort analysis; colorectal cancer; computer model; controlled study; drug repositioning; female; gene expression; genotype; human; intervention study; major clinical study; male; medical expert; middle aged; network analysis; personalized medicine; phenotype; population structure; social stratification; biology; knowledge base; Artificial Intelligence; Computational Biology; Drug Repositioning; Humans; Knowledge Bases; Precision Medicine"
"Altan G.","DeepOCT: An explainable deep learning architecture to analyze macular edema on OCT images","10.1016/j.jestch.2021.101091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123368475&doi=10.1016%2fj.jestch.2021.101091&partnerID=40&md5=888fb7ba2270496ecb8b929b7352b516","Macular edema (ME) is one of the most common retinal diseases that occur as a result of the detachment of the retinal layers on the macula. This study provides computer-aided identification of ME for even small pathologies on OCT using the advantages of Deep Learning. The study aims to identify ME on OCT images using a lightweight explainable Convolutional neural networks (CNN) architecture by composing significant feature activation maps and reducing the trainable parameters. A CNN is an effective Deep Learning algorithm, which consists of feature learning and classification stages. The proposed model, DeepOCT, focuses on reaching high classification performances as well as popular pre-trained architectures using less feature learning and shallow dense layers in addition to visualizing the most responsible regions and pathology on feature activation maps. The DeepOCT encapsulates the block-matching and 3D filtering (BM3D) algorithm, flattening the retinal layers to avoid the effects arising from different macula positions, and excluding non-retinal layers by cropping. DeepOCT identified OCT with ME with the rates of 99.20%, 100%, and 98.40% for accuracy, sensitivity, and specificity, respectively. The DeepOCT provides a standardized analysis, a lightweight architecture by reducing the number of trainable parameters, and high classification performances for both large- and small-scale datasets. It can analyze medical images at different levels with simple feature learning, whereas the literature uses complicated pre-trained feature learning architectures. © 2021 Karabuk University","Convolutional neural networks; Deep learning; DeepOCT; Macular edema; Optical coherence tomography",
"Altenburg T., Giese S.H., Wang S., Muth T., Renard B.Y.","Ad hoc learning of peptide fragmentation from mass spectra enables an interpretable detection of phosphorylated and cross-linked peptides","10.1038/s42256-022-00467-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127553859&doi=10.1038%2fs42256-022-00467-7&partnerID=40&md5=bd2505b0974a1d6a57e31211cc1d16ed","Mass spectrometry-based proteomics provides a holistic snapshot of the entire protein set of living cells on a molecular level. Currently, only a few deep learning approaches exist that involve peptide fragmentation spectra, which represent partial sequence information of proteins. Commonly, these approaches lack the ability to characterize less studied or even unknown patterns in spectra because of their use of explicit domain knowledge. Here, to elevate unrestricted learning from spectra, we introduce ‘ad hoc learning of fragmentation’ (AHLF), a deep learning model that is end-to-end trained on 19.2 million spectra from several phosphoproteomic datasets. AHLF is interpretable, and we show that peak-level feature importance values and pairwise interactions between peaks are in line with corresponding peptide fragments. We demonstrate our approach by detecting post-translational modifications, specifically protein phosphorylation based on only the fragmentation spectrum without a database search. AHLF increases the area under the receiver operating characteristic curve (AUC) by an average of 9.4% on recent phosphoproteomic data compared with the current state of the art on this task. Furthermore, use of AHLF in rescoring search results increases the number of phosphopeptide identifications by a margin of up to 15.1% at a constant false discovery rate. To show the broad applicability of AHLF, we use transfer learning to also detect cross-linked peptides, as used in protein structure analysis, with an AUC of up to 94%. © 2022, The Author(s).",,"Deep learning; Domain Knowledge; Mass spectrometry; Molecular biology; Phosphorylation; Search engines; Ad-hoc learning; Learning approach; Living cell; Mass spectra; Molecular levels; Partial sequences; Peptide fragmentation; Phosphoproteomics; Sequence informations; Spectra's; Peptides"
"Altenburger K.M., Ho D.E.","Is yelp actually cleaning up the restaurant industry? A re-analysis on the relative usefulness of consumer reviews","10.1145/3308558.3313683","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066909047&doi=10.1145%2f3308558.3313683&partnerID=40&md5=9b1486be7a706c85f2c08cb890e5f89e","Social media provides the government with novel methods to improve regulation. One leading case has been the use of Yelp reviews to target food safety inspections. While previous research on data from Seattle finds that Yelp reviews can predict unhygienic establishments, we provide a more cautionary perspective. First, we show that prior results are sensitive to what we call “Extreme Imbalanced Sampling”: extreme because the dataset was restricted from roughly 13k inspections to a sample of only 612 inspections with only extremely high or low inspection scores, and imbalanced by not accounting for class imbalance in the population. We show that extreme imbalanced sampling is responsible for claims about the power of Yelp information in the original classification setup. Second, a re-analysis that utilizes the full dataset of 13k inspections and models the full inspection score (regression instead of classification) shows that (a) Yelp information has lower predictive power than prior inspection history and (b) Yelp reviews do not significantly improve predictions, given existing information about restaurants and inspection history. Contrary to prior claims, Yelp reviews do not appear to aid regulatory targeting. Third, this case study highlights critical issues when using social media for predictive models in governance and corroborates recent calls for greater transparency and reproducibility in machine learning. © 2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.","Consumer reviews; Food safety; Regulation; Replication; Yelp","Classification (of information); Food safety; Learning systems; Social networking (online); Consumer reviews; Food safety inspection; Predictive models; Predictive power; Regulation; Replication; Reproducibilities; Yelp; Inspection"
"Alterovitz R., Arvey A., Sankararaman S., Dallett C., Freund Y., Sjölander K.","ResBoost: Characterizing and predicting catalytic residues in enzymes","10.1186/1471-2105-10-197","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67651205773&doi=10.1186%2f1471-2105-10-197&partnerID=40&md5=1955bcd43c236d7479fbbe872392953e","Background: Identifying the catalytic residues in enzymes can aid in understanding the molecular basis of an enzyme's function and has significant implications for designing new drugs, identifying genetic disorders, and engineering proteins with novel functions. Since experimentally determining catalytic sites is expensive, better computational methods for identifying catalytic residues are needed. Results: We propose ResBoost, a new computational method to learn characteristics of catalytic residues. The method effectively selects and combines rules of thumb into a simple, easily interpretable logical expression that can be used for prediction. We formally define the rules of thumb that are often used to narrow the list of candidate residues, including residue evolutionary conservation, 3D clustering, solvent accessibility, and hydrophilicity. ResBoost builds on two methods from machine learning, the AdaBoost algorithm and Alternating Decision Trees, and provides precise control over the inherent trade-off between sensitivity and specificity. We evaluated ResBoost using cross-validation on a dataset of 100 enzymes from the hand-curated Catalytic Site Atlas (CSA). Conclusion: ResBoost achieved 85% sensitivity for a 9.8% false positive rate and 73% sensitivity for a 5.7% false positive rate. ResBoost reduces the number of false positives by up to 56% compared to the use of evolutionary conservation scoring alone. We also illustrate the ability of ResBoost to identify recently validated catalytic residues not listed in the CSA. © 2009 Alterovitz et al; licensee BioMed Central Ltd.",,"AdaBoost algorithm; Alternating decision trees; Evolutionary conservations; False positive rates; Genetic disorders; Logical expressions; Sensitivity and specificity; Solvent accessibility; Adaptive boosting; Computational methods; Decision trees; Enzymes; 7,8, dihydroneopterin aldolase; bacterial enzyme; fructose bisphosphate aldolase; hydrolyase; scytalone dehydratase; unclassified drug; enzyme; algorithm; amino acid sequence; analytic method; article; catalysis; enzyme active site; enzyme analysis; genetic conservation; hydrophobicity; nonhuman; protein analysis; protein expression; protein function; ResBoost; sensitivity and specificity; Staphylococcus aureus; binding site; biology; catalysis; chemistry; computer program; methodology; protein database; Binding Sites; Catalysis; Computational Biology; Databases, Protein; Enzymes; Software"
"Althoff D., Rodrigues L.N., Silva D.D.D.","Addressing hydrological modeling in watersheds under land cover change with deep learning","10.1016/j.advwatres.2021.103965","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108120792&doi=10.1016%2fj.advwatres.2021.103965&partnerID=40&md5=62feedb40ade31322ba296283667709c","The impacts of land cover change have traditionally been assessed in hydrological modeling with a priori knowledge, e.g., using methods based on the curve number, or by calibrating hydrological models over different time periods. However, how hydrological processes respond to such changes is extremely context-dependent. Thus, there is an opportunity for the development of hydrological models that can learn from large hydrological data sets under the context of severe environmental changes. In this study, a single regional hydrological model is developed based on long short-term memory (LSTM) neural networks using different input configurations. One model considers only meteorological forcings as inputs (I1), another model considers meteorological forcings and static catchment attributes (I2), and a third model also considers meteorological forcings and catchment attributes but where the land cover characteristics are dynamic (I3). The models are trained using information from 411 catchments in the Brazilian Cerrado biome. The data set includes, for each catchment, the daily streamflow observations (target), daily precipitation and reference evapotranspiration (meteorological forcings), and 21 catchment attributes including topography, climate indices, soil characteristics, and land cover characteristics. Considering catchment attributes increases the performance of the LSTM model (I2 and I3 median KGE: 0.69). Considering the land use cover characteristics as dynamic improves the predictions under low-flow conditions (I3 median rNSE: 0.62) when compared to the model considering such characteristics as static (I2 median rNSE: 0.53). This study also uses the deep network with the integrated gradients technique to explore the contribution of the catchment characteristics to streamflow and the number of time steps of influence for the deep network in different regions. © 2021 Elsevier Ltd","Cerrado; Data-driven; Explainable artificial intelligence; Regional hydrological model","Catchments; Land use; Long short-term memory; Runoff; Stream flow; Topography; Cerrado; Data driven; Data set; Explainable artificial intelligence; Hydrological models; Land cover; Land-cover change; Meteorological forcing; Regional hydrological model; Short term memory; Climate models; artificial intelligence; catchment; environmental change; flow modeling; hydrological modeling; numerical model; streamflow; watershed"
"Althoff D., Bazameb H.C., Nascimentob J.G.","Untangling hybrid hydrological models with explainable artificial intelligence","10.2166/H2OJ.2021.066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104407891&doi=10.2166%2fH2OJ.2021.066&partnerID=40&md5=4383b2d0afdac777d9e09b3abc2a96a3","Hydrological models are valuable tools for developing streamflow predictions in unmonitored catchments to increase our understanding of hydrological processes. A recent effort has been made in the development of hybrid (conceptual/machine learning) models that can preserve some of the hydrological processes represented by conceptual models and can improve streamflow predictions. However, these studies have not explored how the data-driven component of hybrid models resolved runoff routing. In this study, explainable artificial intelligence (XAI) techniques are used to turn a black-box model into a glass box model. The hybrid models reduced the root mean-square error of the simulated streamflow values by approximately 27, 50, and 24% for stations 17120000, 27380000, and 33680000, respectively, relative to the traditional method. XAI techniques helped unveil the impor tance of accounting for soil moisture in hydrological models. Differing from purely data-driven hydrological models, the inclusion of the production storage in the proposed hybrid model, which is responsible for estimating the water balance, reduced the short-and long-Term dependencies of input variables for streamflow prediction. In addition, soil moisture controlled water percolation, which was the main predictor of streamflow. This finding is because soil moisture controls the underlying mechanisms of groundwater flow into river streams. © 2021 The Authors.","Gr4j; Individual conditional explanations; Lime; Partial dependence profiles; Regression trees",
"Al-Tirawi A., Reynolds R.G.","Cultural Algorithms as a Framework for the Design of Trustable Evolutionary Algorithms","10.1142/S1793351X22400062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129018652&doi=10.1142%2fS1793351X22400062&partnerID=40&md5=d0a931d59935bed0277b6f6a4a91bdd7","One of the major challenges facing Artificial Intelligence in the future is the design of trustworthy algorithms. The development of trustworthy algorithms will be a key challenge in Artificial Intelligence for years to come. Cultural Algorithms (CAs) are viewed as one framework that can be employed to produce a trustable evolutionary algorithm. They contain features to support both sustainable and explainable computation that satisfy requirements for trustworthy algorithms proposed by Cox [Nine experts on the single biggest obstacle facing AI and algorithms in the next five years, Emerging Tech Brew, January 22, 2021]. Here, two different configurations of CAs are described and compared in terms of their ability to support sustainable solutions over the complete range of dynamic environments, from static to linear to nonlinear and finally chaotic. The Wisdom of the Crowds method was selected for the one configuration since it has been observed to work in both simple and complex environments and requires little long-term memory. The Common Value Auction (CVA) configuration was selected to represent those mechanisms that were more data centric and required more long-term memory content. Both approaches were found to provide sustainable performance across all the dynamic environments tested from static to chaotic. Based upon the information collected in the Belief Space, they produced this behavior in different ways. First, the topologies that they employed differed in terms of the ""in degree""for different complexities. The CVA approach tended to favor reduced ""indegree/outdegree"", while the WM exhibited a higher indegree/outdegree in the best topology for a given environment. These differences reflected the fact the CVA had more information available for the agents about the network in the Belief Space, whereas the agents in the WM had access to less available knowledge. It therefore needed to spread the knowledge that it currently had more widely throughout the population. © 2022 World Scientific Publishing Company.","common value auctions; cultural algorithms; dynamic environments; evolutionary algorithms; resilience; robustness; social networks; sustainable learning; Trustability; wisdom of the crowds","Artificial intelligence; Evolutionary algorithms; Robustness (control systems); Social networking (online); Sustainable development; Topology; Common value auctions; Cultural Algorithm; Dynamic environments; In-Degree; Resilience; Robustness; Social network; Sustainable learning; Trustability; Wisdom of the crowds; Facings"
"Al-Tirawi A., Reynolds R.G., Senior","How to Design a Trustable Cultural Algorithm Using Common Value Auctions","10.1109/TransAI51903.2021.00022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126203746&doi=10.1109%2fTransAI51903.2021.00022&partnerID=40&md5=d66fb9054ad62e01de291c1e532d93f1","One of the major challenges facing Artificial Intelligence in the future is the design of trustworthy algorithms. In this paper four basic features of trustworthy algorithms are presented. A Cultural Algorithm based upon Common Value Auctions is presented. It is demonstrated that this framework is able to support each of these fundamental principles. The basic principles are: fairness, explainability, responsibility, and sustainability. The first three are features that are part of the Cultural Algorithm configuration used here. The fourth properties was established experimentally. It was shown that the CVA based Cultural Algorithm exhibited improved sustainability in terms of both resilience and robustness over the of a Cultural Algorithm based upon a Wisdom of the Crowds or voting approach.. © 2021 IEEE.","Common value auctions; Cultural algorithms; Dynamic environments; Evolutionary algorithms; Resilience; Robustness; Social networks; Sustainable learning; Trustability; Wisdom of the crowds","Commerce; Robustness (control systems); Common value auctions; Cultural Algorithm; Dynamic environments; Fundamental principles; Resilience; Robustness; Social network; Sustainable learning; Trustability; Wisdom of the crowds; Sustainable development"
"Altshuler D., Yu K., Papadopoulos J., Dabestani A.","Is P&T Ready to Add Rapid Cycle Analytics to Formulary?","10.1177/0018578720918341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085508624&doi=10.1177%2f0018578720918341&partnerID=40&md5=2361e451182e80df0da43abc4c3f6bcd","Purpose: The intent of this article is to evaluate a novel approach, using rapid cycle analytics and real world evidence, to optimize and improve the medication evaluation process to help the formulary decision making process, while reducing time for clinicians. Summary: The Pharmacy and Therapeutics (P&T) Committee within each health system is responsible for evaluating medication requests for formulary addition. Members of the pharmacy staff prepare the drug monograph or a medication use evaluation (MUE) and allocate precious clinical resources to review patient charts to assess efficacy and value. We explored a novel approach to evaluate the value of our intravenous acetaminophen (IV APAP) formulary admittance. This new methodology, called rapid cycle analytics, can assist hospitals in meeting and/or exceeding the minimum criteria of formulary maintenance as defined by the Joint Commission Standards. In this particular study, we assessed the effectiveness of IV APAP in total hip arthroplasty (THA) and total knee arthroplasty (TKA) procedures. We assessed the correlation to same-stay opioid utilization, average length of inpatient stay and post anesthesia care unit (PACU) time. Conclusion: We were able to explore and improve our organization’s approach in evaluating medications by partnering with an external analytics expert to help organize and normalize our data in a more robust, yet time efficient manner. Additionally, we were able to use a significantly larger external data set as a point of reference. Being able to perform this detailed analytical exercise for thousands of encounters internally and using a data warehouse of over 130 million patients as a point of reference in a short time has improved the depth of our assessment, as well as reducing valuable clinical resources allocated to MUEs to allow for more direct patient care. This clinically real-world and data-rich analytics model is the necessary foundation for using Artificial or Augmented Intelligence (AI) to make real-time formulary and drug selection decisions © The Author(s) 2020.","benchmarking; data analytics; drug/medical use evaluation; formulary management/P&T; rapid cycle analytics; real world evidence","acetylsalicylic acid; bupivacaine; codeine; dexamethasone; epinephrine; fentanyl; hydrocodone; hydromorphone; ketorolac; methadone; morphine; narcotic analgesic agent; oxycodone; oxymorphone; paracetamol; pregabalin; tramadol; aged; analgesia; Article; artificial intelligence; clinical decision making; Current Procedural Terminology; data analysis; drug efficacy; drug formulary; drug utilization review; female; human; ICD-10; length of stay; major clinical study; male; medical record review; morphine equivalent dose; outcome assessment; patient care; pharmacy and therapeutics committee; postanesthesia care; postoperative analgesia; postoperative pain; rapid cycle analytics; recovery room; spinal anesthesia; total hip replacement; total knee arthroplasty"
"Altunkaynak A., Küllahcı K.","Transfer precipitation learning via patterns of dependency matrix-based machine learning approaches","10.1007/s00521-022-07674-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136950145&doi=10.1007%2fs00521-022-07674-8&partnerID=40&md5=ad0bcc6b1686015ed7d63de76417cdb9","Accurate precipitation prediction is very significant for urban, environmental, and water resources management as well as mitigating the negative effects of drought and flood. However, precipitation prediction is a complex and challenging task which involves meteorological parameters that contain uncertainty. This study attempts to ease the complexity of the problem via proposing a correlation matrix approach. Covariance and correlation matrices are analytical tools that are widely used to identify the interrelationships and possible dependencies throughout the data. Correlation matrices have some advantages over covariance matrices. The main drawback of covariance matrices is their sensitivity to the measurement units of variables. The variables with relatively large variances will dominate the results of multivariate analysis when the covariance matrix is used. Accordingly, the covariance matrix fails to provide useful information when there exist large differences between variances of variables. On the other hand, besides their easy interpretable features, the results of different analyses obtained from correlation matrices can effectively be compared. Therefore, in this study, in order to improve the performances of the predictive models, interrelationships and possible dependencies among data obtained from eighteen precipitation observation stations located in the Upper Euphrates Basin of Turkey (1980–2010) is investigated using correlation matrix approach. Relatedly, dependencies between the stations are resolved by means of examining the correlation matrix and optimal model inputs (data of particular stations) are selected for each prediction scenario. The transfer precipitation learning was performed throughout the period from 1980 to 2010 for eighteen precipitation observation stations located in the Upper Euphrates. Three different data-driven models Fuzzy, K-nearest neighbors (KNN), and multilinear regression (MR) are developed based on the patterns of correlation matrix. Predictive powers of the models are compared by means of performance evaluation criteria, i.e., Nash–Sutcliffe efficiency, mean square error, mean absolute error, and coefficient of determination (R2). Results of this study show that all developed correlation matrix patterns-based Fuzzy, KNN, and MR models have high precipitation prediction performance. However, even though all model results are close to each other, Fuzzy model provided more accurate results with requiring data from a relatively low number of stations. Therefore, patterns of correlation matrix-based Fuzzy model is the most efficient and well-suited approach for precipitation prediction among all the developed models. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Correlation matrix patterns; Fuzzy; KNN; Multilinear regression; Transfer precipitation","Forecasting; Machine learning; Mean square error; Multivariant analysis; Nearest neighbor search; Water resources; Correlation matrix; Correlation matrix pattern; Covariance matrices; Fuzzy; K-near neighbor; Matrix patterns; Multilinear regression; Nearest-neighbour; Precipitation predictions; Transfer precipitation; Covariance matrix"
"Al-Turaiki I., Alshahrani M., Almutairi T.","Building predictive models for MERS-CoV infections using data mining techniques","10.1016/j.jiph.2016.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994517591&doi=10.1016%2fj.jiph.2016.09.007&partnerID=40&md5=f463a55a674e007d4615807fd40b5b02","Background Recently, the outbreak of MERS-CoV infections caused worldwide attention to Saudi Arabia. The novel virus belongs to the coronaviruses family, which is responsible for causing mild to moderate colds. The control and command center of Saudi Ministry of Health issues a daily report on MERS-CoV infection cases. The infection with MERS-CoV can lead to fatal complications, however little information is known about this novel virus. In this paper, we apply two data mining techniques in order to better understand the stability and the possibility of recovery from MERS-CoV infections. Method The Naive Bayes classifier and J48 decision tree algorithm were used to build our models. The dataset used consists of 1082 records of cases reported between 2013 and 2015. In order to build our prediction models, we split the dataset into two groups. The first group combined recovery and death records. A new attribute was created to indicate the record type, such that the dataset can be used to predict the recovery from MERS-CoV. The second group contained the new case records to be used to predict the stability of the infection based on the current status attribute. Results The resulting recovery models indicate that healthcare workers are more likely to survive. This could be due to the vaccinations that healthcare workers are required to get on regular basis. As for the stability models using J48, two attributes were found to be important for predicting stability: symptomatic and age. Old patients are at high risk of developing MERS-CoV complications. Finally, the performance of all the models was evaluated using three measures: accuracy, precision, and recall. In general, the accuracy of the models is between 53.6% and 71.58%. Conclusion We believe that the performance of the prediction models can be enhanced with the use of more patient data. As future work, we plan to directly contact hospitals in Riyadh in order to collect more information related to patients with MERS-CoV infections. © 2016 King Saud Bin Abdulaziz University for Health Sciences","Classification; Data mining; Decision tree; J48; MERS-CoV; Naive Bayes","Article; Bayesian learning; classification algorithm; Coronavirus infection; data mining; decision tree; health care personnel; human; Middle East respiratory syndrome coronavirus; nonhuman; patient coding; predictive value; priority journal; aged; computer simulation; Coronavirus infection; data mining; female; male; middle aged; mortality; pathology; prognosis; Saudi Arabia; survival analysis; treatment outcome; very elderly; Aged; Aged, 80 and over; Computer Simulation; Coronavirus Infections; Data Mining; Female; Humans; Male; Middle Aged; Prognosis; Saudi Arabia; Survival Analysis; Treatment Outcome"
"Al-Turk L., Wawrzynski J., Wang S., Krause P., Saleh G.M., Alsawadi H., Alshamrani A.Z., Peto T., Bastawrous A., Li J., Tang H.L.","Automated feature-based grading and progression analysis of diabetic retinopathy","10.1038/s41433-021-01415-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102987247&doi=10.1038%2fs41433-021-01415-2&partnerID=40&md5=e1760a7ff7607cb70ea99958dbd99248","Background: In diabetic retinopathy (DR) screening programmes feature-based grading guidelines are used by human graders. However, recent deep learning approaches have focused on end to end learning, based on labelled data at the whole image level. Most predictions from such software offer a direct grading output without information about the retinal features responsible for the grade. In this work, we demonstrate a feature based retinal image analysis system, which aims to support flexible grading and monitor progression. Methods: The system was evaluated against images that had been graded according to two different grading systems; The International Clinical Diabetic Retinopathy and Diabetic Macular Oedema Severity Scale and the UK’s National Screening Committee guidelines. Results: External evaluation on large datasets collected from three nations (Kenya, Saudi Arabia and China) was carried out. On a DR referable level, sensitivity did not vary significantly between different DR grading schemes (91.2–94.2.0%) and there were excellent specificity values above 93% in all image sets. More importantly, no cases of severe non-proliferative DR, proliferative DR or DMO were missed. Conclusions: We demonstrate the potential of an AI feature-based DR grading system that is not constrained to any specific grading scheme. © 2021, The Author(s).",,"area under the curve; Article; artificial intelligence; automation; China; controlled study; diabetic macular edema; diabetic retinopathy; diagnostic test accuracy study; eye disease assessment; feature detection; human; Kenya; practice guideline; predictive value; retina image; Saudi Arabia; sensitivity and specificity; diabetes mellitus; diabetic retinopathy; macular edema; mass screening; procedures; retina; software; Diabetes Mellitus; Diabetic Retinopathy; Humans; Macular Edema; Mass Screening; Retina; Software"
"Alufaisan Y., Marusich L.R., Bakdash J.Z., Zhou Y., Kantarcioglu M.","Does Explainable Artificial Intelligence Improve Human Decision-Making?",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121307798&partnerID=40&md5=15f369f0f5c203eb8df287b11e07ea56","Explainable AI provides insights to users into the why for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has typically focused on measures such as interpretability, trust, and usability of the explanation. There are mixed findings whether explainable AI can improve actual human decision-making and the ability to identify the problems with the underlying model. Using real datasets, we compare objective human decision accuracy without AI (control), with an AI prediction (no explanation), and AI prediction with explanation. We find providing any kind of AI prediction tends to improve user decision accuracy, but no conclusive evidence that explainable AI has a meaningful impact. Moreover, we observed the strongest predictor for human decision accuracy was AI accuracy and that users were somewhat able to detect when the AI was correct vs. incorrect, but this was not significantly affected by including an explanation. Our results indicate that, at least in some situations, the why information provided in explainable AI may not enhance user decision-making, and further research may be needed to understand how to integrate explainable AI into real systems. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved",,"Artificial intelligence; Forecasting; Decision accuracies; Decisions makings; Human decision-making; Human decisions; Interpretability; Model prediction; Real data sets; Real systems; Decision making"
"Aluru S.S., Mathew B., Saha P., Mukherjee A.","A Deep Dive into Multilingual Hate Speech Classification","10.1007/978-3-030-67670-4_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103286788&doi=10.1007%2f978-3-030-67670-4_26&partnerID=40&md5=38a7069b2a5d6fd36f8fbd46d01e6dee","Hate speech is a serious issue that is currently plaguing the society and has been responsible for severe incidents such as the genocide of the Rohingya community in Myanmar. Social media has allowed people to spread such hateful content even faster. This is especially concerning for countries which lack hate speech detection systems. In this paper, using hate speech dataset in 9 languages from 16 different sources, we perform the first extensive evaluation of multilingual hate speech detection. We analyze the performance of different deep learning models in various scenarios. We observe that in low resource scenario LASER embedding with Logistic regression perform the best, whereas in high resource scenario, BERT based models perform much better. We also observe that simple techniques such as translating to English and using BERT, achieves competitive results in several languages. For cross-lingual classification, we observe that data from other languages seem to improve the performance, especially in the low resource settings. Further, in case of zero-shot classification, evaluation on Italian and Portuguese dataset achieve good results. Our proposed framework could be used as an efficient solution for low-resource languages. These models could also act as good baselines for future multilingual hate speech detection tasks. Our code (Code: https://github.com/punyajoy/DE-LIMIT ) and models (Models: https://huggingface.co/Hate-speech-CNERG ) are available online. Warning: contains material that many will find offensive or hateful. © 2021, Springer Nature Switzerland AG.","BERT; Classification; Embeddings; Hate speech; Multilingual","Classification (of information); Data mining; Deep learning; HTTP; Logistic regression; Speech; Speech recognition; Cross-lingual; Learning models; Low resource languages; Low-resource settings; Shot classification; Social media; Speech classification; Speech detection; Learning systems"
"Alvanpour A., Das S.K., Robinson C.K., Nasraoui O., Popa D.","Robot Failure Mode Prediction with Explainable Machine Learning","10.1109/CASE48305.2020.9216965","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094101739&doi=10.1109%2fCASE48305.2020.9216965&partnerID=40&md5=e53f8b77567c1af112fc95276da9cb2d","The ability to determine whether a robot's grasp has a high chance of failing, before it actually does, can save significant time and avoid failures by planning for re-grasping or changing the strategy for that special case. Machine Learning (ML) offers one way to learn to predict grasp failure from historic data consisting of a robot's attempted grasps alongside labels of the success or failure. Unfortunately, most powerful ML models are black-box models that do not explain the reasons behind their predictions. In this paper, we investigate how ML can be used to predict robot grasp failure and study the tradeoff between accuracy and interpretability by comparing interpretable (white box) ML models that are inherently explainable with more accurate black box ML models that are inherently opaque. Our results show that one does not necessarily have to compromise accuracy for interpretability if we use an explanation generation method, such as Shapley Additive explanations (SHAP), to add explainability to the accurate predictions made by black box models. An explanation of a predicted fault can lead to an efficient choice of corrective action in the robot's design that can be taken to avoid future failures. © 2020 IEEE.",,"Forecasting; Machine design; Machine learning; Predictive analytics; Robot programming; Robots; Accurate prediction; Black boxes; Black-box model; Corrective actions; Generation method; Historic data; Interpretability; White box; Safety engineering"
"Alvarez K., Urenda J.C., Csiszár O., Csiszár G., Dombi J., Eigner G., Kreinovich V.","Towards fast and understandable computa-tions: Which “and”-and “or”-operations can be represented by the fastest (i.e., 1-layer) neural networks? which activations functions allow such representations?","10.12700/APH.18.2.2021.2.2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101272243&doi=10.12700%2fAPH.18.2.2021.2.2&partnerID=40&md5=3da0b9d0c6dff1843e37020ee8561c3f","We want computations to be fast, and we want them to be understandable. As we show, the need for computations to be fast naturally leads to neural networks, with 1-layer networks being the fastest, and the need to be understandable naturally leads to fuzzy logic and to the corresponding “and”-and “or”-operations. Since we want our computations to be both fast and understandable, a natural question is: which “and”-and “or”-operations of fuzzy logic can be represented by the fastest (i.e., 1-layer) neural network? And a related question is: which activation functions allow such a representation? In this paper, we provide an answer to both questions: the only “and”-and “or”-operations that can be thus represented are max(0, a + b − 1) and min(a + b, 1), and the only activations functions allowing such a representation are equivalent to the rectified linear function – the one used in deep learning. This result provides an additional explanation of why rectified linear neurons are so successful. With also show that with full 2-layer networks, we can compute practically any “and”-and “or”-operation. © 2021, Budapest Tech Polytechnical Institution. All rights reserved.","Explainable AI; Fuzzy logic; Neural networks; Rectified linear neurons; “and”-and “or”-operations",
"Alvarez-Alvarez A., Alonso J.M., Trivino G., Hernández N., Herranz F., Llamazares A., Ocaña M.","Human activity recognition applying computational intelligence techniques for fusing information related to WiFi positioning and body posture","10.1109/FUZZY.2010.5584187","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78549232996&doi=10.1109%2fFUZZY.2010.5584187&partnerID=40&md5=a4d7c6baabc1142afa921bf44e1aa079","This work presents a general framework for people indoor activity recognition. Firstly, a Wireless Fidelity (WiFi) localization system implemented as a Fuzzy Rulebased Classifier (FRBC) is used to obtain an approximate position at the level of discrete zones (office, corridor, meeting room, etc). Secondly, a Fuzzy Finite State Machine (FFSM) is used for human body posture recognition (seated, standing upright or walking). Finally, another FFSM combines bothWiFi localization and posture recognition to obtain a robust, reliable, and easily understandable activity recognition system (working in the desk room, crossing the corridor, having a meeting, etc). Each user carries with a personal digital agenda (PDA) or smart-phone equipped with a WiFi interface for localization task and accelerometers for posture recognition. Our approach does not require adding new hardware to the experimental environment. It relies on the WiFi access points (APs) widely available in most public and private buildings. We include a practical experimentation where good results were achieved. © 2010 IEEE.",,"Access points; Activity recognition; Body postures; Computational intelligence techniques; Experimental environment; Finite state machines; Fuzzy rule-based classifier; Human activity recognition; Human body postures; Indoor activities; Localization system; Posture recognition; Practical experimentation; Wireless fidelities; Accelerometers; Artificial intelligence; Wi-Fi"
"Álvarez-Machancoses Ó., Fernández-Martínez J.L.","Using artificial intelligence methods to speed up drug discovery","10.1080/17460441.2019.1621284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068215658&doi=10.1080%2f17460441.2019.1621284&partnerID=40&md5=f8887389211350d8f4085b2ec4a07885","Introduction: Drug discovery is the process through which potential new compounds are identified by means of biology, chemistry, and pharmacology. Due to the high complexity of genomic data, AI techniques are increasingly needed to help reduce this and aid the adoption of optimal decisions. Phenotypic prediction is of particular use to drug discovery and precision medicine where sets of genes that predict a given phenotype are determined. Phenotypic prediction is an undetermined problem given that the number of monitored genetic probes markedly exceeds the number of collected samples (from patients). This imbalance creates ambiguity in the characterization of the biological pathways that are responsible for disease development. Areas covered: In this paper, the authors present AI methodologies that perform a robust deep sampling of altered genetic pathways to locate new therapeutic targets, assist in drug repurposing and speed up and optimize the drug selection process. Expert opinion: AI is a potential solution to a number of drug discovery problems, though one should, bear in mind that the quality of data predicts the overall quality of the prediction, as in any modeling task in data science. The use of transparent methodologies is crucial, particularly in drug repositioning/repurposing in rare diseases. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.","artificial intelligence; Drug design; drug discovery; phenotype prediction","artificial intelligence; drug absorption; drug design; drug development; drug distribution; drug elimination; drug mechanism; drug metabolism; drug potency; drug repositioning; gene expression; human; measurement accuracy; personalized medicine; phenotype; prediction; prevalence; priority journal; Review; support vector machine; drug development; drug repositioning; procedures; Artificial Intelligence; Drug Discovery; Drug Repositioning; Humans; Phenotype; Precision Medicine"
"Alvarez-Melis D., Fusi N.","Geometric dataset distances via optimal transport",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108098487&partnerID=40&md5=9df5f96d7794d027e41db63df88df317","The notion of task similarity is at the core of various machine learning paradigms, such as domain adaptation and meta-learning. Current methods to quantify it are often heuristic, make strong assumptions on the label sets across the tasks, and many are architecture-dependent, relying on task-specific optimal parameters (e. g., require training a model on each dataset). In this work we propose an alternative notion of distance between datasets that (i) is model-agnostic, (ii) does not involve training, (iii) can compare datasets even if their label sets are completely disjoint and (iv) has solid theoretical footing. This distance relies on optimal transport, which provides it with rich geometry awareness, interpretable correspondences and well-understood properties. Our results show that this novel distance provides meaningful comparison of datasets, and correlates well with transfer learning hardness across various experimental settings and datasets. © 2020 Neural information processing systems foundation. All rights reserved.",,"Heuristic methods; Transfer learning; Domain adaptation; Label sets; Metalearning; Notion of distance; Optimal parameter; Optimal transport; Optimization"
"Álvarez-Mellado E., Holderness E., Miller N., Dhang F., Cawkwell P., Bolton K., Pustejovsky J., Hall M.-H.","Assessing the efficacy of clinical sentiment analysis and topic extraction in psychiatric readmission risk prediction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119360661&partnerID=40&md5=29c086a6f079ba727c3e5a6ae85850e9","Predicting which patients are more likely to be readmitted to a hospital within 30 days after discharge is a valuable piece of information in clinical decision-making. Building a successful readmission risk classifier based on the content of Electronic Health Records (EHRs) has proved, however, to be a challenging task. Previously explored features include mainly structured information, such as sociodemographic data, comorbidity codes and physiological variables. In this paper we assess incorporating additional clinically interpretable NLP-based features such as topic extraction and clinical sentiment analysis to predict early readmission risk in psychiatry patients. © 2019 Association for Computational Linguistics",,"Computational linguistics; Data mining; Decision making; Forecasting; Health risks; Risk assessment; Clinical decision making; Comorbidities; Risk predictions; Sentiment analysis; Sociodemographic data; Structured information; Topic extraction; Sentiment analysis"
"Alves D., Doctor F., Iqbal R., Kattan A.J.","A soft computing methodology based on fuzzy measures and integrals for ranking workers informing labour hiring policies","10.1145/3325112.3325270","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068616401&doi=10.1145%2f3325112.3325270&partnerID=40&md5=f50f44a247c24c872d2ee05520bc3575","Effective policy-making and design for labour nationalization programmes requires a deep understanding of the factors underpinning firms'decisions as regards the hiring of workers across different sectors of the economy, and crucially, how these factors interact in terms of either synergies or redundancies in the overall decision-making process. There is the need to develop a method that predictively determines the stability of employer-employee matches by ranking prospective and employed workers by combining information on firms, workers, and market or institutional variables. The objective of this paper is to present a methodology for transforming criteria in matched employer-employee data into a form expressing directly the variable importance for each match, that can then be used to estimate a fuzzy measure and corresponding Sugeno Fuzzy Integral to create an interpretable regression model that is able to predict the hiring patterns of firms given a pool of applicants. The SFI is explained and compared against three well-known benchmark regression methods in matched employer-employee data from the Kingdom of Saudi Arabia and shown to outperform them. Results on calculating the variable importance with the Shapley Value derived from the estimated fuzzy measures for two selected jobs are also presented, within the scope of a larger intervention model which can be used to aid policy-makers in both designing policies and evaluating their outcome. © 2019 Association for Computing Machinery.","Interaction Index; Labour Market; Logistic Regression; Shapley Value; Sugeno Fuzzy Integral","Artificial intelligence; Commerce; Decision making; Employment; Fuzzy systems; Game theory; Integral equations; Logistic regression; Metadata; Personnel; Soft computing; Decision making process; Interaction index; Kingdom of Saudi Arabia; Labour market; Shapley value; Soft computing methodologies; Sugeno fuzzy integrals; Variable importances; Public policy"
"Alves E., Tanscheit R., Vellasco M.","SENFIS - Selected Ensemble of Fuzzy Inference Systems","10.1109/FUZZ-IEEE.2019.8858805","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073770404&doi=10.1109%2fFUZZ-IEEE.2019.8858805&partnerID=40&md5=8d35f2e19ee7ff2d5ecc9fd66232bd31","Fuzzy Inference Systems (SIF) for classification are machine learning models that employ linguistic rules to describe real problems in an interpretable way. However, when dealing with high dimensional input spaces, these systems tend to suffer from scalability problems and computational cost. This work presents the Selected Ensemble of Fuzzy Inference Systems (SENFIS), an automatic SIF model based on previously developed algorithms (AutoFIS-Class and RandomFIS) and composed of ensemble and subsampling strategies to deal with big datasets. Its performance is compared to those of its predecessors and of others similar fuzzy systems, making use of normal and large benchmarks databases for classification. In terms of accuracy, SENFIS performs as well as its predecessors, but at a lower computational cost. In addition, the resulting rule bases are smaller, especially for high dimensional problems. © 2019 IEEE.","Big Data; Ensemble Methods; Fuzzy Inference Systems; Pattern Classification","Benchmarking; Big data; Classification (of information); Fuzzy systems; Inference engines; Pattern recognition; Computational costs; Ensemble methods; Fuzzy inference systems; High-dimensional; High-dimensional problems; Linguistic rules; Machine learning models; Scalability problems; Fuzzy inference"
"Alves G.F., Montenegro A.A., Trevisan D.G.","Creating Layouts for Virtual Game Controllers Using Generative Design","10.1007/978-3-030-34644-7_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076946265&doi=10.1007%2f978-3-030-34644-7_19&partnerID=40&md5=ebf07f72ca1619fc830cf122051a9947","Video game controllers have a high influence factor on players, as they are responsible for the fun, motivation, and personality of a game. The organization and arrangement of the buttons are one of the relevant factors when developing new controllers since they are responsible for serving as an input of actions within the games. This work presents the construction of a generative design model to support game designers finding different and innovative layouts of virtual controllers for their games. The generative design produces many valid designs or solutions instead of one optimized version of a known solution. This solution was developed by linking genetic algorithms to generate a large number of layouts and machine learning techniques (SVN) to classify individuals between valid and invalid, seeking to facilitate the exploration of the design space by the designer. The tests performed sought to measure the variability of the results generated by the proposed model, showing that several solutions of different controllers with different configurations can be developed for a game. © IFIP International Federation for Information Processing, 2019.","Gamepad; Generative design; Genetic algorithm; Machine learning; Virtual controller","Controllers; Genetic algorithms; Human computer interaction; Learning algorithms; Learning systems; Machine learning; Design spaces; Game designers; Gamepad; Generative design; Machine learning techniques; Video game; Virtual controller; Virtual games; Serious games"
"Alves J., Lau N., Silva F.","Skill Learning for Long-Horizon Sequential Tasks","10.1007/978-3-031-16474-3_58","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138735479&doi=10.1007%2f978-3-031-16474-3_58&partnerID=40&md5=568263b54edcebae8d4dff583745cd78","Solving long-horizon problems is a desirable property in autonomous agents. Learning reusable behaviours can equip the agent with this property, allowing it to adapt them when performing various real-world tasks. Our approach for learning these behaviours is composed of three modules, operating in two separate timescales and it uses a hierarchical model with both discrete and continuous variables. This modular structure allows an independent training process for each stage. These stages are organized using a two-level temporal hierarchy. The first level contains the planner, responsible for issuing the skills that should be executed, while the second level executes the skill. In this latter level, to achieve the desired skill behaviour, the discrete skill is converted to a continuous vector that contains information regarding which environment change must occur. With this approach, we aimed to solve long-horizon sequential tasks with delayed rewards. Contrary to existing work, our method uses both variable types to allow an agent to learn high-level behaviours consisting of an interpretable set of skills. This method allows to compose the discrete skills easily, while keeping the flexibility, provided by the continuous representations, to execute them in several different ways. Using a 2D scenario where the agent has to catch a set of objects in a specific order, we demonstrate that our approach is scalable to scenarios with increasingly longer tasks. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Deep learning; Goal-conditioned policy; Hierarchical model; Reinforcement Learning; Sequential tasks; Skill learning","Autonomous agents; Deep learning; Hierarchical systems; Learning systems; Deep learning; Discrete variables; Goal-conditioned policy; Hierarchical model; Property; Real-world task; Reinforcement learnings; Sequential task; Skill learning; Time-scales; Reinforcement learning"
"Alves M.A., Castro G.Z., Oliveira B.A.S., Ferreira L.A., Ramírez J.A., Silva R., Guimarães F.G.","Explaining machine learning based diagnosis of COVID-19 from routine blood tests with decision trees and criteria graphs","10.1016/j.compbiomed.2021.104335","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103648098&doi=10.1016%2fj.compbiomed.2021.104335&partnerID=40&md5=cffb20fb9e602d2728c65847b6c627ad","The sudden outbreak of coronavirus disease 2019 (COVID-19) revealed the need for fast and reliable automatic tools to help health teams. This paper aims to present understandable solutions based on Machine Learning (ML) techniques to deal with COVID-19 screening in routine blood tests. We tested different ML classifiers in a public dataset from the Hospital Albert Einstein, São Paulo, Brazil. After cleaning and pre-processing the data has 608 patients, of which 84 are positive for COVID-19 confirmed by RT-PCR. To understand the model decisions, we introduce (i) a local Decision Tree Explainer (DTX) for local explanation and (ii) a Criteria Graph to aggregate these explanations and portrait a global picture of the results. Random Forest (RF) classifier achieved the best results (accuracy 0.88, F1–score 0.76, sensitivity 0.66, specificity 0.91, and AUROC 0.86). By using DTX and Criteria Graph for cases confirmed by the RF, it was possible to find some patterns among the individuals able to aid the clinicians to understand the interconnection among the blood parameters either globally or on a case-by-case basis. The results are in accordance with the literature and the proposed methodology may be embedded in an electronic health record system. © 2021 Elsevier Ltd","COVID–19; Criteria graph; Decision tree; Explainable artificial intelligence; Machine learning","Blood; Classification (of information); Data handling; Diagnosis; Machine learning; Automatic tools; Blood test; Coronaviruses; COVID–19; Criteria graph; Explainable artificial intelligence; Learning classifiers; Machine learning techniques; Machine-learning; On-machines; Decision trees; alanine aminotransferase; aspartate aminotransferase; C reactive protein; creatinine; hemoglobin; lactate dehydrogenase; potassium; sodium; urea; adult; alanine aminotransferase blood level; area under the curve; Article; artificial intelligence; aspartate aminotransferase blood level; basophil count; binary classification; blood examination; controlled study; coronavirus disease 2019; creatinine blood level; data processing; decision tree; diagnostic accuracy; diagnostic test accuracy study; electronic health record; Enterovirus infection; eosinophil count; erythrocyte count; false negative result; female; hematocrit; human; influenza A; influenza B; lactate dehydrogenase blood level; leukocyte count; lymphocyte count; machine learning; major clinical study; male; mean corpuscular hemoglobin; mean corpuscular volume; mean platelet volume; monocyte; nested cross validation; neutrophil count; Parainfluenza virus infection; platelet count; potassium blood level; priority journal; random forest; real time polymerase chain reaction; receiver operating characteristic; red blood cell distribution width; reference value; respiratory tract infection; sensitivity and specificity; sodium blood level; urea blood level; blood examination; Brazil; decision tree; machine learning; Brazil; COVID-19; Decision Trees; Hematologic Tests; Humans; Machine Learning; SARS-CoV-2"
"Alves Ribeiro V.H., Reynoso-Meza G., Dos Santos Coelho L.","Multiobjective optimization design procedures for data-driven unmanned aerial vehicles automatic target recognition systems","10.1016/B978-0-12-820276-0.00017-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122628013&doi=10.1016%2fB978-0-12-820276-0.00017-0&partnerID=40&md5=ea394807ff30e2f9ebdb772cf08c5b8c","Within unmanned aerial vehicle (UAV)-related ongoing research topics, automatic target recognition (ATR) is acquiring increasing relevance. This is due to the easiness, in terms of price and accessibility, with which it is possible to acquire UAVs. ATR systems are responsible for detecting aerial vehicles automatically using radar signals. To this end, multiobjective optimization design (MOOD) procedures can be employed for improving the performance in the presented task. This chapter presents an MOOD procedure for ATR system development composed of three steps: (1) the development of a multiobjective problem, where radar data acquisition of three different UAVs is used to train a classifier in terms of accuracy, precision, and sensitivity; (2) the application of an evolutionary multiobjective optimization algorithm, which retrieves a set of nondominated solutions; and (3) the employment of multicriterion decision making techniques for selecting a final preferred solution. For experimental purposes, two scenarios are evaluated on a new real-world UAV radar signal data set, both discriminating different types of UAVs. The first analysis presents the comparison of two different classification techniques, the multilayer perceptron (MLP) and the support vector machine (SVM). The second analysis presents the comparison of three different multiobjective optimization algorithms, nondominated sorting genetic algorithm version II (NSGA-II), multiobjective differential evolution with spherical pruning (spMODE- II), and multiobjective evolutionary algorithm based on decomposition (MOEA/D), used for finding the best possible trade-off for the automatic target recognition system. As a conclusion, the UAV radar signal dataset is made available for future studies, and current results indicate the superiority of SVM over MLP, and of both spMODE-II and MOEA/D over NSGA-II. © 2021 Elsevier Inc. All rights reserved.","Automatic target recognition; Machine learning; Multicriterion decision making; Multiobjective optimization; Unmanned aerial vehicles",
"Alves V.M., Golbraikh A., Capuzzi S.J., Liu K., Lam W.I., Korn D.R., Pozefsky D., Andrade C.H., Muratov E.N., Tropsha A.","Multi-Descriptor Read Across (MuDRA): A Simple and Transparent Approach for Developing Accurate Quantitative Structure-Activity Relationship Models","10.1021/acs.jcim.8b00124","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047993665&doi=10.1021%2facs.jcim.8b00124&partnerID=40&md5=40d57211597f88a5d42ffd196407a983","Multiple approaches to quantitative structure-activity relationship (QSAR) modeling using various statistical or machine learning techniques and different types of chemical descriptors have been developed over the years. Oftentimes models are used in consensus to make more accurate predictions at the expense of model interpretation. We propose a simple, fast, and reliable method termed Multi-Descriptor Read Across (MuDRA) for developing both accurate and interpretable models. The method is conceptually related to the well-known kNN approach but uses different types of chemical descriptors simultaneously for similarity assessment. To benchmark the new method, we have built MuDRA models for six different end points (Ames mutagenicity, aquatic toxicity, hepatotoxicity, hERG liability, skin sensitization, and endocrine disruption) and compared the results with those generated with conventional consensus QSAR modeling. We find that models built with MuDRA show consistently high external accuracy similar to that of conventional QSAR models. However, MuDRA models excel in terms of transparency, interpretability, and computational efficiency. We posit that due to its methodological simplicity and reliable predictive accuracy, MuDRA provides a powerful alternative to a much more complex consensus QSAR modeling. MuDRA is implemented and freely available at the Chembench web portal (https://chembench.mml.unc.edu/mudra). © 2018 American Chemical Society.",,"Computational chemistry; Computational efficiency; HTTP; Molecular graphics; Portals; Structures (built objects); Accurate prediction; Chemical descriptors; Endocrine disruption; Machine learning techniques; Model interpretations; Quantitative structure activity relationship; Quantitative structure-activity relationship modeling; Similarity assessment; Learning systems; mutagenic agent; algorithm; biological model; factual database; human; Internet; quantitative structure activity relation; software; toxicity testing; Algorithms; Databases, Factual; Humans; Internet; Models, Biological; Mutagens; Quantitative Structure-Activity Relationship; Software; Toxicity Tests"
"Alvey B.J., Anderson D.T., Yang C., Buck A., Keller J.M., Yasuda K.E., Ryan H.A.","Characterization of Deep Learning-Based Aerial Explosive Hazard Detection using Simulated Data","10.1109/SSCI50451.2021.9659899","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125814351&doi=10.1109%2fSSCI50451.2021.9659899&partnerID=40&md5=93cb12a386e74a6e4a7266d21ed9578a","Automatic object detection is one of the most common and fundamental tasks in computational intelligence (CI). Neural networks (NNs) are now often the tool of choice for this task. Unlike more traditional approaches that have interpretable parameters, explaining what a NN has learned and characterizing under what conditions the model does and does not perform well is a challenging, yet important task. The most straightforward approach to evaluate performance is to run test imagery through a model. However, the gaining popularity of self-supervised methods among big players such as Tesla and Google serve as evidence that labeled data is scarce in real-world settings. On the other hand, modern high-fidelity graphics simulation is now accessible and programmable, allowing for generation of large amounts of accurately labeled training and testing data for CI. Herein, we describe a framework to assess the performance of a NN model for automatic explosive hazard detection (EHD) from an unmanned aerial vehicle using simulation. The data was generated by the Unreal Engine with Microsoft's AirSim plugin. A workflow for generating simulated data and using it to assess and understand strengths and weaknesses in a learned EHD model is demonstrated. © 2021 IEEE.",,"Antennas; Explosives; Explosives detection; Hazards; Object detection; Condition; Explosive hazard detections; Google+; Labeled data; Neural-networks; Performance; Real world setting; Run test; Supervised methods; Traditional approaches; Deep learning"
"Alwarasneh N., Chow Y.S.S., Yan S.T.M., Lim C.H.","Bridging Explainable Machine Vision in CAD Systems for Lung Cancer Detection","10.1007/978-3-030-66645-3_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102257548&doi=10.1007%2f978-3-030-66645-3_22&partnerID=40&md5=59d11c9618bab5c5ef1b363d28b8b4ec","Computer-aided diagnosis (CAD) systems have grown increasingly popular with aiding physicians in diagnosing lung cancer using medical images in recent years. However, the reasoning behind the state-of-the-art black-box learning and prediction models has become obscured and this resultant lack of transparency has presented a problem whereby physicians are unable to trust the results of these systems. This motivated us to improve the conventional CAD with a more robust and interpretable algorithms to produce a system that achieves high accuracy and explainable diagnoses of lung cancer. The proposed approach uses a novel image processing pipeline to segment nodules from lung CT scan images, and then classifies the nodule using both 2D and 3D Alexnet models that have been trained on lung nodule data from the LIDC-IDRI dataset. The explainability aspect is approached from two angles: 1) LIME that produces a visual explanation of the diagnosis, and 2) a rule-based system that produces a text-based explanation of the diagnosis. Overall, the proposed algorithm has achieved better performance and advance the practicality of CAD systems. © 2020, Springer Nature Switzerland AG.","Cancer detection; Deep learning; Explainable AI; Image processing; Machine vision","Biological organs; Computer aided diagnosis; Computer vision; Diseases; Image segmentation; Intelligent robots; Lime; Medical imaging; Predictive analytics; Robotics; Computer Aided Diagnosis(CAD); High-accuracy; Image processing pipeline; Lung Cancer; Lung cancer detections; Lung nodule; Prediction model; State of the art; Computerized tomography"
"Alwarthan S., Aslam N., Khan I.U.","An Explainable Model for Identifying At-Risk Student at Higher Education","10.1109/ACCESS.2022.3211070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139451582&doi=10.1109%2fACCESS.2022.3211070&partnerID=40&md5=e57c9f6d41059efe172d9a82d5ab4747","Nowadays, researchers from various fields have shown great interest in improving the quality of learning in educational institutes in order to improve student achievement and learning outcomes. The main objective of this study was to predict the at-risk student of failing the preparatory year at an early stage. This study applies several educational data mining algorithms including RF, ANN, and SVM to build three classification models to meet the objectives of this study. Moreover, different features selection methods namely RFE, and GA have been examined to find the best subset of the highest influential features. Furthermore, several sampling approaches are applied to balance the dataset used in this study, including SMOTE, and SMOTE-Tomek Link. Three datasets related to the preparatory year student from the humanities track at IAU were used in this study. The collected datasets are imbalanced datasets, SMOTE-Tomek Link technique has been used to balance the three proposed datasets. The results showed that RF outperformed other techniques as it records the highest performance for building the models. Moreover, RFE with Mutual Information finds the best subset of features to build the first model. Finally, this study not only developed several classification models to identify at-risk students, but it also went a step further by employing XAI techniques such as LIME, SHAP, and the global surrogate model to explain the proposed prediction models, explaining the output and highlighting the reasons for the student failure. © 2013 IEEE.","Educational data mining; identifying at-risk student; LIME; preparatory year; random forest; SHAP","Data mining; Decision trees; Education computing; Lime; Students; Classification algorithm; Classification models; Computational modelling; Educational data mining; Identifying at-risk student; Predictive models; Preparatory year; Radiofrequencies; Random forests; SHAP; Neural networks"
"Alwitt L.F., Mitchell A.A.","Psychological Processes and Advertising Effects: Theory, Research, and Applications","10.4324/9781003047971","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140497164&doi=10.4324%2f9781003047971&partnerID=40&md5=b7c19212e34a2aff5c58d9490291f207","In the 1980s our understanding of how advertising affects consumer behavior was undergoing a dramatic transformation. Recent theoretical and methodological advances in cognitive psychology, social cognition, and artificial intelligence were largely responsible for this transformation. These advances provided a better understanding of the information acquisition process and how information is stored in memory. Consequently, we have been able to incorporate memory, the processing of visual information and affect into our models of advertising effects. However, there were still many unanswered questions. Among these are: (1) Exactly what is the relationship between the different mediators of persuasion? (2) How is memory for advertising related to persuasion? (3) What are the theoretical underpinnings of attitude toward the advertisement? (4) What determines the effect of persuasion over time? (5) What factors affect attention to advertising? (6) What psychological processes occur during the watching of a television commercial? and (7) What factors affect individual differences in the processing of advertising messages? Originally published in 1985, the chapters in this volume provide insights into these questions. They are organized in terms of four psychological processes which contribute to our understanding of how advertising works. These are affective reactions to advertisements, persuasion, psychological processes during television viewing, and involvement. © 1985 Lawrence Erlbaum Associates, Inc.",,
"Aly Abd Elrazek A.E.M., Bilasy S.E., Elbanna A.E.M., Elsherif A.E.A.","Prior to the oral therapy, what do we know about HCV-4 in Egypt: A randomized survey of prevalence and risks using data mining computed analysis","10.1097/MD.0000000000000204","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920202277&doi=10.1097%2fMD.0000000000000204&partnerID=40&md5=c586793eada85412bb6034574c5c87dc","Hepatitis C virus (HCV) affects over 180 million people worldwide and it's the leading cause of chronic liver diseases and hepatocellular carcinoma. HCV is classified into seven major genotypes and a series of subtypes. In general, HCV genotype 4 (HCV-4) is common in the Middle East and Africa, where it is responsible for more than 80% of HCV infections. Although HCV-4 is the cause of approximately 20% of the 180 million cases of chronic hepatitis C worldwide, it has not been a major subject of research yet. The aim of the current study is to survey the morbidities and disease complications among Egyptian population infected with HCV-4 using data mining advanced computing methods mainly and other complementary statistical analysis.Six thousand six hundred sixty subjects, aged between 17 and 58 years old, from different Egyptian Governorates were screened for HCV infection by ELISA and qualitative PCR. HCV-positive patients were further investigated for the incidence of liver cirrhosis and esophageal varices. Obtained data were analyzed by data mining approach.Among 6660 subjects enrolled in this survey, 1018 patients (15.28%) were HCV-positive. Proportion of infected-males was significantly higher than females; 61.6% versus 38.4% (P = 0.0052). Around two-third of infected-patients (635/1018; 62.4%) were presented with liver cirrhosis. Additionally, approximately half of the cirrhotic patients (301/635; 47.4%) showed degrees of large esophageal varices (LEVs), with higher variceal grade observed in males. Age for esophageal variceal development was 47 ± 1. Data mining analysis yielded esophageal wall thickness (>6.5 mm), determined by conventional U/S, as the only independent predictor for esophageal varices.This study emphasizes the high prevalence of HCV infection among Egyptian population, in particular among males. Egyptians with HCV-4 infection are at a higher risk to develop cirrhotic liver and esophageal varices. Data mining, a new analytic technique in medical field, shed light in this study on the clinical importance of esophageal wall thickness as a useful predictor for risky esophageal varices using decision tree algorithm. Copyright © 2014 Wolters Kluwer Health | Lippincott Williams & Wilkins.",,"adolescent; adult; age distribution; Article; data mining; digestive tract parameters; disease severity; echography; Egypt; enzyme linked immunosorbent assay; esophageal wall thickness; esophagus varices; female; health survey; hepatitis C; Hepatitis C virus genotype 4; Hepatitis C virus genotype 4 infection; human; infection rate; infection risk; liver cirrhosis; major clinical study; male; pathophysiology; polymerase chain reaction; prospective study; qualitative analysis; sex ratio"
"Alyuz N., Sezgin T.M.","Interpretable machine learning for generating semantically meaningful formative feedback",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113844855&partnerID=40&md5=4e588574fcf918da86a14f5f130335ea","We express our emotional state through a range of expressive modalities such as facial expressions, vocal cues, or body gestures. However, children on the Autism Spectrum experience difficulties in expressing and recognizing emotions with the accuracy of their neurotypical peers. Research shows that children on the Autism Spectrum can be trained to recognize and express emotions if they are given supportive and constructive feedback. In particular, providing formative feedback, (e.g., feedback given by an expert describing how they need to modify their behavior to improve their expressiveness), has been found valuable in rehabilitation. Unfortunately, generating such formative feedback requires constant supervision of an expert. In this work, we describe a system for automatic formative assessment integrated into an automatic emotion recognition setup. Our system is built on an interpretable machine learning framework that answers the question of what needs to be modified in human behavior to achieve a desired expressive display. It propagates the desired changes to human-understandable attributes through explanation vectors operating on a shared low level feature space. We report experiments conducted on a childrens voice data set with expression variations, showing that the proposed mechanism generates formative feedback aligned with the expectations reported from a clinical perspective. © 2019 IEEE Computer Society. All rights reserved.",,"Biofeedback; Computer vision; Diseases; Machine learning; Vector spaces; Automatic emotion recognition; Constructive feedback; Expression variations; Facial Expressions; Formative assessment; Formative feedbacks; Low-level features; Recognizing emotions; Behavioral research"
"Alzahrani A.S.","An Optimized Approach-Based Machine Learning to Mitigate DDoS Attack in Cloud Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090649698&partnerID=40&md5=e4cccfbbfa4c2a779dbb7fe78b120ca4","With the emerging growth of cloud computing technology and on-demand services, users can access cloud services and software freely and applications based on the ""pay-as-you go"" concept. This innovation reduced service costs and made them cheaper with high reliability. One of the most significant characteristics of the cloud concept is on-demand services. One can access the applications of cloud computing at any time at a much lower cost. In addition to providing cloud users with much-needed services, the cloud also gets rid of security concerns which are not tolerated by the cloud. One of the most security problems in the cloud environment is Distributed Denial of Service (DDoS) attack that are responsible for overloading the cloud servers. This paper highlights a prevention technique (CS-ANN) which detect the DDoS attack and makes the server side more sensitive by integrating a Cuckoo Search (CS) approach with the Artificial Neural Network (ANN) approach. The cloud user features, along with the attacker features, are optimized using CS as a nature-inspired approach. Later on, these optimized features are passed to the ANN structure. The trained features are stored in the database and used during testing process to match the test features with the trained features and hence provide results in terms of attacker and normal cloud users. The test results of CS-ANN show a True Positive Rate (TPR), False Positive Rate (FPR) and detection accuracy of 0.99, 0.0105 and 0.9865% respectively. The proposed approach outperforms in contrast to the other two state-of-the-art techniques. © International Research Publication House.","Artificial Neural Network; Cloud Computing; Cuckoo Search; Distributed Denial of Service",
"Alzantot M., Widdicombe A., Julier S., Srivastava M.","NeuroMask: Explaining predictions of deep neural networks through mask learning","10.1109/SMARTCOMP.2019.00033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070890594&doi=10.1109%2fSMARTCOMP.2019.00033&partnerID=40&md5=804a637a3dc79e7966a1889fcf971239","Deep Neural Networks (DNNs) deliver state-of-the-art performance in many image recognition and understanding applications. However, despite their outstanding performance, these models are black-boxes and it is hard to understand how they make their decisions. Over the past few years, researchers have studied the problem of providing explanations of why DNNs predicted their results. However, existing techniques are either obtrusive, requiring changes in model training, or suffer from low output quality. In this paper, we present a novel method, NeuroMask, for generating an interpretable explanation of classification model results. When applied to image classification models, NeuroMask identifies the image parts that are most important to classifier results by applying a mask that hides/reveals different parts of the image, before feeding it back into the model. The mask values are tuned by minimizing a properly designed cost function that preserves the classification result and encourages producing an interpretable mask. Experiments using state-of-art Convolutional Neural Networks for image recognition on different datasets (CIFAR-10 and ImageNet) show that NeuroMask successfully localizes the parts of the input image which are most relevant to the DNN decision. By showing a visual quality comparison between NeuroMask explanations and those of other methods, we find NeuroMask to be both accurate and interpretable. © 2019 IEEE.","Deep learning; Image recognition; Interpretability; Neural networks","Cost functions; Deep learning; Image recognition; Neural networks; Classification models; Classification results; Convolutional neural network; Interpretability; Model training; Output quality; State-of-the-art performance; Visual qualities; Deep neural networks"
"Alzetta F., Giorgini P., Najjar A., Schumacher M.I., Calvaresi D.","In-Time Explainability in Multi-Agent Systems: Challenges, Opportunities, and Roadmap","10.1007/978-3-030-51924-7_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088557183&doi=10.1007%2f978-3-030-51924-7_3&partnerID=40&md5=03875758769fb22a177ca38b8491d28e","In the race for automation, distributed systems are required to perform increasingly complex reasoning to deal with dynamic tasks, often not controlled by humans. On the one hand, systems dealing with strict-timing constraints in safety-critical applications mainly focused on predictability, leaving little room for complex planning and decision-making processes. Indeed, real-time techniques are very efficient in predetermined, constrained, and controlled scenarios. Nevertheless, they lack the necessary flexibility to operate in evolving settings, where the software needs to adapt to the changes of the environment. On the other hand, Intelligent Systems (IS) increasingly adopted Machine Learning (ML) techniques (e.g., subsymbolic predictors such as Neural Networks). The seminal application of those IS started in zero-risk domains producing revolutionary results. However, the ever-increasing exploitation of ML-based approaches generated opaque systems, which are nowadays no longer socially acceptable—calling for eXplainable AI (XAI). Such a problem is exacerbated when IS tend to approach safety-critical scenarios. This paper highlights the need for on-time explainability. In particular, it proposes to embrace the Real-Time Beliefs Desires Intentions (RT-BDI) framework as an enabler of eXplainable Multi-Agent Systems (XMAS) in time-critical XAI. © 2020, Springer Nature Switzerland AG.","eXplainable autonomous agents; eXplainable BDI model; Multi-Agent Systems; Real-Time Systems","Autonomous agents; Complex networks; Decision making; Distributed database systems; Intelligent agents; Intelligent systems; Real time systems; Safety engineering; Decision making process; Distributed systems; Dynamic tasks; Real-time techniques; Safety critical applications; Sub-symbolic; Time-critical; Timing constraints; Multi agent systems"
"Al-Zogbi L., Singh V., Teixeira B., Ahuja A., Bagherzadeh P.S., Kapoor A., Saeidi H., Fleiter T., Krieger A.","Autonomous Robotic Point-of-Care Ultrasound Imaging for Monitoring of COVID-19–Induced Pulmonary Diseases","10.3389/frobt.2021.645756","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107355276&doi=10.3389%2ffrobt.2021.645756&partnerID=40&md5=e78fe34ca9b7bb82e81fcf635d39b902","The COVID-19 pandemic has emerged as a serious global health crisis, with the predominant morbidity and mortality linked to pulmonary involvement. Point-of-Care ultrasound (POCUS) scanning, becoming one of the primary determinative methods for its diagnosis and staging, requires, however, close contact of healthcare workers with patients, therefore increasing the risk of infection. This work thus proposes an autonomous robotic solution that enables POCUS scanning of COVID-19 patients’ lungs for diagnosis and staging. An algorithm was developed for approximating the optimal position of an ultrasound probe on a patient from prior CT scans to reach predefined lung infiltrates. In the absence of prior CT scans, a deep learning method was developed for predicting 3D landmark positions of a human ribcage given a torso surface model. The landmarks, combined with the surface model, are subsequently used for estimating optimal ultrasound probe position on the patient for imaging infiltrates. These algorithms, combined with a force–displacement profile collection methodology, enabled the system to successfully image all points of interest in a simulated experimental setup with an average accuracy of 20.6 ± 14.7 mm using prior CT scans, and 19.8 ± 16.9 mm using only ribcage landmark estimation. A study on a full torso ultrasound phantom showed that autonomously acquired ultrasound images were 100% interpretable when using force feedback with prior CT and 88% with landmark estimation, compared to 75 and 58% without force feedback, respectively. This demonstrates the preliminary feasibility of the system, and its potential for offering a solution to help mitigate the spread of COVID-19 in vulnerable environments. © Copyright © 2021 Al-Zogbi, Singh, Teixeira, Ahuja, Bagherzadeh, Kapoor, Saeidi, Fleiter and Krieger.","3D deep convolutional network; 3D landmark estimation; autonomous robotics; COVID-19; force feedback; point-of-care ultrasound",
"Alzubaidi A., Tepper J.","Deep Mining from Omics Data","10.1007/978-1-0716-2095-3_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129414134&doi=10.1007%2f978-1-0716-2095-3_15&partnerID=40&md5=72bcd301ff12d8418a2f884c83913e23","Since the advent of high-throughput omics technologies, various molecular data such as genes, transcripts, proteins, and metabolites have been made widely available to researchers. This has afforded clinicians, bioinformaticians, statisticians, and data scientists the opportunity to apply their innovations in feature mining and predictive modeling to a rich data resource to develop a wide range of generalizable prediction models. What has become apparent over the last 10 years is that researchers have adopted deep neural networks (or “deep nets”) as their preferred paradigm of choice for complex data modeling due to the superiority of performance over more traditional statistical machine learning approaches, such as support vector machines. A key stumbling block, however, is that deep nets inherently lack transparency and are considered to be a “black box” approach. This naturally makes it very difficult for clinicians and other stakeholders to trust their deep learning models even though the model predictions appear to be highly accurate. In this chapter, we therefore provide a detailed summary of the deep net architectures typically used in omics research, together with a comprehensive summary of the notable “deep feature mining” techniques researchers have applied to open up this black box and provide some insights into the salient input features and why these models behave as they do. We group these techniques into the following three categories: (a) hidden layer visualization and interpretation; (b) input feature importance and impact evaluation; and (c) output layer gradient analysis. While we find that omics researchers have made some considerable gains in opening up the black box through interpretation of the hidden layer weights and node activations to identify salient input features, we highlight other approaches for omics researchers, such as employing deconvolutional network-based approaches and development of bespoke attribute impact measures to enable researchers to better understand the relationships between the input data and hidden layer representations formed and thus the output behavior of their deep nets. © 2022, The Author(s), under exclusive license to Springer Science+Business Media, LLC, part of Springer Nature.","Deep learning; Deep mining; Explainable AI; Interpretation; Knowledge discovery; Omics data","deep learning; human; knowledge discovery; machine learning; mining; prediction; trust; data mining; support vector machine; Data Mining; Machine Learning; Neural Networks, Computer; Support Vector Machine"
"Alzubaidi A., Tepper J., Lotfi A.","A novel deep mining model for effective knowledge discovery from omics data","10.1016/j.artmed.2020.101821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080038415&doi=10.1016%2fj.artmed.2020.101821&partnerID=40&md5=a2bcbf4c0231fba0be34fd9ad43b991e","Knowledge discovery from omics data has become a common goal of current approaches to personalised cancer medicine and understanding cancer genotype and phenotype. However, high-throughput biomedical datasets are characterised by high dimensionality and relatively small sample sizes with small signal-to-noise ratios. Extracting and interpreting relevant knowledge from such complex datasets therefore remains a significant challenge for the fields of machine learning and data mining. In this paper, we exploit recent advances in deep learning to mitigate against these limitations on the basis of automatically capturing enough of the meaningful abstractions latent with the available biological samples. Our deep feature learning model is proposed based on a set of non-linear sparse Auto-Encoders that are deliberately constructed in an under-complete manner to detect a small proportion of molecules that can recover a large proportion of variations underlying the data. However, since multiple projections are applied to the input signals, it is hard to interpret which phenotypes were responsible for deriving such predictions. Therefore, we also introduce a novel weight interpretation technique that helps to deconstruct the internal state of such deep learning models to reveal key determinants underlying its latent representations. The outcomes of our experiment provide strong evidence that the proposed deep mining model is able to discover robust biomarkers that are positively and negatively associated with cancers of interest. Since our deep mining model is problem-independent and data-driven, it provides further potential for this research to extend beyond its cognate disciplines. © 2020 Elsevier B.V.","AI; Data mining; Deep learning; Knowledge discovery; Omics data analysis; Precision medicine; Predictive modelling","Artificial intelligence; Deep learning; Diseases; Learning systems; Personalized medicine; Signal to noise ratio; Biological samples; Cognate disciplines; Deep feature learning; High dimensionality; Key determinants; Multiple projections; Predictive modelling; Small Sample Size; Data mining; Article; autoencoder; data mining; deep learning; knowledge discovery; methodology; omics; phenotype; priority journal; sparse autoencoder; data mining; genetics; human; machine learning; neoplasm; signal noise ratio; Data Mining; Humans; Knowledge Discovery; Machine Learning; Neoplasms; Signal-To-Noise Ratio"
"Alzubi H.S., Al-Nuaimy W., Young I.S.","Horse stress analysis using biomechanical modelling and machine learning approach","10.1109/SSD.2016.7473755","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974604321&doi=10.1109%2fSSD.2016.7473755&partnerID=40&md5=80a5411c2736b7c216001a6ead1679d4","Horse transport is a common practice in the equestrian industry, especially with the expansion of this industry around the world. Research has proved that horse transport by road is responsible for high stress levels, which sometimes exceed stress levels caused by exercising during professional horse races. Stress symptoms are reflected in the physiological functions of horses leading to horses suffering from horses fatigue or the injury. The horses stand still in a small box during transport to ensure safety and avoid falls or injuries. The weight is held by the four limbs while the vehicle is moving and vibration forces keep interrupting the balance. This requires the horse to counter these forces in order to keep its balance which demands high energy consumption even for short trips. The horse blood circulation system tries to support the muscles with enough oxygen forcing the heart to beat at high rates. This paper suggests an analytical biomechanical model for the vibration forces to understand how these forces move through horse limbs. This model is proposed to associate vibration forces with high stress levels during transport. Such a direct relationship between vehicle vibration forces and high stress levels will lead to a low cost non-invasive early stress detection system without the need to measure any direct physiological response of the horse. This relationship will also shed light on the importance of optimised vehicle design to reduce vibrations. © 2016 IEEE.","Biomechanical model; Horse; Horse transport; Stress","Artificial intelligence; Biomechanics; Cardiovascular system; Chemical detection; Energy utilization; Learning systems; Physiological models; Physiology; Stress analysis; Stresses; Vehicles; Vibrations (mechanical); Bio-mechanical models; Biomechanical modelling; High energy consumption; Horse; Horse transport; Machine learning approaches; Physiological functions; Physiological response; Vibration analysis"
"Alzweiri M., Watson D.G., Parkinson J.A.","Metabonomics as a clinical tool of analysis: LC-MS approaches","10.1080/10826076.2011.644054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872316623&doi=10.1080%2f10826076.2011.644054&partnerID=40&md5=01d728cfdcd539c7567e70d867f14812","Metabolic differences between test and control groups (i.e., metabonomics) are routinely accomplished by using multivariate analysis for data obtained commonly from NMR, GC-MS, and LC-MS. Multivariate analysis (e.g., principal component analysis PCA) is commonly used to extract potential metabolites responsible for clinical observations. Metabonomics applied to the clinical field is challenging because the physiological variabilities like gender, age, race, etc. might govern the cluster pattern obtained by multivariate analysis instead of the tested differences. This review focuses on the challenges facing the clinical applications of metabonomics and introduces their possible solutions as mentioned in the literature. © 2013 Taylor and Francis Group, LLC.","LC-MS; metabolic profiling; metabonomics; multivariate analysis; NMR; Principal Component Analysis","Clinical application; Clinical observation; Clinical tools; Cluster patterns; Control groups; LC-MS; Metabolic profiling; Metabonomics; Multi variate analysis; Metabolism; Nuclear magnetic resonance; Principal component analysis; Multivariant analysis; organic solvent; trifluoroacetic acid; analyzer; article; atmospheric pressure ionization; biomics; blood sampling; data mining; electrospray; human; hydrophilic interaction chromatography; ionization; liquid chromatography; mass spectrometry; metabonomics; multiple linear regression analysis; nuclear magnetic resonance spectroscopy; nuclear Overhauser effect; partial least squares regression; pH; precipitation; principal component analysis; quadrupole mass spectrometry; quantitative structure activity relation; solid phase extraction; time of flight mass spectrometry; ultrafiltration; urinalysis"
"Alzyoud F.Y., Maqableh W., Shrouf F.A.","A Semi Smart Adaptive Approach for Trash Classification","10.15837/ijccc.2021.4.4172","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113407739&doi=10.15837%2fijccc.2021.4.4172&partnerID=40&md5=7940dbe1213f8737b2523a6a95c3836e","Waste management and recycling play a crucial factor in world economy sustainability as they prevent the squander of useful materials which can lead in garbage landfill reduction and cost reduction respectively. Garbage sorting into different categories plays an important role in recycling and waste management; but unfortunately, most garbage sorting still depends on labor which has a reverse impact on mankind and world economy, so there are different approaches to replace human separation by intelligent machines. In this article, we propose a comprehensive approach, Semi Smart Trash Separator to classify garbage and trash using the following technique: precycling by assigning a barcode or QR code to each material, which will enable the separation process as per assigned code; Magnetic separator helps in collecting conductive metal, then the non-conductive materials are classified according to their hardness. This test is a unique idea used in trash classification. Finally, if there is ambiguity in waste material classification barcode or material properties, the classification will be done using neural network techniques depending on the shapes of trash. Mat lab software is modified to handle convolutional neural networks in the image recognition (AlexNet and GoogLeNet) to be used in the trash classification processes and to test their accuracy. The tests are performed using a trustable data set. The material recognition accuracy rate from the obtained results on AlexNet and GoogLeNet are 75% and 83% respectively © 2021 by the authors. Licensee Agora University, Oradea, Romania. This is an open access article distributed under the terms and conditions of the Creative Commons Attribution-NonCommercial 4.0 International License. Journal’s webpage: http://univagora.ro/jour/index.php/ijccc/","Artificial Intelligence; convolutional Neural Network; Machine Learning; Neural Network; Semi Smart Trash Separator",
"Amador-Domínguez E., Serrano E., Manrique D., Bajo J.","A case-based reasoning model powered by deep learning for radiology report recommendation","10.9781/ijimai.2021.08.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121046667&doi=10.9781%2fijimai.2021.08.011&partnerID=40&md5=ab7649417924dd59b073bc1903f8bb06","Case-Based Reasoning models are one of the most used reasoning paradigms in expert-knowledge-driven areas. One of the most prominent fields of use of these systems is the medical sector, where explainable models are required. However, these models are considerably reliant on user input and the introduction of relevant curated data. Deep learning approaches offer an analogous solution, where user input is not required. This paper proposes a hybrid Case-Based Reasoning, Deep Learning framework for medical-related applications, focusing on the generation of medical reports. The proposal combines the explainability and user-focused approach of case-based reasoning models with the deep learning techniques performance. Moreover, the framework is fully modular to fit a wide variety of tasks and data, such as real-time sensor captured data, images, or text, to name a few. An implementation of the proposed framework focusing on radiology report generation assistance is provided. This implementation is used to evaluate the proposal, showing that it can provide meaningful and accurate corrections, even when the amount of information available is minimal. Additional tests on the optimization degree of the case base are also performed, evidencing how the proposed framework can optimize this base to achieve optimal performance. © 2021, Universidad Internacional de la Rioja. All rights reserved.","Case-Based Reasoning; Deep Learning; Entity Recognition; Medical Radiology; Natural Language Processing",
"Amani M., Kakooei M., Moghimi A., Ghorbanian A., Ranjgar B., Mahdavi S., Davidson A., Fisette T., Rollin P., Brisco B., Mohammadzadeh A.","Application of google earth engine cloud computing platform, sentinel imagery, and neural networks for crop mapping in Canada","10.3390/rs12213561","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094844639&doi=10.3390%2frs12213561&partnerID=40&md5=b59a405e6dfdc6c87553ff564e2257cb","The ability of the Canadian agriculture sector to make better decisions and manage its operations more competitively in the long term is only as good as the information available to inform decision-making. At all levels of Government, a reliable flow of information between scientists, practitioners, policy-makers, and commodity groups is critical for developing and supporting agricultural policies and programs. Given the vastness and complexity of Canada’s agricultural regions, space-based remote sensing is one of the most reliable approaches to get detailed information describing the evolving state of the country’s environment. Agriculture and Agri-Food Canada (AAFC)-the Canadian federal department responsible for agriculture-produces the Annual Space-Based Crop Inventory (ACI) maps for Canada. These maps are valuable operational space-based remote sensing products which cover the agricultural land use and non-agricultural land cover found within Canada’s agricultural extent. Developing and implementing novel methods for improving these products are an ongoing priority of AAFC. Consequently, it is beneficial to implement advanced machine learning and big data processing methods along with open-access satellite imagery to effectively produce accurate ACI maps. In this study, for the first time, the Google Earth Engine (GEE) cloud computing platform was used along with an Artificial Neural Networks (ANN) algorithm and Sentinel-1,-2 images to produce an object-based ACI map for 2018. Furthermore, different limitations of the proposed method were discussed, and several suggestions were provided for future studies. The Overall Accuracy (OA) and Kappa Coefficient (KC) of the final 2018 ACI map using the proposed GEE cloud method were 77% and 0.74, respectively. Moreover, the average Producer Accuracy (PA) and User Accuracy (UA) for the 17 cropland classes were 79% and 77%, respectively. Although these levels of accuracies were slightly lower than those of the AAFC’s ACI map, this study demonstrated that the proposed cloud computing method should be investigated further because it was more efficient in terms of cost, time, computation, and automation. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Agriculture; Big data; Canada; Cloud computing; Cropland classification; Google earth engine; Neural network; Remote sensing; Sentinel","Agricultural robots; Cloud computing; Crops; Data handling; Decision making; Engines; Land use; Remote sensing; Satellite imagery; Space optics; Agricultural land use; Agricultural policies; Agriculture sectors; Cloud computing platforms; Data processing methods; Federal Department; Non-agricultural lands; Overall accuracies; Neural networks"
"Amara J., Kaur P., Owonibi M., Bouaziz B.","Convolutional neural network based chart image classification",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072733786&partnerID=40&md5=846bd5e85cd94e01910ec2e008abbb0d","Charts are frequently embedded objects in digital documents and are used to convey a clear analysis of research results or commercial data trends. These charts are created through different means and may be represented by a variety of patterns such as column charts, line charts and pie charts. Chart recognition is as important as text recognition to automatically comprehend the knowledge within digital document. Chart recognition consists on identifying the chart type and decoding its visual contents into computer understandable values. Previous work in chart image identification has relied on hand crafted features which often fails when dealing with a large amount of data that could contain significant varieties and less common char types. Hence, as a first step towards this goal, in this paper we propose to use a deep learning-based approach that automates the feature extraction step. We present an improved version of the LeNet [LeCu 89] convolutional neural network architecture for chart image classification. We derive 11 classes of visualization (Scatter Plot, Column Chart, etc.) which we use to annotate 3377 chart images. Results show the efficiency of our proposed method with 89.5 % of accuracy rate. © Vaclav Skala – UNION Agency.","Chart Image Classification; Data Visualization; Dataset Annotation; Deep Learning","Character recognition; Classification (of information); Computer vision; Convolution; Data visualization; Deep learning; Image enhancement; Network architecture; Neural networks; Visualization; Convolutional neural network; Dataset Annotation; Digital Documents; Embedded object; Image identification; Learning-based approach; Research results; Text recognition; Image classification"
"Amarasinghe K., Manic M.","Explaining What a Neural Network has Learned: Toward Transparent Classification","10.1109/FUZZ-IEEE.2019.8858899","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073781664&doi=10.1109%2fFUZZ-IEEE.2019.8858899&partnerID=40&md5=caba2bfabcbd5875f8d3391e2842e336","Deep Neural Networks (DNNs) have limited ability to explain their acquired knowledge or decision rationale. As a result, end-users perceive DNNs as black-boxes and are hesitant to fully adopt them in safety-critical applications. Therefore, developing explainable DNNs has become a prime interest in neural network research. This paper presents a methodology for linguistically explaining the knowledge a DNN classifier has acquired in training. The main objective is to help users understand what the DNN has learned about each class. The presented methodology is fuzzy logic based and involves end-users of the system in the explanation process, enabling users to customize the explanations to match their requirements. This paper presents the explanation methodology, metrics of explanation quality, validation steps, and a discussion of advantages and limitations. The explanation methodology was implemented on a benchmark classification problem. Experimental results demonstrated the method's capability to explain the DNN-knowledge and validated the explanations. © 2019 IEEE.","Explainable Artificial Intelligence; Explainable Neural Networks; Fuzzy Logic; Linguistic Summarization","Classification (of information); Computer circuits; Deep neural networks; Fuzzy neural networks; Fuzzy systems; Linguistics; Safety engineering; Benchmark classification; Black boxes; End users; Linguistic summarization; Safety critical applications; Fuzzy logic"
"Amarasinghe K., Manic M.","Improving user trust on deep neural networks based intrusion detection systems","10.1109/IECON.2018.8591322","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061553933&doi=10.1109%2fIECON.2018.8591322&partnerID=40&md5=7df225f543aa55955b66871818f769b0","Deep Neural Networks based intrusion detection systems (DNN-IDS) have proven to be effective. However, in domains like critical infrastructure security, user trust on the DNN-IDS is imperative and high accuracy isn't sufficient. The black-box nature of DNNs hinders transparency of the DNN-IDS, which is necessary for building trust. The main objective of this work is to improve user trust by improving transparency of the DNN-IDS by making it more communicative. This paper presents a methodology to generate offline and online feedback to the user on the decision making process of the DNN-IDS. Offline, the user is reported the input features that are most relevant in detecting each type of intrusion by the trained DNN-IDS. Online, for each detection, the user is reported the inputs features that contributed most to the detection. The presented method was implemented on the KDD-NSL dataset with a multi-layer perceptron (MLP) based DNN-IDS. Binary and multi-class classification was carried out on the dataset. Further, several DNN-IDS architectures with different depth were tested to study the factors that drive classification. It was observed that despite showing very similar accuracy results, the factors that drove the decisions were different across architectures. This evidences that the qualitative analysis that is enabled through reporting relevant input features is important for the user to make a more informed decision in choosing a DNN-IDS. This online and offline feedback leads to improving the transparency of the DNN-IDS and helps build trust prior to and during deployment. © 2018 IEEE.","Anomaly Detection; Deep Learning; Deep Neural Networks; Explainable AI; Intrusion Detection; Layer wise Relevance Propagation","Anomaly detection; Classification (of information); Computer crime; Decision making; Deep learning; Feature extraction; Industrial electronics; Intrusion detection; Network architecture; Transparency; Critical infrastructure securities; Decision making process; Informed decision; Intrusion Detection Systems; Layer-wise; Multi layer perceptron; Multi-class classification; Qualitative analysis; Deep neural networks"
"Amarasinghe K., Wickramasinghe C., Marino D., Rieger C., Manicl M.","Framework for Data Driven Health Monitoring of Cyber-Physical Systems","10.1109/RWEEK.2018.8473535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055791666&doi=10.1109%2fRWEEK.2018.8473535&partnerID=40&md5=0fcd64da6b3d1429e44b035553074f5a","Modern infrastructure is heavily reliant on systems with interconnected computational and physical resources, named Cyber-Physical Systems (CPSs). Hence, building resilient CPSs is a prime need and continuous monitoring of the CPS operational health is essential for improving resilience. This paper presents a framework for calculating and monitoring of health in CPSs using data driven techniques. The main advantages of this data driven methodology is that the ability of leveraging heterogeneous data streams that are available from the CPSs and the ability of performing the monitoring with minimal a priori domain knowledge. The main objective of the framework is to warn the operators of any degradation in cyber, physical or overall health of the CPS. The framework consists of four components: 1) Data acquisition and feature extraction, 2) state identification and real time state estimation, 3) cyber-physical health calculation and 4) operator warning generation. Further, this paper presents an initial implementation of the first three phases of the framework on a CPS testbed involving a Microgrid simulation and a cyber-network which connects the grid with its controller. The feature extraction method and the use of unsupervised learning algorithms are discussed. Experimental results are presented for the first two phases and the results showed that the data reflected different operating states and visualization techniques can be used to extract the relationships in data features. © 2018 IEEE.","Anomaly Detection; Cyber-Physical Systems; Explainable AI; Health Monitoring; Resilience; Unsupervised learning","Cyber Physical System; Data acquisition; Data mining; Data visualization; Embedded systems; Extraction; Feature extraction; Health; Knowledge management; Learning algorithms; Unsupervised learning; Anomaly detection; Cyber physical systems (CPSs); Data driven technique; Feature extraction methods; Health monitoring; Modern infrastructure; Resilience; Visualization technique; Monitoring"
"Amarasinghe K., Kenney K., Manic M.","Toward explainable deep neural network based anomaly detection","10.1109/HSI.2018.8430788","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052729377&doi=10.1109%2fHSI.2018.8430788&partnerID=40&md5=04e9bdbc5c6ada07cbe1b1c2aa6fb39d","Anomaly detection in industrial processes is crucial for general process monitoring and process health assessment. Deep Neural Networks (DNNs) based anomaly detection has received increased attention in recent work. Albeit their high accuracy, the black-box nature of DNNs is a drawback in practical deployment. Especially in industrial anomaly detection systems, explanations of DNN detected anomalies are crucial. This paper presents a framework for DNN based anomaly detection which provides explanations of detected anomalies. The framework answers the following questions during online processing: 1) 'why is it an anomaly?' and 2) 'what is the confidence?' Further, the framework can be used offline to evaluate the 'knowledge' of the trained DNN. The framework reduces the opaqueness of the DNN based anomaly detector and thus improves human operators' trust in the algorithm. This paper implements the first steps of the presented framework on the benchmark KDD-NSL dataset for Denial of Service (DoS) attack detection. Offline DNN explanations showed that the DNN was detecting DoS attacks based on features indicating destination of connection, frequency and amount of data transferred while showing an accuracy around 97%. © 2018 IEEE.","Anomaly Detection; Deep Learning; Deep Neural Networks; Explainable AI; Layer wise Relevance Propagation","Deep learning; Denial-of-service attack; Process monitoring; Anomaly detection; Anomaly detection systems; Anomaly detector; Denial of Service; Health assessments; Industrial processs; Layer-wise; Online processing; Deep neural networks"
"Amarpuri L., Yadav N., Kumar G., Agrawal S.","Prediction of CO2 emissions using deep learning hybrid approach: A Case Study in Indian Context","10.1109/IC3.2019.8844902","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073241738&doi=10.1109%2fIC3.2019.8844902&partnerID=40&md5=65dba719df74a88a1f9364ba81a4be1e","Carbon Dioxide (CO2), which accounts for 1% of the atmospheric gases but responsible for about 81% of the total emissions, is a major component of Green House Gas (GHG) Emissions. Produced naturally (decomposition, respiration and ocean drive) or released as a consequence of human activities (burning of fossil fuels, cement production, automotive exhausts etc.), CO2has played a critical role in global warming. The concern over CO2is the significant change in its level over a short period of time. The net result is melting of polar ice caps or rising annual global temperature. India, a developing nation and an emerging economy, is the fourth largest producer of Carbon Dioxide emissions following China, United States of America and the European Union. India has now overtaken Russia to become the third largest producer of electricity but it still relies on coal as the biggest source of electricity. Awaken by the adversities of Carbon Dioxide Emissions, India has signed the Paris Agreement and pledged to reduce the Carbon Dioxide levels to 30-35% of the level in the year 2005. This agreement will start from the year 2020. The research is aimed at predicting the CO2levels in the year 2020 to have a better understanding of the challenge posed to Government of India. The prediction technique used is a deep learning hybrid model of Convolution Neural Network and Long Short-Term Memory Network (CNN-LSTM). © 2019 IEEE.","CNN; Deep Learning; Green House Gas; LSTM; Neural Networks","Carbon dioxide; Deep learning; Deep neural networks; Forecasting; Fossil fuels; Gas emissions; Global warming; Long short-term memory; Neural networks; Carbon dioxide emissions; Convolution neural network; Emerging economies; Global temperatures; Government of India; LSTM; Prediction techniques; United States of America; Greenhouse gases"
"Amato F., Ferraro A., Galli A., Moscato F., Moscato V., Sperlí G.","Credit Score Prediction Relying on Machine Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137451300&partnerID=40&md5=8eed9bf9b9b8a613384cd8cf197731ff","Financial institutions use a variety of methodologies to define their commercial and strategic policies, and a significant role is played by credit risk assessment. In recent years, different credit risk assessment services arose, providing Social Lending platforms to connect lenders and borrowers in a direct way without assisting of financial institutions. Despite the pros of these platforms in supporting fundraising process, there are different stems from multiple factors including lack of experience of lenders, missing or uncertain information about the borrower's credit history. In order to handle these problems, credit risk assessments of financial transactions are usually modeled as a binary problem based on debt repayment, going to apply Machine Learning (ML) techniques. The paper represents an extended abstract of a recent work, where some of the authors performed a benchmarking among the most used credit risk assessment ML models in the field of predicting whether a loan will be repaid in a P2P platform. The experimental analysis is based on a real dataset of Social Lending (Lending Club), going to evaluate several evaluation metrics including AUC, sensitivity, specificity and explainability of the models. © 2022 CEUR-WS. All rights reserved.","Credit Score Prediction; eXplainable Artificial Intelligence; Machine Learning","Finance; Machine learning; Risk assessment; Commercial policy; Credit risk assessment; Credit score prediction; Credit scores; Explainable artificial intelligence; Financial institution; Machine-learning; Multiple factors; On-machines; Strategic policy; Forecasting"
"Amato F., Di Cicco F., Fonisto M., Giacalone M.","A Survey on Neural Recommender Systems: Insights from a Bibliographic Analysis","10.1007/978-3-030-99619-2_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128786261&doi=10.1007%2f978-3-030-99619-2_10&partnerID=40&md5=342512c2a752296bbe6b629d124bcbd6","In recent years, deep learning has gotten a lot of attention, notably in fields like Computer Vision and Natural Language Processing. With the growing amount of online information, recommender systems have shown to be an effective technique for coping with information overload. The purpose of this article is to provide a comprehensive overview of recent deep learning-based recommender systems. Furthermore, it provides an experimental assessment of prominent topics within the latest published papers in the field. Results showed that explainable AI and Graph Neural Networks are two of the most attractive topics in the field to this day, and that the adoption of deep learning methods is increasing over. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Deep learning; Recommender systems; Research trend analysis",
"Amato Mangiameli A.C.","Algorithms and big data. The rules and principles of robotics [Algoritmi e big data Dalla carta sulla robotica]","10.4477/93369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088306357&doi=10.4477%2f93369&partnerID=40&md5=34bd685d803a076a989fc0a45234983a","Is the activity of software a neutral one? The set of algorithms and big data which help the framework of an artificial intelligence in recognizing, classifying or elaborating data, could be considered truly objective? As a matter of fact, data are never objective: Statistical models represent reality as they modify it. Furthermore, we are living in the era of learning machines: Robots are everyday more intelligent, useful and independent from human control. As they are implied in a long list of activities, we need to know which rules they have to respect. Most of all, we need to establish who, and in what terms, could be responsible for their faults. That is why the European Union is elaborating a set of principles and rules on Robots and Artificial Intelligence: New technologies have a great potential for the benefit of humanity, provided that ethics and law do not give up on ruling them. © 2019 Società editrice il Mulino.","Algorithms; Big Data; European Union.; Neural Networks; Robots",
"Amazonas M., Vasconcelos V., Brandão A., Kienem G., Castro T., Gadelha B., Fuks H.","Collaborative music composition based on sonic interaction design","10.1007/978-3-319-91125-0_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050465020&doi=10.1007%2f978-3-319-91125-0_28&partnerID=40&md5=ba2f6ba2f2f07ddf67149f9286ae6619","This work proposes a way to create sonic interaction through collaborative composition of spatialized sounds in real time, based on SID (sound interaction design). There is no centralizing figure for the sound designer, and each person may begin a sonic interaction spontaneous and independently, or join an ongoing interaction. Therefore, each person is responsible for the result of his own interaction. Every movement a person makes within the reach of the determined space, the sound is captured and processed and then externalized as feedback in the multichannel sound system, perceived as a unified sound. The concepts and app designed for sonic interaction discussed in this work is intended for use in multidisciplinary contexts, raising important technical challenges. We have devised a design process resulting in four different prototypes, attending to different perspectives. Each prototype had its own experiment. © Springer International Publishing AG, part of Springer Nature 2018.","Collaborative music composition; Interaction design; Real time spatialization","Artificial intelligence; Computer science; Computers; Interaction design; Multichannel sounds; Music composition; Sonic interaction design; Sonic interactions; Sound interactions; Spatialization; Technical challenges; Design"
"Amber K.P., Aslam M.W.","Energy-related environmental and economic performance analysis of two different types of electrically heated student residence halls","10.1080/14786451.2016.1174703","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963807322&doi=10.1080%2f14786451.2016.1174703&partnerID=40&md5=c57903deb5d7c138c10f3b7191543129","Student residence halls occupy 26% of the total area of a typical university campus in the UK and are directly responsible for 24% of university’s annual CO2 emissions. Based on five years measured data, this paper aims to investigate the energy-related environmental and economic performance of electrically heated residence halls in which space heating is provided by two different types of electric heaters, that is, panel heater (PHT) and storage heater (SHT). Secondly, using statistical and machine learning methods, the paper attempts to investigate the relationship between daily electricity consumption and five factors (ambient temperature, solar radiation, relative humidity, wind speed and type of day). Data analysis revealed that electricity consumption of both halls is mainly driven by ambient temperature only, whereas SHT residence has 39% higher annual electricity bill and emits 70% higher CO2 emissions on a per square metre basis compared to the PHT residence hall. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","CO2 emissions; Electricity consumption; student residence halls; university residences","Artificial intelligence; Digital storage; Electric power utilization; Learning systems; Students; Temperature; Wind; Electric heater; Electricity bill; Electricity-consumption; Environmental and economic performance; Machine learning methods; Residence hall; University campus; university residences; Carbon dioxide; carbon dioxide; carbon emission; electricity supply; energy use; environmental economics; indoor air; performance assessment; university sector; ventilation; United Kingdom"
"Ambika G.N., Singh B.P., Sah B., Tiwari D.","Air quality index prediction using linear regression","10.35940/ijrte.B2437.078219","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071419958&doi=10.35940%2fijrte.B2437.078219&partnerID=40&md5=13954d90c166349cae0c6f7a80eecaa8","Controlling and preserving the better air excellence has become one of the most indispensible events in numerous manufacturing plus metropolitan regions at present. The excellence of air is harmfully affecting payable to the various forms of contamination affected via the transportation, power, fuels expenditures, etc. The installation of destructive fumes is spawning the severe hazard for the class of natural life in developed metropolises. Through cumulative air contamination, we require implementing competent air excellence monitoring models which gathers the statistics about the absorption of air impurities and be responsible for calculation of air contamination in each zone. Hence, air excellence estimation plus calculation has come to be a significant study area. The superiority of air is exaggerated by multi-dimensional influences comprising place, time plus indeterminate parameters. The intention of this development is to examine the machine learning based methods for air quality prediction. © BEIESP.","Air Quality Index; Auto ARIMA Model; Jupyter Notebook; Linear Regression; Python; Stepwise Regression; Tableau Public",
"Ambika M., Keerthana S.S., Sarika M., Swetha G., Thasnim M.S.","Smart card based electronic passport system using rfid","10.5373/JARDCS/V12I6/S20201081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090717323&doi=10.5373%2fJARDCS%2fV12I6%2fS20201081&partnerID=40&md5=deead89600cc8641045a12608029eef6","Now-a-days smart card plays a vital role in almost every field. Smart card is the card which can provide functions like data exchange, storage, and manipulation and used to access any other resources. Radio Frequency Identifier (RFID) refers to an identification badge or card that transfers its contents to the reader. The prime aim of this article is to promote smart card-based support for E-passport system. This system can support updation, renewal and user authentication process. It also enables us to obtain required patterns from the database using data mining techniques. This card contains the name, unique identification (UID) number, date of birth, nationality and all other required details. An Integrated Chip used on this card is responsible for storing and processing the information through modulation and demodulation on the RF signal which will be transmitted by the reader. The RFID reader reads the data present in the card and sends the data through wireless stream using RF transceiver. As soon as the card is placed over the card reader, it verifies the data present in the system and if it matches, the details of the card holder will be displayed. Moreover, the proposed system can enable the retrieval of frequent patterns. The proposed system shows better performance in terms of accuracy when compared with other existing systems. Thus, the proposed system reduces the burden of documentation which thereby reduces time consumption. Thus, the system provides enhanced security and ease of access to the user. © 2020, Institute of Advanced Scientific Research, Inc.. All rights reserved.","Electronic Passport System; Identification; IOT; RFID reader; Security; Smart card; Smart card reader",
"Amblard M.","For a responsible natural language processing [Pour un TAL responsable]",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024124969&partnerID=40&md5=47a1705ac9483c73feb15569320a2179","Artificial intelligence (AI) has evolved in recent years along with societal concerns. Various committees were introduced in order to brainstorm on the consequences of these developments. These authorities are also concerned by Natural Language Processing (NLP), not only as a subfield of AI but also as a specific field with which it interacts. In this article we review the links between AI and NLP but also where they differ. We focus on ethical clues for both of them. Finally we argue for not using ethics as a unique solution, but rather as the way to abstract over our researches. In the end, we go back on how to interpret machine learning methods in the context of NLP.",,
"Ambrosino M., Albanese S., De Vivo B., Guagliardi I., Guarino A., Lima A., Cicchella D.","Identification of Rare Earth Elements (REEs) distribution patterns in the soils of Campania region (Italy) using compositional and multivariate data analysis","10.1016/j.gexplo.2022.107112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140335244&doi=10.1016%2fj.gexplo.2022.107112&partnerID=40&md5=026093a8736278bffc14941d72c453e7","A big monitoring programme was carried out in Italy from 2015 to 2018 with the aim to assess the environmental conditions of the Campania region territory. Among the different environmental media, 7300 samples of topsoil (10–15 cm) were collected and analysed for 52 elements through an analytical methodology combining ICP-MS and ICP-ES. Specifically, this paper focuses on the geochemical distribution of La, Ce, Sc and Y with implications for the whole REEs group. Machine learning techniques, Compositional Data Analysis (CoDA) and multivariate statistics were used in order to understand the behaviour of these elements in different geogenic and anthropogenic conditions and their interaction with other elements. Lasso regression (LR) was performed to select the variables most related with REEs. Subsequently, robust Principal Component Analysis (rPCA) was performed, using 20 elements selected through the LR to identify the main geochemical associations. Finally, using rPCA scores and additive log-ratio (alr) coordinates, distribution maps were generated to assess the patterns of the REEs sources identified. The results show that median concentration values of the investigated elements (Ce = 80.3, La = 41.3, Y = 15.4 and Sc = 3.3 mg/kg) are significantly higher than those of European and Italian soils. The type of lithologic substrate and the degree of soil alteration are the main factors responsible of the REEs concentrations. Cerium and La have the same behaviour and show higher concentrations in volcanic soils, especially the more altered ones, which are associated with high values of low-mobile elements like Th, Fe, Hf, Zr, Mn, or where the pyroclastic cover lies on limestone rocks. Otherwise, Sc is enriched in soils formed on siliciclastic and calcareous rocks while Y shows an intermediate behaviour between Sc and, La and Ce. This study has shown that Th, Fe and Mn are the elements most closely related to REEs in soils of Campania region, therefore they can be used to estimate their natural concentration in soil. © 2022 Elsevier B.V.","Campania Region; Geochemical mapping; Lasso regression; Robust Principal Component Analysis; Soil samples","Data handling; Exploratory geochemistry; Information analysis; Learning systems; Lime; Multivariant analysis; Principal component analysis; Soils; Volcanoes; Campania; Campanium region; Compositional data analysis; Distribution patterns; Element distribution; Geochemical mapping; Geochemicals; Lasso regressions; Robust principal component analysis; Soil sample; Rare earths"
"Ambroszkiewicz S.","On higher order computations and synaptic meta-plasticity in the human brain","10.1007/978-3-319-44781-0_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988418395&doi=10.1007%2f978-3-319-44781-0_18&partnerID=40&md5=fae8586096325b1d3dc9cbea33aabab6","Glia modify neuronal connectivity by creating structural changes in the neuronal connectome. Glia also influence the functional connectome by modifying the flow of information through neural networks (Fields et al. 2015 [6]). There are strong experimental evidences that glia are responsible for synaptic meta-plasticity. Synaptic plasticity is the modification of the strength of connections between neurons. Meta-plasticity, i.e. plasticity of synaptic plasticity, may be viewed as mechanisms for dynamic reconfiguration of neural circuits. Since synapse creation corresponds to the mathematical notion of function composition, the mechanisms may serve as a grounding for functionals, i.e. higher order functions that take functions as their arguments. © Springer International Publishing Switzerland 2016.",,"Dynamic models; Functions; Machine learning; Neurons; Dynamic re-configuration; Experimental evidence; Higher order functions; Higher-order; Mathematical notion; Neural circuits; Neuronal connectivity; Synaptic plasticity; Neural networks"
"Ambroszkiewicz S.","Agent based approach to service description and composition","10.1007/978-3-540-45173-0_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-7044272228&doi=10.1007%2f978-3-540-45173-0_10&partnerID=40&md5=5f601f69595e573a8d88c7c7a9113a3e","An approach to service description and composition is presented. It is based on agent technology, and on the idea of separating description and composition language from biding, i.e., from specification of data format (exchanged by applications) and transport protocol. Usually, the biding is an integral part of description language, e.g., WSDL, and DAML-S. Starting with this idea, a simple service description language is constructed as well as a composition protocol is specified. Agents play crucial role in our approach; they are responsible for service composition. © Springer-Verlag Berlin Heidelberg 2003.",,"Composition protocols; Service composition; Service description; Transport protocols; Agent technology; Agent-based approach; Composition languages; Description languages; Service compositions; Service description; Service description language; Transport protocols; Computer programming languages; Data reduction; Network protocols; Artificial intelligence; Computer science; Computers; Multi agent systems; Computational linguistics"
"Ambsdorf J., Munir A., Wei Y., Degkwitz K., Harms H.M., Stannek S., Ahrens K., Becker D., Strahl E., Weber T., Wermter S.","Explain yourself! Effects of Explanations in Human-Robot Interaction","10.1109/RO-MAN53752.2022.9900558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139608738&doi=10.1109%2fRO-MAN53752.2022.9900558&partnerID=40&md5=a29918e62de60ac05d483dcab444bdb2","Recent developments in explainable artificial intelligence promise the potential to transform human-robot interaction: Explanations of robot decisions could affect user perceptions, justify their reliability, and increase trust. However, the effects on human perceptions of robots that explain their decisions have not been studied thoroughly. To analyze the effect of explainable robots, we conduct a study in which two simulated robots play a competitive board game. While one robot explains its moves, the other robot only announces them. Providing explanations for its actions was not sufficient to change the perceived competence, intelligence, likeability or safety ratings of the robot. However, the results show that the robot that explains its moves is perceived as more lively and human-like. This study demonstrates the need for and potential of explainable human-robot interaction and the wider assessment of its effects as a novel research direction. © 2022 IEEE.",,"Intelligent robots; Man machine systems; Board games; Human like; Human perception; Humans-robot interactions; Likeability; Perceived competence; Safety ratings; Simulated robot; User perceptions; Human robot interaction"
"Ameen Z.S., Saleh Mubarak A., Altrjman C., Alturjman S., Abdulkadir R.A.","Explainable Residual Network for Tuberculosis Classification in the IoT Era","10.1109/FoNeS-AIoT54873.2021.00012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129634806&doi=10.1109%2fFoNeS-AIoT54873.2021.00012&partnerID=40&md5=7a6e649de4241e9d2ec7b2155125a7fc","Tuberculosis (TB) is a lung disease that tops the world mortality rate. The widely employed method of diagnosis is the X-ray images which gives the pictorial information of the lungs. In the era of the Internet of Things (IoT), Artificial Intelligence (AI) using deep learning is among the most efficient methods in detecting lung-related diseases, and in classifying the related X-ray images. To trust the achieved decision, in this study ResNet-50 was used to classify TB and normal patients X-ray images, also, a Gradient-weighted Class Activation Mapping (Grad-CAM) was used to extract the features of the last pooling layer of the ResNet model to know what makes the model classify the X-ray images based on the given classes. © 2021 IEEE.","Deep Learning; Explainable AI; IoT; Tuberculosis","Biological organs; Deep learning; Diagnosis; Image classification; Activation mapping; Deep learning; Explainable artificial intelligence; Image-based; Mortality rate; Tuberculosis; X-ray image; Internet of things"
"Ameli M., Pfanschilling V., Amirli A., Maaß W., Kersting K.","Unsupervised Multi-sensor Anomaly Localization with Explainable AI","10.1007/978-3-031-08333-4_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133290546&doi=10.1007%2f978-3-031-08333-4_41&partnerID=40&md5=5c3b57597ec0c86f1fb3dbc3783fa201","Multivariate and Multi-sensor data acquisition for the purpose of device monitoring had a significant impact on recent research in Anomaly Detection. Despite the wide range of anomaly detection approaches, localization of detected anomalies in multivariate and Multi-sensor time-series data remains a challenge. Interpretation and anomaly attribution is critical and could improve the analysis and decision-making for many applications. With anomaly attribution, explanations can be leveraged to understand, on a per-anomaly basis, which sensors cause the root of anomaly and which features are the most important in causing an anomaly. To this end, we propose using saliency-based Explainable-AI approaches to localize the essential sensors responsible for anomalies in an unsupervised manner. While most Explainable AI methods are considered as interpreters of AI models, we show for the first time that Saliency Explainable AI can be utilized in Multi-sensor Anomaly localization applications. Our approach is demonstrated for localizing the detected anomalies in an unsupervised multi-sensor setup, and the experiments show promising results. We evaluate and compare different classes of saliency explainable AI approach on the Server Machine Data (SMD) Dataset and compared the results with the state-of-the-art OmniAnomaly Localization approach. The results of our empirical analysis demonstrate a promising performance. © 2022, IFIP International Federation for Information Processing.","Anomaly localization; Explainable artificial intelligence; Multi-sensor data; Multivariate time-series; Unsupervised anomaly detection","Artificial intelligence; Data acquisition; Decision making; Time series; Anomaly detection; Anomaly localizations; Explainable artificial intelligence; Localisation; Multi sensor; Multi-sensor data; Multivariate sensors; Multivariate time series; Recent researches; Unsupervised anomaly detection; Anomaly detection"
"Amendola A., Riva A., Camici S., Turolla A., Zampato M.","A novel machine learning-based support tool for source rock identification from biomarkers data: Theory and case studies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066778595&partnerID=40&md5=ffbbf999de7b98f2b09aceb2b42d79e5","Petroleum biomarkers are complex carbon-based molecules derived from formerly living organisms and found in crude oils. These molecules are used by geochemists to get information on the source rocks responsible for the oils generation, such as lithology, depositional environment, organic matter, maturity and age. So they are of paramount importance for Petroleum System Modelling and more generally for exploration de-risking and sedimentary basin characterization purposes. Very often, biomarkers datasets are very large and interpretation process by geochemists can take several months to complete. For this reason, we developed an innovative Machine Learning-based support tool to facilitate and speed-up the whole process of biomarkers examination and interpretation. The core of tool is an advanced clustering method that allows expressing biomarkers data as a combination (mixing) of underlying components, directly ascribable to different source rocks. Non-negative constraint is a key aspect: the objective is to express each data sample, i.e. a vector with mainly non-negative values such as biomarkers concentrations and/or concentration ratios, as an additive combination of some of the underlying components, whereas subtracting components would not have any physical interpretation. A sparsity constraint is added to find solutions that allow to represent data as an additive combination of few source rock components. Both constraints greatly reduce non-uniqueness of the solution, greatly enhancing interpretability of the results. The tool then groups data in clusters, each one having a specific geochemical signature given by a set of scores for each of the different biomarkers’ parameters. Each sample is assigned to a specific cluster with a “purity” percentage indicator. Geochemists can then easily use the high-purity samples to label the relevant samples as belonging to different source rocks. Moreover, the tool is able to distinguish the amount of mixing between different source rocks, through accurate deconvolution algorithms. Two applications of the tool are here presented, borrowed by real exploration case studies. In both cases the tool was able to separate samples into clusters that geochemists successfully recognized as lacustrine, marine and in some cases, transitional, with less than 10% of misclassifications, isolating also strongly biodegraded samples. This tool opens the doors also to the insertion and integration of other types of data (light hydrocarbons, diamondoids, etc.) for the whole ‘Big Data’ geochemical characterization of a sedimentary basin. © 2019 Offshore Mediterranean Conference (OMC). All rights reserved.",,"Additives; Biology; Biomarkers; Crude oil; Deconvolution; Gasoline; Geochemistry; Large dataset; Lithology; Machine learning; Mixing; Molecules; Offshore oil well production; Offshore petroleum prospecting; Rocks; Sedimentology; Settling tanks; Deconvolution algorithm; Depositional environment; Geochemical characterization; Geochemical signatures; Petroleum biomarkers; Physical interpretation; Sparsity constraints; Underlying components; Petroleum geology"
"Amendola G., Cosconati S.","PyRMD: A New Fully Automated AI-Powered Ligand-Based Virtual Screening Tool","10.1021/acs.jcim.1c00653","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111495940&doi=10.1021%2facs.jcim.1c00653&partnerID=40&md5=6613dbedb7e9bae1b8161165fd8cf586","Artificial intelligence (AI) algorithms are dramatically redefining the current drug discovery landscape by boosting the efficiency of its various steps. Still, their implementation often requires a certain level of expertise in AI paradigms and coding. This often prevents the use of these powerful methodologies by non-expert users involved in the design of new biologically active compounds. Here, the random matrix discriminant (RMD) algorithm, a high-performance AI method specifically tailored for the identification of new ligands, was implemented in a new fully automated tool, PyRMD. This ligand-based virtual screening tool can be trained using target bioactivity data directly downloaded from the ChEMBL repository without manual intervention. The software automatically splits the available training compounds into active and inactive sets and learns the distinctive chemical features responsible for the compounds' activity/inactivity. PyRMD was designed to easily screen millions of compounds in hours through an automated workflow and intuitive input files, allowing fine tuning of each parameter of the calculation. Additionally, PyRMD features a wealth of benchmark metrics, to accurately probe the model performance, which were used here to gauge its predictive potential and limitations. PyRMD is freely available on GitHub (https://github.com/cosconatilab/PyRMD) as an open-source tool. ©",,"Automation; Benchmarking; Bioactivity; Ligands; Open source software; Outsourcing; Automated workflow; Benchmark metrics; Biologically active compounds; Chemical features; Manual intervention; Model performance; Open source tools; Virtual Screening; Artificial intelligence"
"Amendola G., Libkin L.","Explainable certain answers","10.24963/ijcai.2018/233","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055700811&doi=10.24963%2fijcai.2018%2f233&partnerID=40&md5=699f2a0c6c38ce18f2ecc40d151f080d","When a dataset is not fully specified and can represent many possible worlds, one commonly answers queries by computing certain answers to them. A natural way of defining certainty is to say that an answer is certain if it is consistent with query answers in all possible worlds, and is furthermore the most informative answer with this property. However, the existence and complexity of such answers is not yet well understood even for relational databases. Thus in applications one tends to use different notions, essentially the intersection of query answers in possible worlds. However, justification of such notions has long been questioned. This leads to two problems: are certain answers based on informativeness feasible in applications? and can a clean justification be provided for intersection-based notions? Our goal is to answer both. For the former, we show that such answers may not exist, or be very large, even in simple cases of querying incomplete data. For the latter, we add the concept of explanations to the notion of informativeness: it shows not only that one object is more informative than the other, but also says why this is so. This leads to a modified notion of certainty: explainable certain answers. We present a general framework for reasoning about them, and show that for open and closed world relational databases, they are precisely the common intersection-based notions of certainty. © 2018 International Joint Conferences on Artificial Intelligence. All right reserved.",,"Artificial intelligence; Certain answers; Incomplete data; Informative ness; Possible worlds; Relational Database; Query processing"
"Amendola J., Tannuri E.A., Cozman F.G., Reali Costa A.H.","Port channel navigation subjected to environmental conditions using reinforcement learning","10.1115/OMAE2019-96120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075809293&doi=10.1115%2fOMAE2019-96120&partnerID=40&md5=2cae97a398c70a88f42f7507f41335ce","This paper proposes a machine learning agent for automatically navigating a vessel in a confined channel subject to environmental conditions. The agent is trained and tested using a Ship Maneuvering Simulator and is responsible for commanding the rudder, so as to keep the vessel inside the channel with minimum distance from the center line, and to reach the final part of the channel with a prescribed thruster rotation level. The algorithm is based on deep reinforcement learning method and uses an efficient state-space representation. The advantage of using reinforcement learning is that it does not require any expert to directly teach the agent how to behave under particular conditions. The novelty of this work is that: it does not require previous knowledge on the vessel dynamic model and the maneuvering scenario; it is robust against fluctuations of environmental forces such as wind and current; it considers discrete actions of rudder commands emulating the pilot actions in a real maneuver. The developed method is convenient for simulations in scenarios or areas that were never navigated before, in which no previous navigation data can be used to train a conventional supervised learning agent. One direct application for this work is the integration with a realistic fast-time maneuvering simulator for new ports or operations. Both training and validation experiments focused on the unsheltered approach channel of the Suape Port, in Brazil; these experiments were run in a SMH-USP maneuvering simulator (real environmental conditions measured on-site were employed in simulations). Copyright © 2019 ASME.",,"Arctic engineering; Deep learning; Machine learning; Ocean engineering; Offshore oil well production; Rudders; Simulators; State space methods; Approach channels; Channel navigation; Environmental conditions; Environmental forces; Machine learning agents; Particular condition; Reinforcement learning method; State space representation; Reinforcement learning"
"Ament S.E., Stein H.S., Guevarra D., Zhou L., Haber J.A., Boyd D.A., Umehara M., Gregoire J.M., Gomes C.P.","Multi-component background learning automates signal detection for spectroscopic data","10.1038/s41524-019-0213-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069536756&doi=10.1038%2fs41524-019-0213-0&partnerID=40&md5=30ff3425f50ba83cb26e38e7e8eb829d","Automated experimentation has yielded data acquisition rates that supersede human processing capabilities. Artificial Intelligence offers new possibilities for automating data interpretation to generate large, high-quality datasets. Background subtraction is a long-standing challenge, particularly in settings where multiple sources of the background signal coexist, and automatic extraction of signals of interest from measured signals accelerates data interpretation. Herein, we present an unsupervised probabilistic learning approach that analyzes large data collections to identify multiple background sources and establish the probability that any given data point contains a signal of interest. The approach is demonstrated on X-ray diffraction and Raman spectroscopy data and is suitable to any type of data where the signal of interest is a positive addition to the background signals. While the model can incorporate prior knowledge, it does not require knowledge of the signals since the shapes of the background signals, the noise levels, and the signal of interest are simultaneously learned via a probabilistic matrix factorization framework. Automated identification of interpretable signals by unsupervised probabilistic learning avoids the injection of human bias and expedites signal extraction in large datasets, a transformative capability with many applications in the physical sciences and beyond. © 2019, The Author(s).",,"Data acquisition; Extraction; Factorization; Probabilistic logics; Signal detection; Automated identification; Automatic extraction; Background subtraction; Data interpretation; Probabilistic Learning; Probabilistic matrix factorizations; Processing capability; Signals of interests; Large dataset"
"Amer A., Ye X., Zolgharni M., Janan F.","ResDUnet: Residual Dilated UNet for Left Ventricle Segmentation from Echocardiographic Images","10.1109/EMBC44109.2020.9175436","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091031300&doi=10.1109%2fEMBC44109.2020.9175436&partnerID=40&md5=1efd749c209b73b6ad4645f353ea4956","Echocardiography is the modality of choice for the assessment of left ventricle function. Left ventricle is responsible for pumping blood rich in oxygen to all body parts. Segmentation of this chamber from echocardiographic images is a challenging task, due to the ambiguous boundary and inhomogeneous intensity distribution. In this paper we propose a novel deep learning model named ResDUnet. The model is based on U-net incorporated with dilated convolution, where residual blocks are employed instead of the basic U-net units to ease the training process. Each block is enriched with squeeze and excitation unit for channel-wise attention and adaptive feature re-calibration. To tackle the problem of left ventricle shape and size variability, we chose to enrich the process of feature concatenation in U-net by integrating feature maps generated by cascaded dilation. Cascaded dilation broadens the receptive field size in comparison with traditional convolution, which allows the generation of multi-scale information which in turn results in a more robust segmentation. Performance measures were evaluated on a publicly available dataset of 500 patients with large variability in terms of quality and patients pathology. The proposed model shows a dice similarity increase of 8.4% when compared to deeplabv3 and 1.2% when compared to the basic U-net architecture. Experimental results demonstrate the potential use in clinical domain. © 2020 IEEE.",,"Convolution; Deep learning; Echocardiography; Large dataset; Medicine; Adaptive features; Echocardiographic images; Intensity distribution; Left ventricle shapes; Multi-scale informations; Performance measure; Receptive field sizes; Robust segmentation; Image segmentation"
"Ameri F., Yoder R., Zandbiglari K.","SKOS Tool: A Tool for Creating Knowledge Graphs to Support Semantic Text Classification","10.1007/978-3-030-57997-5_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090171505&doi=10.1007%2f978-3-030-57997-5_31&partnerID=40&md5=aa9f9b0ad98ed95b771194f7f5f30b35","Knowledge graphs are being increasingly adopted in industry in order to add meaning to data and improve the intelligence of data analytics methods. Simple Knowledge Management System (SKOS) is a W3C standard for representation of knowledge graphs in a web-native and machine-understandable format. This paper introduces SKOS Tool; a web-based application developed at the Engineering Informatics Lab at Texas State University. It can be used for creating knowledge graphs and concept schemes based on the SKOS standard. The main feature and functions of SKOS Tool are described in this paper. Beyond creating knowledge graphs, SKOS Tool has additional features that can be used to support semantic document classification based on the Bag of Concepts technique. To demonstrate the utilities of SKOS Tool, a use case related to classifications of manufacturing suppliers with Medical Grade Polymer Tubing capabilities is presented. © 2020, IFIP International Federation for Information Processing.","Artificial intelligence; Knowledge graph; Natural language processing; Semantic classifier; SKOS","Classification (of information); Data Analytics; Functional polymers; Graphic methods; Industrial management; Information retrieval systems; Knowledge based systems; Knowledge management; Knowledge representation; Manufacture; Semantics; Document Classification; Engineering informatics; Knowledge graphs; Knowledge management system; Medical grade polymers; Text classification; W3C standards; Web-based applications; Text processing"
"Ameur C., Faquir S., Yahyaouy A., Abdelouahed S.","Enhancing hybrid renewable energy performance through deep Q-learning networks improved by fuzzy reward control","10.11591/ijece.v12i4.pp4302-4314","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129675207&doi=10.11591%2fijece.v12i4.pp4302-4314&partnerID=40&md5=0dc2ae10d9f31c71061af9e5e5634e01","In a stand-alone system, the use of renewable energies, load changes, and interruptions to transmission lines can cause voltage drops, impacting its reliability. A way to offset a change in the nature of hybrid renewable energy immediately is to utilize energy storage without needing to turn on other plants. Photovoltaic panels, a wind turbine, and a wallbox unit (responsible for providing the vehicle’s electrical need) are the components of the proposed system; in addition to being a power source, batteries also serve as a storage unit. Taking advantage of deep learning, particularly convolutional neural networks, and this new system will take advantage of recent advances in machine learning. By employing algorithms for deep Q-learning, the agent learns from the data of the various elements of the system to create the optimal policy for enhancing performance. To increase the learning efficiency, the reward function is implemented using a fuzzy Mamdani system. Our proposed experimental results shows that the new system with fuzzy reward using deep Q-learning networks (DQN) keeps the battery and the wallbox unit optimally charged and less discharged. Moreover confirms the economic advantages of the proposed approach performs better approximate to +25% Moreover, it has dynamic response capabilities and is more efficient over the existing optimization approach using deep learning without fuzzy logic. © 2022 Institute of Advanced Engineering and Science. All rights reserved.","Fuzzy logic controller; Photovoltaic panels; Reinforcement Q-learning; Renewable energy; Wind turbine",
"Amgad M., Atteya L.A., Hussein H., Mohammed K.H., Hafiz E., Elsebaie M.A.T., Mobadersany P., Manthey D., Gutman D.A., Elfandy H., Cooper L.A.D.","Explainable nucleus classification using Decision Tree Approximation of Learned Embeddings","10.1093/bioinformatics/btab670","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126278928&doi=10.1093%2fbioinformatics%2fbtab670&partnerID=40&md5=94dd290a0ed4a2e0f0b4af43f8918432","Motivation: Nucleus detection, segmentation and classification are fundamental to high-resolution mapping of the tumor microenvironment using whole-slide histopathology images. The growing interest in leveraging the power of deep learning to achieve state-of-the-art performance often comes at the cost of explainability, yet there is general consensus that explainability is critical for trustworthiness and widespread clinical adoption. Unfortunately, current explainability paradigms that rely on pixel saliency heatmaps or superpixel importance scores are not well-suited for nucleus classification. Techniques like Grad-CAM or LIME provide explanations that are indirect, qualitative and/or nonintuitive to pathologists. Results: In this article, we present techniques to enable scalable nuclear detection, segmentation and explainable classification. First, we show how modifications to the widely used Mask R-CNN architecture, including decoupling the detection and classification tasks, improves accuracy and enables learning from hybrid annotation datasets like NuCLS, which contain mixtures of bounding boxes and segmentation boundaries. Second, we introduce an explainability method called Decision Tree Approximation of Learned Embeddings (DTALE), which provides explanations for classification model behavior globally, as well as for individual nuclear predictions. DTALE explanations are simple, quantitative, and can flexibly use any measurable morphological features that make sense to practicing pathologists, without sacrificing model accuracy. Together, these techniques present a step toward realizing the promise of computational pathology in computer-aided diagnosis and discovery of morphologic biomarkers. © 2021 The Author(s) 2021. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.",,
"Amgad M., Atteya L.A., Hussein H., Mohammed K.H., Hafiz E., Elsebaie M.A.T., Alhusseiny A.M., Almoslemany M.A., Elmatboly A.M., Pappalardo P.A., Sakr R.A., Mobadersany P., Rachid A., Saad A.M., Alkashash A.M., Ruhban I.A., Alrefai A., Elgazar N.M., Abdulkarim A., Farag A.-A., Etman A., Elsaeed A.G., Alagha Y., Amer Y.A., Raslan A.M., Nadim M.K., Elsebaie M.A.T., Ayad A., Hanna L.E., Gadallah A., Elkady M., Drumheller B., Jaye D., Manthey D., Gutman D.A., Elfandy H., Cooper L.A.D.","NuCLS: A scalable crowdsourcing approach and dataset for nucleus classification and segmentation in breast cancer","10.1093/gigascience/giac037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130126960&doi=10.1093%2fgigascience%2fgiac037&partnerID=40&md5=0189be22644a3773bc8a9dc6c19d3cf3","Background: Deep learning enables accurate high-resolution mapping of cells and tissue structures that can serve as the foundation of interpretable machine-learning models for computational pathology. However, generating adequate labels for these structures is a critical barrier, given the time and effort required from pathologists. Results: This article describes a novel collaborative framework for engaging crowds of medical students and pathologists to produce quality labels for cell nuclei. We used this approach to produce the NuCLS dataset, containing >220,000 annotations of cell nuclei in breast cancers. This builds on prior work labeling tissue regions to produce an integrated tissue region- and cell-level annotation dataset for training that is the largest such resource for multi-scale analysis of breast cancer histology. This article presents data and analysis results for single and multi-rater annotations from both non-experts and pathologists. We present a novel workflow that uses algorithmic suggestions to collect accurate segmentation data without the need for laborious manual tracing of nuclei. Our results indicate that even noisy algorithmic suggestions do not adversely affect pathologist accuracy and can help non-experts improve annotation quality. We also present a new approach for inferring truth from multiple raters and show that non-experts can produce accurate annotations for visually distinctive classes. Conclusions: This study is the most extensive systematic exploration of the large-scale use of wisdom-of-the-crowd approaches to generate data for computational pathology applications. © 2022 The Author(s) 2022","breast cancer; crowdsourcing; deep learning; nucleus classification; nucleus segmentation","algorithm; Article; breast carcinoma; cancer growth; cancer localization; cancer tissue; cell nucleus; computer model; controlled study; crowdsourcing; disease classification; genetic association; histology; histopathology; human; human cell; human tissue; image segmentation; major clinical study; medical student; molecular genetics; observational study; pathologist; staining; workflow; breast tumor; cell nucleus; female; machine learning; pathology; procedures; Breast Neoplasms; Cell Nucleus; Crowdsourcing; Female; Humans; Machine Learning"
"Amich A., Eshete B.","EG-Booster: Explanation-Guided Booster of ML Evasion Attacks","10.1145/3508398.3511510","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130607226&doi=10.1145%2f3508398.3511510&partnerID=40&md5=362e66191a395e41b4e9bf0163d1b219","The widespread usage of machine learning (ML) in a myriad of domains has raised questions about its trustworthiness in high-stakes environments. Part of the quest for trustworthy ML is assessing robustness to test-time adversarial examples. Inline with the trustworthy ML goal, a useful input to potentially aid robustness evaluation is feature-based explanations of model predictions. In this paper, we present a novel approach, called EG-Booster, that leverages techniques from explainable ML to guide adversarial example crafting for improved robustness evaluation of ML models. The key insight in EG-Booster is the use of feature-based explanations of model predictions to guide adversarial example crafting by adding consequential perturbations (likely to result in model evasion) and avoiding non-consequential perturbations (unlikely to contribute to evasion). EG-Booster is agnostic to model architecture, threat model, and supports diverse distance metrics used in the literature. We evaluate EG-Booster using image classification benchmark datasets: MNIST and CIFAR10. Our findings suggest that EG-Booster significantly improves the evasion rate of state-of-the-art attacks while performing a smaller number of perturbations. Through extensive experiments that cover four white-box and three black-box attacks, we demonstrate the effectiveness of EG-Booster against two undefended neural networks trained on MNIST and CIFAR10, and an adversarially-trained ResNet model trained on CIFAR10. Furthermore, we introduce a stability assessment metric and evaluate the reliability of our explanation-based attack boosting approach by tracking the similarity between the model's predictions across multiple runs of EG-Booster. Our results over 10 separate runs suggest that EG-Booster's output is stable across distinct runs. Combined with state-of-the-art attacks, we hope EG-Booster will be used towards improved robustness assessment of ML models against evasion attacks. © 2022 ACM.","adversarial examples; explainable ml; ml evasion","Classification (of information); Forecasting; Adversarial example; Explainable ml; Feature-based; Machine learning models; Ml evasion; Model prediction; Modeling architecture; Robustness evaluation; State of the art; Test time; System stability"
"Amich A., Eshete B.","Explanation-Guided Diagnosis of Machine Learning Evasion Attacks","10.1007/978-3-030-90019-9_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120091306&doi=10.1007%2f978-3-030-90019-9_11&partnerID=40&md5=54545689b6cfd3d7bc9fe5dc0c3899d5","Machine Learning (ML) models are susceptible to evasion attacks. Evasion accuracy is typically assessed using aggregate evasion rate, and it is an open question whether aggregate evasion rate enables feature-level diagnosis on the effect of adversarial perturbations on evasive predictions. In this paper, we introduce a novel framework that harnesses explainable ML methods to guide high-fidelity assessment of ML evasion attacks. Our framework enables explanation-guided correlation analysis between pre-evasion perturbations and post-evasion explanations. Towards systematic assessment of ML evasion attacks, we propose and evaluate a novel suite of model-agnostic metrics for sample-level and dataset-level correlation analysis. Using malware and image classifiers, we conduct comprehensive evaluations across diverse model architectures and complementary feature representations. Our explanation-guided correlation analysis reveals correlation gaps between adversarial samples and the corresponding perturbations performed on them. Using a case study on explanation-guided evasion, we show the broader usage of our methodology for assessing robustness of ML models. © 2021, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.","Explainable machine learning; Machine learning evasion","Aggregates; Correlation methods; Malware; Correlation analysis; Explainable machine learning; Feature level; High-fidelity; Level correlation; Machine learning evasion; Machine learning methods; Machine learning models; Machine-learning; Systematic assessment; Machine learning"
"Amilpur S., Bhukya R.","EDeepSSP: Explainable deep neural networks for exact splice sites prediction","10.1142/S0219720020500249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090505120&doi=10.1142%2fS0219720020500249&partnerID=40&md5=445e47f288e3e117e8909cee0f433d2f","Splice site prediction is crucial for understanding underlying gene regulation, gene function for better genome annotation. Many computational methods exist for recognizing the splice sites. Although most of the methods achieve a competent performance, their interpretability remains challenging. Moreover, all traditional machine learning methods manually extract features, which is tedious job. To address these challenges, we propose a deep learning-based approach (EDeepSSP) that employs convolutional neural networks (CNNs) architecture for automatic feature extraction and effectively predicts splice sites. Our model, EDeepSSP, divulges the opaque nature of CNN by extracting significant motifs and explains why these motifs are vital for predicting splice sites. In this study, experiments have been conducted on six benchmark acceptors and donor datasets of humans, cress, and fly. The results show that EDeepSSP has outperformed many state-of-The-Art approaches. EDeepSSP achieves the highest area under the receiver operating characteristic curve (AUC-ROC) and area under the precision-recall curve (AUC-PR) of 99.32% and 99.26% on human donor datasets, respectively. We also analyze various filter activities, feature activations, and extracted significant motifs responsible for the splice site prediction. Further, we validate the learned motifs of our model against known motifs of JASPAR splice site database. © 2020 World Scientific Publishing Europe Ltd.","convolutional neural network; EDeepSSP; feature activation; genome annotation; motifs; Splice site","animal; Arabidopsis; biology; Drosophila melanogaster; genetic database; genetics; human; procedures; receiver operating characteristic; RNA splice site; Animals; Arabidopsis; Computational Biology; Data Visualization; Databases, Genetic; Deep Learning; Drosophila melanogaster; Humans; Neural Networks, Computer; RNA Splice Sites; ROC Curve"
"Amin K., Kapetanakis S., Polatidis N., Althoff K.-D., Dengel A.","DeepKAF: A Heterogeneous CBR Deep Learning Approach for NLP Prototyping","10.1109/INISTA49547.2020.9194679","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091997286&doi=10.1109%2fINISTA49547.2020.9194679&partnerID=40&md5=f10ab4ca73dbb65f5392fae0ee93addb","With widespread modernization, digitization and transformations of most of industries, Artificial Intelligence (AI) has become the key enabler in that modernization journey. AI offers substantial capabilities to solve new problems and optimise existing solutions specialising on specific problems and learning from different domains. AI solutions can be either explainable or black box ones with the latter being urged to improve since they cannot trust. Case-based Reasoning (CBR) is an explainable AI approach where solutions are provided along with relevant explanations in terms of why a solution was selected. However, CBR, like most other explainable approaches, has several limitations in terms of scalability, large data volumes, domain complexity, that reduce its ability to scale any CBR system in industrial applications. In this paper, we provide a heterogeneous CBR framework - DeepKAF where we combine CBR paradigm with Deep Learning architectures to solve complicated Natural Language Processing (NLP) problems (eg. mixed language and grammatically incorrect text).DeepKAF is built based on continuous research in the area of Deep Learning and CBR. DeepKAF has been implemented and used across different domains, test use cases and research models as an ensemble deep learning and CBR Architecture. © 2020 IEEE.","Case-based Reasoning; Deep Learning; Natural Language Processing","Case based reasoning; Intelligent systems; Natural language processing systems; Casebased reasonings (CBR); Different domains; Large data volumes; Learning approach; Learning architectures; NAtural language processing; Research models; Specific problems; Deep learning"
"Amin M., Shah B., Sharif A., Ali T., Kim K.-I., Anwar S.","Android malware detection through generative adversarial networks","10.1002/ett.3675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069892612&doi=10.1002%2fett.3675&partnerID=40&md5=9864c889326f4582315c9677290be317","Mobile and cell devices have empowered end users to tweak their cell phones more than ever and introduce applications just as we used to with personal computers. Android likewise portrays an uprise in mobile devices and personal digital assistants. It is an open-source versatile platform fueling incalculable hardware units, tablets, televisions, auto amusement frameworks, digital boxes, and so forth. In a generally shorter life cycle, Android also has additionally experienced a mammoth development in application malware. In this context, a toweringly large measure of strategies has been proposed in theory for the examination and detection of these harmful applications for the Android platform. These strategies attempt to both statically reverse engineer the application and elicit meaningful information as features manually or dynamically endeavor to quantify the runtime behavior of the application to identify malevolence. The overgrowing nature of Android malware has enormously debilitated the support of protective measures, which leaves the platforms such as Android feeble for novel and mysterious malware. Machine learning is being utilized for malware diagnosis in mobile phones as a common practice and in Android distinctively. It is important to specify here that these systems, however, utilize and adapt the learning-based techniques, yet the overhead of hand-created features limits ease of use of such methods in reality by an end user. As a solution to this issue, we mean to make utilization of deep learning–based algorithms as the fundamental arrangement for malware examination on Android. Deep learning turns up as another way of research that has bid the scientific community in the fields of vision, speech, and natural language processing. Of late, models set up on deep convolution networks outmatched techniques utilizing handmade descriptive features at various undertakings. Likewise, our proposed technique to cater malware detection is by design a deep learning model making use of generative adversarial networks, which is responsible to detect the Android malware via famous two-player game theory for a rock-paper-scissor problem. We have used three state-of-the-art datasets and augmented a large-scale dataset of opcodes extracted from the Android Package Kit bytecode and used in our experiments. Our technique achieves F1 score of 99% with a receiver operating characteristic of 99% on the bytecode dataset. This proves the usefulness of our technique and that it can generally be adopted in real life. © 2019 John Wiley & Sons, Ltd.",,"Android (operating system); Cellular telephones; Deep learning; Digital devices; Game theory; Large dataset; Learning systems; Life cycle; Malware; Natural language processing systems; Open systems; Personal digital assistants; Adversarial networks; Android platforms; Large-scale dataset; NAtural language processing; Protective measures; Receiver operating characteristics; Scientific community; Shorter life cycles; Mobile security"
"Amin M.G., Erol B.","Understanding deep neural networks performance for radar-based human motion recognition","10.1109/RADAR.2018.8378780","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049967092&doi=10.1109%2fRADAR.2018.8378780&partnerID=40&md5=e7ab72d1da47fbcebcb35c5139ec72c9","Deep neural networks have recently emerged as a promising tool for radar-based human motion recognition. Their nonlinear structure makes them successful in classifying large-scale datasets. However, due to their complexity, it is difficult to interpret the classification results and identify pixels with the biggest impact on the classification score. In this paper, we investigate recently proposed linear-wise relevance propagation (LRP) method which finds relevant pixels within the image. Based on this method, it is possible to recognize pixels which contain evidence for or against the prediction made by a classifier. Experimental results demonstrate that the LRP method can be successfully applied to detect regions within the radar images responsible for distinguishing human motions. © 2018 IEEE.","Deep learning; human motion recognition; radar; time-frequency domain","Classification (of information); Deep learning; Deep neural networks; Frequency domain analysis; Pixels; Radar; Radar imaging; Classification results; Human motion recognition; Human motions; Large-scale datasets; LRP methods; Nonlinear structure; Time frequency domain; Motion estimation"
"Amin R., Tang J., Ellejmi M., Kirby S., Abbass H.A.","An evolutionary goal-programming approach towards scenario design for air-traffic human-performance experiments","10.1109/CIVTS.2013.6612291","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886047896&doi=10.1109%2fCIVTS.2013.6612291&partnerID=40&md5=dd516c8e8fd0034cf3db577bf29ee911","Air traffic controllers are responsible for maintaining a safe and efficient flow of air traffic in controlled airspace. Many aspects of air traffic control are impacted by the performance of air traffic controllers such as separation assurance tasks. In order to design more advanced air traffic management systems, there is a need for more experiments to be conducted which evaluate the impact of human performance on the system. The design of scenarios that satisfy/meet specific traffic characteristics needed by the analyst is a daunting task. For example, it is often required to design scenarios for a specific sector that have a specific number of conflicts to evaluate human task load. To schedule the aircraft within the time specified for the experiment, and given all the constraints imposed by the route structure and airspace design parameters for the sector in question, are far from trivial problems. In this paper, an evolutionary goal programming approach has been presented which generates a set of scenarios for use in these experiments. The evolutionary goal programming system aimed to generate scenarios meeting the criteria of a set number of conflicts in each of four conflict angle groups. Differential evolution was employed in addition to three modified methods for the optimization of the problem. It was found that the three modified methods outperformed the standard method by producing a greater number of scenarios meeting the set criteria. © 2013 IEEE.",,"Air traffic controller; Air Traffic Management Systems; Differential Evolution; Goal programming; Human-performance; Route structures; Separation assurances; Traffic characteristics; Air traffic control; Artificial intelligence; Evolutionary algorithms; Experiments; Transportation; Design"
"Amin S.A., Ghosh K., Mondal D., Jha T., Gayen S.","Exploring indole derivatives as myeloid cell leukaemia-1 (Mcl-1) inhibitors with multi-QSAR approach: A novel hope in anti-cancer drug discovery","10.1039/d0nj03863f","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094209303&doi=10.1039%2fd0nj03863f&partnerID=40&md5=8c19ea4f5a35c948c038faad7bc8610d","In humans, the over-expression of Mcl-1 protein causes different cancers and it is also responsible for cancer resistance to different cytotoxic agents. Thus, discovery of potential inhibitors of Mcl-1 is very important and both the pharmaceutical industry and academia are looking at it in the quest for new anticancer drugs. In the present study, different molecular modelling techniques such as recursive partitioning, Bayesian classification, structural and physico-chemical interpretation analysis and pharmacophore mapping were employed in order to identify the crucial structural fingerprints important for the optimization of 143 indole derivatives as Mcl-1 inhibitors. These modelling studies emphasize that hydrophobic naphthyl rings, methyl-substituted 1H-pyrazole moiety, N(1)-tethered morpholinoethyl group, chloro substitutions at the 6th position of indole nucleus etc. are beneficial for Mcl-1 binding. Finally, statistically validated classical QSAR and machine learning-based models were also developed for screening and prediction of different indole derivatives as Mcl-1 inhibitors. The modelling analyses will help medicinal chemists to design potent Mcl-1 inhibitors in future. Thus, the present study was an attempt to speed up the anticancer drug discovery of indole-based Mcl-1 inhibitors. This journal is © The Royal Society of Chemistry and the Centre National de la Recherche Scientifique.",,"Chemical analysis; Computational chemistry; Diseases; Drug products; Molecular modeling; Regression analysis; Anticancer drug discovery; Bayesian classification; Indole derivatives; Modelling techniques; Pharmaceutical industry; Pharmacophore mapping; Potential inhibitors; Recursive Partitioning; Polycyclic aromatic hydrocarbons; antineoplastic agent; indole derivative; naphthalene; protein inhibitor; protein mcl 1; protein mcl 1 inhibitor; pyrazole; unclassified drug; Article; controlled study; drug design; drug potency; human; hydrophobicity; molecular model; pharmacophore; prediction; priority journal; protein binding; quantitative structure activity relation; recursive partitioning; substitution reaction"
"Amin S.A., Adhikari N., Jha T.","Diverse classes of HDAC8 inhibitors: In search of molecular fingerprints that regulate activity","10.4155/fmc-2018-0005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050526898&doi=10.4155%2ffmc-2018-0005&partnerID=40&md5=0a5bb7249d578858faa4f96726056c96","Aim: HDAC8 is one of the crucial enzymes involved in malignancy. Structural explorations of HDAC8 inhibitory activity and selectivity are required. Materials & methods: A mathematical framework was constructed to explore important molecular fragments responsible for HDAC8 inhibition. Bayesian classification models were developed on a large set of structurally diverse HDAC8 inhibitors. Results: This study helps to understand the structural importance of HDAC8 inhibitors. The hydrophobic aryl cap function is important for HDAC8 inhibition whereas benzamide moiety shows a negative impact on HDAC8 inhibition. Conclusion: This work validates our previously proposed structural features for better HDAC8 inhibition. The comparative learning between the statistical and intelligent methods will surely enrich future drug design aspects of HDAC8 inhibitors. © 2018 Newlands Press.","cancer; ECFP6; HDAC8 inhibitor; molecular fingerprints; naive Bayes classification model; QSAR; quantitative structure-Activity relationship","histone deacetylase 8; histone deacetylase 8 inhibitor; histone deacetylase inhibitor; unclassified drug; benzamide; benzamide derivative; HDAC8 protein, human; histone deacetylase; histone deacetylase inhibitor; repressor protein; Article; Bayes theorem; classifier; drug activity; drug design; drug structure; enzyme inhibition; machine learning; mathematical model; priority journal; algorithm; antagonists and inhibitors; chemistry; drug design; drug development; human; metabolism; molecular library; molecular model; pharmacology; procedures; quantitative structure activity relation; Algorithms; Bayes Theorem; Benzamides; Drug Design; Drug Discovery; Histone Deacetylase Inhibitors; Histone Deacetylases; Humans; Models, Molecular; Quantitative Structure-Activity Relationship; Repressor Proteins; Small Molecule Libraries"
"Aminan A.W., Zailani S.Z., Tajuddin S.N., Ramli A.N.M.","Isolation and cloning of sesquiterpene synthases (amgs3 and amgs4) and chalcone synthase (amchs) from aquilaria malaccensis responsible for agarwood formation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110152116&partnerID=40&md5=39182b857e873d792fb53403ce9ea0eb","Sesquiterpene and phenylethyl chromone, two types of agarwood marker compounds, have been extensively studied. However, genetic studies of agarwood (Aquilaria malaccensis) are still scarce. This study describes the isolation and cloning of sesquiterpene synthase genes (AmGS3 and AmGS4), and chalcone synthase gene (AmCHS) identified from A. malaccensis transcriptome data mining. The sizes of AmGS3, AmGS4, and AmCHS were 1162, 1466, and 1623 bp in length. The open reading frames (ORFs) of AmGS3, AmGS4, and AmCHS detected were 948, 1047, and 1185 bp, with a polypeptide length of 348, 315, and 394 amino acids. The full-length sequences of AmGS3, AmGS4, and AmCHS were successfully isolated from the infected stem of A. malaccensis, amplified via polymerase chain reaction (PCR), cloned into the pGEM-T Easy Vector, and transformed into prepared Escherichia coli DH5a competent cells. The sequencing result and BLASTn analysis revealed that the ORFs of AmGS3 and AmGS4 are highly homologous to putative delta-guaiene synthase from Aquilaria sinensis, with a similarity of 98.1% and 98.08% respectively, while the ORF of AmCHS is highly homologous to chalcone synthase from A. sinensis with a similarity of 99.24%. These results demonstrated the successful isolation of sesquiterpene synthase and chalcone synthase genes that may play important roles in forming agarwood sesquiterpene and phenylethyl chromone in A. malaccensis. © 2021 Malaysian Institute of Chemistry. All rights reserved.","Agarwood; Aquilaria malaccensis; Chalcone synthase; Sesquiterpene synthase",
"Amini A., Muggleton S.H., Lodhi H., Sternberg M.J.E.","A novel logic-based approach for quantitative toxicology prediction","10.1021/ci600223d","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250876925&doi=10.1021%2fci600223d&partnerID=40&md5=525e94e6a0931c3331edeac45c62b443","There is a pressing need for accurate in silico methods to predict the toxicity of molecules that are being introduced into the environment or are being developed into new pharmaceuticals. Predictive toxicology is in the realm of structure activity relationships (SAR), and many approaches have been used to derive such SAR. Previous work has shown that inductive logic programming (ILP) is a powerful approach that circumvents several major difficulties, such as molecular superposition, faced by some other SAR methods. The ILP approach reasons with chemical substructures within a relational framework and yields chemically understandable rules. Here, we report a general new approach, support vector inductive logic programming (SVILP), which extends the essentially qualitative ILP-based SAR to quantitative modeling. First, ILP is used to learn rules, the predictions of which are then used within a novel kernel to derive a support-vector generalization model. For a highly heterogeneous dataset of 576 molecules with known fathead minnow fish toxicity, the cross-validated correlation coefficients (Rcv2) from a chemical descriptor method (CHEM) and SVILP are 0.52 and 0.66, respectively. The ILP, CHEM, and SVILP approaches correctly predict 55, 58, and 73%, respectively, of toxic molecules. In a set of 165 unseen molecules, the R2 values from the commercial software TOPKAT and SVILP are 0.26 and 0.57, respectively. In all calculations, SVILP showed significant improvements in comparison with the other methods. The SVILP approach has a major advantage in that it uses ILP automatically and consistently to derive rules, mostly novel, describing fragments that are toxicity alerts. The SVILP is a general machine-learning approach and has the potential of tackling many problems relevant to chemoinformatics including in silico drug design. © 2007 American Chemical Society.",,"Computer simulation; Drug products; Logic programming; Molecular dynamics; Support vector machines; Toxicity; Chemoinformatics; Silico methods; Structure activity relationships (SAR); Formal logic; drug; algorithm; article; artificial intelligence; chemical structure; combinatorial chemistry; computer program; drug design; logic; methodology; structure activity relation; toxicology; Algorithms; Artificial Intelligence; Combinatorial Chemistry Techniques; Drug Design; Logic; Molecular Structure; Pharmaceutical Preparations; Software; Structure-Activity Relationship; Toxicology"
"Amini M., Bagheri A., Delen D.","Discovering injury severity risk factors in automobile crashes: A hybrid explainable AI framework for decision support","10.1016/j.ress.2022.108720","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134753162&doi=10.1016%2fj.ress.2022.108720&partnerID=40&md5=158d6244503c93ab39060bca398f675b","Millions of car crashes occur annually in the US, leaving tens of thousands of deaths and many more severe injuries. Thus, understanding the most impactful contributors to severe injuries in automobile crashes and mitigating their effects are of great importance in traffic safety improvement. This paper develops a hybrid framework involving predictive analytics, explainable AI, and heuristic optimization techniques to investigate and explain the injury severity risk factors in automobile crashes. First, our framework examines various machine learning models to identify the one with the best prediction performance as the base model. Then, it utilizes two popular state-of-the-art explainable AI techniques from the literature (i.e., leave-one-covariate-out and TreeExplainer) and our proposed explanation method based on the variable neighborhood search procedure to construe the importance of the variables. Finally, by applying an information fusion technique, our approach identifies a unified ranking list of the most important variables contributing to severe car crash injuries. Transportation safety planners and policymakers can use our findings to reduce the severity of car accidents, improve traffic safety, and save many lives. © 2022","Explainable AI; Information fusion; Injury severity risk factors; Machine learning; Traffic safety; Variable neighborhood search","Accident prevention; Accidents; Automobiles; Decision support systems; Machine learning; Optimization; Predictive analytics; Car crashes; Decision supports; Explainable AI; Injury severity; Injury severity risk factor; Machine-learning; Risk factors; Safety improvement; Traffic safety; Variable neighborhood search; Information fusion"
"Amini S., Ghaemmaghami S.","Towards Improving Robustness of Deep Neural Networks to Adversarial Perturbations","10.1109/TMM.2020.2969784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090159230&doi=10.1109%2fTMM.2020.2969784&partnerID=40&md5=a29172b02eaae46da2c18028e7b2ab40","Deep neural networks have presented superlative performance in many machine learning based perception and recognition tasks, where they have even outperformed human precision in some applications. However, it has been found that human perception system is much more robust to adversarial perturbation, as compared to these artificial networks. It has been shown that a deep architecture with a lower Lipschitz constant can generalize better and tolerate higher level of adversarial perturbation. Smooth regularization has been proposed to control the Lipschitz constant of a deep architecture and in this work, we show how a deep convolutional neural network (CNN), based on non-smooth regularization of convolution and fully connected layers, can present enhanced generalization and robustness to adversarial perturbation, simultaneously. We propose two non-smooth regularizers that present specific features for adversarial samples with different levels of signal-to-noise ratios. The regularizers build direct interconnections for the weight matrices in each layer, through which they control the Lipschitz constant of architecture and improve the consistency of input-output mapping of the network. This leads to more reliable and interpretable network mapping and reduces abrupt changes in the networks output. We develop an efficient algorithm to solve the non-smooth learning problems, which presents a gradual complexity addition property. Our simulation results over three benchmark datasets signify the superiority of the proposed formulations over previously reported methods for improving the robustness of deep architecture, towards human robustness to adversarial samples. © 1999-2012 IEEE.","Convolutional neural network; gradient descent; interpretable; proximal operator; regularizer; robust","Convolution; Convolutional neural networks; Deep learning; Mapping; Multilayer neural networks; Network architecture; Robustness (control systems); Signal to noise ratio; Artificial networks; Benchmark datasets; Deep architectures; Human perception systems; Input-output mapping; Learning problem; Lipschitz constant; Perception and recognition; Deep neural networks"
"Aminpour A., Ebrahimi M., Widjaja E.","Deep learning-based lesion segmentation in paediatric epilepsy","10.1117/12.2582144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103686472&doi=10.1117%2f12.2582144&partnerID=40&md5=5c269adc5ea8298e03128dfc3fa8c97e","In this research, our goal is to implement a Convolutional Neural Network (CNN) to segment Focal Cortical Dysplasia (FCD). FCD is a common lesion responsible for paediatric medically intractable focal epilepsy. MRI features of FCD can be subtle and may be missed by a radiologist. Recent advances in deep learning techniques in different fields have motivated us to develop a deep learning-based model to detect and segment the lesion responsible for FCD. We proposed a Fully Convolutional Network (FCN) for the task of FCD detection and localization. Our proposed model has four blocks of two convolutional layers followed by a pooling layer, as feature extraction part. Then, we have added three up-sampling blocks which include one convolutional layer and one up-sampling layer. The convolutional layers' kernels are 3×3 and we are utilizing 4x and 2x upsampling layers in the decoder part. We are using skip layers as well, to get a more refined up-sampled output. We are adding the respective down-sampled feature map from the encoder part. To train and evaluate the model leaveone-out technique has been utilized where one test subject is left out of training in each experiment. We have identified 13 out 13 healthy subjects as healthy. The model has identified the lesion in 15 out of 17 MR-positive FCD subjects with 73 percent lesion coverage. For MR-negative cases, 11 out of 13 subjects were identified with lesion coverage of 64 percent. Based on our experiments, FCN holds the potential to assist specialists in detecting and localizing the lesion. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Epilepsy; Focal Cortical Dysplasia (FCD); Fully Convolutional Network (FCN); Magnetic Resonance Imaging (MRI); Segmentation","Computer aided diagnosis; Convolution; Convolutional neural networks; Learning systems; Medical imaging; Neurology; Pediatrics; Signal sampling; Convolutional networks; Detection and localization; Focal cortical dysplasias; Focal epilepsy; Healthy subjects; Learning Based Models; Learning techniques; Lesion segmentations; Deep learning"
"Amir O., Doshi-Velez F., Sarne D.","Summarizing agent strategies","10.1007/s10458-019-09418-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069858323&doi=10.1007%2fs10458-019-09418-w&partnerID=40&md5=8fefbcb2adb2cdcb1b453fd84367088b","Intelligent agents and AI-based systems are becoming increasingly prevalent. They support people in different ways, such as providing users with advice, working with them to achieve goals or acting on users’ behalf. One key capability missing in such systems is the ability to present their users with an effective summary of their strategy and expected behaviors under different conditions and scenarios. This capability, which we see as complementary to those currently under development in the context of “interpretable machine learning” and “explainable AI”, is critical in various settings. In particular, it is likely to play a key role when a user needs to collaborate with an agent, when having to choose between different available agents to act on her behalf, or when requested to determine the level of autonomy to be granted to an agent or approve its strategy. In this paper, we pose the challenge of developing capabilities for strategy summarization, which is not addressed by current theories and methods in the field. We propose a conceptual framework for strategy summarization, which we envision as a collaborative process that involves both agents and people. Last, we suggest possible testbeds that could be used to evaluate progress in research on strategy summarization. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Explainable AI; Human–agent interaction; Strategy summarization","Intelligent agents; Agent interaction; Collaborative process; Conceptual frameworks; Current theories; Level of autonomies; Strategy summarization; User need; Autonomous agents"
"Amir O., Doshi-Velez F., Same D.","Agent strategy summarization: Blue sky ideas track",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054695158&partnerID=40&md5=04687fb701573134ec833f7e9a5e2ccc","Intelligent agents and Al-based systems are becoming increasingly prevalent. They support people in different ways, such as providing users with advice, working with them to achieve goals or acting on users' behalf. One key capability missing in such systems is the ability to present their users with an effective summary of their strategy and expected behaviors under different conditions and scenarios. This capability, which we see as complimentary to those currently under development in the context of ""interpretable machine learning"" and ""explainable AI"", is critical in various settings. In particular, it is likely to play a key role whenever a user needs to understand the strategy of an agent she is working along with, when having to choose between different available agents to act on her behalf, or when requested to determine the level of autonomy to be granted to the agent or approve its strategy. In this paper, we pose the challenge of developing capabilities for strategy summarization, which is not addressed by current theories and methods in the field. We propose a conceptual framework for strategy summarization, which we envision as a collaborative process that involves both agents and people. Last, we suggest possible testbeds that could be used to evaluate progress in research on strategy summarization. © 2018 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","Explainable AI; Strategy summarization","Intelligent agents; Learning systems; Multi agent systems; Collaborative process; Conceptual frameworks; Current theories; Level of autonomies; Strategy summarization; User need; Autonomous agents"
"Amiri S.S., Lee E.R., Hoque S.","Explainable artificial intelligence applied to household transportation energy consumption",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101627462&partnerID=40&md5=f1fe444474bcab5f1fa4fd59cccd8c88",[No abstract available],"Artificial neural network; Interpretable machine learning; LIME; Transportation energy use",
"Amirian M., Tuggener L., Chavarriaga R., Satyawan Y.P., Schilling F.-P., Schwenker F., Stadelmann T.","Two to Trust: AutoML for Safe Modelling and Interpretable Deep Learning for Robustness","10.1007/978-3-030-73959-1_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105964825&doi=10.1007%2f978-3-030-73959-1_23&partnerID=40&md5=6a34818efe7bab2dc8a75aed31b8380f","With great power comes great responsibility. The success of machine learning, especially deep learning, in research and practice has attracted a great deal of interest, which in turn necessitates increased trust. Sources of mistrust include matters of model genesis (“Is this really the appropriate model?”) and interpretability (“Why did the model come to this conclusion?”, “Is the model safe from being easily fooled by adversaries?”). In this paper, two partners for the trustworthiness tango are presented: recent advances and ideas, as well as practical applications in industry in (a) Automated machine learning (AutoML), a powerful tool to optimize deep neural network architectures and fine-tune hyperparameters, which promises to build models in a safer and more comprehensive way; (b) Interpretability of neural network outputs, which addresses the vital question regarding the reasoning behind model predictions and provides insights to improve robustness against adversarial attacks. © 2021, Springer Nature Switzerland AG.","Adversarial attacks; Automated Deep Learning (AutoDL)","Deep neural networks; Learning systems; Network architecture; Neural networks; Appropriate models; Automated machines; Hyperparameters; Interpretability; Model prediction; Deep learning"
"Amirian M., Schwenker F.","Radial Basis Function Networks for Convolutional Neural Networks to Learn Similarity Distance Metric and Improve Interpretability","10.1109/ACCESS.2020.3007337","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088630920&doi=10.1109%2fACCESS.2020.3007337&partnerID=40&md5=8067a6c710f06e0455b1fa7945cb013e","Radial basis function neural networks (RBFs) are prime candidates for pattern classification and regression and have been used extensively in classical machine learning applications. However, RBFs have not been integrated into contemporary deep learning research and computer vision using conventional convolutional neural networks (CNNs) due to their lack of adaptability with modern architectures. In this paper, we adapt RBF networks as a classifier on top of CNNs by modifying the training process and introducing a new activation function to train modern vision architectures end-to-end for image classification. The specific architecture of RBFs enables the learning of a similarity distance metric to compare and find similar and dissimilar images. Furthermore, we demonstrate that using an RBF classifier on top of any CNN architecture provides new human-interpretable insights about the decision-making process of the models. Finally, we successfully apply RBFs to a range of CNN architectures and evaluate the results on benchmark computer vision datasets. © 2013 IEEE.","CNN-RBFs; convolutional neural networks (CNNs); Radial basis function neural networks (RBFs); similarity distance metric; supervised learning; unsupervised learning","Computer architecture; Computer vision; Convolution; Decision making; Deep learning; Functions; Network architecture; Radial basis function networks; Decision making process; Interpretability; Machine learning applications; Modern architectures; New activation functions; Radial basis function neural networks; Similarity distance; Training process; Convolutional neural networks"
"Amit, Roy R., Tanwar R., Singh V.","Evaluating Models for Better Life Expectancy Prediction","10.1007/978-981-19-3391-2_30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137545951&doi=10.1007%2f978-981-19-3391-2_30&partnerID=40&md5=04bfb2a4f1aa5ccc78f73e58b92b22bc","Life Expectancy is the average number of years some person or group of people tends to live. Being able to predict life expectancy may help to deliver potential insights, e.g., the prosperity of a community in the future, and volatile environments, etc. Life Expectancy also emerged as a key factor to evaluate a governing body’s performance in improving the welfare of the population in general and improving health status in particular, whether it is a decision about increasing the expenditure in any sector by the state or for people to choose which country could provide the best conditions for life. In this paper, exploratory analysis is conducted (i) to outline the most important factors that are responsible for affecting life expectancy through feature selection and (ii) to predict the most suitable machine learning algorithm to deliver the objectives. The systematic analysis is performed over the generic dataset that fits with socio-economic and health records. Though data available for countries are very little and inconsistent. The proposed prediction model achieved the aimed accurate forecast with a reduced number of features, as predictions are satisfactory over a dataset with inconsistent and missing values. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Big data; Life expectancy; Pattern recognition; Predictive analysis","Forecasting; Learning algorithms; Machine learning; Pattern recognition; Average numbers; Condition; Evaluating models; Exploratory analysis; Governing bodies; Health status; Key factors; Life expectancies; Performance; Volatile environments; Big data"
"Ammar A., Elouedi Z., Lingras P.","Semantically enhanced clustering in retail using possibilistic K-modes","10.1007/978-3-319-11740-9_69","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908663758&doi=10.1007%2f978-3-319-11740-9_69&partnerID=40&md5=b57c1250be9c4f9b844394c989cf4368","Possibility theory can be used to translate numeric values into semantically more meaningful representation with the help of linguistic variables. The data mining applied to a dataset with linguistic variables can lead to results that are easily interpretable due to the inherent semantics in the representation. Moreover, the data mining algorithms based on these linguistic variables tend to orient themselves based on underlying semantics. This paper describes how to transform a realworld dataset consisting of numeric values using linguistic variables based on possibilistic variables. The transformed dataset is clustered using a recently proposed possibilistic k-modes algorithm. The resulting cluster profiles are semantically accessible with very little numerical analysis. © Springer International Publishing Switzerland 2014.","K-modes method; Possibility distribution; Possibility theory; Retail databases","k-Modes; Possibilistic; Possibility distributions; Possibility theory"
"Ammar M.A., Abdel-Latif M.S., Badran K.M., Hassan H.A.","A new Dataset of Wideband Radar Signals for Training Deep Neural Networks on Classification and Detection Tasks","10.1109/CSDE53843.2021.9718398","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127843197&doi=10.1109%2fCSDE53843.2021.9718398&partnerID=40&md5=75136659b0f0d97f86d07dcfd1d02e99","in the deep learning field, the availability of datasets is a very important requirement for developing deep neural network models and benchmarking. This paper introduces a new dataset of wideband radar signals (WBR-DS-1) that is essential for training, developing, and benchmarking deep neural network models for the classification and detection of wideband radar signals. Typical ESM receiver parameters, propagation channels, and environmental parameters are simulated to guarantee the dataset's usability. The Electronic Support Measures (ESM) systems are responsible for the interception and characterization of the different radar signals. In this paper, the ESM sensor is assumed to be a ground-based one. Two scenarios are proposed to describe the geometric relationship between the ground-based ESM sensor and both the airborne and ground-based radar systems. The air-to-ground scenario is corresponding to airborne radars in front of the ground-based ESM sensor while the groundto-ground scenario is corresponding to ground radars in front of ground-based ESM. One of the most important impairments that signals are subjected to during propagation is multipath fading. The multipath fading causes random variance in features and parameters of the radar signals. Both Rayleigh and Rician multipath channels with typical path losses and Doppler shifts are applied to simulate the environment. To verify that the dataset is suitable for training deep neural network models, a convolutional neural network (CNN) model has been trained and tested for classification of radar signals and detection of frequency modulated continuous wave (FMCW) radars. © IEEE 2022.","Deep learning; ESM; wideband radars","Backpropagation; Classification (of information); Convolution; Convolutional neural networks; Deep neural networks; Frequency modulation; Tracking radar; Classification tasks; Deep learning; Detection tasks; Electronic support measures; Ground based; Learning fields; Neural network model; Radar signals; Wide band radar; Wideband radar signals; Benchmarking"
"Ammar N., Shaban-Nejad A.","Explainable artificial intelligence recommendation system by leveraging the semantics of adverse childhood experiences: Proof-of-concept prototype development","10.2196/18752","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097463874&doi=10.2196%2f18752&partnerID=40&md5=9ef23f6a3d11660a8287945846b0bd4e","Background: The study of adverse childhood experiences and their consequences has emerged over the past 20 years. Although the conclusions from these studies are available, the same is not true of the data. Accordingly, it is a complex problem to build a training set and develop machine-learning models from these studies. Classic machine learning and artificial intelligence techniques cannot provide a full scientific understanding of the inner workings of the underlying models. This raises credibility issues due to the lack of transparency and generalizability. Explainable artificial intelligence is an emerging approach for promoting credibility, accountability, and trust in mission-critical areas such as medicine by combining machine-learning approaches with explanatory techniques that explicitly show what the decision criteria are and why (or how) they have been established. Hence, thinking about how machine learning could benefit from knowledge graphs that combine “common sense” knowledge as well as semantic reasoning and causality models is a potential solution to this problem. Objective: In this study, we aimed to leverage explainable artificial intelligence, and propose a proof-of-concept prototype for a knowledge-driven evidence-based recommendation system to improve mental health surveillance. Methods: We used concepts from an ontology that we have developed to build and train a question-answering agent using the Google DialogFlow engine. In addition to the question-answering agent, the initial prototype includes knowledge graph generation and recommendation components that leverage third-party graph technology. Results: To showcase the framework functionalities, we here present a prototype design and demonstrate the main features through four use case scenarios motivated by an initiative currently implemented at a children’s hospital in Memphis, Tennessee. Ongoing development of the prototype requires implementing an optimization algorithm of the recommendations, incorporating a privacy layer through a personal health library, and conducting a clinical trial to assess both usability and usefulness of the implementation. Conclusions: This semantic-driven explainable artificial intelligence prototype can enhance health care practitioners’ ability to provide explanations for the decisions they make. ©Nariman Ammar, Arash Shaban-Nejad.","Adverse childhood experiences; Digital assistant; Explainable artificial intelligence; Knowledge-based recommendation; Mental health surveillance; Semantic web",
"Ammour N., Alhichri H., Bazi Y., Alajlan N.","LwF-ECG: Learning-without-forgetting approach for electrocardiogram heartbeat classification based on memory with task selector","10.1016/j.compbiomed.2021.104807","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114332368&doi=10.1016%2fj.compbiomed.2021.104807&partnerID=40&md5=20e7479241405f2c0111c854738b3642","Most existing Electrocardiogram (ECG) classification methods assume that all arrhythmia classes are known during the training phase. In this paper, the problem of learning several successive tasks is addressed, where, in each new task, there are new arrhythmia classes to learn. Unfortunately, in machine learning it is known that when a model is retrained onto a new task, the machine tends to forget the old task. This is known in machine learning, as ‘the catastrophic forgetting phenomenon’. To this end, a learn-without-forgetting (LwF) approach to solve this problem is proposed. This novel deep LwF method for ECG heartbeat classification is the first work of its kind in the field. This proposed LwF approach consists of a deep learning architecture that includes the following important aspects: feature extraction module, classification layers for each learned task, memory module to store one prototype for each task, and a task selection module able to identify the most suitable task for each input sample. The feature extraction module constitutes another contribution of this work. It starts with a set of deep layers that convert an ECG heartbeat signal into an image, then the pre-trained DenseNet169 CNN takes the obtained image and extracts rich and powerful features that are effective inputs for the classifications layers of the model. Whenever a new task is to be learned, the network expands with a new classification layer having a Softmax activation function. The newly added layer is responsible for learning the classes of the new task. When the network is trained for the new task, the shared layers, as well as the output layers of the old tasks, are also fine-tuned using pseudo labels. This helps in retaining knowledge of old tasks. Finally, the task selector stores feature prototypes for each task, and using a distance matching network, is trained to select which task is more suitable to classify a new test sample. The whole network uses end-to-end learning to optimize one loss functions, which is a weighted combination of the loss functions of the different network modules. The proposed model was tested on three common ECG datasets, namely the MIT-BIH, INCART, and SVDB datasets. The results obtained demonstrate the success of the proposed method in learning, without forgetting, successive ECG heartbeat classification tasks. © 2021 The Author(s)","Catastrophic forgetting; Deep neural networks; Electrocardiogram classification; Learning-without-forgetting continual learning","Deep neural networks; Diseases; Extraction; Feature extraction; Catastrophic forgetting; Classification methods; Electrocardiogram classification; Features extraction; Heartbeat classifications; Learn+; Learning-without-forgetting continual learning; Loss functions; Machine-learning; Neural-networks; Electrocardiography; Article; body weight; convolutional neural network; deep learning; deep neural network; electrocardiogram; feature extraction; heart beat; information processing; loss of function mutation; electrocardiography; heart arrhythmia; heart rate; human; machine learning; Arrhythmias, Cardiac; Electrocardiography; Heart Rate; Humans; Machine Learning; Neural Networks, Computer"
"Amores V.J., Montáns F.J., Cueto E., Chinesta F.","Crossing Scales: Data-Driven Determination of the Micro-scale Behavior of Polymers From Non-homogeneous Tests at the Continuum-Scale","10.3389/fmats.2022.879614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132403555&doi=10.3389%2ffmats.2022.879614&partnerID=40&md5=a68b5b8f56c7cad3acc5cd390b23b336","We propose an efficient method to determine the micro-structural entropic behavior of polymer chains directly from a sufficiently rich non-homogeneous experiment at the continuum scale. The procedure is developed in 2 stages: First, a Macro-Micro-Macro approach; second, a finite element method. Thus, we no longer require the typical stress-strain curves from standard homogeneous tests, but we use instead the applied/reaction forces and the displacement field obtained, for example, from Digital Image Correlation. The approach is based on the P-spline local approximation of the constituents behavior at the micro-scale (a priori unknown). The sought spline vertices determining the polymer behavior are first pushed up from the micro-scale to the integration point of the finite element, and then from the integration point to the element forces. The polymer chain behavior is then obtained immediately by solving a linear system of equations which results from a least squares minimization error, resulting in an inverse problem which crosses material scales. The result is physically interpretable and directly linked to the micro-structure of the material, and the resulting polymer behavior may be employed in any other finite element simulation. We give some demonstrative examples (academic and from actual polymers) in which we demonstrate that we are capable of recovering “unknown” analytical models and spline-based constitutive behavior previously obtained from homogeneous tests. Copyright © 2022 Amores, Montáns, Cueto and Chinesta.","data-driven modeling; digital image correlation; hyperelasticity; machine learning; polymers; splines","E-learning; Finite element method; Image correlation; Inverse problems; Linear systems; Splines; Strain measurement; Stress-strain curves; Data driven; Data-driven model; Digital image correlations; Hyper-elasticity; Integration points; Machine-learning; Macro micro; Micro-structural; Non-homogeneous; Polymer chains; Machine learning"
"Amorim J.P., Domingues I., Abreu P.H., Santos J.","Interpreting deep learning models for ordinal problems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055678955&partnerID=40&md5=b1ab527a404acd2735cd535e21579cf7","Machine learning algorithms have evolved by exchanging simplicity and interpretability for accuracy, which prevents their adoption in critical tasks such as healthcare. Progress can be made by improving interpretability of complex models while preserving performance. This work introduces an extension of interpretable mimic learning which teaches in-terpretable models to mimic predictions of complex deep neural networks, not only on binary problems but also in ordinal settings. The results show that the mimic models have comparative performance to Deep Neural Network models, with the advantage of being interpretable. © ESANN 2018 - Proceedings, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning.",,"Complex networks; Deep neural networks; Learning algorithms; Neural networks; Binary problems; Comparative performance; Complex model; Critical tasks; Interpretability; Learning models; Neural network model; Machine learning"
"Amoroso N., Pomarico D., Fanizzi A., Didonna V., Giotta F., La Forgia D., Latorre A., Monaco A., Pantaleo E., Petruzzellis N., Tamborra P., Zito A., Lorusso V., Bellotti R., Massafra R.","A roadmap towards breast cancer therapies supported by explainable artificial intelligence","10.3390/app11114881","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107528780&doi=10.3390%2fapp11114881&partnerID=40&md5=37d8f4e1301d75f27d0ae708cfdf79da","In recent years personalized medicine reached an increasing importance, especially in the design of oncological therapies. In particular, the development of patients’ profiling strategies suggests the possibility of promising rewards. In this work, we present an explainable artificial intelligence (XAI) framework based on an adaptive dimensional reduction which (i) outlines the most important clinical features for oncological patients’ profiling and (ii), based on these features, determines the profile, i.e., the cluster a patient belongs to. For these purposes, we collected a cohort of 267 breast cancer patients. The adopted dimensional reduction method determines the relevant subspace where distances among patients are used by a hierarchical clustering procedure to identify the corresponding optimal categories. Our results demonstrate how the molecular subtype is the most important feature for clustering. Then, we assessed the robustness of current therapies and guidelines; our findings show a striking correspondence between available patients’ profiles determined in an unsupervised way and either molecular subtypes or therapies chosen according to guidelines, which guarantees the interpretability characterizing explainable approaches to machine learning techniques. Accordingly, our work suggests the possibility to design data-driven therapies to emphasize the differences observed among the patients. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Breast cancer; Cluster analysis; Explainable artificial intelligence; Molecular subtype; Relevant features",
"Amparore E., Perotti A., Bajardi P.","To trust or not to trust an explanation: using LEAF to evaluate local linear XAI methods","10.7717/peerj-cs.479","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109682532&doi=10.7717%2fpeerj-cs.479&partnerID=40&md5=1d8edaf66cbc0f22be2e2c8088af735f","The main objective of eXplainable Artificial Intelligence (XAI) is to provide effective explanations for black-box classifiers. The existing literature lists many desirable properties for explanations to be useful, but there is a scarce consensus on how to quantitatively evaluate explanations in practice. Moreover, explanations are typically used only to inspect black-box models, and the proactive use of explanations as a decision support is generally overlooked. Among the many approaches to XAI, a widely adopted paradigm is Local Linear Explanations—with LIME and SHAP emerging as state-of-the-art methods. We show that these methods are plagued by many defects including unstable explanations, divergence of actual implementations from the promised theoretical properties, and explanations for the wrong label. This highlights the need to have standard and unbiased evaluation procedures for Local Linear Explanations in the XAI field. In this paper we address the problem of identifying a clear and unambiguous set of metrics for the evaluation of Local Linear Explanations. This set includes both existing and novel metrics defined specifically for this class of explanations. All metrics have been included in an open Python framework, named LEAF. The purpose of LEAF is to provide a reference for end users to evaluate explanations in a standardised and unbiased way, and to guide researchers towards developing improved explainable techniques. © 2021 Amparore et al. All Rights Reserved.","eXplainable AI; LIME; Local linear explanation; Machine Learning Auditing; SHAP","Decision support systems; Lime; Black boxes; Black-box model; Decision supports; End users; Local linear; State-of-the-art methods; Artificial intelligence"
"Ampe C., Libbrecht J., Van Troys M.","β-actin knock-out mouse embryonic fibroblasts show increased expression of LIM-, CH-, EFh-domain containing proteins with predicted common upstream regulators","10.1002/cm.21147","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888205592&doi=10.1002%2fcm.21147&partnerID=40&md5=6a68e56bb606a8492738b43a82306102","β-actin depletion from mouse embryonic fibroblasts results in an altered transcriptional response rendering these cells a myofibroblast like phenotype. The proteins and upstream regulatory factors responsible for this acquired phenotype, with prominent focal adhesions and stress fibres, are unknown. Data-mining of the changed proteome revealed that actin binding proteins associated with stress fiber or focal adhesion formation are overexpressed in the β-actin knock-out cells and that many of these contain CH-, LIM- or EFh- domains. Furthermore in silico analysis predicts potential common upstream regulators that may, at least partly, coordinate the altered transcriptional response. © 2013 Wiley Periodicals, Inc.","Actin cytoskeleton; CH-domain; LIM-domain; Stress fiber; Transcription factor","actin binding protein; beta actin; calponin; calponin homology; LIM protein; proteome; unclassified drug; animal cell; article; data mining; embryo; fibroblast; focal adhesion; gene overexpression; knockout mouse; mouse; nonhuman; phenotype; priority journal; protein expression; protein motif; stress fiber; actin cytoskeleton; CH-domain; LIM-domain; stress fiber; transcription factor; Actin Cytoskeleton; Actins; Animals; Cytoskeletal Proteins; Embryo, Mammalian; Fibroblasts; LIM Domain Proteins; Mice; Mice, Knockout; Protein Structure, Tertiary; Proteome; Proteomics; Regulatory Sequences, Nucleic Acid; Software; Stress Fibers"
"Amram M., Dunn J., Zhuo Y.D.","Optimal policy trees","10.1007/s10994-022-06128-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126131761&doi=10.1007%2fs10994-022-06128-5&partnerID=40&md5=d80efdcca3664457a17e487bc904ca86","We propose an approach for learning optimal tree-based prescription policies directly from data, combining methods for counterfactual estimation from the causal inference literature with recent advances in training globally-optimal decision trees. The resulting method, Optimal Policy Trees, yields interpretable prescription policies, is highly scalable, and handles both discrete and continuous treatments. We conduct extensive experiments on both synthetic and real-world datasets and demonstrate that these trees offer best-in-class performance across a wide variety of problems. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.","Decision trees; Machine learning; Prescriptive decision making","Decision trees; Causal inferences; Combining method; Continuous treatments; Counterfactuals; Decisions makings; Optimal decisions; Optimal policies; Prescriptive decision making; Real-world datasets; Tree-based; Machine learning"
"Amrane M., Oukid S., Ensari T.","MOLECULES GENERATION TO INHIBIT COVID-19 DISEASE USING ENCODER-DECODER LSTM ARCHITECTURE AND PCA PROPERTIES","10.1142/S0219519422500725","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139082787&doi=10.1142%2fS0219519422500725&partnerID=40&md5=7e133c9d63af1b5026de540888d54640","COVID-19 has become the world's worst pandemic and has claimed over six million lives as of March 2022. The virus is now in alongside cancer as one of the most common causes of death. Likewise, there is no definitive or unique treatment for COVID-19 outside of a selected few drugs approved by the Food and Drug Administration (FDA). While Artificial Intelligence (AI) can be used to generate molecules that target Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), the virus responsible for COVID-19, such molecules are novel and do not yet exist in the market. With the emergence and availability of several drug datasets related to COVID-19 (tests, images, graphs, and ChEMBLs), recent works based on Deep Learning (DL) techniques have been employed to generate molecules and check the effectiveness of existing molecules on COVID-19. In our study, we investigated the benefits of an Encoder-Decoder (ED) architecture based on Long Short-Term Memory (LSTM) cells. As a result, the molecules were converted into a vector during the encoding phase, which was then decoded back into SMILES molecules strings. We propose an approach to incorporate four features of Principal Components Analysis (PCA) with Encoder-Decoder Long Short-Term Memory (ED-LSTM) for regularization, which means that, instead of avoiding linear mapping, we assumed that the data could be linearly separable. We concluded that ED-LSTM with unit norm constraint has the best reconstruction accuracy in the context of generating molecules. The resulting dataset was used with the aid of virtual screening and convolutional neural networks to check the drugs that have the best binding affinity with SARS-CoV-2. We achieved an accuracy of 87.35% on the test set. © 2022 World Scientific Publishing Company.","Auto encoder; COVID-19; deep learning; drug discovery; LSTM; molecules generation; SMILES","Binding energy; Brain; Convolutional neural networks; Coronavirus; Decoding; Diagnosis; Long short-term memory; Memory architecture; Molecules; Network architecture; Principal component analysis; Quality control; Signal encoding; Auto encoders; Causes of death; Deep learning; Drug discovery; Encoder-decoder; Food and Drug Administration; Molecule generation; Principal-component analysis; Property; SMILES; COVID-19"
"Amrani G., Adadi A., Berrada M., Souirti Z., Boujraf S.","EEG signal analysis using deep learning: A systematic literature review","10.1109/ICDS53782.2021.9626707","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123480267&doi=10.1109%2fICDS53782.2021.9626707&partnerID=40&md5=f9651ff4bfdec708724ef5686899cf7a","Objective: Electroencephalography (EEG) is very crucial for understanding the dynamic healthy and pathological complex processes in the brain. However, the manual analysis of the EEG signal is very complex, time-consuming, and depends on the expertise and experience of the users. Hence, nowadays research is conducted on automated EEG signal analysis using artificial intelligence and computer-aided technologies. This would allow fast and highly accurate results. The goal of this paper is to provide an extensive review of the EEG signal analysis using deep learning (DL).Methods: This systematic literature review of EEG processing using Deep Learning (DL) was achieved on Web of Science, PubMed, and Science Direct databases, resulting in 403 identified papers. All collected studies were analyzed based on main disorders studied, type of tasks performed, data source, stages of analysis, and DL architecture.Results: DL in EEG processing is promising in various research applications. It covered the common neurological disorders diagnosis such as epilepsy, movement disorder, depression, schizophrenia, autism, alcohol use, attention, memory, sleep, pain, etc. The main tasks covered by the included studies are detection and classification. The average range of data sources utilized by the included studies is 127 subjects with an EEG recording a total duration of 458 hours. In fact, we identified the use of a plethora of DL architecture for EEG analysis. 57% of papers used Convolutional Neural Networks (CNNs), whereas Recurrent Neural Networks (RNNs) were the architecture choice of about 12% of papers. Combinations of CNNs and Long Short-Term Memory (LSTM) were used in 13% of studies. Generative Adversarial Networks (GAN) and Autoencoder (AEs) were used in 5% and 4% of papers respectively. Restricted Boltzmann Machine (RBMs), Deep Belief Networks (DBNs), and other hybrid architectures appeared in 1% of studies.Conclusion: This systematic review showed that DL is a powerful tool to process, analyze, and interpret EEG data without requiring extraction steps. These intelligent models can allow self-learning from the training data. On the other hand, DL models need a lot of data to learn, while suffering from a lack of confidence due to their black-box nature. Hence, studies on transfer learning and Explainable Artificial Intelligence (XAI) could help in solving such issues. Big Data, Cloud Computing, the Internet of Things (IoT), and closed-loop technology can also help DL-based systems in achieving fast, and accurate processing of EEG recordings. © 2021 IEEE.","Deep learning; EEG; Electroencephalography; Machine Learning; Systematic Literature Review","Biomedical signal processing; Brain; Complex networks; Computer aided analysis; Convolutional neural networks; Electroencephalography; Electrophysiology; Internet of things; Long short-term memory; Memory architecture; Network architecture; Neuroimaging; Neurology; Signal analysis; Complex Processes; Computer-aided technologies; Convolutional neural network; Data-source; Deep learning; Learning architectures; Machine-learning; Manual analysis; Signals analysis; Systematic literature review; Diseases"
"Amraoui H., Mhamdi F., Elloumi M.","Fast exhaustive search algorithm for discovering relevant association rules","10.18293/SEKE2019-157","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071393384&doi=10.18293%2fSEKE2019-157&partnerID=40&md5=491c25bffbd455680e3eb244798e75b3","Association Rules Mining (ARM) is one of the most important tasks of Data mining. The purpose of ARM is to discover relationships having an interest between attributes/patterns stored in very large databases. Nowadays several efficient algorithms have been developed by the researchers for the discovery of relevant Association Rules (ARs). These latter are responsible for decision making in several domains, such as medicine, finance, marketing and many other fields. In this paper, we propose a new algorithm based on exhaustive search to find relevant AR to make the decision and to predict the chance of occurring the Diabetes Mellitus (DM). We develop an algorithm to mine data in less time and less complexity without losing information. Finally, we test our approach using a real database to evaluate the efficiency of our algorithm compared to Apriori algorithm. © 2019 Knowledge Systems Institute Graduate School. All rights reserved.","Association rules; Confidence; Fast exhaustive search algorithm; Fitness; Support","Association rules; Data mining; Decision making; Knowledge engineering; Learning algorithms; Supports; Apriori algorithms; Association rules mining; Confidence; Diabetes mellitus; Exhaustive search algorithms; Fitness; Real database; Very large database; Software engineering"
"Amri E., Dardouillet P., Benoit A., Courteille H., Bolon P., Dubucq D., Credoz A.","Offshore Oil Slick Detection: From Photo-Interpreter to Explainable Multi-Modal Deep Learning Models Using SAR Images and Contextual Data","10.3390/rs14153565","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137106783&doi=10.3390%2frs14153565&partnerID=40&md5=69a1b4a3f8e08cd04b46065debea0cc3","Ocean surface monitoring, emphasizing oil slick detection, has become essential due to its importance for oil exploration and ecosystem risk prevention. Automation is now mandatory since the manual annotation process of oil by photo-interpreters is time-consuming and cannot process the data collected continuously by the available spaceborne sensors. Studies on automatic detection methods mainly focus on Synthetic Aperture Radar (SAR) data exclusively to detect anthropogenic (spills) or natural (seeps) oil slicks, all using limited datasets. The main goal is to maximize the detection of oil slicks of both natures while being robust to other phenomena that generate false alarms, called “lookalikes”. To this end, this paper presents the automation of offshore oil slick detection on an extensive database of real and recent oil slick monitoring scenarios, including both types of slicks. It relies on slick annotations performed by expert photo-interpreters on Sentinel-1 SAR data over four years and three areas worldwide. In addition, contextual data such as wind estimates and infrastructure positions are included in the database as they are relevant data for oil detection. The contributions of this paper are: (i) A comparative study of deep learning approaches using SAR data. A semantic and instance segmentation analysis via FC-DenseNet and Mask R-CNN, respectively. (ii) A proposal for Fuse-FC-DenseNet, an extension of FC-DenseNet that fuses heterogeneous SAR and wind speed data for enhanced oil slick segmentation. (iii) An improved set of evaluation metrics dedicated to the task that considers contextual information. (iv) A visual explanation of deep learning predictions based on the SHapley Additive exPlanation (SHAP) method adapted to semantic segmentation. The proposed approach yields a detection performance of up to (Formula presented.) of good detection with a false alarm reduction ranging from (Formula presented.) to (Formula presented.) compared to mono-modal models. These results provide new solutions to improve the detection of natural and anthropogenic oil slicks by providing tools that allow photo-interpreters to work more efficiently on a wide range of marine surfaces to be monitored worldwide. Such a tool will accelerate the oil slick detection task to keep up with the continuous sensor acquisition. This upstream work will allow us to study its possible integration into an industrial production pipeline. In addition, a prediction explanation is proposed, which can be integrated as a step to identify the appropriate methodology for presenting the predictions to the experts and understanding the obtained predictions and their sensitivity to contextual information. Thus it helps them to optimize their way of working. © 2022 by the authors.","AI explanation; data fusion; deep learning; meteorological data; offshore detection; oil slicks; SAR images","Data fusion; Deep learning; Errors; Image enhancement; Learning systems; Offshore oil well production; Radar imaging; Semantics; Wind; AI explanation; Deep learning; Meteorological data; Offshore detection; Offshore oil; Offshores; Oil-slick detection; Oils slick; Radar data; Synthetic aperture radar images; Synthetic aperture radar"
"Amrollahi F., Shashikumar S.P., Meier A., Ohno-Machado L., Nemati S., Wardi G.","Inclusion of social determinants of health improves sepsis readmission prediction models","10.1093/jamia/ocac060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132453482&doi=10.1093%2fjamia%2focac060&partnerID=40&md5=ed01a085324be40791b948459befd7ef","Objective: Sepsis has a high rate of 30-day unplanned readmissions. Predictive modeling has been suggested as a tool to identify high-risk patients. However, existing sepsis readmission models have low predictive value and most predictive factors in such models are not actionable. Materials and Methods: Data from patients enrolled in the AllofUs Research Program cohort from 35 hospitals were used to develop a multicenter validated sepsis-related unplanned readmission model that incorporates clinical and social determinants of health (SDH) to predict 30-day unplanned readmissions. Sepsis cases were identified using concepts represented in the Observational Medical Outcomes Partnership. The dataset included over 60 clinical/laboratory features and over 100 SDH features. Results: Incorporation of SDH factors into our model of clinical and demographic features improves model area under the receiver operating characteristic curve (AUC) significantly (from 0.75 to 0.80; P <. 001). Model-Agnostic interpretability techniques revealed demographics, economic stability, and delay in getting medical care as important SDH predictive features of unplanned hospital readmissions. Discussion: This work represents one of the largest studies of sepsis readmissions using objective clinical data to date (8935 septic index encounters). SDH are important to determine which sepsis patients are more likely to have an unplanned 30-day readmission. The AllofUS dataset provides granular data from a diverse set of individuals, making this model potentially more generalizable than prior models. Conclusion: Use of SDH improves predictive performance of a model to identify which sepsis patients are at high risk of an unplanned 30-day readmission. © The Author(s) 2022. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.","interpretable; machine learning; OMOP; readmission; sepsis","adult; Article; Charlson Comorbidity Index; clinical feature; cohort analysis; controlled study; demography; economic stability; female; hospital readmission; hospitalization; human; infectious complication; intensive care unit; machine learning; major clinical study; male; multicenter study (topic); observational study; patient selection; prediction; predictive value; retrospective study; sepsis; social determinants of health; clinical trial; multicenter study; risk factor; social determinants of health; statistical model; Humans; Logistic Models; Patient Readmission; Retrospective Studies; Risk Factors; Sepsis; Social Determinants of Health"
"Amyrotos C., Andreou P., Germanakos P.","Adaptive Business Data Visualizations and Exploration: A Human-centred Perspective",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104413421&partnerID=40&md5=3bdde5a5049562adf4123a49dca097e2","Today's business environments are characterized by an indisputable growth in the volume, complexity and multivariate nature of business processes, data structures and sources. For the business end-users, this is many times an overwhelming and demotivating experience when interacting with rich business data visualizations and scenarios. As they need to explore demanding use cases, create fast an understanding and make informed decisions so to meet their business goals. This position paper addresses this challenge by introducing a human-centred model that consists of four main dimensions: User, Visualizations, Data, and Tasks, and which is maintained at the core of an adaptive data analytics platform in the business domain. The aim is two-fold: To provide (a) best-fit representation of data for the unique end-users, and (b) personalized and transparent path of exploration towards accomplishing purposeful end-to-end business activities. Thus, enabling explainable and intuitive interactions for accurate decision making and problem solving-saving time and costs © 2021 Copyright c 2021 for this paper by its authors.","Adaptation; Artificial intelligence; Business analytics; Data visualizations; Human factors; Personalization; User modelling","Data Analytics; Decision making; Visualization; Business activities; Business domain; Business environments; Business goals; Business Process; Informed decision; Intuitive interaction; Position papers; User interfaces"
"An H., Shin H.-G., Ji S., Jung W., Oh S., Shin D., Park J., Lee J.","DeepResp: Deep learning solution for respiration-induced B0 fluctuation artifacts in multi-slice GRE","10.1016/j.neuroimage.2020.117432","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093650890&doi=10.1016%2fj.neuroimage.2020.117432&partnerID=40&md5=d8db8501728abe42e5238ee1d2e93b05","Respiration-induced B0 fluctuation corrupts MRI images by inducing phase errors in k-space. A few approaches such as navigator have been proposed to correct for the artifacts at the expense of sequence modification. In this study, a new deep learning method, which is referred to as DeepResp, is proposed for reducing the respiration-artifacts in multi-slice gradient echo (GRE) images. DeepResp is designed to extract the respiration-induced phase errors from a complex image using deep neural networks. Then, the network-generated phase errors are applied to the k-space data, creating an artifact-corrected image. For network training, the computer-simulated images were generated using artifact-free images and respiration data. When evaluated, both simulated images and in-vivo images of two different breathing conditions (deep breathing and natural breathing) show improvements (simulation: normalized root-mean-square error (NRMSE) from 7.8 ± 5.2% to 1.3 ± 0.6%; structural similarity (SSIM) from 0.88 ± 0.08 to 0.99 ± 0.01; ghost-to-signal-ratio (GSR) from 7.9 ± 7.2% to 0.6 ± 0.6%; deep breathing: NRMSE from 13.9 ± 4.6% to 5.8 ± 1.4%; SSIM from 0.86 ± 0.03 to 0.95 ± 0.01; GSR 20.2 ± 10.2% to 5.7 ± 2.3%; natural breathing: NRMSE from 5.2 ± 3.3% to 4.0 ± 2.5%; SSIM from 0.94 ± 0.04 to 0.97 ± 0.02; GSR 5.7 ± 5.0% to 2.8 ± 1.1%). Our approach does not require any modification of the sequence or additional hardware, and may therefore find useful applications. Furthermore, the deep neural networks extract respiration-induced phase errors, which is more interpretable and reliable than results of end-to-end trained networks. © 2020","Deep learning; Deep neural network; Phase error in GRE; Respiration-induced B0 fluctuations","analytical error; Article; artifact; artificial neural network; breathing; computer simulation; controlled study; deep learning; image analysis; image processing; in vivo study; priority journal; brain; diagnostic imaging; human; image processing; nuclear magnetic resonance imaging; procedures; Artifacts; Brain; Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Respiration"
"An J., Joe I.","Attention Map-Guided Visual Explanations for Deep Neural Networks","10.3390/app12083846","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128847089&doi=10.3390%2fapp12083846&partnerID=40&md5=ba386977e63f85d79e339fff07784411","Deep neural network models perform well in a variety of domains, such as computer vision, recommender systems, natural language processing, and defect detection. In contrast, in areas such as healthcare, finance, and defense, deep neural network models, due to their lack of explainability, are not trusted by users. In this paper, we focus on attention-map-guided visual explanations for deep neural networks. We employ an attention mechanism to find the most important region of an input image. The Grad-CAM method is used to extract the feature map for deep neural networks, and then the attention mechanism is used to extract the high-level attention maps. The attention map, which highlights the important region in the image for the target class, can be seen as a visual explanation of a deep neural network. We evaluate our method using two common metrics: average drop and percentage increase. For a more effective experiment, we also propose a new metric to evaluate our method. The experiments were carried out to show that the proposed method works better than the state-of-the-art explainable artificial intelligence method. Our approach can provide a lower average drop and higher percent increase when compared to other methods and find a more explanatory region, especially in the first twenty percent region of the input image. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","attention mechanism; explainable artificial intelligence; visual explanation",
"An N., Shi X., Zhang Y., Lv N., Feng L., Di X., Han N., Wang G., Cheng S., Zhang K.","Discovery of a novel immune gene signature with profound prognostic value in colorectal cancer: A model of cooperativity disorientation created in the process from development to cancer","10.1371/journal.pone.0137171","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943327668&doi=10.1371%2fjournal.pone.0137171&partnerID=40&md5=c47eaa34d849ea11f296bb9ccdb5d0ed","Immune response-related genes play a major role in colorectal carcinogenesis by mediating inflammation or immune-surveillance evasion. Although remarkable progress has been made to investigate the underlying mechanism, the understanding of the complicated carcinogenesis process was enormously hindered by large-scale tumor heterogeneity. Development and carcinogenesis share striking similarities in their cellular behavior and underlying molecular mechanisms. The association between embryonic development and carcinogenesis makes embryonic development a viable reference model for studying cancer thereby circumventing the potentially misleading complexity of tumor heterogeneity. Here we proposed that the immune genes, responsible for intra-immune cooperativity disorientation (defined in this study as disruption of developmental expression correlation patterns during carcinogenesis), probably contain untapped prognostic resource of colorectal cancer. In this study, we determined the mRNA expression profile of 137 human biopsy samples, including samples from different stages of human colonic development, colorectal precancerous progression and colorectal cancer samples, among which 60 were also used to generate miRNA expression profile. We originally established Spearman correlation transition model to quantify the cooperativity disorientation associated with the transition from normal to precancerous to cancer tissue, in conjunction with miRNA-mRNA regulatory network and machine learning algorithm to identify genes with prognostic value. Finally, a 12-gene signature was extracted, whose prognostic value was evaluated using Kaplan-Meier survival analysis in five independent datasets. Using the log-rank test, the 12-gene signature was closely related to overall survival in four datasets (GSE17536, n = 177, p = 0.0054; GSE17537, n = 55, p = 0.0039; GSE39582, n = 562, p = 0.13; GSE39084, n = 70, p = 0.11), and significantly associated with disease-free survival in four datasets (GSE17536, n = 177, p = 0.0018; GSE17537, n = 55, p = 0.016; GSE39582, n = 557, p = 4.4e-05; GSE14333, n = 226, p = 0.032). Cox regression analysis confirmed that the 12-gene signature was an independent factor in predicting colorectal cancer patient's overall survival (hazard ratio: 1.759; 95% confidence interval: 1.126-2.746; p = 0.013], as well as disease-free survival (hazard ratio: 2.116; 95% confidence interval: 1.324-3.380; p = 0.002). Copyright: © 2015 An et al.",,"messenger RNA; messenger RNA; microRNA; adult; aged; Article; cancer growth; cancer prognosis; carcinogenesis; cell function; colorectal cancer; controlled study; cooperation; disease free survival; disorientation; embryo development; female; genetic heterogeneity; human; immune response; immunosurveillance; Kaplan Meier method; machine learning; major clinical study; male; molecular mechanics; overall survival; colorectal tumor; gene expression profiling; gene regulatory network; genetics; immunology; middle aged; pathology; prognosis; survival analysis; validation study; Aged; Colorectal Neoplasms; Female; Gene Expression Profiling; Gene Regulatory Networks; Humans; Male; MicroRNAs; Middle Aged; Prognosis; RNA, Messenger; Survival Analysis"
"An S., Boussaid F., Bennamoun M., Sohel F.","Exploiting layerwise convexity of rectifier networks with sign constrained weights","10.1016/j.neunet.2018.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048860948&doi=10.1016%2fj.neunet.2018.06.005&partnerID=40&md5=54e71d91d49b59fc9229b37c1383a4f2","By introducing sign constraints on the weights, this paper proposes sign constrained rectifier networks (SCRNs), whose training can be solved efficiently by the well known majorization–minimization (MM) algorithms. We prove that the proposed two-hidden-layer SCRNs, which exhibit negative weights in the second hidden layer and negative weights in the output layer, are capable of separating any number of disjoint pattern sets. Furthermore, the proposed two-hidden-layer SCRNs can decompose the patterns of each class into several clusters so that each cluster is convexly separable from all the patterns from the other classes. This provides a means to learn the pattern structures and analyse the discriminant factors between different classes of patterns. Experimental results are provided to show the benefits of sign constraints in improving classification performance and the efficiency of the proposed MM algorithm. © 2018 Elsevier Ltd","Geometrically interpretable neural network; Rectifier neural network; The majorization–minimization algorithm","Optimization; Classification performance; Different class; Hidden layers; Minimization algorithms; Output layer; Pattern set; Pattern structure; Sign constraints; Rectifying circuits; algorithm; analytic method; Article; artificial neural network; cluster analysis; decomposition; discriminant analysis; feedback system; nonlinear system; priority journal; machine learning; Machine Learning; Neural Networks (Computer)"
"Ana Lúcia L., Enguita F.J.","Synthetic biology approaches for secondary metabolism engineering","10.1016/B978-0-12-821477-0.00022-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126769627&doi=10.1016%2fB978-0-12-821477-0.00022-2&partnerID=40&md5=2ea755e8c89a4c82f9fbcf879fc2e193","Secondary metabolism is a rich and important source of chemical compounds with potential applications in several fields, including the human health. The genes involved in the biosynthesis of secondary metabolites in microorganisms and plants are often clustered and functionally structured into biosynthetic and regulatory modules. This structure made them ideal targets for metabolic engineering methods that follow the postulates of the synthetic biology. Considering that biological systems are composed by functional and structural modules, synthetic biology methods will allow the localization, decoding, and characterization of these modules that could be artificially assembled to build living entities with new functions and properties. The modern genomics and systems biology techniques allowed the characterization of secondary metabolites and the genes and enzymes responsible for their biosynthesis. Applying the ideas of the synthetic biology, the modern methods of genome analysis and manipulation, and the artificial intelligence algorithms, we can rationally design and build new secondary metabolites and improve the production of the already existing ones. This chapter will summarize the current methods and strategies for the use of synthetic biology in the rational design and optimization of secondary metabolite production. © 2021 Elsevier Inc. All rights reserved.","Antibiotic; Biological engineering; Biosynthetic cluster; Genome editing; Secondary metabolism; Synthetic biology",
"Anagnostis A., Moustakidis S., Papageorgiou E., Bochtis D.","A Hybrid Bimodal LSTM Architecture for Cascading Thermal Energy Storage Modelling","10.3390/en15061959","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126271834&doi=10.3390%2fen15061959&partnerID=40&md5=c2c8ed2462722cf331da70fab9d72f26","Modelling of thermal energy storage (TES) systems is a complex process that requires the development of sophisticated computational tools for numerical simulation and optimization. Until recently, most modelling approaches relied on analytical methods based on equations of the physical processes that govern TES systems’ operations, producing high-accuracy and interpretable results. The present study tackles the problem of modelling the temperature dynamics of a TES plant by exploring the advantages and limitations of an alternative data-driven approach. A hybrid bimodal LSTM (H2M-LSTM) architecture is proposed to model the temperature dynamics of different TES components, by utilizing multiple temperature readings in both forward and bidirectional fashion for fine-tuning the predictions. Initially, a selection of methods was employed to model the temperature dynamics of individual components of the TES system. Subsequently, a novel cascading modelling framework was realised to provide an integrated holistic modelling solution that takes into account the results of the individual modelling components. The cascading framework was built in a hierarchical structure that considers the interrelationships between the integrated energy components leading to seamless modelling of whole operation as a single system. The performance of the proposed H2M-LSTM was compared against a variety of well-known machine learning algorithms through an extensive experimental analysis. The efficacy of the proposed energy framework was demonstrated in comparison to the modelling performance of the individual components, by utilizing three prediction performance indicators. The findings of the present study offer: (i) insights on the low-error performance of tailor-made LSTM architectures fitting the TES modelling problem, (ii) deeper knowledge of the behaviour of integral energy frameworks operating in fine timescales and (iii) an alternative approach that enables the real-time or semi-real time deployment of TES modelling tools facilitating their use in real-world settings. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Bi-modal LSTM; Cascading energy framework; District heating; Thermal energy storage","Digital storage; Dynamics; Learning algorithms; Long short-term memory; Thermal energy; Bi-modal LSTM; Cascading energy framework; Complex Processes; Energy; Individual components; Real- time; Storage modelling; Temperature dynamics; Thermal energy storage; Thermal energy storage systems; Heat storage"
"Anagnostou M., Karvounidou O., Katritzidaki C., Kechagia C., Melidou K., Mpeza E., Konstantinidis I., Kapantai E., Berberidis C., Magnisalis I., Peristeras V.","Characteristics and challenges in the industries towards responsible AI: a systematic literature review","10.1007/s10676-022-09634-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137139116&doi=10.1007%2fs10676-022-09634-1&partnerID=40&md5=486922f8dd03ca1d111c5af9c7127234","Today humanity is in the midst of the massive expansion of new and fundamental technology, represented by advanced artificial intelligence (AI) systems. The ongoing revolution of these technologies and their profound impact across various sectors, has triggered discussions about the characteristics and values that should guide their use and development in a responsible manner. In this paper, we conduct a systematic literature review with the aim of pointing out existing challenges and required principles in AI-based systems in different industries. We discuss our findings and provide general recommendations to be considered during AI deployment in production. The results have shown many gaps and concerns towards responsible AI and integration of complex AI models in the industry that the research community could address. © 2022, The Author(s), under exclusive licence to Springer Nature B.V.","Artificial intelligence; Business; Challenges; Characteristics; Ethics; Responsible AI","Ethical technology; Artificial intelligence systems; Challenge; Characteristic; Intelligence models; Research communities; Responsible artificial intelligence; Systematic literature review; Artificial intelligence"
"Anand A., Kadian T., Shetty M.K., Gupta A.","Explainable AI decision model for ECG data of cardiac disorders","10.1016/j.bspc.2022.103584","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126143087&doi=10.1016%2fj.bspc.2022.103584&partnerID=40&md5=156ee56a43d72ccbf6c0cded21ccea51","Electrocardiogram (ECG) data is used to monitor the electrical activity of the heart. It is known that ECG data could help in detecting cardiac (heart) abnormalities. AI-enabled automated analysis of ECG waves has many applications in the medical domain, such as diagnostic of heart diseases, prediction of stress level, etc. In this study, we implemented a number of deep neural networks on a publicly available dataset of PTB-XL of ECG signals for the detection of cardiac disorders. Our proposed ST-CNN-GAP-5 model produced better results compared to the existing state-of-the-art results on this dataset, achieving an AUC of 93.41%. The same network architecture is tested on another ECG dataset of arrhythmia patients to assess the generalizability of our DL model for ECG datasets, yielding an accuracy of 95.8% and an AUC of 99.46%, which is competitive in performance to the state-of-the-art models. Finally, we analyzed the ECG data using SHapley Additive exPlanations (SHAP) on the trained ST-CNN-GAP-5 to assess the explainability or interpretability of the decisions of this deep convolution network model. Results indicate that the model is able to highlight relevant alterations of the ECG waves as required by clinicians, making it explainable for diagnostic purposes. Deployment of such models can help in easing the burden on medical infrastructure in low- and middle-income populous countries. © 2022","CNN; Deep Learning; ECG Waves; Electrocardiogram; Interpretability; Residual Networks; SHAP; XAI","Cardiology; Deep neural networks; Diseases; Heart; Network architecture; Cardiac disorders; CNN; Deep learning; Electrocardiogram wave; Interpretability; Residual network; Shapley; Shapley additive explanation; State of the art; XAI; Electrocardiography; accuracy; aged; Article; artificial intelligence; cardiovascular disease; child; convolutional neural network; deep learning; deep neural network; electrocardiogram; feature extraction; female; Fourier transform; heart arrhythmia; heart disease; heart infarction; human; machine learning; major clinical study; male; signal processing; Stockwell transform; very elderly"
"Anand A., Chamberlain D., Kodgule R., Fletcher R.R.","Pulmonary Screener: A Mobile Phone Screening Tool for Pulmonary and Respiratory Disease","10.1109/GHTC.2018.8601821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061778801&doi=10.1109%2fGHTC.2018.8601821&partnerID=40&md5=4cdc55fb75e4eaf087b917ba4816aacd","Pulmonary and respiratory diseases comprise a large proportion of the global disease burden, responsible for both mortality and disability. This burden is especially concentrated in the developing world, where air pollution levels are generally high and resources for diagnosing these diseases are very limited. Health workers and many general practitioner doctors in developing countries are not trained to diagnose pulmonary diseases, leading to high rates of misdiagnosis and underdiagnosis. Motivated by this need, we have developed a mobile toolkit that can be used for screening and diagnostic guidance for three of the most common pulmonary and respiratory diseases (Asthma, Chronic Obstructive Pulmonary Disease (COPD) and Allergic Rhinitis (AR)). The toolkit consists of a mobile phone app, known as Pulmonary Screener, which is used in conjunction with a low-cost (<US10) peak flow meter. Machine vision software enables the phone camera to automatically track and capture the reading from the peak flow meter without the need for any electronics, Bluetooth radio, or batteries. Using the peak flow meter reading as well as an integrated clinical questionnaire, a machine learning model is then used to calculate the individual probabilities of a patient having a specific pulmonary disease (Asthma, COPD, AR, other) or co-morbidities (Asthma + AR, COPD + AR). The machine learning models used in the application were trained using diagnostic data from 325 patients collected at a pulmonary clinic over the past 3 years. Based on 50 iterations of a held-out test set, the Pulmonary Screener achieves accuracy (AUC) values above 0.90 for all diseases and combinations, with the exception of Asthma with AUC = 0.84. Sensitivity and specificity values were for all diseases was also greater than 0.90. To our knowledge, this is the first clinically validated mobile tool that is capable of diagnosing multiple pulmonary diseases in a single app. Future versions of the Pulmonary Screener will make use of ongoing data collection to expand support for infectious diseases as well, including pneumonia and tuberculosis, and include features extracted from auscultation and cough sounds. © 2018 IEEE.","asthma; COPD; diagnostic; disease; global health; mobile; phone; pulmonary; respirator; screening","Cellular telephones; Developing countries; Diagnosis; Diseases; E-learning; Flow measurement; Flowmeters; Machine learning; Respirators; Screening; asthma; COPD; diagnostic; Global health; mobile; phone; pulmonary; Pulmonary diseases"
"Anand A., Fogel G., Tang E.K., Suganthan P.N.","Feature selection approach for quantitative prediction of transcriptional activities","10.1109/CIBCB.2006.331012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547853358&doi=10.1109%2fCIBCB.2006.331012&partnerID=40&md5=81f565d8f930722e9d0492e5c3bf9d57","Protein-DNA interactions play a crucial role in transcriptional regulation and other biological processes. Quantitative predictive models of protein-DNA binding affinities can increase our understanding of molecular interaction and help validate putative transcription factor binding sites or other regulatory features. Such predictive models must take into account context-specific features associated with both DNA and proteins. Given the large complexity associated with such features, here we consider only the contextual features of DNA associated with binding affinity. Two types of features are considered in this paper, 1) features accounting for conformational and physicochemical properties of nucleotide sequence and 2) another set of features accounting for conservation of evolutionary information in the form of position-specific weight matrices. A feature selection approach, named, leave-one-out sequential forward selection (LOOSFS), is presented. The feature selection method employs leave-one-out cross-validation error of the least square support vector machines (LS-SVM) to estimate the test error of quantitative prediction model. The method is used to identify important features possibly responsible for differences in transcriptional activities of 130 DNA sequences. These sequences were obtained by single base substitutions within promoter of the mouse beta-major globin gene. The selected features and predicted activity values correlate well with experimental results. © 2006 IEEE.",,"Artificial intelligence; Binding energy; Binding sites; Biochemistry; Biocommunications; Bioinformatics; Chlorine compounds; Crack propagation; Curve fitting; DNA; Error analysis; Flow interactions; Forecasting; Genes; Information science; Intelligent control; Ketones; Least squares approximations; Mathematical models; Nucleic acids; Organic acids; Predictive control systems; Proteins; Research aircraft; Support vector machines; Transcription; Transcription factors; Binding affinities; Biological processing; Computational biology; Computational intelligence; Contextual features; Evolutionary information; Feature selection; Feature selection methods; Globin genes; Least square support vector machines; Leave-one-out; Leave-one-out cross-validation; Nucleotide sequencing; Physico-chemical properties; Predictive modeling; Protein-DNA binding; Protein-DNA interactions; Putative transcription factors; Quantitative prediction; Quantitative predictive models; Sequential forward selection; Transcriptional activities; Transcriptional regulations; Two types; Weight matrices; DNA sequences"
"Anand D., Yashashwi K., Kumar N., Rane S., Gann P.H., Sethi A.","Weakly supervised learning on unannotated H&E-stained slides predicts BRAF mutation in thyroid cancer with high accuracy","10.1002/path.5773","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114791161&doi=10.1002%2fpath.5773&partnerID=40&md5=8acad49be4aa27e6d72039cde4ae19c3","Deep neural networks (DNNs) that predict mutational status from H&E slides of cancers can enable inexpensive and timely precision oncology. Although expert knowledge is reliable for annotating regions informative of malignancy and other known histologic patterns (strong supervision), it is unreliable for identifying regions informative of mutational status. This poses a serious impediment to obtaining higher prognostic accuracy and discovering new knowledge of pathobiology. We used a weakly supervised learning technique to train a DNN to predict BRAF V600E mutational status, determined using DNA testing, in H&E-stained images of thyroid cancer tissue without regional annotations. Our discovery cohort was a tissue microarray of only 85 patients from a single hospital. On a large independent external cohort of 444 patients from other hospitals, the trained model gave an area under the receiver operating characteristic curve of 0.98 (95% CI 0.97–1.00), which is much higher than the previously reported results for detecting any mutation using H&E by DNNs trained using strong supervision. We also developed a visualization technique that can automatically highlight regions the DNN found most informative for predicting mutational status. Our visualization is spatially granular and highly specific in highlighting strong negative and positive regions and moves us toward explainable artificial intelligence. Using t-tests, we confirmed that the proportions of follicular or papillary histology and oncocytic cytology, as noted for each patient by a pathologist who was blinded to the mutational status, were significantly different between mutated and wildtype patients. However, based solely on these features noted by the pathologist, a logistic regression classifier gave an average area under the receiver operating characteristic curve of 0.78 in five-fold cross-validation, which is much lower than that obtained using the DNN. These results highlight the potential of weakly supervised learning for training DNN models for problems where the informative visual patterns and their locations are not known a priori. © 2021 The Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd. © 2021 The Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.","BRAF V600E; computational pathology; deep learning; H&E; thyroid cancer; weakly supervised learning","B Raf kinase; B Raf kinase; BRAF protein, human; adult; anaplastic thyroid carcinoma; area under the curve; Article; artificial intelligence; aspiration cytology; cancer prognosis; cancer tissue; carcinogenesis; cohort analysis; controlled study; cross validation; deep neural network; DNA extraction; female; gene mutation; histology; human; human tissue; image quality; major clinical study; male; molecular genetics; molecular pathology; pathologist; poorly differentiated thyroid cancer; prediction; receiver operating characteristic; staining; supervised machine learning; thyroid cancer; thyroid follicular carcinoma; thyroid gland tissue; thyroid papillary carcinoma; tissue microarray; aged; genetics; middle aged; mutation; staining; thyroid tumor; Adult; Aged; Female; Humans; Male; Middle Aged; Mutation; Neural Networks, Computer; Proto-Oncogene Proteins B-raf; Staining and Labeling; Thyroid Neoplasms"
"Anand L., Rane K.P., Bewoor L.A., Bangare J.L., Surve J., Raghunath M.P., Sankaran K.S., Osei B.","Development of Machine Learning and Medical Enabled Multimodal for Segmentation and Classification of Brain Tumor Using MRI Images","10.1155/2022/7797094","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137225838&doi=10.1155%2f2022%2f7797094&partnerID=40&md5=0525efd73a1772363004bc09b6ff7b9b","The improper and excessive growth of brain cells may lead to the formation of a brain tumor. Brain tumors are the major cause of death from cancer. As a direct consequence of this, it is becoming more challenging to identify a treatment that is effective for a specific kind of brain tumor. The brain may be imaged in three dimensions using a standard MRI scan. Its primary function is to examine, identify, diagnose, and classify a variety of neurological conditions. Radiation therapy is employed in the treatment of tumors, and MRI segmentation is used to guide treatment. Because of this, we are able to assess whether or not a piece that was spotted by an MRI is a tumor. Using MRI scans, this study proposes a machine learning and medically assisted multimodal approach to segmenting and classifying brain tumors. MRI pictures contain noise. The geometric mean filter is utilized during picture preprocessing to facilitate the removal of noise. Fuzzy c-means algorithms are responsible for segmenting an image into smaller parts. The identification of a region of interest is facilitated by segmentation. The GLCM Grey-level co-occurrence matrix is utilized in order to carry out the process of dimension reduction. The GLCM algorithm is used to extract features from photographs. The photos are then categorized using various machine learning methods, including SVM, RBF, ANN, and AdaBoost. The performance of the SVM RBF algorithm is superior when it comes to the classification and detection of brain tumors. © 2022 L. Anand et al.",,"Adaptive boosting; Brain; Clustering algorithms; Fuzzy clustering; Image classification; Image segmentation; Learning systems; Medical imaging; Radial basis function networks; Radiotherapy; Support vector machines; Tumors; Brain cells; Brain tumors; Causes of death; Condition; Machine-learning; MRI Image; MRI scan; Multi-modal; Primary functions; Three dimensions; Magnetic resonance imaging; algorithm; brain; brain tumor; diagnostic imaging; human; machine learning; nuclear magnetic resonance imaging; pathology; procedures; Algorithms; Brain; Brain Neoplasms; Humans; Machine Learning; Magnetic Resonance Imaging"
"Anand R., Mishra R.K., Khan R.","Plant diseases detection using artificial intelligence","10.1016/B978-0-323-90550-3.00007-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138101747&doi=10.1016%2fB978-0-323-90550-3.00007-2&partnerID=40&md5=675418cdbf65236424386963c6a803af","The agricultural sector plays a very important role in the development of a country. The agricultural sector is not only responsible for feeding the growing population, but it is also a vital source of energy and is a solution to global warming. But crop diseases can harm entire crops, which may lead to economic losses and starvation. Plant diseases are detected by using manual observation of the disease symptoms present on parts of plants, especially leaves. This method has significant complexity and, without proper knowledge of the diseases which affect crops, farmers use an excessive number of pesticides, insecticides, etc. for the treatment of plant diseases. Therefore it is the need of the hour to use ever-growing technology for diagnosing diseases so that the right amount and correct chemicals can be used. This review discusses different machine learning and image processing-based techniques that were proposed in different literature for treating plant diseases, as most of these diseases occur only on plant leaves, so surveyed literature consists of those methods and techniques which can diagnose diseases in plants through leaf images. © 2022 Elsevier Inc. All rights reserved.","feature extraction; Image classification; image processing; image segmentation; machine learning; plant diseases",
"Anand R., Vaid A., Singh P.K.","Association rule mining using multi-objective evolutionary algorithms: Strengths and challenges","10.1109/NABIC.2009.5393878","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949616664&doi=10.1109%2fNABIC.2009.5393878&partnerID=40&md5=623298111ea78a7864ab236c1837c5e6","Association rule mining based on support and confidence generates a large number of rules. However, post analysis is required to obtain interesting rules as many of the generated rules are useless. We pose mining association rules as multi-objective optimization problem where objective functions are rule interestingness measures and use NSGA-II, a well known multi-objective evolutionary algorithm (MOEA), to solve the problem. We compare our results vis-à-vis results obtained by a traditional rule mining algorithm - Apriori and contrary to the other works reported in the literature clearly highlight the quality of obtained rules and challenges while using MOEAs for mining association rules. Though none of the algorithm emerged as clear winner, some of the rules obtained by MOEA could not be obtained by traditional data mining algorithm. We treat the whole process from data mining perspective and discuss the pitfalls responsible for relatively poor performance of the MOEA which has been shown as a good performer in other paradigms. ©2009 IEEE.","Association rule mining; Genetic algorithms; Interestingness measures; Multi-objective optimization","Apriori; Association rule mining; Data mining algorithm; Interesting rules; Interestingness measures; Mining association rules; Multi objective evolutionary algorithms; Multi-objective optimization problem; NSGA-II; Objective functions; Poor performance; Post analysis; Rule interestingness; Rule mining algorithms; Support and confidence; Whole process; Association rules; Associative processing; Data mining; Evolutionary algorithms; Multiobjective optimization"
"Ananraya K., Ammarapala V.","The development of highways assets management system","10.1109/ICSSSM.2010.5530102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955961170&doi=10.1109%2fICSSSM.2010.5530102&partnerID=40&md5=fb6530eeea07eff0cc103b4f95c877ed","This paper presents a development of the assets management system that purposely aspires to assist the Department of Highways (DOH), Ministry of Transportation, Thailand in managing their assets. Currently, the DOH has over hundred billions Thai Baht or approximately three billions USD in assets under management, which require a constant maintenance. The study initially focuses on an assessment of assets such as assets conditions, and the importance of each asset, in regards of engineering, economic and social impacts. After that, the results of the assessment are transformed into three indices, which are Condition Rating (CR), Asset Serviceability Index (ASI) and Priority Index (PI). Based on the indices, the system prioritizes all routine maintenance jobs under the responsible of DOH. This prioritization helps support the DOH executives in making proper judgments on how to set up an annual routine maintenance program of their assets, and manage their limited routine maintenance budget. Additionally, this system could be extended to include the scope of a budget tracking system. In this paper, the authors illustrate the development of an assets management system for twenty three groups of the DOH's assets. During the construction phrase, the research team gathered the assets information from five sub-districts in all five regions of the country. The collected data were analyzed and utilized in the pilot test of the system. In the end, the authors make conclusion and suggestions about how this system can be enhanced in the future. ©2010 IEEE.","Assets conditions; Assets management system; Conditions assessment; Decision Support Systems (DSS)","Assets conditions; Assets management; Assets management system; Conditions assessment; Constant maintenance; Decision supports; Pilot tests; Prioritization; Research teams; Routine maintenance; Serviceability index; Social impact; Thailand; Tracking system; Artificial intelligence; Budget control; Decision support systems; Decision theory; Highway administration; Maintenance; Decision making"
"Ananthi S., Dhanalakshmi P.","Syllable based concatenative synthesis for text to speech conversion","10.1007/978-81-322-2202-6_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920020106&doi=10.1007%2f978-81-322-2202-6_6&partnerID=40&md5=49c4a697638e746154bd4952e77ae8ca","Speech Synthesis functions as a medium which converts text into speech. Speech Recognition and Speech Synthesis plays a vital role in Human-Machine Interaction. Synthesized speeches are extracted from concatenating the pieces of pre-recorded speech utterances from the database. The proposed work converts the written text into a syllables (syllable text representation) using rule based approach and subsequently it converts the syllable representation to modified syllable waveform clips that can be combined together to produce as sound. Syllabic transcription attempts to describe the individual variations that occur between speakers of a dialect or language. Syllable based concatenative synthesis aims to record the syllables that a speaker uses rather than the actual spoken variants of those syllables that are produced when a speaker converse a word. The Concatenative Speech Synthesis methods provide highly understandable speech utterance. © Springer India 2015.","Concatenate wave segments; Concatenative speech synthesis; Speech processing; Speech synthesis (SS); Syllable; Syllable transcription; Text normalization; Text to speech (TTS) conversion; Waveform concatenation","Artificial intelligence; Data mining; Speech processing; Speech synthesis; Transcription; Concatenate wave segments; Concatenative speech synthesis; Syllable; Text normalizations; Text to speech; Wave forms; Speech recognition"
"Anaya A.R., Boticario J.G.","Reveal the collaboration in a open learning environment","10.1007/978-3-642-02264-7_48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350700920&doi=10.1007%2f978-3-642-02264-7_48&partnerID=40&md5=84bd3561ba397c29ed77f7030b73293a","The management and characterization of collaboration to improve students' learning is still an open issue, which needs standardized models and inferring methods for effective collaboration indicators, especially when online courses are based on open approaches where students are not following CSCL scripts. We have supplied our students with a scrutable (manageable and understandable) web application that shows an ontology, which includes collaborative features. The ontology structures collaboration context information, which has been obtained form explicit (based on questionnaires) and implicit methods (supported by several machine learning techniques). From two consecutive years of experiences with hundreds of students we researched students' interactions to find implicit methods to identify and characterize students' collaboration. Based on the outcomes of our experiments we claim that showing useful and structured information to students and tutors about students' collaborative features can have a twofold beneficial impact on students learning and on the management of their collaboration. © 2009 Springer Berlin Heidelberg.",,"Context information; Implicit methods; Machine learning techniques; Online course; Open learning; Standardized models; Structured information; Students learning; WEB application; Learning algorithms; Ontology; Students; Teaching"
"Anaya A.R., Boticario J.G.","Clustering learners according to their collaboration","10.1109/CSCWD.2009.4968115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-69649083146&doi=10.1109%2fCSCWD.2009.4968115&partnerID=40&md5=4d09552407c5909fcbec4daecc025b34","When students are responsible for their own collaborative learning process, they need information on collaboration to manage the process. We note that collaborative environments fail to provide users with information on the collaboration process. This research proposes an open collaborative learning experience design that provides learners with an environment where they can manage the collaboration. The design provides information on collaboration to improve management. An inferring method, which uses statistical indicators of student interaction in forums and data mining technology, works to obtain information on student collaboration and groups students according to their level of collaboration. This information will be shown to students and tutors to improve the learning process and its management. © 2009 IEEE.","Collaboration analysis; Data mining; ELearning","Collaboration analysis; Collaboration process; Collaborative environments; Collaborative learning; Collaborative learning process; Data mining technology; Learning process; Statistical indicators; Student collaboration; Student interactions; Computer supported cooperative work; Design; Groupware; Interactive computer systems; Interoperability; Mining; Students"
"Anaya K., Isaza C., Zavala J.P., Rizzo-Sierra J.A., Mosquera J.C.","Semiautomatic acousto-optical tunable filter calibration from spectrometry in the visible range with deep learning","10.1117/12.2541049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077345048&doi=10.1117%2f12.2541049&partnerID=40&md5=c2f798058a77dd7a1f48aeea36c26d59","Acousto-optical filters are devices in which light dispersion occurs through a crystalline translucent material. Particularly, light interacts with a sound-induced spatially distributed. Post-interaction, diffracted light can be analyzed for different purposes. Although acousto-optics has been studied for decades, practical devices applying its principles are relatively recent. Here, experimental and technical procedures are used to obtain the transfer function of an acousto-optical tunable filter (AOTF) based system used as a hyper-spectral photometer. The reflectance responses at given wavelengths are measured and adjusted from a commercially available color pattern set, while typically, those values are set up manually. We propose a semiautomatic strategy to calibrate as a single black box all components of the system including: The light source, the signal generator power with its frequency-Amplitude deviation from the full radio frequency set point, the radio-frequency amplifier, the transmission lines, the piezoelectric impedance, and the filter's own transfer function among others. To achieve that, we explored the capability of neural networks with deep learning. The system's input is reflectance data measured with a spectrophotometer at wavelengths from 400 to 700 nm with a step of 10 nm. Then, the AOTF system was used to gather reflectance data from those color pattern tiles from 400 to 700 nm with a step of 1 nm. Both reflectance datasets were adjusted using the proposed deep learning neural network. Results show that it is possible to calibrate an AOTF system by using ceramic tile color patterns and measuring reference reflectance values with a spectrophotometer in the visible range. Furthermore, a neural network can be trained to learn the compensation values, deriving trustable spectral information with a better wavelength resolution. © 2019 SPIE.","Acousto-optcial filter calibration; Acousto-optics; Deep neural net-works; Spectrometry in visible range","Calibration; Color; Color printing; Light; Light amplifiers; Light sources; Meteorological instruments; Optical filters; Radio frequency amplifiers; Radio transmission; Radio waves; Reflection; Spectrometry; Spectrophotometers; Transfer functions; Acousto-optical filter; Acousto-optical tunable filter; Amplitude deviation; Learning neural networks; Spectral information; Translucent materials; Visible range; Wavelength resolution; Deep neural networks"
"Anbajagane D., Evrard A.E., Farahi A.","Baryonic imprints on DM haloes: Population statistics from dwarf galaxies to galaxy clusters","10.1093/mnras/stab3177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121209833&doi=10.1093%2fmnras%2fstab3177&partnerID=40&md5=8f14c4a26cd369b0b2f2efda40939c44","In a purely cold dark matter (CDM) universe, the initial matter power spectrum and its subsequent gravitational growth contain no special mass-or time-scales, and so neither do the emergent population statistics of internal dark matter (DM) halo properties. Using 1.5 million haloes from three illustristng realizations of a ΛCDM universe, we show that galaxy formation physics drives non-monotonic features ('wiggles') into DM property statistics across six decades in halo mass, from dwarf galaxies to galaxy clusters. We characterize these features by extracting the halo mass-dependent statistics of five DM halo properties-velocity dispersion, NFW concentration, density-and velocity-space shapes, and formation time-using kernel-localized linear regression (Kllr). Comparing precise estimates of normalizations, slopes, and covariances between realizations with and without galaxy formation, we find systematic deviations across all mass-scales, with maximum deviations of 25 per cent at the Milky Way mass of 1012, M_\odot. The mass-dependence of the wiggles is set by the interplay between different cooling and feedback mechanisms, and we discuss its observational implications. The property covariances depend strongly on halo mass and physics treatment, but the correlations are mostly robust. Using multivariate Kllr and interpretable machine learning, we show the halo concentration and velocity-space shape are principal contributors, at different mass, to the velocity dispersion variance. Statistics of mass accretion rate and DM surface pressure energy are provided in an appendix. We publicly release halo property catalogues and kllr parameters for the TNG runs at 20 epochs up to z = 12. © 2021 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society.","dark matter; galaxies: haloes; galaxies: statistics","Dispersions; Gravitation; Population statistics; Velocity; Dark matter; Dark matter halos; Dwarf galaxies; Galaxies clusters; Galaxies: statistics; Galaxy formations; Galaxy:halo; Halo mass; Mass scale; Property; Galaxies"
"Anbalagan S., Bashir A.K., Raja G., Dhanasekaran P., Vijayaraghavan G., Tariq U., Guizani M.","Machine-Learning-Based Efficient and Secure RSU Placement Mechanism for Software-Defined-IoV","10.1109/JIOT.2021.3069642","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103773575&doi=10.1109%2fJIOT.2021.3069642&partnerID=40&md5=6a12f830cc5d2da7ed6db376bbb22f57","The massive increase in computing and network capabilities has resulted in a paradigm shift from vehicular networks to the Internet of Vehicles (IoV). Owing to the dynamic and heterogeneous nature of IoV, it requires efficient resource management using smart technologies, such as software-defined network (SDN), machine learning (ML), and so on. Roadside units (RSUs) in software-defined-IoV (SD-IoV) networks are responsible for network efficiency and offer several safety functions. However, it is not viable to deploy enough RSUs, and also the existing RSU placement lacks universal coverage within a region. Furthermore, any disruption in network performance or security impacts vehicular activities severely. Thus, this work aims to improve network efficiency through optimal RSU placement and enhance security with a malicious IoV detection algorithm in an SD-IoV network. Therefore, the memetic-based RSU (M-RSU) placement algorithm is proposed to reduce communication delay and increase the coverage area among IoV devices through an optimum RSU deployment. Besides the M-RSU algorithm, the work also proposes a distributed ML (DML)-based intrusion detection system (IDS) that prevents the SD-IoV network from disastrous security failures. The simulation results show that M-RSU placement reduces the transmission delay. The DML-based IDS detects the malicious IoV with an accuracy of 89.82% compared to traditional ML algorithms. © 2014 IEEE.","Internet of Vehicles (IoV); intrusion detection system (IDS); machine learning (ML); roadside unit (RSU) placement; software-defined network (SDN)","Efficiency; Intrusion detection; Machine learning; Vehicle to vehicle communications; Communication delays; Detection algorithm; Intrusion Detection Systems; Network capability; Placement algorithm; Resource management; Transmission delays; Vehicular networks; Network security"
"Anbananthen K.S.M., Sainarayanan G., Chekima A., Teo J.","Artificial neural network tree approach in data mining","10.22452/mjcs.vol20no1.5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548392334&doi=10.22452%2fmjcs.vol20no1.5&partnerID=40&md5=db87522c3ff8b43196f8244e5fe24376","Artificial neural networks (ANN) have demonstrated good predictive performance in a wide variety of real world problems. However, there are strong arguments as to why ANNs are insufficient for data mining. The arguments are the poor comprehensibility of the learned ANNs, which is the inability to represent the learned knowledge in an understandable way to the users. In this paper, Artificial Neural Network Tree (ANNT), i.e. ANN training preceded by Decision Tree rules extraction method, is presented to overcome the comprehensibility problem of ANN. Experimental results on three data sets show that the proposed algorithm generates rules that are better than C4.5. This paper provides an evaluation of the proposed method in terms of accuracy, comprehensibility and fidelity.","Artificial neural network; Comprehensibility; Data mining; Decision tree",
"Ancona M., Ceolini E., Öztireli C., Gross M.","Gradient-Based Attribution Methods","10.1007/978-3-030-28954-6_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072844355&doi=10.1007%2f978-3-030-28954-6_9&partnerID=40&md5=1258b372232386e4fe3083d38811f899","The problem of explaining complex machine learning models, including Deep Neural Networks, has gained increasing attention over the last few years. While several methods have been proposed to explain network predictions, the definition itself of explanation is still debated. Moreover, only a few attempts to compare explanation methods from a theoretical perspective has been done. In this chapter, we discuss the theoretical properties of several attribution methods and show how they share the same idea of using the gradient information as a descriptive factor for the functioning of a model. Finally, we discuss the strengths and limitations of these methods and compare them with available alternatives. © Springer Nature Switzerland AG 2019.","Attribution methods; Deep Neural Networks; Explainable artificial intelligence","Artificial intelligence; Attribution methods; Complex machines; Gradient based; Gradient informations; Network prediction; Deep neural networks"
"And I.A.A.E.-M., Darwish S.M.","Towards Designing a Trusted Routing Scheme in Wireless Sensor Networks: A New Deep Blockchain Approach","10.1109/ACCESS.2021.3098933","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111002730&doi=10.1109%2fACCESS.2021.3098933&partnerID=40&md5=498d395cde8efd8bf75fe08a63b0c61c","Routing is a critical process in Wireless Sensor Networks (WSNs) since it is responsible for data transmission to base stations. Routing attacks are capable of completely destroying and degrading the function of WSNs. A trustworthy routing system is critical for ensuring routing security and WSN efficiency. Numerous studies have been conducted to increase trust between routing nodes, including cryptographic techniques, and centralized routing decisions. Nonetheless, the majority of routing methods are impractical in practice, since it is difficult to identify untrusted activities of routing nodes effectively. Meanwhile, there is no efficient method of preventing malicious node attacks. As a result of these issues, this article offers a trusted routing method that combines deep blockchain and Markov Decision Processes (MDPs) in order to enhance the routing security and efficiency of WSNs. To authenticate the process of transmitting the node, the proposed approach utilizes a Proof of Authority (PoA) method inside the blockchain network. The validation group necessary for proofing is selected using a deep learning methodology that focuses on the properties of each node. MDPs are then used to choose the appropriate next hop as a forwarding node capable of transferring messages simply and securely. According to testing data, our routing system still performs well in a 50% malicious node routing environment when compared to existing routing algorithms. © 2013 IEEE.","blockchain; deep learning; Markov decision; trusted routing; Wireless sensor networks","Blockchain; Deep learning; Efficiency; Markov chains; Network security; Sensor nodes; Well testing; Cryptographic techniques; Forwarding nodes; Malicious nodes; Markov Decision Processes; Routing attacks; Routing decisions; Routing security; Wireless sensor network (WSNs); Network routing"
"Anderkova V., Babic F.","Better understandability of prediction models: A case study for data-based road safety management system","10.1109/CINTI53070.2021.9668314","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124969778&doi=10.1109%2fCINTI53070.2021.9668314&partnerID=40&md5=da837294396d685daee81e96725068c3","The road safety management system aims to ensure a safe transport system for all road users. Analyses of data about traffic accidents can provide important knowledge to support relevant decision-makers or processes. This fact motivated our case study covering the analytical process over publicly available data about traffic accidents in England, Scotland, and Wales. Based on our previous experience with this dataset, we aimed not on the prediction models and their accuracy, but on their explanations for the end-users with limited knowledge from data mining, machine learning, or artificial intelligence. For this purpose, we improved the generated decision models with selected explainable methods like The Local Interpretable Model-Agnostic Explanation (LIME) and SHap Additive exPlanations (SHAP) values. The final visualizations show which attributes and to what extent they contribute to each type of accident. © 2021 IEEE.",,"Accident prevention; Accidents; Data mining; Highway administration; Highway planning; Information management; Intelligent systems; Lime; Motor transportation; Roads and streets; Analytical process; Case-studies; Decision makers; Decision process; Prediction modelling; Road safety; Road users; Safety management systems; Transport systems; Understandability; Decision making"
"Anders C.J., Weber L., Neumann D., Samek W., Müller K.-R., Lapuschkin S.","Finding and removing Clever Hans: Using explanation methods to debug and improve deep models","10.1016/j.inffus.2021.07.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113458167&doi=10.1016%2fj.inffus.2021.07.015&partnerID=40&md5=266f4e856422a7b806d5207ac06c06d3","Contemporary learning models for computer vision are typically trained on very large (benchmark) datasets with millions of samples. These may, however, contain biases, artifacts, or errors that have gone unnoticed and are exploitable by the model. In the worst case, the trained model does not learn a valid and generalizable strategy to solve the problem it was trained for, and becomes a “Clever Hans” predictor that bases its decisions on spurious correlations in the training data, potentially yielding an unrepresentative or unfair, and possibly even hazardous predictor. In this paper, we contribute by providing a comprehensive analysis framework based on a scalable statistical analysis of attributions from explanation methods for large data corpora. Based on a recent technique — Spectral Relevance Analysis — we propose the following technical contributions and resulting findings: (a) a scalable quantification of artifactual and poisoned classes where the machine learning models under study exhibit Clever Hans behavior, (b) several approaches we collectively denote as Class Artifact Compensation, which are able to effectively and significantly reduce a model's Clever Hans behavior, i.e., we are able to un-Hans models trained on (poisoned) datasets, such as the popular ImageNet data corpus. We demonstrate that Class Artifact Compensation, defined in a simple theoretical framework, may be implemented as part of a neural network's training or fine-tuning process, or in a post-hoc manner by injecting additional layers, preventing any further propagation of undesired Clever Hans features, into the network architecture. Using our proposed methods, we provide qualitative and quantitative analyses of the biases and artifacts in, e.g., the ImageNet dataset, the Adience benchmark dataset of unfiltered faces, and the ISIC 2019 skin lesion analysis dataset. We demonstrate that these insights can give rise to improved, more representative, and fairer models operating on implicitly cleaned data corpora. © 2021 The Authors","Class Artifact Compensation; Clever Hans predictors; Deep Neural Networks; Explainable Artificial Intelligence; Feature unlearning; Spectral Relevance Analysis","Backpropagation; Image analysis; Large dataset; Multilayer neural networks; Network architecture; Benchmark datasets; Comprehensive analysis; Learning models; Machine learning models; Qualitative and quantitative analysis; Relevance analysis; Technical contribution; Theoretical framework; Learning systems"
"Andersen H., Lensen A., Browne W.N.","Improving the search of learning classifier systems through interpretable feature clustering","10.1145/3520304.3534027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136332258&doi=10.1145%2f3520304.3534027&partnerID=40&md5=e31a20ec06c65be7767d98befd2a896a","Learning Classifier Systems (LCS) are a well-known machine learning method, producing sets of interpretable rules in order to solve a variety of problems. Despite this, a common issue that these systems run into is the creation of unhelpful rules, caused by having multiple features in the data representing similar areas of knowledge. While we can logically know that these rules will not be useful in conjunction with each other, this is much more difficult for the algorithm to innately know. This paper presents an exploration into using clustering algorithms for feature selection in LCS, selecting features that represent each major cluster of feature information. Combined with the innate power of LCS at finding nonlinear decision boundaries, these selected features can achieve results close to that of the full feature set while reducing the training time required to reach those results. The feature selection performed is highly interpretable, allowing for different features to be selected while maintaining the information spread in the feature subset. Learning Classifier Systems (LCS) [1] are an Evolutionary Computation method that learns a population of rules in order to perform tasks. One common application for LCS systems is classification, where each rule makes an individual mapping of the class from an input, and these heuristics are combined to form an overall class prediction. © 2022 ACM.",,"Calculations; Classification (of information); Clustering algorithms; Learning systems; Decision boundary; Feature clustering; Feature information; Features selection; Interpretable rules; Know-that; Learning classifier system; Machine learning methods; Multiple features; Power; Feature Selection"
"Andersen H., Lensen A., Browne W.N., Mei Y.","Evolving Counterfactual Explanations with Particle Swarm Optimization and Differential Evolution","10.1109/CEC55065.2022.9870283","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138740143&doi=10.1109%2fCEC55065.2022.9870283&partnerID=40&md5=156cc90ef3db142b1e1d13ee33839051","Counterfactual explanations are a popular eXplainable AI technique, used to provide contrastive answers to 'what-if' questions. These explanations are consistent with the way that an everyday person will explain an event, and have been shown to satisfy the 'right to explanation' of the European data regulations. Despite this, current work to generate counterfactual explanations either makes assumptions about the model being explained or utlises algorithms that perform suboptimally on continuous data. This work presents two novel algorithms to generate counterfactual explanations using Particle Swarm Optimization (PSO) and Differential Evolution (DE). These are shown to provide effective post-hoc explanations that make no assumptions about the underlying model or data structure. In particular, PSO is shown to generate counterfactual explanations that utilise significantly fewer features to generate sparser explanations when compared to previous related work. © 2022 IEEE.","counterfactual explanation; differential evolution; explainable AI; particle swarm optimization","Artificial intelligence; 'current; AI techniques; Counterfactual explanation; Counterfactuals; Data regulations; Differential Evolution; Explainable AI; Particle swarm; Particle swarm optimization; Swarm optimization; Particle swarm optimization (PSO)"
"Andersen J.K.H., Hubel M.S., Savarimuthu T.R., Rasmussen M.L., Sørensen S.L.B., Grauslund J.","A digital online platform for education and certification of diabetic retinopathy health care professionals in the Region of Southern Denmark","10.1111/aos.15123","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126131078&doi=10.1111%2faos.15123&partnerID=40&md5=e0d01f87365ef5efae6415b7e162a8dd","Purpose: The incidence of diabetes continues to increase across the world. As the number of patients rises, so does the need for educated health care professionals. Diabetic retinopathy (DR) remains one of the primary complications in diabetes, and screening has proved to be a cost-effective measure to avoid DR-related blindness. Denmark has an established screening programme, but no formal training of the people responsible for analysing retinal images. Methods: We here present an online learning platform that offers a diabetic eye screening course for health care professionals undertaking screening responsibility in the Region of Southern Denmark. The course is divided into lectures, each focussed on identifying different levels of DR or detecting related lesions. The course is free to use on-demand, contains instructional videos, interactive tests and exercises, and it is concluded with a certification test. The tools on the platform can in addition be used to generate data for research purposes, such as comparing users or experts in detection of lesions or annotating data for the development of machine learning models. Results: More than 150 participants have so far completed the course, and the platform is being adopted for education in other regions of Denmark. © 2022 The Authors. Acta Ophthalmologica published by John Wiley & Sons Ltd on behalf of Acta Ophthalmologica Scandinavica Foundation.","artificial intelligence; certification; diabetic retinopathy; online education; screening","certification; Denmark; diabetes mellitus; diabetic retinopathy; health care personnel; human; machine learning; mass screening; procedures; Certification; Denmark; Diabetes Mellitus; Diabetic Retinopathy; Health Personnel; Humans; Machine Learning; Mass Screening"
"Andersen M.R., Winther O., Hansen L.K., Poldrack R., Koyejo O.","Bayesian structure learning for dynamic brain connectivity",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067805909&partnerID=40&md5=780843526bdfb4009c6254e595b07857","Human brain activity as measured by fMRI exhibits strong correlations between brain regions which are believed to vary over time. Importantly, dynamic connectivity has been linked to individual differences in physiology, psychology and behavior, and has shown promise as a biomarker for disease. The state of the art in computational neuroimaging is to estimate the brain networks as relatively short sliding window covariance matrices, which leads to high variance estimates, thereby resulting in high overall error. This manuscript proposes a novel Bayesian model for dynamic brain connectivity. Motivated by the underlying neuroscience, the model estimates covariances which vary smoothly over time, with an instantaneous decomposition into a collection of spatially sparse components – resulting in parsimonious and highly interpretable estimates of dynamic brain connectivity. Simulated results are presented to illustrate the performance of the model even when it is mis-specified. For real brain imaging data with unknown ground truth, in addition to qualitative evaluation, we devise a simple classification task which suggests that the estimated brain networks better capture the underlying structure. Copyright 2018 by the author(s).",,"Artificial intelligence; Bayesian networks; Behavioral research; Brain mapping; Covariance matrix; Bayesian structure learning; Brain connectivity; Brain-imaging data; Classification tasks; Covariance matrices; Individual Differences; Qualitative evaluations; Strong correlation; Brain"
"Andersen P.-A., Goodwin M., Granmo O.-C.","Interpretable Option Discovery Using Deep Q-Learning and Variational Autoencoders","10.1007/978-3-030-71711-7_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103515635&doi=10.1007%2f978-3-030-71711-7_11&partnerID=40&md5=5f084ea3dbe3e1d984ea5d4e8ffe2f0a","Deep Reinforcement Learning (RL) is unquestionably a robust framework to train autonomous agents in a wide variety of disciplines. However, traditional deep and shallow model-free RL algorithms suffer from low sample efficiency and inadequate generalization for sparse state spaces. The options framework with temporal abstractions [18] is perhaps the most promising method to solve these problems, but it still has noticeable shortcomings. It only guarantees local convergence, and it is challenging to automate initiation and termination conditions, which in practice are commonly hand-crafted. Our proposal, the Deep Variational Q-Network (DVQN), combines deep generative- and reinforcement learning. The algorithm finds good policies from a Gaussian distributed latent-space, which is especially useful for defining options. The DVQN algorithm uses MSE with KL-divergence as regularization, combined with traditional Q-Learning updates. The algorithm learns a latent-space that represents good policies with state clusters for options. We show that the DVQN algorithm is a promising approach for identifying initiation and termination conditions for option-based reinforcement learning. Experiments show that the DVQN algorithm, with automatic initiation and termination, has comparable performance to Rainbow and can maintain stability when trained for extended periods after convergence. © 2021, Springer Nature Switzerland AG.","Clustering; Deep reinforcement learning; Hierarchical reinforcement learning; Latent-space representation; Options","Autonomous agents; Clustering algorithms; Learning algorithms; Reinforcement learning; Autoencoders; Gaussian distributed; KL-divergence; Local Convergence; Model free; Q-learning; Temporal abstraction; Termination condition; Deep learning"
"Anderson A., Douglas P.K., Kerr W.T., Haynes V.S., Yuille A.L., Xie J., Wu Y.N., Brown J.A., Cohen M.S.","Non-negative matrix factorization of multimodal MRI, fMRI and phenotypic data reveals differential changes in default mode subnetworks in ADHD","10.1016/j.neuroimage.2013.12.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908384066&doi=10.1016%2fj.neuroimage.2013.12.015&partnerID=40&md5=33f7d08b46265a85995f6cd73ce9e9e4","In the multimodal neuroimaging framework, data on a single subject are collected from inherently different sources such as functional MRI, structural MRI, behavioral and/or phenotypic information. The information each source provides is not independent; a subset of features from each modality maps to one or more common latent dimensions, which can be interpreted using generative models. These latent dimensions, or ""topics,"" provide a sparse summary of the generative process behind the features for each individual. Topic modeling, an unsupervised generative model, has been used to map seemingly disparate features to a common domain. We use Non-Negative Matrix Factorization (NMF) to infer the latent structure of multimodal ADHD data containing fMRI, MRI, phenotypic and behavioral measurements. We compare four different NMF algorithms and find that the sparsest decomposition is also the most differentiating between ADHD and healthy patients. We identify dimensions that map to interpretable, recognizable dimensions such as motion, default mode network activity, and other such features of the input data. For example, structural and functional graph theory features related to default mode subnetworks clustered with the ADHD-Inattentive diagnosis. Structural measurements of the default mode network (DMN) regions such as the posterior cingulate, precuneus, and parahippocampal regions were all related to the ADHD-Inattentive diagnosis. Ventral DMN subnetworks may have more functional connections in ADHD-I, while dorsal DMN may have less. ADHD topics are dependent upon diagnostic site, suggesting diagnostic differences across geographic locations. We assess our findings in light of the ADHD-200 classification competition, and contrast our unsupervised, nominated topics with previously published supervised learning methods. Finally, we demonstrate the validity of these latent variables as biomarkers by using them for classification of ADHD in 730 patients. Cumulatively, this manuscript addresses how multimodal data in ADHD can be interpreted by latent dimensions. © 2013 Elsevier Inc.","ADHD; Attention deficit; Biomarkers; Default mode; FMRI; Latent variables; Machine learning; MRI; Multimodal data; NMF; Phenotype; Sparsity; Topic modeling","algorithm; attention deficit disorder; behavior; brain function; default mode network; disease classification; functional magnetic resonance imaging; human; machine learning; neuroimaging; non negative matrix factorization; nuclear magnetic resonance imaging; parahippocampal gyrus; phenotype; posterior cingulate; precuneus; Review; statistical analysis; adolescent; Attention Deficit Disorder with Hyperactivity; child; female; genetics; male; multimodal imaging; neuroimaging; phenotype; young adult; Adolescent; Algorithms; Attention Deficit Disorder with Hyperactivity; Child; Female; Humans; Magnetic Resonance Imaging; Male; Multimodal Imaging; Neuroimaging; Phenotype; Young Adult"
"Anderson A., Han D., Douglas P.K., Bramen J., Cohen M.S.","Real-time functional MRI classification of brain states using Markov-SVM hybrid models: Peering inside the rt-fMRI black box","10.1007/978-3-642-34713-9_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870034605&doi=10.1007%2f978-3-642-34713-9_31&partnerID=40&md5=7d3e66891c373528cbf7b33802d70861","Real-time functional MRI (rt-fMRI) methods provide the ability to predict and detect online changes in cognitive states. Applications require appropriate selection of features, preprocessing routines, and efficient computational models in order to be both practical to implement and deliver interpretable results. We predict video activity in nicotine-addicted subjects using both regional spatial averages and pre-constructed independent component spatial maps we refer to as an ""IC dictionary."" We found that this dictionary predicted better than the anatomical summaries and was less sensitive to preprocessing steps. When prior state information was incorporated using hybrid SVM-Markov models, the online models were able to predict even more accurately in real-time whether an individual was viewing a video while either resisting or indulging in nicotine cravings. Collectively, this work proposes and evaluates models that could be used for biofeedback. The IC dictionary offered an interpretable feature set proposing functional networks responsible for cognitive activity. We explore what is inside the black box of real-time fMRI, and examine both the advantages and shortcomings when machine learning methods are applied to predict and interpret cognitive states in the real-time context. © 2012 Springer-Verlag.",,"Black boxes; Brain state; Cognitive activities; Cognitive state; Computational model; Feature sets; Functional MRI; Functional network; Hybrid model; Independent components; Machine learning methods; Online models; Pre-processing step; Spatial average; Spatial maps; State information; Video activity; Biofeedback; Brain models; Data processing; Forecasting; Learning systems; Markov processes; Pyridine; Neuroimaging"
"Anderson A.B., Grazal C.F., Balazs G.C., Potter B.K., Dickens J.F., Forsberg J.A.","Can Predictive Modeling Tools Identify Patients at High Risk of Prolonged Opioid Use after ACL Reconstruction?","10.1097/CORR.0000000000001251","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099557083&doi=10.1097%2fCORR.0000000000001251&partnerID=40&md5=b75f95bc1cf5925432fe1fc3eeed645e","Background Machine-learning methods such as the Bayesian belief network, random forest, gradient boosting machine, and decision trees have been used to develop decision-support tools in other clinical settings. Opioid abuse is a problem among civilians and military service members, and it is difficult to anticipate which patients are at risk for prolonged opioid use.Questions/purposes(1) To build a cross-validated model that predicts risk of prolonged opioid use after a specific orthopaedic procedure (ACL reconstruction), (2) To describe the relationships between prognostic and outcome variables, and (3) To determine the clinical utility of a predictive model using a decision curve analysis (as measured by our predictive system's ability to effectively identify high-risk patients and allow for preventative measures to be taken to ensure a successful procedure process).MethodsWe used the Military Analysis and Reporting Tool (M2) to search the Military Health System Data Repository for all patients undergoing arthroscopically assisted ACL reconstruction (Current Procedure Terminology code 29888) from January 2012 through December 2015 with a minimum of 90 days postoperative follow-up. In total, 10,919 patients met the inclusion criteria, most of whom were young men on active duty. We obtained complete opioid prescription filling histories from the Military Health System Data Repository's pharmacy records. We extracted data including patient demographics, military characteristics, and pharmacy data. A total of 3.3% of the data was missing. To curate and impute all missing variables, we used a random forest algorithm. We shuffled and split the data into 80% training and 20% hold-out sets, balanced by outcome variable (Outcome90Days). Next, the training set was further split into training and validation sets. Each model was built on the training data set, tuned with the validation set as applicable, and finally tested on the separate hold-out dataset. We chose four predictive models to develop, at the end choosing the best-fit model for implementation. Logistic regression, random forest, Bayesian belief network, and gradient boosting machine models were the four chosen models based on type of analysis (classification). Each were trained to estimate the likelihood of prolonged opioid use, defined as any opioid prescription filled more than 90 days after anterior cruciate reconstruction. After this, we tested the models on our holdout set and performed an area under the curve analysis concordance statistic, calculated the Brier score, and performed a decision curve analysis for validation. Then, we chose the method that produced the most suitable analysis results and, consequently, predictive power across the three calculations. Based on the calculations, the gradient boosting machine model was selected for future implementation. We systematically selected features and tuned the gradient boosting machine to produce a working predictive model. We performed area under the curve, Brier, and decision curve analysis calculations for the final model to test its viability and gain an understanding of whether it is possible to predict prolonged opioid use.ResultsFour predictive models were successfully developed using gradient boosting machine, logistic regression, Bayesian belief network, and random forest methods. After applying the Boruta algorithm for feature selection based on a 100-tree random forest algorithm, features were narrowed to a final seven features. The most influential features with a positive association with prolonged opioid use are preoperative morphine equivalents (yes), particular pharmacy ordering sites locations, shorter deployment time, and younger age. Those observed to have a negative association with prolonged opioid use are particular pharmacy ordering sites locations, preoperative morphine equivalents (no), longer deployment, race (American Indian or Alaskan native) and rank (junior enlisted).On internal validation, the models showed accuracy for predicting prolonged opioid use with AUC greater than our benchmark cutoff 0.70; random forest were 0.76 (95% confidence interval 0.73 to 0.79), 0.76 (95% CI 0.73 to 0.78), 0.73 (95% CI 0.71 to 0.76), and 0.72 (95% CI 0.69 to 0.75), respectively. Although the results from logistic regression and gradient boosting machines were very similar, only one model can be used in implementation. Based on our calculation of the Brier score, area under the curve, and decision curve analysis, we chose the gradient boosting machine as the final model. After selecting features and tuning the chosen gradient boosting machine, we saw an incremental improvement in our implementation model; the final model is accurate, with a Brier score of 0.10 (95% CI 0.09 to 0.11) and area under the curve of 0.77 (95% CI 0.75 to 0.80). It also shows the best clinical utility in a decision curve analysis.ConclusionsThese scores support our claim that it is possible to predict which patients are at risk of prolonged opioid use, as seen by the appropriate range of hold-out analysis calculations. Current opioid guidelines recommend preoperative identification of at-risk patients, but available tools for this purpose are crude, largely focusing on identifying the presence (but not relative contributions) of various risk factors and screening for depression. The power of this model is that it will permit the development of a true clinical decision-support tool, which risk-stratifies individual patients with a single numerical score that is easily understandable to both patient and surgeon. Probabilistic models provide insight into how clinical factors are conditionally related. Not only will this gradient boosting machine be used to help understand factors contributing to opiate misuse after ACL reconstruction, but also it will allow orthopaedic surgeons to identify at-risk patients before surgery and offer increased support and monitoring to prevent opioid abuse and dependency.Level of EvidenceLevel III, therapeutic study. © 2020 Royal Society Publishing. All rights reserved.",,"morphine; narcotic analgesic agent; narcotic antagonist; adult; African American; Alaska Native; American Indian; anterior cruciate ligament reconstruction; area under the curve; arthroscopic surgery; Article; Asian; Bayesian network; clinical feature; contingency table; controlled study; demography; drug use; feature selection; female; follow up; high risk patient; human; major clinical study; male; military health; outcome variable; pharmacy (shop); prescription; priority journal; random forest; retrospective study; risk assessment; adverse event; anterior cruciate ligament injury; anterior cruciate ligament reconstruction; arthroscopy; clinical decision making; decision support system; drug administration; factual database; machine learning; military medicine; opiate addiction; postoperative pain; reproducibility; risk assessment; risk factor; time factor; treatment outcome; young adult; Adult; Anterior Cruciate Ligament Injuries; Anterior Cruciate Ligament Reconstruction; Arthroscopy; Clinical Decision-Making; Databases, Factual; Decision Support Techniques; Drug Administration Schedule; Female; Humans; Machine Learning; Male; Military Medicine; Narcotic Antagonists; Opioid-Related Disorders; Pain, Postoperative; Reproducibility of Results; Retrospective Studies; Risk Assessment; Risk Factors; Time Factors; Treatment Outcome; Young Adult"
"Anderson C., Vasudevan R., Johnson-Roberson M.","Off the Beaten Sidewalk: Pedestrian Prediction in Shared Spaces for Autonomous Vehicles","10.1109/LRA.2020.3023713","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091307929&doi=10.1109%2fLRA.2020.3023713&partnerID=40&md5=f4ac29ff1e462e3908c586fc049631a9","Pedestrians and drivers interact closely in a wide range of environments. Autonomous vehicles (AVs) correspondingly face the need to predict pedestrians' future trajectories in these same environments. Traditional model-based prediction methods have been limited to making predictions in highly structured scenes with signalized intersections, marked crosswalks, or curbs. Deep learning methods have instead leveraged datasets to learn predictive features that generalize across scenes, at the cost of model interpretability. This letter aims to achieve both widely applicable and interpretable predictions by proposing a risk-based attention mechanism to learn when pedestrians yield, and a model of vehicle influence to learn how yielding affects motion. A novel probabilistic method, Off the Sidewalk Predictions (OSP), uses these to achieve accurate predictions in both shared spaces and traditional scenes. Experiments on urban datasets demonstrate that the realtime method achieves state-of-the-art performance. © 2016 IEEE.","Autonomous agents; autonomous vehicle navigation; motion trajectory prediction","Agricultural robots; Crosswalks; Deep learning; Forecasting; Learning systems; Pavements; Pedestrian safety; Accurate prediction; Attention mechanisms; Interpretability; Learning methods; Probabilistic methods; Signalized intersection; State-of-the-art performance; Traditional models; Autonomous vehicles"
"Anderson H.E., Liden T., Berger B.K., Zanella D., Manh L.H., Wang S., Schug K.A.","Profiling of contemporary beer styles using liquid chromatography quadrupole time-of-flight mass spectrometry, multivariate analysis, and machine learning techniques","10.1016/j.aca.2021.338668","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107062034&doi=10.1016%2fj.aca.2021.338668&partnerID=40&md5=733df72afac80806118a1de6841355e8","Although all beer is brewed using the same four classes of ingredients, contemporary beer styles show wide variation in flavor and color, suggesting differences in their chemical profiles. A selection of 32 beers covering five styles (India pale ale, blonde, stout, wheat, and sour) were investigated to determine chemical features, which discriminate between popular beer styles. The beers were analyzed in an untargeted fashion using liquid chromatography quadrupole time-of-flight mass spectrometry (LC-QTOF-MS). The separation and detection method were tuned to include compounds from important beer components, namely iso-α-acids and phenolic compounds. Due to the sheer number of unknown compounds in beer, multivariate analysis and machine learning techniques were used to pinpoint some of the compounds most influential in distinguishing beer styles. It was determined that while many phenols and iso-α-acids were present in the beers, they were not the compounds most responsible for the variations between styles. However, it was possible to discriminate each beer style using multivariate analysis. Principal component analysis (PCA) was able to separate and cluster the individual beer samples by style. A combination of statistical tools were used to predict formulas for some of the most influential metabolites from each style. Machine learning models accurately classified patterns in the five beer styles, indicating that they can be precisely distinguished by their nonvolatile chemical profile. © 2021 Elsevier B.V.","Craft beer; Discriminant analysis; High resolution mass spectrometry; Partial least squares; Principal component analysis; Untargeted analysis","Beer; Discriminant analysis; Learning algorithms; Liquid chromatography; Machine learning; Mass spectrometry; Multivariant analysis; Phenols; Principal component analysis; Statistical mechanics; Chemical profiles; Craft beer; High resolution mass spectrometry; Liquid chromatography-quadrupole time-of-flight mass spectrometries; Liquid chromatography/quadrupole-time of flight mass spectrometries; Machine learning techniques; Multi variate analysis; Partial least-squares; Principal-component analysis; Untargeted analyse; Least squares approximations; analytic method; Article; beer; chemical analysis; chemical composition; controlled study; limit of detection; liquid chromatography; machine learning; separation technique; time of flight mass spectrometry; liquid chromatography; mass spectrometry; multivariate analysis; Beer; Chromatography, Liquid; Machine Learning; Mass Spectrometry; Multivariate Analysis"
"Anderson M., Anderson S.L.","Machine ethics","10.1017/CBO9780511978036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927108085&doi=10.1017%2fCBO9780511978036&partnerID=40&md5=af1352cce51e0ae767f6bccb0bab8b29","The new field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making. Developing ethics for machines, in contrast to developing ethics for human beings who use machines, is by its nature an interdisciplinary endeavor. The essays in this volume represent the first steps by philosophers and artificial intelligence researchers toward explaining why it is necessary to add an ethical dimension to machines that function autonomously, what is required in order to add this dimension, philosophical and practical challenges to the machine ethics project, various approaches that could be considered in attempting to add an ethical dimension to machines, work that has been done to date in implementing these approaches, and visions of the future of machine ethics research. © Cambridge University Press 2011.",,"Artificial intelligence; Ethical decision making; Ethical dilemma; Ethical principles; Human being; Interdisciplinary endeavor; Philosophical aspects"
"Anderson S., Radić V.","Interpreting Deep Machine Learning for Streamflow Modeling Across Glacial, Nival, and Pluvial Regimes in Southwestern Canada","10.3389/frwa.2022.934709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133940867&doi=10.3389%2ffrwa.2022.934709&partnerID=40&md5=5ab1d5ab20ead83461c04f9698f71e3d","The interpretation of deep learning (DL) hydrological models is a key challenge in data-driven modeling of streamflow, as the DL models are often seen as “black box” models despite often outperforming process-based models in streamflow prediction. Here we explore the interpretability of a convolutional long short-term memory network (CNN-LSTM) previously trained to successfully predict streamflow at 226 stream gauge stations across southwestern Canada. To this end, we develop a set of sensitivity experiments to characterize how the CNN-LSTM model learns to map spatiotemporal fields of temperature and precipitation to streamflow across three streamflow regimes (glacial, nival, and pluvial) in the region, and we uncover key spatiotemporal patterns of model learning. The results reveal that the model has learned basic physically-consistent principles behind runoff generation for each streamflow regime, without being given any information other than temperature, precipitation, and streamflow data. In particular, during periods of dynamic streamflow, the model is more sensitive to perturbations within/nearby the basin where streamflow is being modeled, than to perturbations far away from the basins. The sensitivity of modeled streamflow to the magnitude and timing of the perturbations, as well as the sensitivity of day-to-day increases in streamflow to daily weather anomalies, are found to be specific for each streamflow regime. For example, during summer months in the glacial regime, modeled daily streamflow is increasingly generated by warm daily temperature anomalies in basins with a larger fraction of glacier coverage. This model's learning of “glacier runoff” contributions to streamflow, without any explicit information given about glacier coverage, is enabled by a set of cell states that learned to strongly map temperature to streamflow only in glacierized basins in summer. Our results demonstrate that the model's decision making, when mapping temperature and precipitation to streamflow, is consistent with a basic physical understanding of the system. Copyright © 2022 Anderson and Radić.","convolutional neural networks; deep machine learning; hydrology; interpretable machine learning; long short-term memory neural networks",
"Anderson S., Radić V.","Evaluation and interpretation of convolutional long short-term memory networks for regional hydrological modelling","10.5194/hess-26-795-2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125082696&doi=10.5194%2fhess-26-795-2022&partnerID=40&md5=66f6726e36fbd6a6a4b91f850eddf93d","Deep learning has emerged as a useful tool across geoscience disciplines; however, there remain outstanding questions regarding the suitability of unexplored model architectures and how to interpret model learning for regional-scale hydrological modelling. Here we use a convolutional long short-term memory network, a deep learning approach for learning both spatial and temporal patterns, to predict streamflow at 226 stream gauges across southwestern Canada. The model is forced by gridded climate reanalysis data and trained to predict observed daily streamflow between 1980 and 2015. To interpret the model's learning of both spatial and temporal patterns, we introduce a set of experiments with evaluation metrics to track the model's response to perturbations in the input data. The model performs well in simulating daily streamflow over the testing period, with a median Nash-Sutcliffe efficiency (NSE) of 0.68 and 35 % of stations having NSE>0.8. When predicting streamflow, the model is most sensitive to perturbations in the input data prescribed near and within the basins being predicted, demonstrating that the model is automatically learning to focus on physically realistic areas. When uniformly perturbing input temperature time series to obtain relatively warmer and colder input data, the modelled indicator of freshet timing and peak flow changes in accordance with the transition timing from below- to above-freezing temperatures. We also demonstrate that modelled August streamflow in partially glacierized basins is sensitive to perturbations in August temperature, and that this sensitivity increases with glacier cover. The results demonstrate the suitability of a convolutional long short-term memory network architecture for spatiotemporal hydrological modelling, making progress towards interpretable deep learning hydrological models. Copyright: © 2022 Sam Anderson.",,"Brain; Climate models; Convolutional neural networks; Forecasting; Hydrology; Long short-term memory; Memory architecture; Network architecture; Stream flow; Well testing; Geosciences; Hydrological models; Input datas; Learning approach; Memory network; Model learning; Modeling architecture; Regional hydrological model; Regional scale; Spatial and temporal patterns; Convolution; glacier; hydrological modeling; machine learning; peak flow; streamflow; time series; Canada"
"Andert Ed P., Frasher Chuck","Verifiable, autonomous satellite control system",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024926362&partnerID=40&md5=610a358b331b88b035c4d8de43c83391","The authors describe an autonomous embedded expert system for satellite-based electronic control systems using multiple sensors. The autonomous expert control system methodology was developed with emphasis on expert system verification. It was demonstrated with a multisensor satellite thermal control subsystem responsible for real-time health and status monitoring, subsystem commanding, and decision assistance. A propositional logic rule base was used to create a rule structure that can be verified and easily transferred into a hardware implementation. The 400 propositional logic rule demonstration of the thermal control subsystem showed the feasibility and testability of the compact expert system approach for autonomous embedded applications.",,"Artificial Intelligence--Expert Systems; Computer Programming--Program Diagnostics; Control Systems--Space Applications; Autonomous Satellite Control; Command and Control; Expert Control System; Expert System Verification; Multiple Sensors; Rule Based Systems; Satellites"
"Andini M., Ciani E., de Blasio G., D'Ignazio A., Salvestrini V.","Targeting with machine learning: An application to a tax rebate program in Italy","10.1016/j.jebo.2018.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055561772&doi=10.1016%2fj.jebo.2018.09.010&partnerID=40&md5=9c1c11a0b773f7d95f86c5c8df0af0b0","This paper shows how machine learning (ML) methods can be used to improve the effectiveness of public schemes and inform policy decisions. Focusing on a massive tax rebate scheme introduced in Italy in 2014, it shows that the effectiveness of the program would have significantly increased if the beneficiaries had been selected according to a transparent and easily interpretable ML algorithm. Then, some issues in estimating and using ML for the actual implementation of public policies, such as transparency and accountability, are critically discussed. © 2018 Elsevier B.V.","Fiscal stimulus; Machine learning; Prediction; Program evaluation",
"Andoh-Baidoo F.K., Osei-Bryson K.-M.","Exploring the characteristics of Internet security breaches that impact the market value of breached firms","10.1016/j.eswa.2006.01.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750966470&doi=10.1016%2fj.eswa.2006.01.020&partnerID=40&md5=1a44a5bb8d991fdcd16236a75ca34535","The impact of Internet security breaches on firms has been a concern to both researchers and practitioners. One measure of the damage to the breached firm is the observed cumulative abnormal stock market return (CAR) when there is announcement of the attack in the public media. To develop effective Internet security investment strategies for preventing such damage, firms need to understand the factors that lead to the occurrence of CAR. While previous research have involved the use of regression analysis to explore the relationship between firm and attack characteristics and the occurrence of CAR, in this paper we use decision tree (DT) induction to explore this relationship. The results of our DT-based analysis indicate that both attack and firm characteristics determine CAR. While each of our results is consistent with that of at least one previous study, no previous single study has provided evidence that both firm and attack characteristics are determinants of CAR. Further, the DT-based analysis provides an interpretable model in the form of understandable and actionable rules that may be used by decision makers. The DT-based approach thus provides additional insights beyond what may be provided by the regression approach that has been employed in previous research. The paper makes methodological, theoretical and practical contribution to understanding the predictors of damage when a firm is breached. © 2006.","Attack characteristics; Data mining; Decision tree induction; Event study; Exploratory data analysis; Firm characteristics; Internet security breaches; Market value; Regression","Data mining; Decision making; Decision tables; Internet; Inventory control; Regression analysis; Trees (mathematics); Cumulative abnormal stock market return (CAR); Decision tree induction; Firm characteristics; Internet security breaches; Market values; Security of data"
"Andrade R.B., Costa G.A.O.P., Mota G.L.A., Ortega M.X., Feitosa R.Q., Soto P.J., Heipke C.","Evaluation of semantic segmentation methods for deforestation detection in the amazon","10.5194/isprs-archives-XLIII-B3-2020-1497-2020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091163041&doi=10.5194%2fisprs-archives-XLIII-B3-2020-1497-2020&partnerID=40&md5=9758929890ad2bdbc0d7443d01f177cc","Deforestation is a wide-reaching problem, responsible for serious environmental issues, such as biodiversity loss and global climate change. Containing approximately ten percent of all biomass on the planet and home to one tenth of the known species, the Amazon biome has faced important deforestation pressure in the last decades. Devising efficient deforestation detection methods is, therefore, key to combat illegal deforestation and to aid in the conception of public policies directed to promote sustainable development in the Amazon. In this work, we implement and evaluate a deforestation detection approach which is based on a Fully Convolutional, Deep Learning (DL) model: the DeepLabv3+. We compare the results obtained with the devised approach to those obtained with previously proposed DL-based methods (Early Fusion and Siamese Convolutional Network) using Landsat OLI-8 images acquired at different dates, covering a region of the Amazon forest. In order to evaluate the sensitivity of the methods to the amount of training data, we also evaluate them using varying training sample set sizes. The results show that all tested variants of the proposed method significantly outperform the other DL-based methods in terms of overall accuracy and F1-score. The gains in performance were even more substantial when limited amounts of samples were used in training the evaluated methods. © 2020 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives.","Amazon Forest; Change Detection; Deep Learning; DeepLabv3+; Deforestation; Semantic Segmentation","Biodiversity; Climate change; Convolution; Convolutional neural networks; Deep learning; Environmental protection; Semantics; Sustainable development; Biodiversity loss; Convolutional networks; Detection approach; Detection methods; Environmental issues; Global climate changes; Overall accuracies; Semantic segmentation; Deforestation"
"Andre B., Vercauteren T., Buchner A.M., Wallace M.B., Ayache N.","Learning semantic and visual similarity for endomicroscopy video retrieval","10.1109/TMI.2012.2188301","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861873452&doi=10.1109%2fTMI.2012.2188301&partnerID=40&md5=add65083369713096a8484a6ab765589","Content-based image retrieval (CBIR) is a valuable computer vision technique which is increasingly being applied in the medical community for diagnosis support. However, traditional CBIR systems only deliver visual outputs, i.e., images having a similar appearance to the query, which is not directly interpretable by the physicians. Our objective is to provide a system for endomicroscopy video retrieval which delivers both visual and semantic outputs that are consistent with each other. In a previous study, we developed an adapted bag-of-visual-words method for endomicroscopy retrieval, called Dense-Sift, that computes a visual signature for each video. In this paper, we present a novel approach to complement visual similarity learning with semantic knowledge extraction, in the field of in vivo endomicroscopy. We first leverage a semantic ground truth based on eight binary concepts, in order to transform these visual signatures into semantic signatures that reflect how much the presence of each semantic concept is expressed by the visual words describing the videos. Using cross-validation, we demonstrate that, in terms of semantic detection, our intuitive Fisher-based method transforming visual-word histograms into semantic estimations outperforms support vector machine (SVM) methods with statistical significance. In a second step, we propose to improve retrieval relevance by learning an adjusted similarity distance from a perceived similarity ground truth. As a result, our distance learning method allows to statistically improve the correlation with the perceived similarity. We also demonstrate that, in terms of perceived similarity, the recall performance of the semantic signatures is close to that of visual signatures and significantly better than those of several state-of-the-art CBIR methods. The semantic signatures are thus able to communicate high-level medical knowledge while being consistent with the low-level visual signatures and much shorter than them. In our resulting retrieval system, we decide to use visual signatures for perceived similarity learning and retrieval, and semantic signatures for the output of an additional information, expressed in the endoscopist own language, which provides a relevant semantic translation of the visual retrieval outputs. © 2012 IEEE.","Bag-of-visual-words (BoW); content-based image retrieval (CBIR); endomicroscopy; semantic and visual similarity; semantic gap; similarity learning","Bag-of-visual-words; Content-Based Image Retrieval; endomicroscopy; Semantic gap; similarity learning; Visual similarity; Computer vision; Content based retrieval; Diagnosis; Distance education; Semantics; Support vector machines; Visual languages; algorithm; article; artificial intelligence; automated pattern recognition; capsule endoscopy; colon tumor; computer assisted diagnosis; hospital information system; human; image enhancement; image subtraction; information retrieval; methodology; pathology; reproducibility; semantics; sensitivity and specificity; video microscopy; Algorithms; Artificial Intelligence; Capsule Endoscopy; Colonic Neoplasms; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Microscopy, Video; Pattern Recognition, Automated; Radiology Information Systems; Reproducibility of Results; Semantics; Sensitivity and Specificity; Subtraction Technique"
"Andre C.M., Soukoulis C.","Food quality assessed by chemometrics","10.3390/foods9070897","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091295170&doi=10.3390%2ffoods9070897&partnerID=40&md5=9ebe6bb81d8c08b0b96d55459d0a9f00","Food market globalization, food security as well as increasing consumer demand for safe, minimally processed and healthy food impose the need to establish new approaches for identifying and assessing food quality markers. Nowadays, food industry stakeholders are challenged to assure food quality and safety without compromising several prerequisites such as sustainable and ecologically resilient food production, prolonged shelf life, satisfactory sensory quality, enhanced nutritional value and health-promoting properties. In addition, food fraud related to deliberate product mislabeling or economically intended adulteration is of major concern for both industry and regulatory authorities due to cost and public health implications. Notwithstanding the great number of state-of-the-art analytical tools available for quantifying food quality markers, their implementation results in highly complex and big datasets, which are not easily interpretable. In this context, chemometrics e.g., supervised and unsupervised multivariate exploratory analyses, design-of-experiment methodology, univariate or multivariate regression modelling etc., are commonly implemented as part of food process optimization and food quality assessment. In this Special Issue, we aimed to publish innovative research and perspective papers on chemometric-assisted case studies relating to food quality assessment, food authenticity, mathematical modelling and optimization of processes involved in food manufacturing. © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).","Chemometrics; Data mining; Food authenticity; Food product development; Food quality; Foodomics; Process optimization and modelling",
"Andreakis N., D'Aniello S., Albalat R., Patti F.P., Garcia-Fernndez J., Procaccini G., Sordino P., Palumbo A.","Evolution of the nitric oxide synthase family in metazoans","10.1093/molbev/msq179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650483490&doi=10.1093%2fmolbev%2fmsq179&partnerID=40&md5=d4bb31bb42245983cc373bbd1f22a21c","Nitric oxide (NO) is essential to many physiological functions and operates in several signaling pathways. It is not understood how and when the different isoforms of nitric oxide synthase (NOS), the enzyme responsible for NO production, evolved in metazoans. This study investigates the number and structure of metazoan NOS enzymes by genome data mining and direct cloning of Nos genes from the lamprey. In total, 181 NOS proteins are analyzed from 33 invertebrate and 63 vertebrate species. Comparisons among protein and gene structures, combined with phylogenetic and syntenic studies, provide novel insights into how NOS isoforms arose and diverged. Protein domains and gene organization - that is, intron positions and phases - of animal NOS are remarkably conserved across all lineages, even in fast-evolving species. Phylogenetic and syntenic analyses support the view that a proto-NOS isoform was recurrently duplicated in different lineages, acquiring new structural configurations through gains and losses of protein motifs. We propose that in vertebrates a first duplication took place after the agnathan-gnathostome split followed by a paralog loss. A second duplication occurred during early tetrapod evolution, giving rise to the three isoforms - I, II, and III - in current mammals. Overall, NOS family evolution was the result of multiple gene and genome duplication events together with changes in protein architecture. © 2010 The Author.","evolution; intron; nitric oxide synthase; synteny","endothelial nitric oxide synthase; inducible nitric oxide synthase; neuronal nitric oxide synthase; article; comparative study; enzyme structure; enzyme synthesis; gene duplication; gene structure; genome analysis; isoenzyme analysis; lamprey; metazoon; molecular cloning; molecular evolution; phylogenetic tree; protein motif; vertebrate; Animals; Biological Evolution; Databases, Genetic; Enzyme Stability; Evolution, Molecular; Humans; Introns; Isoenzymes; Lampreys; Likelihood Functions; Molecular Sequence Data; Multigene Family; Nitric Oxide Synthase; Phylogeny; Synteny; Animalia; Invertebrata; Mammalia; Metazoa; Petromyzontidae; Tetrapoda; Vertebrata"
"Andreas J., Klein D., Levine S.","Modular multitask reinforcement learning with policy sketches",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048689528&partnerID=40&md5=d560affefbd7769456dcb4202f199e68","We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them-specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor-critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks. © 2017 International Machine Learning Society (IMLS). All rights reserved.",,"Artificial intelligence; Deep learning; Actor critic; Continuous control; Intrinsic motivation; Learning policy; Learning tasks; Reward function; Structural relationship; Subtasks; Reinforcement learning"
"Andreassen A., Feige I., Frye C., Schwartz M.D.","JUNIPR: a framework for unsupervised machine learning in particle physics","10.1140/epjc/s10052-019-6607-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060998624&doi=10.1140%2fepjc%2fs10052-019-6607-9&partnerID=40&md5=4f8f6754b881c736c564704e8ef7a72a","In applications of machine learning to particle physics, a persistent challenge is how to go beyond discrimination to learn about the underlying physics. To this end, a powerful tool would be a framework for unsupervised learning, where the machine learns the intricate high-dimensional contours of the data upon which it is trained, without reference to pre-established labels. In order to approach such a complex task, an unsupervised network must be structured intelligently, based on a qualitative understanding of the data. In this paper, we scaffold the neural network’s architecture around a leading-order model of the physics underlying the data. In addition to making unsupervised learning tractable, this design actually alleviates existing tensions between performance and interpretability. We call the framework Junipr: “Jets from UNsupervised Interpretable PRobabilistic models”. In this approach, the set of particle momenta composing a jet are clustered into a binary tree that the neural network examines sequentially. Training is unsupervised and unrestricted: the network could decide that the data bears little correspondence to the chosen tree structure. However, when there is a correspondence, the network’s output along the tree has a direct physical interpretation. Junipr models can perform discrimination tasks, through the statistically optimal likelihood-ratio test, and they permit visualizations of discrimination power at each branching in a jet’s tree. Additionally, Junipr models provide a probability distribution from which events can be drawn, providing a data-driven Monte Carlo generator. As a third application, Junipr models can reweight events from one (e.g. simulated) data set to agree with distributions from another (e.g. experimental) data set. © 2019, The Author(s).",,
"Andreeva A.V., Gritskov Y.V., Pavlov A.P., Fenvesh T.A.","Artificial Intelligence as a Factor in the Sociodynamics of Daily Order","10.1007/978-3-030-69415-9_38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108512608&doi=10.1007%2f978-3-030-69415-9_38&partnerID=40&md5=63bc540b43dbfb6c6c0c9ca1ba23f0cc","Artificial intelligence is defined by researchers as a component of social and cultural dynamics of daily order. The analysis of available research has shown that the study of the influence of artificial intelligence on society and man is dominated by approaches that seem insufficient and one-sided, especially in the interpretation of its impact on social and cultural processes, daily order. According to the authors, these approaches are based mainly on the principles of technicism, the essence of which is the consideration of artificial intelligence as emancipated from the human mind, from the everyday order and culture of the subject. There is a proprietary approach to the study of artificial intelligence, its social and humanitarian component in the article. A number of opportunities for the effective use of artificial intelligence in solving social and humanitarian problems, including it in the processes of social dynamics of daily order are offered. These possibilities are connected with interaction, mutual complementing (N. Bohr’s complementarity principle) of artificial intellect with culture: the sphere in which a personality is formed that is responsible to society and to itself. It is proven that the sociodynamics of the everyday order, effective ways of social reproduction (autopoiesis) are possible only on the basis of dialogue, mutual complementing (N. Bohr’s complementarity principle) of cultural resources (as a sphere “responsible” for the development of personality) and actually the resources of artificial intelligence. The absence of dialogue leads to the loss of artificial intelligence from the social autopoiesis. Artificial intelligence does not execute the function assigned to it: the constructs of social networks that ensure the reproduction of people as social actors. The problem with artificial intelligence is not that robots and machines take over the human world, but that people themselves give up the role of the actors of their own being voluntarily, entrusting this role to artificial intelligence (robots, machines, Internet options and applications). © 2021, Springer Nature Switzerland AG.","Artificial intelligence; Autopoiesis; Ethoses",
"Andrejevic N., Chen Z., Nguyen T., Fan L., Heiberger H., Zhou L.-J., Zhao Y.-F., Chang C.-Z., Grutter A., Li M.","Elucidating proximity magnetism through polarized neutron reflectometry and machine learning","10.1063/5.0078814","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126906687&doi=10.1063%2f5.0078814&partnerID=40&md5=efd6df5e392db808bb4bd51d908b4982","Polarized neutron reflectometry is a powerful technique to interrogate the structures of multilayered magnetic materials with depth sensitivity and nanometer resolution. However, reflectometry profiles often inhabit a complicated objective function landscape using traditional fitting methods, posing a significant challenge for parameter retrieval. In this work, we develop a data-driven framework to recover the sample parameters from polarized neutron reflectometry data with minimal user intervention. We train a variational autoencoder to map reflectometry profiles with moderate experimental noise to an interpretable, low-dimensional space from which sample parameters can be extracted with high resolution. We apply our method to recover the scattering length density profiles of the topological insulator-ferromagnetic insulator heterostructure Bi2Se3/EuS exhibiting proximity magnetism in good agreement with the results of conventional fitting. We further analyze a more challenging reflectometry profile of the topological insulator-antiferromagnet heterostructure (Bi,Sb)2Te3/Cr2O3 and identify possible interfacial proximity magnetism in this material. We anticipate that the framework developed here can be applied to resolve hidden interfacial phenomena in a broad range of layered systems. © 2022 Author(s).",,"Electric insulators; Machine learning; Magnetic materials; Magnetism; Neutron reflection; Reflectometers; Data driven; Depth sensitivity; Fitting method; Multi-layered; Nanometer resolutions; Objective functions; Parameter retrieval; Polarized neutron reflectometry; Reflectometry; Topological insulators; Reflection"
"Andrés-Andrés A., Gómez-Sánchez E., Bote-Lorenzo M.L.","Incremental rule pruning for fuzzy ARTMAP neural network",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646238949&partnerID=40&md5=1ef8e7eead297723671784c105a6f7df","Fuzzy ARTMAP is capable of incrementally learning interpretable rules. To remove unused or inaccurate rules, a rule pruning method has been proposed in the literature. This paper addresses its limitations when incremental learning is used, and modifies it so that it does not need to store previously learnt samples. Experiments show a better performance, especially in concept drift problems. © Springer-Verlag Berlin Heidelberg 2005.",,"Artificial intelligence; Computer science; Fuzzy control; Fuzzy sets; Learning systems; Logic programming; Concept drift problems; Fuzzy ARTMAP; Learning interpretable rules; Rule pruning method; Neural networks"
"Andresini G., Appice A., Caforio F.P., Malerba D., Vessio G.","ROULETTE: A neural attention multi-output model for explainable Network Intrusion Detection","10.1016/j.eswa.2022.117144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128464559&doi=10.1016%2fj.eswa.2022.117144&partnerID=40&md5=f0c09931977eaa9c3f5d9ee7c990cd15","Network Intrusion Detection (NID) systems are one of the most powerful forms of defense for protecting public and private networks. Most of the prominent methods applied to NID problems consist of Deep Learning methods that have achieved outstanding accuracy performance. However, even though they are effective, these systems are still too complex to interpret and explain. In recent years this lack of interpretability and explainability has begun to be a major drawback of deep neural models, even in NID applications. With the aim of filling this gap, we propose ROULETTE: a method based on a new neural model with attention for an accurate, explainable multi-class classification of network traffic data. In particular, attention is coupled with a multi-output Deep Learning strategy that helps to discriminate better between network intrusion categories. We report the results of extensive experimentation on two benchmark datasets, namely NSL-KDD and UNSW-NB15, which show the beneficial effects of the proposed attention mechanism and multi-output learning strategy on both the accuracy and explainability of the decisions made by the method. © 2022 Elsevier Ltd","Attention; Deep learning; Explainable artificial intelligence; Multi-class classification; Multi-output learning; Network intrusion detection","Classification (of information); Deep learning; Intrusion detection; Attention; Deep learning; Explainable artificial intelligence; Learning strategy; Multi-output; Multi-output learning; Network intrusion detection; Network intrusion detection systems; Neural modelling; Public networks; Classifiers"
"Andresini G., Pendlebury F., Pierazzi F., Loglisci C., Appice A., Cavallaro L.","INSOMNIA: Towards Concept-Drift Robustness in Network Intrusion Detection","10.1145/3474369.3486864","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120893404&doi=10.1145%2f3474369.3486864&partnerID=40&md5=8f83c4092694983bf2b27377a796c874","Despite decades of research in network traffic analysis and incredible advances in artificial intelligence, network intrusion detection systems based on machine learning (ML) have yet to prove their worth. One core obstacle is the existence of concept drift, an issue for all adversary-facing security systems. Additionally, specific challenges set intrusion detection apart from other ML-based security tasks, such as malware detection. In this work, we offer a new perspective on these challenges. We propose INSOMNIA, a semi-supervised intrusion detector which continuously updates the underlying ML model as network traffic characteristics are affected by concept drift. We use active learning to reduce latency in the model updates, label estimation to reduce labeling overhead, and apply explainable AI to better interpret how the model reacts to the shifting distribution. To evaluate INSOMNIA, we extend TESSERACT-a framework originally proposed for performing sound time-Aware evaluations of ML-based malware detectors-to the network intrusion domain. Our evaluation shows that accounting for drifting scenarios is vital for effective intrusion detection systems. © 2021 ACM.","machine learning; network security","Intrusion detection; Learning systems; Machine learning; Malware; Concept drifts; In networks; Intelligence network; Intrusion-Detection; Machine-learning; Network intrusion detection; Network intrusion detection systems; Network traffic analysis; Networks security; On-machines; Network security"
"Andreu-Perez J., Emberson L.L., Kiani M., Filippetti M.L., Hagras H., Rigato S.","Explainable artificial intelligence based analysis for interpreting infant fNIRS data in developmental cognitive neuroscience","10.1038/s42003-021-02534-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115241586&doi=10.1038%2fs42003-021-02534-y&partnerID=40&md5=0ba726f1f2353b868340e1b9fd0548d7","In the last decades, non-invasive and portable neuroimaging techniques, such as functional near infrared spectroscopy (fNIRS), have allowed researchers to study the mechanisms underlying the functional cognitive development of the human brain, thus furthering the potential of Developmental Cognitive Neuroscience (DCN). However, the traditional paradigms used for the analysis of infant fNIRS data are still quite limited. Here, we introduce a multivariate pattern analysis for fNIRS data, xMVPA, that is powered by eXplainable Artificial Intelligence (XAI). The proposed approach is exemplified in a study that investigates visual and auditory processing in six-month-old infants. xMVPA not only identified patterns of cortical interactions, which confirmed the existent literature; in the form of conceptual linguistic representations, it also provided evidence for brain networks engaged in the processing of visual and auditory stimuli that were previously overlooked by other methods, while demonstrating similar statistical performance. © 2021, The Author(s).",,"artificial intelligence; cognitive neuroscience; devices; growth; human; infant; near infrared spectroscopy; neuroimaging; procedures; Artificial Intelligence; Cognitive Neuroscience; Growth; Humans; Infant; Neuroimaging; Spectroscopy, Near-Infrared"
"Andrews P.L.R., Sanger G.J.","Nausea and the quest for the perfect anti-emetic","10.1016/j.ejphar.2013.09.072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892842948&doi=10.1016%2fj.ejphar.2013.09.072&partnerID=40&md5=e22fee7012d49d48313ac150e5297925","The discovery of anti-emetic agents is reviewed to illustrate the large database (&gt;129,000 papers in PubMed) available for potential data mining and to provide a background to the shift in interest to nausea from vomiting. Research on nausea extends to identification of biomarkers for diagnosis/clinical trials and to understanding why nausea is such a common dose-limiting toxicity of diverse therapeutic agents. The lessons learned for translation from animals to humans, from the discovery of the anti-vomiting effects of 5-HT3 and NK1 receptor antagonists, is discussed in terms of the similarities between the emetic pathways and their pharmacology, and also in terms of the limitations of rodent models of ""nausea"" (pica, conditioned taste aversion, conditioned gaping and disgust). The review focuses on the established view that anti-emetics are more efficacious against vomiting than nausea. In particular we examine studies of 5-HT3, NK1 and D2 receptor antagonists, gabapentin and various receptor agonists. The potential for targeting anti-nausea agents is then considered, by targeting mechanisms which correct delayed gastric emptying (prokinetics), the rise in plasma vasopressin (AVP) and/or act at central targets revealed by the growing knowledge of cortical regions activated/inhibited in subjects reporting nausea. Modulation of the projections from the brainstem to the cortical areas responsible for the genesis of the sensation of nausea provides the most likely approach to a target at which an anti-nausea drug could be targeted with the expectation that it would affect nausea from multiple causes. © 2013 Elsevier B.V.","Animal model; Anti-emetic; Nausea; Prokinetic; Vasopressin; Vomiting","antiemetic agent; aprepitant; chlorpromazine; cisplatin; dexamethasone; dolasetron mesilate; domperidone; dopamine 2 receptor blocking agent; dronabinol; droperidol; fluphenazine; gabapentin; ghrelin; granisetron; haloperidol; maropitant; metoclopramide; nabilone; neurokinin 1 receptor antagonist; olanzapine; ondansetron; palonosetron; prochlorperazine; prokinetic agent; ranitidine; serotonin 3 antagonist; serotonin 4 agonist; thiethylperazine; unindexed drug; vasopressin; anesthesia; animal welfare; article; brain cortex; brain region; brain stem; cancer chemotherapy; cancer radiotherapy; chemotherapy induced nausea and vomiting; disease model; drug efficacy; drug targeting; electrogastrography; human; medical research; nausea; nonhuman; postoperative nausea and vomiting; pregnancy; priority journal; seasickness; self report; sensation; signal transduction; stomach emptying; vasopressin blood level; Animal model; Anti-emetic; Nausea; Prokinetic; Vasopressin; Vomiting; Animals; Antiemetics; Disease Models, Animal; Drug Discovery; Humans; Nausea; Translational Medical Research; Vomiting"
"Andrews R., van Dun C.G.J., Wynn M.T., Kratsch W., Röglinger M.K.E., ter Hofstede A.H.M.","Quality-informed semi-automated event log generation for process mining","10.1016/j.dss.2020.113265","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081010515&doi=10.1016%2fj.dss.2020.113265&partnerID=40&md5=d82415aaa25bacdb539899ee536462ce","Process mining, as with any form of data analysis, relies heavily on the quality of input data to generate accurate and reliable results. A fit-for-purpose event log nearly always requires time-consuming, manual pre-processing to extract events from source data, with data quality dependent on the analyst's domain knowledge and skills. Despite much being written about data quality in general, a generalisable framework for analysing event data quality issues when extracting logs for process mining remains unrealised. Following the DSR paradigm, we present RDB2Log, a quality-aware, semi-automated approach for extracting event logs from relational data. We validated RDB2Log's design against design objectives extracted from literature and competing artifacts, evaluated its design and performance with process mining experts, implemented a prototype with a defined set of quality metrics, and applied it in laboratory settings and in a real-world case study. The evaluation shows that RDB2Log is understandable, of relevance in current research, and supports process mining in practice. © 2020 Elsevier B.V.","Data quality; Event log; Log extraction; Process mining","Automation; Data reduction; Quality control; Automated approach; Data quality; Design objectives; Domain knowledge; Event log; Log extractions; Process mining; Reliable results; Data mining"
"Andrienko N., Andrienko G., Adilova L., Wrobel S.","Visual Analytics for Human-Centered Machine Learning","10.1109/MCG.2021.3130314","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123905854&doi=10.1109%2fMCG.2021.3130314&partnerID=40&md5=144409f52551710076354df70147c83a","We introduce a new research area in visual analytics (VA) aiming to bridge existing gaps between methods of interactive machine learning (ML) and eXplainable Artificial Intelligence (XAI), on one side, and human minds, on the other side. The gaps are, first, a conceptual mismatch between ML/XAI outputs and human mental models and ways of reasoning, and second, a mismatch between the information quantity and level of detail and human capabilities to perceive and understand. A grand challenge is to adapt ML and XAI to human goals, concepts, values, and ways of thinking. Complementing the current efforts in XAI towards solving this challenge, VA can contribute by exploiting the potential of visualization as an effective way of communicating information to humans and a strong trigger of human abstractive perception and thinking. We propose a cross-disciplinary research framework and formulate research directions for VA. © 2022 IEEE.","Bridges; Buildings; Computational modeling; Computer science; Human intelligence; Machine learning; Visual analytics","Machine learning; Computational modelling; Human intelligence; Human mind; Information levels; Information quantity; Interactive machine learning; Machine-learning; Mental model; Research areas; Visual analytics; Visualization; artificial intelligence; human; machine learning; Artificial Intelligence; Humans; Machine Learning"
"Andrzejak A., Langner F., Zabala S.","Interpretable models from distributed data via merging of decision trees","10.1109/CIDM.2013.6597210","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885590924&doi=10.1109%2fCIDM.2013.6597210&partnerID=40&md5=7431b9c3c8a19e97d37383f8363d9937","Learning from distributed data becomes increasingly important. Factors contributing to this trend include emergence of data sets exceeding RAM sizes and inherently distributed scenarios such as mobile environments. Also in these cases interpretable models are favored: they facilitate identifying artifacts and understanding the impact of individual variables. Given the distributed environment, even if the individual learner on each site is interpretable, the overall model usually is not (as e.g. in case of voting schemes). To overcome this problem we propose an approach for efficient merging of decision trees (each learned independently) into a single decision tree. The method complements the existing distributed decision trees algorithms by providing interpretable intermediate models and tolerating constraints on bandwidth and RAM size. The latter properties are achieved by trading RAM and communication constraints for accuracy. Our method and the mentioned trade-offs are validated in experiments on real-world data sets. © 2013 IEEE.",,"Communication constraints; Distributed data; Distributed decision; Distributed environments; Intermediate model; Mobile environments; Single decision; Trees algorithm; Artificial intelligence; Commerce; Data mining; Decision trees; Virtual reality; Merging"
"Anelli V.W., Bellogín A., Noia T.D., Donini F.M., Paparella V., Pomo C.","An Analysis of Local Explanation with LIME-RS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136234963&partnerID=40&md5=5a0f66ec4f108012b1e32a7b8580d6fe","Explainable Recommendation has attracted a lot of attention due to a renewed interest in explainable artificial intelligence. In particular, post-hoc approaches have proved to be the most easily applicable ones, since they treat as black boxes the increasingly complex recommendation models. Recent literature has shown that for post-hoc explanations based on local surrogate models, there are problems related to the robustness of the approach itself. This consideration becomes even more relevant in human-related tasks, from transparency or trustworthiness points of view – like recommendation. We show how the behavior of LIME-RS – a classical post-hoc model based on surrogates – is strongly model-dependent and does not prove to be accountable for the explanations generated. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","explainable recommendation; local surrogate model; post-hoc explanation","Black boxes; Explainable recommendation; IS problems; Local surrogate model; Model-based OPC; Post-hoc explanation; Surrogate modeling; Lime"
"Anelli V.W., Bellogín A., Di Noia T., Donini F.M., Paparella V., Pomo C.","Adherence and constancy in LIME-RS explanations for recommendation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116923105&partnerID=40&md5=ce76e3add396931d0a7af4a03fc33ccf","Explainable Recommendation has attracted a lot of attention due to a renewed interest in explainable artificial intelligence. In particular, post-hoc approaches have proved to be the most easily applicable ones to increasingly complex recommendation models, which are then treated as black boxes. The most recent literature has shown that for post-hoc explanations based on local surrogate models, there are problems related to the robustness of the approach itself. This consideration becomes even more relevant in human-related tasks like recommendation. The explanation also has the arduous task of enhancing increasingly relevant aspects of user experience such as transparency or trustworthiness. This paper aims to show how the characteristics of a classical post-hoc model based on surrogates is strongly model-dependent and does not prove to be accountable for the explanations generated. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Explainable recommendation; Local surrogate model; Post-hoc explanation","Lime; Black boxes; Explainable recommendation; IS problems; Local surrogate model; Model-based OPC; Post-hoc explanation; Surrogate modeling; Users' experiences; User interfaces"
"Ang E.T.Y., Nambiar M., Soh Y.S., Tan V.Y.F.","An Interpretable Intensive Care Unit Mortality Risk Calculator","10.1109/EMBC46164.2021.9631058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122535032&doi=10.1109%2fEMBC46164.2021.9631058&partnerID=40&md5=047497600739c38fdb9f55fceec815a8","Mortality risk is a major concern to patients who have just been discharged from the intensive care unit (ICU). Many studies have been directed to construct machine learning models to predict such risk. Although these models are highly accurate, they are less amenable to interpretation and clinicians are typically unable to gain further insights into the patients' health conditions and the underlying factors that influence their mortality risk. In this paper, we use patients' profiles extracted from the MIMIC-III clinical database to construct risk calculators based on different machine learning techniques such as logistic regression, decision trees, random forests, k-nearest neighbors and multilayer perceptrons. We perform an extensive benchmarking study that compares the most salient features as predicted by various methods. We observe a high degree of agreement across the considered machine learning methods; in particular, age, blood urea nitrogen level and the indicator variable - whether the patient is discharged from the cardiac surgery recovery unit are commonly predicted to be the most salient features for determining patients' mortality risks. Our work has the potential to help clinicians interpret risk predictions. © 2021 IEEE.",,"Cardiovascular surgery; Health risks; Intensive care units; Logistic regression; Machine learning; Nearest neighbor search; Patient rehabilitation; Random forests; Urea; Clinical database; Health condition; Highly accurate; Logistics regressions; Machine learning models; Machine learning techniques; Mortality risk; Patient health; Salient features; Underlying factors; Decision trees; factual database; human; intensive care unit; machine learning; statistical model; Databases, Factual; Humans; Intensive Care Units; Logistic Models; Machine Learning; Neural Networks, Computer"
"Ang K.K., Chin Z.Y., Zhang H., Guan C.","Composite filter bank common spatial pattern for motor imagery-based brain-computer interface","10.1109/CCMB.2011.5952108","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961144639&doi=10.1109%2fCCMB.2011.5952108&partnerID=40&md5=7b0affded94747186a0bdc9ef6bf0b31","The Filter Bank Common Spatial Pattern (FBCSP) algorithm employs multiple spatial filters across a bank of band-pass filtered EEC using the CSP algorithm. This is as opposed to the commonly used single spatial filter from band-pass filtered EEC. Hence, the FBCSP yields improved performance in autonomous selection of key temporal-spatial discriminative EEC characteristics in motor imagery-based Brain-Computer Interfaces (MI-BCI). However, the multiple spatial filtering involves multiple estimations of covariance matrices across the different frequency bands. Thus, the use of multiple spatial filters increases the sensitivity of the FBCSP algorithm to noise, artifacts and outliers compared to the CSP algorithm. Furthermore, the multiple spatial patterns are also less interpretable than a single spatial pattern. Hence this paper proposes a Composite FBCSP algorithm that employs a single spatial filter instead of multiple spatial filters. The composite spatial filter is computed from a weighted sum of covariance matrices whereby the weights are determined from the mutual information across selected frequency band. The performance of the Composite FBCSP is compared to the FBCSP on a publicly available dataset and data collected from 5 healthy subjects using session-to-session transfer kappa values on the independent test data. The results revealed improvements in accuracy and interpretability in the spatial patterns. © 2011 IEEE.",,"Band pass; Common spatial patterns; Composite filter; Covariance matrices; Data sets; Different frequency; Healthy subjects; Interpretability; Kappa values; Multiple estimation; Mutual informations; Selected frequency band; Spatial filterings; Spatial filters; Spatial patterns; Test data; Weighted Sum; Algorithms; Artificial intelligence; Covariance matrix; Filter banks; Frequency bands; Frequency estimation; Interfaces (computer); Statistical tests; Brain computer interface"
"Ang Y., Li S., Ong M.E.H., Xie F., Teo S.H., Choong L., Koniman R., Chakraborty B., Ho A.F.W., Liu N.","Development and validation of an interpretable clinical score for early identification of acute kidney injury at the emergency department","10.1038/s41598-022-11129-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129314752&doi=10.1038%2fs41598-022-11129-4&partnerID=40&md5=dee6cda98e006f85cc846ec225a37d79","Acute kidney injury (AKI) in hospitalised patients is a common syndrome associated with poorer patient outcomes. Clinical risk scores can be used for the early identification of patients at risk of AKI. We conducted a retrospective study using electronic health records of Singapore General Hospital emergency department patients who were admitted from 2008 to 2016. The primary outcome was inpatient AKI of any stage within 7 days of admission based on the Kidney Disease Improving Global Outcome (KDIGO) 2012 guidelines. A machine learning-based framework AutoScore was used to generate clinical scores from the study sample which was randomly divided into training, validation and testing cohorts. Model performance was evaluated using area under the curve (AUC). Among the 119,468 admissions, 10,693 (9.0%) developed AKI. 8491 were stage 1 (79.4%), 906 stage 2 (8.5%) and 1296 stage 3 (12.1%). The AKI Risk Score (AKI-RiSc) was a summation of the integer scores of 6 variables: serum creatinine, serum bicarbonate, pulse, systolic blood pressure, diastolic blood pressure, and age. AUC of AKI-RiSc was 0.730 (95% CI 0.714–0.747), outperforming an existing AKI Prediction Score model which achieved AUC of 0.665 (95% CI 0.646–0.679) on the testing cohort. At a cut-off of 4 points, AKI-RiSc had a sensitivity of 82.6% and specificity of 46.7%. AKI-RiSc is a simple clinical score that can be easily implemented on the ground for early identification of AKI and potentially be applied in international settings. © 2022, The Author(s).",,"creatinine; acute kidney failure; female; hospital emergency service; human; male; retrospective study; risk assessment; Acute Kidney Injury; Creatinine; Emergency Service, Hospital; Female; Humans; Male; Retrospective Studies; Risk Assessment"
"Angelini F., Widera P., Mobasheri A., Blair J., Struglics A., Uebelhoer M., Henrotin Y., Marijnissen A.C.A., Kloppenburg M., Blanco F.J., Haugen I.K., Berenbaum F., Ladel C., Larkin J., Bay-Jensen A.C., Bacardit J.","Osteoarthritis endotype discovery via clustering of biochemical marker data","10.1136/annrheumdis-2021-221763","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128431123&doi=10.1136%2fannrheumdis-2021-221763&partnerID=40&md5=f0367f115a29ebdbedd4eb4ed8f9f78d","Objectives Osteoarthritis (OA) patient stratification is an important challenge to design tailored treatments and drive drug development. Biochemical markers reflecting joint tissue turnover were measured in the IMI-APPROACH cohort at baseline and analysed using a machine learning approach in order to study OA-dominant phenotypes driven by the endotype-related clusters and discover the driving features and their disease-context meaning. Method Data quality assessment was performed to design appropriate data preprocessing techniques. The k-means clustering algorithm was used to find dominant subgroups of patients based on the biochemical markers data. Classification models were trained to predict cluster membership, and Explainable AI techniques were used to interpret these to reveal the driving factors behind each cluster and identify phenotypes. Statistical analysis was performed to compare differences between clusters with respect to other markers in the IMI-APPROACH cohort and the longitudinal disease progression. Results Three dominant endotypes were found, associated with three phenotypes: C1) low tissue turnover (low repair and articular cartilage/subchondral bone turnover), C2) structural damage (high bone formation/resorption, cartilage degradation) and C3) systemic inflammation (joint tissue degradation, inflammation, cartilage degradation). The method achieved consistent results in the FNIH/OAI cohort. C1 had the highest proportion of non-progressors. C2 was mostly linked to longitudinal structural progression, and C3 was linked to sustained or progressive pain. Conclusions This work supports the existence of differential phenotypes in OA. The biomarker approach could potentially drive stratification for OA clinical trials and contribute to precision medicine strategies for OA progression in the future. Trial registration number NCT03883568. ©","epidemiology; knee; osteoarthritis","adult; aged; Article; artificial intelligence; biochemistry; bone turnover; cartilage degeneration; classification; cohort analysis; data processing; data quality; disease exacerbation; female; human; inflammation; longitudinal study; machine learning; major clinical study; male; osteoarthritis; osteolysis; pain; prediction; articular cartilage; cluster analysis; knee osteoarthritis; osteolysis; biological marker; Biomarkers; Bone Resorption; Cartilage, Articular; Cluster Analysis; Disease Progression; Humans; Inflammation; Osteoarthritis, Knee"
"Angelini F., Naqvi S.M.","Joint RGB-Pose Based Human Action Recognition for Anomaly Detection Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081789864&partnerID=40&md5=e687b3a223a43266f69cff6249c58c74","Human Action Recognition (HAR) and Human Behaviour Anomaly Detection (HBAD) systems require intelligent and multimodal features extraction for classification. The RGB deep learning based methods represent the state-of-the-art for HAR and HBAD. On the other hand, human poses extracted by popular RGB-based detectors have shown promising results for posture-level HAR and HBAD. However, both modalities present limitations, e.g. the RGB-based methods are difficult to extract explainable features relevant to generalisation, especially when contextual data is dominant. Furthermore, human poses cannot model complex human actions, i.e. involving objects or with high contextual information. To overcome the above limitations, three Joint RGB-Pose based multimodal networks are proposed. Combinations of CNNs, 3DCNNs, RNNs, MLSTMs, and ResNet-152 pre-trained CNN networks are exploited. The proposed three methods for joint learning are compared with the correspondent RGB-based and Pose-based methods, in the context of HAR for HBAD applications. Experimental results are provided on the challenging datasets UCF101 and MPOSE2019, showing promising results in terms of recognition accuracy and processing time. © 2019 ISIF-International Society of Information Fusion.","Human Action Recognition; Joint learning; Pose; RGB","Behavioral research; Deep learning; Information fusion; Learning systems; Contextual information; Human-action recognition; Joint learning; Learning-based methods; Multimodal features; Multimodal network; Pose; Recognition accuracy; Anomaly detection"
"Angelino E., Larus-Stone N., Alabi D., Seltzer M., Rudin C.","Learning certifiably optimal rule lists","10.1145/3097983.3098047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029035317&doi=10.1145%2f3097983.3098047&partnerID=40&md5=5c30814c0a4e9de68253bf7de34faf5c","We present the design and implementation of a custom discrete optimization technique for building rule lists over a categorical feature space. Our algorithm provides the optimal solution, with a certificate of optimality. By leveraging algorithmic bounds, efficient data structures, and computational reuse, we achieve several orders of magnitude speedup in time and a massive reduction of memory consumption. We demonstrate that our approach produces optimal rule lists on practical problems in seconds. This framework is a novel alternative to CART and other decision tree methods. © 2017 Copyright held by the owner/author(s).","Decision trees; Interpretable models; Optimization; Rule lists","Decision trees; Optimization; Categorical features; Decision tree method; Design and implementations; Discrete optimization techniques; Efficient data structures; Orders of magnitude; Practical problems; Rule lists; Data mining"
"Angelopoulos N., Chatzipli A., Nangalia J., Maura F., Campbell P.J.","Bayesian networks elucidate complex genomic landscapes in cancer","10.1038/s42003-022-03243-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127512933&doi=10.1038%2fs42003-022-03243-w&partnerID=40&md5=6b54a50d181eef69c0cd39ca743b110b","Bayesian networks (BNs) are disciplined, explainable Artificial Intelligence models that can describe structured joint probability spaces. In the context of understanding complex relations between a number of variables in biological settings, they can be constructed from observed data and can provide a guiding, graphical tool in exploring such relations. Here we propose BNs for elucidating the relations between driver events in large cancer genomic datasets. We present a methodology that is specifically tailored to biologists and clinicians as they are the main producers of such datasets. We achieve this by using an optimal BN learning algorithm based on well established likelihood functions and by utilising just two tuning parameters, both of which are easy to set and have intuitive readings. To enhance value to clinicians, we introduce (a) the use of heatmaps for families in each network, and (b) visualising pairwise co-occurrence statistics on the network. For binary data, an optional step of fitting logic gates can be employed. We show how our methodology enhances pairwise testing and how biologists and clinicians can use BNs for discussing the main relations among driver events in large genomic cohorts. We demonstrate the utility of our methodology by applying it to 5 cancer datasets revealing complex genomic landscapes. Our networks identify central patterns in all datasets including a central 4-way mutual exclusivity between HDR, t(4,14), t(11,14) and t(14,16) in myeloma, and a 3-way mutual exclusivity of three major players: CALR, JAK2 and MPL, in myeloproliferative neoplasms. These analyses demonstrate that our methodology can play a central role in the study of large genomic cancer datasets. © 2022, The Author(s).",,"algorithm; artificial intelligence; Bayes theorem; genetics; genomics; human; neoplasm; Algorithms; Artificial Intelligence; Bayes Theorem; Genomics; Humans; Neoplasms"
"Angelov P., Soares E.","Towards Deep Machine Reasoning: A Prototype-based Deep Neural Network with Decision Tree Inference","10.1109/SMC42975.2020.9282812","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098848408&doi=10.1109%2fSMC42975.2020.9282812&partnerID=40&md5=c4695e91969ae14d0dfdd7dc8b9eb264","In this paper we introduce the DMR - a prototype-based method and network architecture for deep learning which is using a decision tree (DT)- based inference and synthetic data to balance the classes. It builds upon the recently introduced xDNN method addressing more complex multi-class problems, specifically when classes are highly imbalanced. DMR moves away from a direct decision based on all classes towards a layered DT of pair-wise class comparisons. In addition, it forces the prototypes to be balanced between classes regardless of possible class imbalances of the training data. It has two novel mechanisms, namely i) using a DT to determine the winning class label, and ii) balancing the classes by synthesizing data around the prototypes determined from the available training data. As a result, we improved significantly the performance of the resulting fully explainable DNN as evidenced on the well know benchmark problem Caltech-101. Furthermore, we also achieved high results in terms of accuracy for the well known Caltech-256 dataset, as well as surpassed the results of other approaches on Faces-1999 problem. In summary, we propose a new approach specifically advantageous for imbalanced multi-class problems on well known hard benchmark datasets. Moreover, DMR offers full explainability, does not require GPUs and can continue to learn from new data by adding new prototypes preserving the previous ones but not requiring full retraining. © 2020 IEEE.",,"Balancing; Benchmarking; Decision trees; Deep learning; Network architecture; Neural networks; Program processors; Trees (mathematics); Bench-mark problems; Benchmark datasets; Class imbalance; Decision-based; Multi-class problems; New approaches; Synthetic data; Training data; Deep neural networks"
"Angelov P., Soares E.","Towards explainable deep neural networks (xDNN)","10.1016/j.neunet.2020.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087911322&doi=10.1016%2fj.neunet.2020.07.010&partnerID=40&md5=a42afcd99290340d0b66abc201b768db","In this paper, we propose an elegant solution that is directly addressing the bottlenecks of the traditional deep learning approaches and offers an explainable internal architecture that can outperform the existing methods, requires very little computational resources (no need for GPUs) and short training times (in the order of seconds). The proposed approach, xDNN is using prototypes. Prototypes are actual training data samples (images), which are local peaks of the empirical data distribution called typicality as well as of the data density. This generative model is identified in a closed form and equates to the pdf but is derived automatically and entirely from the training data with no user- or problem-specific thresholds, parameters or intervention. The proposed xDNN offers a new deep learning architecture that combines reasoning and learning in a synergy. It is non-iterative and non-parametric, which explains its efficiency in terms of time and computational resources. From the user perspective, the proposed approach is clearly understandable to human users. We tested it on challenging problems as the classification of different lighting conditions for driving scenes (iROADS), object detection (Caltech-256, and Caltech-101), and SARS-CoV-2 identification via computed tomography scan (COVID CT-scans dataset). xDNN outperforms the other methods including deep learning in terms of accuracy, time to train and offers an explainable classifier. © 2020 Elsevier Ltd","Deep-learning; Explainable AI; Interpretability; Prototype-based models","Classification (of information); Computerized tomography; Deep neural networks; Iterative methods; Learning systems; Network architecture; Neural networks; Object detection; Program processors; Computational resources; Computed tomography scan; Generative model; Internal architecture; Learning approach; Learning architectures; Lighting conditions; User perspectives; Deep learning; article; classifier; deep learning; deep neural network; human; illumination; nonhuman; reasoning; Severe acute respiratory syndrome coronavirus 2; x-ray computed tomography; Coronavirus infection; diagnostic imaging; image processing; pandemic; procedures; virus pneumonia; Coronavirus Infections; Deep Learning; Humans; Image Processing, Computer-Assisted; Pandemics; Pneumonia, Viral; Tomography, X-Ray Computed"
"Angelov P., Gu X.","A cascade of deep learning fuzzy rule-based image classifier and SVM","10.1109/SMC.2017.8122697","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043377418&doi=10.1109%2fSMC.2017.8122697&partnerID=40&md5=aabdbcda4b23447af3fb70f578653e14","In this paper, a fast, transparent, self-evolving, deep learning fuzzy rule-based (DLFRB) image classifier is proposed. This new classifier is a cascade of the recently introduced DLFRB classifier called MICE and an auxiliary SVM. The DLFRB classifier serves as the main engine and can identify a number of human interpretable fuzzy rules through a very short, transparent, highly parallelizable training process. The SVM based auxiliary plays the role of a conflict resolver when the DLFRB classifier produces two highly confident labels for a single image. Only the fundamental image transformation techniques (rotation, scaling and segmentation) and feature descriptors (GIST and HOG) are used for pre-processing and feature extraction, but the proposed approach significantly outperforms the state-of-art methods in terms of both time and precision. Numerical experiments based on a handwritten digits recognition problem are used to demonstrate the highly accurate and repeatable performance of the proposed approach. © 2017 IEEE.","Cascade classifiers; Deep learning; Fuzzy rule-based classifier; Handwritten digits recognition; SVM","Character recognition; Classification (of information); Cybernetics; Fuzzy inference; Fuzzy rules; Image classification; Image segmentation; Cascade classifiers; Fuzzy rule-based classifier; Handwritten digits recognition; Image transformations; Interpretable fuzzy rules; Learning fuzzy rules; Numerical experiments; State-of-art methods; Deep learning"
"Angelov P., Gu X.","MICE: Multi-layer multi-model images classifier ensemble","10.1109/CYBConf.2017.7985788","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027844569&doi=10.1109%2fCYBConf.2017.7985788&partnerID=40&md5=964f8cec519c3d9559e3811f30046605","In this paper, a new type of fast deep learning (DL) network for handwriting recognition is proposed. In contrast to the existing DL networks, the proposed approach has a clearly interpretable structure that is entirely data- driven and free from user- or problem-specific assumptions. Firstly, the fundamental image transformation techniques (rotation and scaling) used by other existing DL methods are used to improve the generalization. The commonly used descriptors are then used to extract the global features from the training set and based on them a bank/ensemble of zero order AnYa type fuzzy rule-based (FRB) models is built in parallel through the recently introduced Autonomous Learning Multiple Model (ALMMo) method. The final decision about the winning class label is made by a committee on the basis of the fuzzy mixture of the trained zero order ALMMo models. The training of the proposed MICE system is very efficient and highly parallelizable. It significantly outperforms the best-known methods in terms of time and is on par in terms of precision/accuracy. Critically, it offers a high level of interpretability, transparency of the classification model, full repeatability (unlike the methods that use probabilistic elements) of the results. Moreover, it allows an evolving scenario whereby the data is provided in an incremental, online manner and the system structure evolves in parallel with the classification which opens opportunities for online and real-time applications (on a sample by sample basis). Numerical examples from the well-known handwritten digits recognition problem (MNIST) were used and the results demonstrated the very high repeatable performance after a very short training process exhibiting high level of interpretability, transparency. © 2017 IEEE.",,"Cybernetics; Fuzzy inference; Image classification; Mammals; Transparency; Autonomous learning; Classification models; Classifier ensembles; Handwriting recognition; Handwritten digits recognition; Image transformations; Real-time application; System structures; Character recognition"
"Angelov P., Gu X., Principe J.","Fast feedforward non-parametric deep learning network with automatic feature extraction","10.1109/IJCNN.2017.7965899","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027862674&doi=10.1109%2fIJCNN.2017.7965899&partnerID=40&md5=d3f2e15a4b3dd48ede50f1a6df6127f1","In this paper, a new type of feedforward non-parametric deep learning network with automatic feature extraction is proposed. The proposed network is based on human-understandable local aggregations extracted directly from the images. There is no need for any feature selection and parameter tuning. The proposed network involves nonlinear transformation, segmentation operations to select the most distinctive features from the training images and builds RBF neurons based on them to perform classification with no weights to train. The design of the proposed network is very efficient (computation and time wise) and produces highly accurate classification results. Moreover, the training process is parallelizable, and the time consumption can be further reduced with more processors involved. Numerical examples demonstrate the high performance and very short training process of the proposed network for different applications. © 2017 IEEE.","Deep learning; Fast training; Feature extraction; Feedforward; Learning network","Deep learning; Extraction; Image segmentation; Learning systems; Mathematical transformations; Automatic feature extraction; Classification results; Feed-Forward; Learning network; Non-linear transformations; Parameter-tuning; Time consumption; Training process; Feature extraction"
"Angelov P., Filev D.P., Kasabov N.","Evolving Intelligent Systems: Methodology and Applications","10.1002/9780470569962","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891585323&doi=10.1002%2f9780470569962&partnerID=40&md5=5990eec4bc97e4b110c17b21872bb106","From theory to techniques, the first all-in-one resource for EIS There is a clear demand in advanced process industries, defense, and Internet and communication (VoIP) applications for intelligent yet adaptive/evolving systems. Evolving Intelligent Systems is the first self- contained volume that covers this newly established concept in its entirety, from a systematic methodology to case studies to industrial applications. Featuring chapters written by leading world experts, it addresses the progress, trends, and major achievements in this emerging research field, with a strong emphasis on the balance between novel theoretical results and solutions and practical real-life applications. Explains the following fundamental approaches for developing evolving intelligent systems (EIS): the Hierarchical Prioritized Structure the Participatory Learning Paradigm the Evolving Takagi-Sugeno fuzzy systems (eTS+) the evolving clustering algorithm that stems from the well-known Gustafson-Kessel offline clustering algorithm Emphasizes the importance and increased interest in online processing of data streams Outlines the general strategy of using the fuzzy dynamic clustering as a foundation for evolvable information granulation Presents a methodology for developing robust and interpretable evolving fuzzy rule-based systems Introduces an integrated approach to incremental (real-time) feature extraction and classification Proposes a study on the stability of evolving neuro-fuzzy recurrent networks Details methodologies for evolving clustering and classification Reveals different applications of EIS to address real problems in areas of: evolving inferential sensors in chemical and petrochemical industry learning and recognition in robotics Features downloadable software resources Evolving Intelligent Systems is the one-stop reference guide for both theoretical and practical issues for computer scientists, engineers, researchers, applied mathematicians, machine learning and data mining experts, graduate students, and professionals. © 2010 Institute of Electrical and Electronics Engineers.",,
"Angelov P.P., Soares E.A., Jiang R., Arnold N.I., Atkinson P.M.","Explainable artificial intelligence: an analytical review","10.1002/widm.1424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110049592&doi=10.1002%2fwidm.1424&partnerID=40&md5=f15cac4f7a392e14767297ab686721d6","This paper provides a brief analytical review of the current state-of-the-art in relation to the explainability of artificial intelligence in the context of recent advances in machine learning and deep learning. The paper starts with a brief historical introduction and a taxonomy, and formulates the main challenges in terms of explainability building on the recently formulated National Institute of Standards four principles of explainability. Recently published methods related to the topic are then critically reviewed and analyzed. Finally, future directions for research are suggested. This article is categorized under: Technologies > Artificial Intelligence Fundamental Concepts of Data and Knowledge > Explainable AI. © 2021 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.","black-box models; deep learning; explainable AI; machine learning; prototype-based models; surrogate models","Deep learning; Analytical reviews; Fundamental concepts; National institute of standards; State of the art; Advanced Analytics"
"Angelov P.P., Gu X.","Brief Introduction to Computational Intelligence","10.1007/978-3-030-02384-3_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055331182&doi=10.1007%2f978-3-030-02384-3_3&partnerID=40&md5=851901e9d9592129ca9e525442f5e379","This chapter provides a detailed introduction to the basic concepts and the general principles of the fuzzy sets and systems theory. Three major types of FRB systems are also covered and their differences are analyzed. The design of FRB systems is also covered. This chapter further moves on to the ANNs, which include the feedforward neural networks and three types of deep learning models. Both of the FRB systems and the ANNs have been proven universal approximators and can be designed based on the data. FRB systems have transparent, human-interpretable internal representation and can take advantage of the human domain expert knowledge. They are excellent in dealing with uncertainties, and they can self-organize, self-update both the structures and parameters in an online, dynamic environment. While ANNs are excellent in providing high precisions in most cases, they are fragile when facing new data patterns. They are typical examples of “black box” systems, their training process is usually limited to offline mode and requires huge amount of computation resources and data. © 2019, Springer Nature Switzerland AG.",,
"Angelov P.P., Gu X.","Deep rule-based classifier with human-level performance and characteristics","10.1016/j.ins.2018.06.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049325171&doi=10.1016%2fj.ins.2018.06.048&partnerID=40&md5=5e096b24dc1c803078ef972f9b09b8a7","In this paper, a new type of multilayer rule-based classifier is proposed and applied to image classification problems. The proposed approach is entirely data-driven and fully automatic. It is generic and can be applied to various classification and prediction problems, but in this paper we focus on image processing, in particular. The core of the classifier is a fully interpretable, understandable, self-organized set of IF…THEN… fuzzy rules based on the prototypes autonomously identified by using a one-pass type training process. The classifier can self-evolve and be updated continuously without a full retraining. Due to the prototype-based nature, it is non-parametric; its training process is non-iterative, highly parallelizable and computationally efficient. At the same time, the proposed approach is able to achieve very high classification accuracy on various benchmark datasets surpassing most of the published methods, be comparable with the human abilities. In addition, it can start classification from the first image of each class in the same way as humans do, which makes the proposed classifier suitable for real-time applications. Numerical examples of benchmark image processing demonstrate the merits of the proposed approach. © 2018 Elsevier Inc.","Deep learning; Fuzzy rule based classifiers; Non-iterative; Non-parametric; Self-evolving structure","Classification (of information); Deep learning; Fuzzy inference; Fuzzy rules; Iterative methods; Classification accuracy; Computationally efficient; Fuzzy rule-based classifier; Human-level performance; Non-iterative; Non-parametric; Real-time application; Rule-based classifier; Image classification"
"Angenent M.N., Barata A.P., Takes F.W.","Large-scale machine learning for business sector prediction","10.1145/3341105.3374084","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083034311&doi=10.1145%2f3341105.3374084&partnerID=40&md5=a6167b929544cbbe62bb77f066e7da72","In this study we use machine learning to perform explainable business sector prediction from financial statements. Financial statements are a valuable source of information on the financial state and performance of firms. Recently, large-scale data on financial statements has become available in the form of open data sets. Previous work on such data mainly focused on predicting fraud and bankruptcy. In this paper we devise a model for business sector prediction, which has several valuable applications, including automated error and fraud detection. In addition, such a predictive model may help in completing similar datasets with missing sector information. The proposed method employs a supervised learning approach based on random forests that addresses business sector prediction as a classification task. Using a dataset from the Netherlands Chamber of Commerce, containing over 1.5 million financial statements from Dutch companies, we created an adequately-performing model for business sector prediction. By assessing which features are instrumental in the final classification model, we found that a small number of attributes is crucial for predicting the majority of business sectors. Interestingly, in some cases the presence or absence of a feature was more important than the value itself. The resulting insights may also prove useful in accounting, where the relation between financial statements and characteristics of the company is a frequently studied topic. © 2020 Owner/Author.","Business sector prediction; Data mining; Explainable machine learning; Financial statements","Crime; Decision trees; Finance; Machine learning; Open Data; Chamber of Commerce; Classification models; Classification tasks; Financial statements; Large scale data; Large-scale machine learning; Predictive modeling; Supervised learning approaches; Forecasting"
"Angenent-Mari N.M., Garruss A.S., Soenksen L.R., Church G., Collins J.J.","A deep learning approach to programmable RNA switches","10.1038/s41467-020-18677-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092267308&doi=10.1038%2fs41467-020-18677-1&partnerID=40&md5=62fa8a380b8a386e7059219f075a04c4","Engineered RNA elements are programmable tools capable of detecting small molecules, proteins, and nucleic acids. Predicting the behavior of these synthetic biology components remains a challenge, a situation that could be addressed through enhanced pattern recognition from deep learning. Here, we investigate Deep Neural Networks (DNN) to predict toehold switch function as a canonical riboswitch model in synthetic biology. To facilitate DNN training, we synthesize and characterize in vivo a dataset of 91,534 toehold switches spanning 23 viral genomes and 906 human transcription factors. DNNs trained on nucleotide sequences outperform (R2 = 0.43–0.70) previous state-of-the-art thermodynamic and kinetic models (R2 = 0.04–0.15) and allow for human-understandable attention-visualizations (VIS4Map) to identify success and failure modes. This work shows that deep learning approaches can be used for functionality predictions and insight generation in RNA synthetic biology. © 2020, The Author(s).",,"nucleotide; RNA; transcription factor; transcription factor; algorithm; biology; detection method; pattern recognition; RNA; Article; binding site; controlled study; deep learning; deep neural network; feed forward neural network; human; long short term memory network; multilayer perceptron; nonhuman; nucleotide sequence; protein secondary structure; ribosome; riboswitch; synthetic biology; translation initiation; virus genome; genetic engineering; genetics; information processing; kinetics; procedures; riboswitch; thermodynamics; Datasets as Topic; Deep Learning; Genetic Engineering; Genome, Viral; Humans; Kinetics; Riboswitch; Synthetic Biology; Thermodynamics; Transcription Factors"
"Angiulli F., Fassetti F., Nisticò S., Palopoli L.","Outlier Explanation Through Masking Models","10.1007/978-3-031-15740-0_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137990775&doi=10.1007%2f978-3-031-15740-0_28&partnerID=40&md5=1c349573fb6adfeea772ca9b2b414a45","Given a database and one single anomalous data point, the Outlying Aspect Mining problem consists in explaining the abnormality of that data point w.r.t. the data population stored in the input database. Thus, the problem requires the discovery of the sets of attributes and associated values that account for the abnormality of a data point within a given data set. In this setting, the abnormality of the data point at hand is stated beforehand, e.g., as the result of some outlier detection techniques (which, for the most part, do not provide information about why the selected data points are actually anomalous). This paper proposes a solution to the OAM problem exploiting a deep learning architecture. Besides explaining the input data point abnormality by singling out the smallest set of pairs attribute-value justifying it, our technique also provides new values for those attributes that would transform the input outlier into an inlier. Several experiments are also presented that assess the effectiveness of our approach. © 2022, Springer Nature Switzerland AG.","Deep learning; Explainable artificial intelligence; Outlier aspect mining","Deep learning; Aspect mining; Data population; Data set; Datapoints; Deep learning; Explainable artificial intelligence; Masking models; Mining problems; Outlier aspect mining; Outlier Detection; Population statistics"
"Angiulli F., Fassetti F., Nisticò S.","Finding Local Explanations Through Masking Models","10.1007/978-3-030-91608-4_46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126427433&doi=10.1007%2f978-3-030-91608-4_46&partnerID=40&md5=60669b37731e5f6b0fc6ccdb06e88fd9","Among the XAI (eXplainable Artificial Intelligence) techniques, local explanations are witnessing increasing interest due to the user need to trust specific black-box decisions. In this work we explore a novel local explanation approach appliable to any kind of classifier based on generating masking models. The idea underlying the method is to learn a transformation of the input leading to a novel instance able to confuse the black-box and simultaneously minimizing dissimilarity with the instance to explain. The transformed instance then highlights the parts of the input that need to be (de-)emphasized and acts as an explanation for the local decision. We clarify differences with existing local explanation methods and experiment our approach on different image classification scenarios, pointing out advantages and peculiarities of the proposal. © 2021, Springer Nature Switzerland AG.","Deep learning; eXplainable Artificial Intelligence; Local explanations for machine learning","Artificial intelligence techniques; Black boxes; Deep learning; Explainable artificial intelligence; Images classification; Learn+; Local decisions; Local explanation for machine learning; Masking models; User need; Deep learning"
"Angiulli F., Fassetti F., Nisticò S.","Local Interpretable Classifier Explanations with Self-generated Semantic Features","10.1007/978-3-030-88942-5_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118130831&doi=10.1007%2f978-3-030-88942-5_31&partnerID=40&md5=da22659e62fd2b8b4abd38292406ea5b","Explaining predictions of classifiers is a fundamental problem in eXplainable Artificial Intelligence (XAI). LIME (for Local Interpretable Model-agnostic Explanations) is a recently proposed XAI technique able to explain any classifier by providing an interpretable model which approximates the black-box locally to the instance under consideration. In order to build interpretable local models, LIME requires the user to explicitly define a space of interpretable components, also called artefacts, associated with the input instance. To reconstruct local black-box behaviour, the instance neighbourhood is explored by generating instance neighbours as random subsets of the provided artefacts. In this work we note that the above depicted strategy has two main flaws: first, it requires user intervention in the definition of the interpretable space and, second, the local explanation is limited to be expressed in terms the user-provided artefacts. To overcome these two limitations, in this work we propose S-LIME, a variant of the basic LIME method exploiting unsupervised learning to replace user-provided interpretable components with self-generated semantic features. This characteristics enables our approach to sample instance neighbours in a more semantic-driven fashion and to greatly reduce the bias associated with explanations. We demonstrate the applicability and effectiveness of our proposal in the text classification domain. Comparison with the baseline highlights superior quality of the explanations provided adopting our strategy. © 2021, Springer Nature Switzerland AG.","Adversarial autoencoders; Explainable machine learning; Local interpretable explanations","Classification (of information); Lime; Machine learning; Semantics; Adversarial autoencoder; Auto encoders; Black boxes; Explainable machine learning; Local interpretable explanation; Local model; Neighbourhood; Random subsets; Semantic features; User intervention; Text processing"
"Angiulli F., Greco G., Palopoli L.","Discovering anomalies in evidential knowledge by logic programming","10.1007/978-3-540-30227-8_48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-22944486369&doi=10.1007%2f978-3-540-30227-8_48&partnerID=40&md5=5f31534d80cf6ec0ab6879d44def7184","The development of effective knowledge discovery techniques has become in the recent few years a very active research area due to the important impact it has in several relevant application areas. One interesting task thereof is that of singling out anomalous individuals from a given population, e.g., to detect rare events in time-series analysis settings, or to identify objects whose behavior is deviant w.r.t. a codified standard set of ""social"" rules. Such exceptional individuals are usually referred to as outliers in the literature. Recently, outlier detection has also emerged as a relevant KR&R problem in the context of default logic [2]. For instance, detection algorithms can be used by rational agents to single out those observations that are anomalous to some extent w.r.t. their own, trustable knowledge about the world encoded in the form of a suitable logic theory. In this paper, we formally state the concept of outliers in the context of logic programming. Besides the novel formalization we propose which helps in shedding some lights on the real nature of outliers, a major contribution of the work lies in the exploitation of a minimality criteria in their detection. Moreover, the computational complexity of outlier detection problems arising in this novel setting is thoroughly investigated and accounted for in the paper as well. Finally, we also propose a rewriting algorithm that transforms any outlier problem into an equivalent answer set computation problem, thereby making outlier computation effective and realizable on top of any answer set engine. © Springer-Verlag Berlin Heidelberg 2004.",,"Artificial intelligence; Data handling; Logic programming; Time series analysis; Algorithms; Computational methods; Information analysis; Knowledge engineering; Mathematical models; Problem solving; Application area; Computation problems; Detection algorithm; Knowledge discovery techniques; Minimality criteria; Outlier Detection; Rational agents; Rewriting algorithm; Statistics; Logic programming; Knowledge discovery; Logic programs; Minimality criteria; Rewriting algorithm"
"Anglin N.L., Amri A., Kehel Z., Ellis D.","A case of need: Linking traits to genebank accessions","10.1089/bio.2018.0033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055079038&doi=10.1089%2fbio.2018.0033&partnerID=40&md5=85a4b3bcb68c28993e7cba5aeebd7c45","Genebanks are responsible for collecting, maintaining, characterizing, documenting, and distributing plant genetic resources for research, education, and breeding purposes. The rationale for requests of plant materials varies highly from areas of anthropology, social science, small-holder farmers, the commercial sector, rehabilitation of degraded systems, all the way to crop improvement and basic research. Matching ""the right"" accessions to a particular request is not always a straightforward process especially when genetic resource collections are large and the user does not already know which accession or even which species they want to study. Some requestors have limited knowledge of the crop; therefore, they do not know where to begin and thus, initiate the search by consultation with crop curators to help direct their request to the most suitable germplasm. One way to enhance the use of genebank material and aid in the selection of genetic resources is to have thoroughly cataloged agronomic, biochemical, genomic, and other traits linked to genebank accessions. In general, traits of importance to most users include genotypes that thrive under various biotic and abiotic stresses, morphological traits (color, shape, size of fruits), plant architecture, disease resistance, nutrient content, yield, and crop specific quality traits. In this review, we discuss methods for linking traits to genebank accessions, examples of linked traits, and some of the complexities involved, while reinforcing why it is critical to have well characterized accessions with clear trait data publicly available. © Noelle L. Anglin, et al., 2018; Published by Mary Ann Liebert, Inc. 2018.","FIGS; genebanks; genetic resources; GWAS; molecular markers; trait association","molecular marker; abiotic stress; accuracy; algorithm; anthropology; Article; biobank; correlation analysis; crop improvement; disease resistance; genebank; genetic resource; genome-wide association study; genomics; genotype; germplasm; machine learning; morphological trait; nonhuman; nutrient content; phenotype; plant; priority journal; sensitivity and specificity; sociology; algorithm; conservational seed bank; genetic variation; genetics; metabolism; physiological stress; plant development; Algorithms; Genetic Variation; Genome-Wide Association Study; Genotype; Plant Development; Plants; Seed Bank; Stress, Physiological"
"Angrick S., Bals B., Hastrich N., Kleissl M., Schmidt J., Doskoc V., Molitor L., Friedrich T., Katzmann M.","Towards explainable real estate valuation via evolutionary algorithms","10.1145/3512290.3528801","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135214921&doi=10.1145%2f3512290.3528801&partnerID=40&md5=610ed04ccb659b5288cd5ff86e374e10","Human lives are increasingly influenced by algorithms, which therefore need to meet higher standards not only in accuracy but also with respect to explainability. This is especially true for high-stakes areas such as real estate valuation. Unfortunately, the methods applied there often exhibit a trade-off between accuracy and explainability. One explainable approach is case-based reasoning (CBR), where each decision is supported by specific previous cases. However, such methods can be wanting in accuracy. The unexplainable machine learning approaches are often observed to provide higher accuracy but are not scrutable in their decision-making. In this paper, we apply evolutionary algorithms (EAs) to CBR predictors in order to improve their performance. In particular, we deploy EAs to the similarity functions (used in CBR to find comparable cases), which are fitted to the data set at hand. As a consequence, we achieve higher accuracy than state-of-the-art deep neural networks (DNNs), while keeping interpretability and explainability. These results stem from our empirical evaluation on a large data set of real estate offers where we compare known similarity functions, their EA-improved counterparts, and DNNs. Surprisingly, DNNs are only on par with standard CBR techniques. However, using EA-learned similarity functions does yield an improved performance. © 2022 ACM.","case-based reasoning; Evolutionary algorithms; neural networks; real estate valuation","Decision making; Deep neural networks; Economic and social effects; Evolutionary algorithms; Casebased reasonings (CBR); High standards; High-accuracy; Human lives; Machine learning approaches; Neural-networks; Performance; Real estate valuations; Similarity functions; Trade off; Case based reasoning"
"Anguita-Ruiz A., Segura-Delgado A., Alcalá R., Aguilera C.M., Alcalá-Fdez J.","EXplainable Artificial Intelligence (XAI) for the identification of biologically relevant gene expression patterns in longitudinal human studies, insights from obesity research","10.1371/journal.pcbi.1007792","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084026406&doi=10.1371%2fjournal.pcbi.1007792&partnerID=40&md5=941c869264fa9d7d8c5ceef2f48a5eb5","Until date, several machine learning approaches have been proposed for the dynamic modeling of temporal omics data. Although they have yielded impressive results in terms of model accuracy and predictive ability, most of these applications are based on “Black-box” algorithms and more interpretable models have been claimed by the research community. The recent eXplainable Artificial Intelligence (XAI) revolution offers a solution for this issue, were rule-based approaches are highly suitable for explanatory purposes. The further integration of the data mining process along with functional-annotation and pathway analyses is an additional way towards more explanatory and biologically soundness models. In this paper, we present a novel rule-based XAI strategy (including pre-processing, knowledge-extraction and functional validation) for finding biologically relevant sequential patterns from longitudinal human gene expression data (GED). To illustrate the performance of our pipeline, we work on in vivo temporal GED collected within the course of a long-term dietary intervention in 57 subjects with obesity (GSE77962). As validation populations, we employ three independent datasets following the same experimental design. As a result, we validate primarily extracted gene patterns and prove the goodness of our strategy for the mining of biologically relevant gene-gene temporal relations. Our whole pipeline has been gathered under open-source software and could be easily extended to other human temporal GED applications. © 2020 Anguita-Ruiz et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Application programs; Artificial intelligence; Data mining; Gene expression; Mammals; Nutrition; Open source software; Open systems; 'omics'; Black box algorithms; Dynamics models; Gene Expression Data; Gene expression patterns; Human study; Machine learning approaches; Model predictive; Modeling accuracy; Predictive abilities; Pipelines; fatty acid synthase; glycerol 3 phosphate acyltransferase; Notch3 receptor; scatter factor; transcriptome; Article; artificial intelligence; data analysis software; data extraction; data mining; diet therapy; Egfl6 gene; gene; gene expression; genetic analysis; human; in vivo study; information processing; low calorie diet; major clinical study; medical research; Notch3 gene; obesity; omics; prediction; very low calorie diet; algorithm; artificial intelligence; biology; gene expression; gene expression profiling; genetic database; genetics; longitudinal study; machine learning; obesity; procedures; software; Algorithms; Artificial Intelligence; Computational Biology; Data Mining; Databases, Genetic; Gene Expression; Gene Expression Profiling; Humans; Longitudinal Studies; Machine Learning; Obesity; Software; Transcriptome"
"Angulo C., Gonzalez-Abril L., Raya C., Ortega J.A.","A Proposal to Evolving Towards Digital Twins in Healthcare","10.1007/978-3-030-45385-5_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085198631&doi=10.1007%2f978-3-030-45385-5_37&partnerID=40&md5=a9c00cc3ebc7cfc96810e7aaf7a73da6","The main objective in this proposal is to orchestrate an ecosystem of manipulation of reliable and safe data, applied to the field of health, specifically lung cancer, by introducing the creation of digital twins for personalised healthcare about the behaviour of this disease on patients. Digital twins is a very popular and novel approach in digitisation units in industry which will be used by both kind of experts: (i) data analysts, who will design expert recommender systems and extract knowledge – explainable Artificial Intelligence (AI); and (ii) professionals in medicine, who will consume that knowledge generated with their research for better diagnosis. This knowledge generation/extraction process will work in the form of a lifelong learning system by iterative and continuous use. The produced software platform will be abstracted so it can be applied like a general purpose service tool in other domains of knowledge, specially health and industry. Furthermore, a rule extraction module will be made available for explainability issues. © Springer Nature Switzerland AG 2020.","Digital twin; GAN; Healthcare; Proposal","Bioinformatics; Biomedical engineering; Diagnosis; Health care; Learning systems; Service industry; Data analysts; Design-expert; Digitisation; Knowledge generations; Life long learning; Rule extraction; Service tools; Software platforms; Digital twin"
"Angulo C., Ortega J.A., Gonzalez-Abril L.","Towards a healthcare digital twin","10.3233/FAIA190139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085040224&doi=10.3233%2fFAIA190139&partnerID=40&md5=945eebedb2ae4906600d3a023d3eb8a4","The main aim of this short position paper is to introduce a proposal to orchestrate an ecosystem of manipulation of reliable and safe data, applied to the field of health, proposing the creation of digital twins for personalised healthcare. These will be used by both kind of experts: (i) data analysts, who will design expert recommender systems and extract knowledge (explainable AI); and (ii) professionals in medicine, who will consume that knowledge generated with their research for better diagnosis. Knowledge generation / extraction processes will work in the form of a lifelong learning system by iterative and continuous use. The produced platform would be abstracted so it can be applied like a general purpose service tool in other domains. A rule extraction module would be also made available. © 2019 The authors and IOS Press. All rights reserved.","Anonymization; Decision support system; Digital twin; Healthcare","Artificial intelligence; Diagnosis; Digital twin; Health care; Data analysts; Design-expert; Knowledge generations; Life long learning; Rule extraction; Service tools; Short position; Learning systems"
"Anh N.K., Toi N.K., Van Linh N.","An Interpretable method for text summarization based on simplicial non-negative matrix factorization","10.1145/2676585.2676604","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962505125&doi=10.1145%2f2676585.2676604&partnerID=40&md5=00d61e491e4a379b5c8b7c34ee6a5c4d","Automatic text summarization plays an important role in information retrieval and text mining. Furthermore, it provides an useful solution to the information overload problem. In this paper, we propose a simplicial NMF-based unsuper-vised generic document summarization method which can inherit some advantages of simplicial NMF such as easy in-terpretability, low complexity, convexity and sparsity. By focusing on the major topics contained within every sentence as well as entire document, our method generates better summaries with less repetition. The effectiveness of our method is proved by experimental results. On the summarization performance, our approach obtains mostly higher ROUGE scores than NMF-based method. Copyright 2014 ACM.","Maximal marginal relevance; Simplicial non-negative matrix factorization; Text summarization","Data mining; Matrix algebra; Natural language processing systems; Text processing; Automatic text summarization; Document summarization; Information overloads; Maximal marginal relevance; Nonnegative matrix factorization; Text mining; Text summarization; Factorization"
"Aniceto N., Bonifácio V.D.B., Guedes R.C., Martinho N.","Exploring the Chemical Space of Urease Inhibitors to Extract Meaningful Trends and Drivers of Activity","10.1021/acs.jcim.2c00150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133344616&doi=10.1021%2facs.jcim.2c00150&partnerID=40&md5=266adced7b9d93849c0ce8453ee6c27c","Blocking the catalytic activity of urease has been shown to have a key role in different diseases as well as in different agricultural applications. A vast array of molecules have been tested against ureases of different species, but the clinical translation of these compounds has been limited due to challenges of potency, chemical and metabolic stability as well as promiscuity against other proteins. The design and development of new compounds greatly benefit from insights from previously tested compounds; however, no large-scale studies surveying the urease inhibitors' chemical space exist that can provide an overview of developed compounds to data. Therefore, given the increasing interest in developing new compounds for this target, we carried out a comprehensive analysis of the activity landscape published so far. To do so, we assembled and curated a data set of compounds tested against urease. To the best of our knowledge, this is the largest data set of urease inhibitors to date, composed of 3200 compounds of diverse structures. We characterized the data set in terms of chemical space coverage, molecular scaffolds, distribution with respect to physicochemical properties, as well as temporal trends of drug development. Through these analyses, we highlighted different substructures and functional groups responsible for distinct activity and inactivity against ureases. Furthermore, activity cliffs were assessed, and the chemical space of urease inhibitors was compared to DrugBank. Finally, we extracted meaningful patterns associated with activity using a decision tree algorithm. Overall, this study provides a critical overview of urease inhibitor research carried out in the last few decades and enabled finding underlying SAR patterns such as under-reported chemical functional groups that contribute to the overall activity. With this work, we propose different rules and practical implications that can guide the design or selection of novel compounds to be screened as well as lead optimization. © 2022 American Chemical Society.",,"Catalyst activity; Chemical stability; Data mining; Decision trees; Blockings; Chemical space; Clinical translation; Comprehensive analysis; Data set; Design and Development; Large datasets; Large-scale studies; Metabolic stability; Urease inhibitors; Physicochemical properties; enzyme inhibitor; urease; Enzyme Inhibitors; Urease"
"Anilkumar C., Sunitha N.C., Harikrishna, Devate N.B., Ramesh S.","Advances in integrated genomic selection for rapid genetic gain in crop improvement: a review","10.1007/s00425-022-03996-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138464541&doi=10.1007%2fs00425-022-03996-y&partnerID=40&md5=89f15b10b4068b57d4ddf2d537a8deaf","Main conclusion: Genomic selection and its importance in crop breeding. Integration of GS with new breeding tools and developing SOP for GS to achieve maximum genetic gain with low cost and time. Abstract: The success of conventional breeding approaches is not sufficient to meet the demand of a growing population for nutritious food and other plant-based products. Whereas, marker assisted selection (MAS) is not efficient in capturing all the favorable alleles responsible for economic traits in the process of crop improvement. Genomic selection (GS) developed in livestock breeding and then adapted to plant breeding promised to overcome the drawbacks of MAS and significantly improve complicated traits controlled by gene/QTL with small effects. Large-scale deployment of GS in important crops, as well as simulation studies in a variety of contexts, addressed G × E interaction effects and non-additive effects, as well as lowering breeding costs and time. The current study provides a complete overview of genomic selection, its process, and importance in modern plant breeding, along with insights into its application. GS has been implemented in the improvement of complex traits including tolerance to biotic and abiotic stresses. Furthermore, this review hypothesises that using GS in conjunction with other crop improvement platforms accelerates the breeding process to increase genetic gain. The objective of this review is to highlight the development of an appropriate GS model, the global open source network for GS, and trans-disciplinary approaches for effective accelerated crop improvement. The current study focused on the application of data science, including machine learning and deep learning tools, to enhance the accuracy of prediction models. Present study emphasizes on developing plant breeding strategies centered on GS combined with routine conventional breeding principles by developing GS-SOP to achieve enhanced genetic gain. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Accelerated crop improvement; Cross-validation; Genetic gain; Integrated GS; SOP for GS","genetic selection; genetics; genomics; phenotype; plant breeding; plant genome; Genome, Plant; Genomics; Phenotype; Plant Breeding; Selection, Genetic"
"Anis K., Zakia H., Mohamed D., Jeffrey C.","Detecting depression severity by interpretable representations of motion dynamics","10.1109/FG.2018.00116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049391259&doi=10.1109%2fFG.2018.00116&partnerID=40&md5=75f2459a0102fb192b795efc7da1caf6","Recent breakthroughs in deep learning using automated measurement of face and head motion have made possible the first objective measurement of depression severity. While powerful, deep learning approaches lack interpretability. We developed an interpretable method of automatically measuring depression severity that uses barycentric coordinates of facial landmarks and a Lie-algebra based rotation matrix of 3D head motion. Using these representations, kinematic features are extracted, preprocessed, and encoded using Gaussian Mixture Models (GMM) and Fisher vector encoding. A multi-class SVM is used to classify the encoded facial and head movement dynamics into three levels of depression severity. The proposed approach was evaluated in adults with history of chronic depression. The method approached the classification accuracy of state-of-the-art deep learning while enabling clinically and theoretically relevant findings. The velocity and acceleration of facial movement strongly mapped onto depression severity symptoms consistent with clinical data and theory. © 2018 IEEE.","Barycentric coordinates; Depression Severity Assessment; Face and head dynamics; Lie algebra","Algebra; Computational mechanics; Dynamics; Gesture recognition; Automated measurement; Barycentric coordinates; Classification accuracy; Gaussian Mixture Model; Interpretable representation; Lie Algebra; Objective measurement; Severity assessments; Deep learning"
"Anish P.R., Joshi V., Sainani A., Ghaisas S.","Towards enhanced accountability in complying with healthcare regulations","10.1109/SEH.2019.00012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072911037&doi=10.1109%2fSEH.2019.00012&partnerID=40&md5=06861c60cc13ff47a69aeda4e19f8ea4","The healthcare ecosystem is highly complex. It is composed of multiple stakeholders with intersecting interests. The healthcare regulations such as Health Insurance Portability and Accountability Act, much like the systems they protect, are complex and often difficult to interpret. Regulations contain obligations that must be fulfilled by healthcare systems that form the backbone of modern healthcare. In this paper, we present a model for extracting and deconstructing obligations. The Obligation Model allows for capturing the essence of obligations in terms of their attributes such as Responsible entity, Trigger, Action, Deadline and Reference. We augment the extracted obligations with auxiliary information present in regulation documents and provide an ownership based rendering of the deconstructed obligation in an HTML format. This helps in establishing an explicit ownership of obligations and contributes towards enhancing accountability of stakeholders towards fulfilling the obligations. The rendering will be useful for building compliant healthcare systems by making the legal text more comprehensible for system designers. © 2019 IEEE.","Extraction; Healthcare; HIPAA; Machine Learning; Obligation; Ownership based rendering","Extraction; Health care; Health insurance; Learning systems; Software engineering; Auxiliary information; Health insurance portability and accountability acts; Health-care system; HIPAA; Multiple stakeholders; Obligation; Ownership based rendering; System designers; Laws and legislation"
"Anjaiah Gujjary V., Saxena A.","A neural network approach for data masking","10.1016/j.neucom.2011.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953035470&doi=10.1016%2fj.neucom.2011.01.002&partnerID=40&md5=4660d0d95fad94ca4336a20a723d3cdf","In this letter we present a neural network based data masking solution, in which the database information remains internally consistent yet is not inadvertently exposed in an interpretable state. The system differs from the classic data masking in the sense that it can understand the semantics of the original data and mask it using a neural network which is a priori trained by some rules. Our adaptive data masking (ADM) concentrates on data masking techniques such as shuffling, substitution, masking and number variance in an intelligent fashion with the help of adaptive neural network. The very nature of being adaptive makes data masking easier and content agnostic, and thus finds place in various vertical domains and systems. © 2011 Elsevier B.V.","Adaptive data masking (ADM); Data privacy; Neural network","Adaptive neural networks; Data masking; Database information; Intelligent fashion; Data privacy; Semantics; Neural networks; article; artificial intelligence; artificial neural network; data extraction; data masking; information processing; information storage; priority journal; privacy"
"Anjana P.S., Wankar R., Rao C.R.","Design of a cloud brokerage architecture using fuzzy rough set technique","10.1007/978-3-319-69456-6_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034222040&doi=10.1007%2f978-3-319-69456-6_5&partnerID=40&md5=db4e9e153ee4a75d0328a2e0160d034c","Cloud computing offers numerous services to the cloud consumers such as infrastructure, platform, software, etc. Due to the vast diversity in available cloud services from the user point of view, it leads to several challenges to rank and select the potential cloud service. One of the plausible solutions for the problem can be obtained with the use of Rough Set Theory (RST) and available in the literature. Unfortunately, Rough Set Theory cannot deal with numerical values. One of the classical solutions to this problem can be obtained by using Fuzzy Rough Set. To the best of our knowledge, there is no working Fuzzy-Rough Set based brokerage architecture available which is used for minimizing attributes, search space and for ranking the service providers. In this paper, we proposed a Fuzzy Rough Set based Cloud Brokerage (FRSCB) architecture, which is responsible for service selection based on consumers Quality of Service (QoS) request. We propose to use Fuzzy Rough Set Theory (FRST) to minimize the number of attributes and searching space. We also did the QoS attribute categorization to identify functional and non-functional requirements and behavior of the attributes (static/dynamic). Finally, we develop an algorithm that recommends potential cloud services to the cloud consumers. © Springer International Publishing AG 2017.","Cloud brokerage; Cloud computing; Cloud service provider ranking; Fuzzy Rough Set; Quality of Service; Reduct","Artificial intelligence; Cloud computing; Computation theory; Computer architecture; Distributed database systems; Quality of service; Set theory; Telecommunication services; Web services; Classical solutions; Cloud service providers; Fuzzy rough set theory; Fuzzy-rough sets; Non-functional requirements; Reduct; Rough set theory (RST); Service selection; Rough set theory"
"Anjomshoae S., Omeiza D., Jiang L.","Context-based image explanations for deep neural networks","10.1016/j.imavis.2021.104310","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115994867&doi=10.1016%2fj.imavis.2021.104310&partnerID=40&md5=1a2bda269b078b44769a01541c183711","With the increased use of machine learning in decision-making scenarios, there has been a growing interest in explaining and understanding the outcomes of machine learning models. Despite this growing interest, existing works on interpretability and explanations have been mostly intended for expert users. Explanations for general users have been neglected in many usable and practical applications (e.g., image tagging, caption generation). It is important for non-technical users to understand features and how they affect an instance-specific prediction to satisfy the need for justification. In this paper, we propose a model-agnostic method for generating context-based explanations aiming for general users. We implement partial masking on segmented components to identify the contextual importance of each segment in scene classification tasks. We then generate explanations based on feature importance. We present visual and text-based explanations: (i) saliency map presents the pertinent components with a descriptive textual justification, (ii) visual map with a color bar graph showing the relative importance of each feature for a prediction. Evaluating the explanations using a user study (N = 50), we observed that our proposed explanation method visually outperformed existing gradient and occlusion based methods. Hence, our proposed explanation method could be deployed to explain models’ decisions to non-expert users in real-world applications. © 2021 The Authors","Contextual importance; DNNs; Explainable AI; Visual explanations","Decision making; User interfaces; Context-based; Contextual importance; Decisions makings; DNN; Expert users; Explainable AI; Image tagging; Interpretability; Machine learning models; Visual explanation; Deep neural networks"
"Anjomshoae S., Jiang L., Främling K.","Visual Explanations for DNNs with Contextual Importance","10.1007/978-3-030-82017-6_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113329479&doi=10.1007%2f978-3-030-82017-6_6&partnerID=40&md5=522c1163a9542ae1c0337c79b2a3d093","Autonomous agents and robots with vision capabilities powered by machine learning algorithms such as Deep Neural Networks (DNNs) are taking place in many industrial environments. While DNNs have improved the accuracy in many prediction tasks, it is shown that even modest disturbances in their input produce erroneous results. Such errors have to be detected and dealt with for making the deployment of DNNs secure in real-world applications. Several explanation methods have been proposed to understand the inner workings of these models. In this paper, we present how Contextual Importance (CI) can make DNN results more explainable in an image classification task without peeking inside the network. We produce explanations for individual classifications by perturbing an input image through over-segmentation and evaluating the effect on a prediction score. Then the output highlights the most contributing segments for a prediction. Results are compared with two explanation methods, namely mask perturbation and LIME. The results for the MNIST hand-written digit dataset produced by the three methods show that CI provides better visual explainability. © 2021, Springer Nature Switzerland AG.","Contextual importance; Deep learning; Explainable artificial intelligence; Image classification","Autonomous agents; Deep neural networks; Forecasting; Image segmentation; Industrial robots; Learning algorithms; Lime; Machine learning; Industrial environments; Input image; Over segmentation; Prediction tasks; Real-world; Vision capability; Multi agent systems"
"Anjomshoae S., Calvaresi D., Najjar A., Främling K.","Explainable agents and robots: Results from a systematic literature review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076471241&partnerID=40&md5=d033776219d3d7943f545e105cbe555c","Humans are increasingly relying on complex systems that heavily adopts Artificial Intelligence (AI) techniques Such systems are employed in a growing number of domains, and making them explainable is an impelling priority Recently, the domain of explainable Artificial Intelligence (XAI) emerged with the aims of fostering transparency and trustworthiness Several reviews have been conducted Nevertheless, most of them deal with data-driven XAI to overcome the opaqueness of black-box algorithms Contributions addressing goal-driven XAI (e.g., explainable agency for robots and agents) are still missing This paper aims at filling this gap, proposing a Systematic Literature Review The main findings are (I) a considerable portion of the papers propose conceptual studies, or lack evaluations or tackle relatively simple scenarios; (H) almost all of the studied papers deal with robots/agents explaining their behaviors to the human users, and very few works addressed inter-robot (inter-agent) explainability Finally, iit) while providing explanations to non-expert users has been outlined as a necessity, only a few works addressed the issues of personalization and context-awareness. © 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org) Ail rights reserved.","Autonomous agents; Explainable ai; Goal-based xai; Human-robot interaction",
"Anjomshoae S., Främling K., Najjar A.","Explanations of black-box model predictions by contextual importance and utility","10.1007/978-3-030-30391-4_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072851529&doi=10.1007%2f978-3-030-30391-4_6&partnerID=40&md5=59cf646cfe5df107ea6c06d05648ba84","The significant advances in autonomous systems together with an immensely wider application domain have increased the need for trustable intelligent systems. Explainable artificial intelligence is gaining considerable attention among researchers and developers to address this requirement. Although there is an increasing number of works on interpretable and transparent machine learning algorithms, they are mostly intended for the technical users. Explanations for the end-user have been neglected in many usable and practical applications. In this work, we present the Contextual Importance (CI) and Contextual Utility (CU) concepts to extract explanations that are easily understandable by experts as well as novice users. This method explains the prediction results without transforming the model into an interpretable one. We present an example of providing explanations for linear and non-linear models to demonstrate the generalizability of the method. CI and CU are numerical values that can be represented to the user in visuals and natural language form to justify actions and explain reasoning for individual instances, situations, and contexts. We show the utility of explanations in car selection example and Iris flower classification by presenting complete (i.e. the causes of an individual prediction) and contrastive explanation (i.e. contrasting instance against the instance of interest). The experimental results show the feasibility and validity of the provided explanation methods. © Springer Nature Switzerland AG 2019.","Black-box models; Contextual importance; Contextual utility; Contrastive explanations; Explainable AI","Autonomous agents; Forecasting; Intelligent agents; Intelligent systems; Learning algorithms; Machine learning; Visual languages; Autonomous systems; Black-box model; Contextual importance; Contextual utility; Contrastive explanations; Individual prediction; Natural languages; Non-linear model; Multi agent systems"
"Anjum A., Ilyas M.U.","Activity recognition using smartphone sensors","10.1109/CCNC.2013.6488584","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875973665&doi=10.1109%2fCCNC.2013.6488584&partnerID=40&md5=7a4e40ebc562e0db901b0ad7fb264807","Motion sensor embedded smartphones have provided a new platform for activity inference. These sensors, initially used for cell phone feature enhancement, are now being used for a variety of applications. Providing cell phone users information about their own physical activity in an understandable format can enable users to make more informed and healthier lifestyle choices. In this work, we built a smartphone application which tracks users' physical activities and provide feedback requiring no user input during routine operation. The application reports estimates of the calories burned, broken up by physical activities. Detectable physical activities include walking, running, climbing stairs, descending stairs, driving, cycling and being inactive. We evaluated a number of classification algorithms from the area of Machine Learning, including Naïve Bayes, Decision Tree, K-Nearest Neighbor and Support Vector Machine classifiers. For training and verification of classifiers, we collected a dataset of 510 activity traces using cell phone sensors. We developed a smartphone app that performs activity recognition that does not require any user intervention. The classifier implemented in the Android app performs at an average true positives rate of greater than 95%, false positives rate of less than 1.5% and an ROC area of greater than 98%. © 2013 IEEE.",,"Activity inference; Activity recognition; Classification algorithm; Feature enhancement; K-nearest neighbors; Smart-phone applications; Support vector machine classifiers; User intervention; Decision trees; Mobile phones; Pattern recognition; Signal encoding; Smartphones; Stairs; Telecommunication equipment; Sensors"
"Anjum M.M., Iqbal S., Hamelin B.","ANUBIS: A Provenance Graph-Based Framework for Advanced Persistent Threat Detection","10.1145/3477314.3507097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130414802&doi=10.1145%2f3477314.3507097&partnerID=40&md5=7153283f1fd9a5e9f13dc2565ef6f119","We present ANUBIS, a highly effective machine learning-based APT detection system. Our design philosophy for ANUBIS involves two principal components. Firstly, we intend ANUBIS to be effectively utilized by cyber-response teams. Therefore, prediction explainability is one of the main focuses of ANUBIS design. Secondly, ANUBIS uses system provenance graphs to capture causality and thereby achieves high detection performance. At the core of the predictive capability of ANUBIS, there is a Bayesian Neural Network that can tell how confident it is in its predictions. We evaluate ANUBIS against a recent APT dataset (DARPA OpTC) and show that ANUBIS can detect malicious activity akin to APT campaigns with high accuracy. Moreover, ANUBIS learns about high-level patterns that allow it to explain its predictions to threat analysts. The high predictive performance with explainable attack story reconstruction makes ANUBIS an effective tool to use for enterprise cyber defense. © 2022 ACM.","advanced persistent threat; bayesian neural network; provenance graph analysis","Graphic methods; Network security; Neural networks; Advanced persistent threat; Bayesian neural networks; Design philosophy; Detection performance; Detection system; Graph analysis; Graph-based; Principal Components; Provenance graph analyse; Threat detection; Forecasting"
"Anjum O., Almasri M., Xiong J., Hwu W.-M.","Phrasescope: An effective and unsupervised framework for mining high quality phrases",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120958377&partnerID=40&md5=9116bef9b7519d14e581cd5a6f71982f","Phrase mining is one of the fundamental NLP tasks that can have significant impact on the efficacy of many downstream applications. Many supervised and unsupervised phrase mining approaches have been proposed. Some rely on linguistic analyzers, and others are language agnostic. A daunting challenge in this task is to distinguish quality phrases from noise phrases, which tightly coexists with quality phrases in the entire frequency spectrum. Most existing approaches to phrase mining, however, rely on frequency-based statistics, hence suffer from quality loss. In this paper, we propose an unsupervised phrase mining framework, “PhraseScope”, which consists of a sequence of filters, namely cohesion, domain, and graph filters, to remove noise phrase. Each filter is responsible for removing noise phrase of particular characteristics. Collectively, our proposed filters are capable of detecting and removing noise phrases effectively while preserving quality phrases. Our results show significant improvement in both recall and precision over state-of-the-art frameworks when tested on three different domains of datasets. © 2021 by SIAM.",,"Data mining; Space division multiple access; Different domains; Downstream applications; Frequency spectra; High quality; Quality loss; Recall and precision; REmove noise; Removing noise; State of the art; Filtration"
"Anker A.S., Kjær E.T.S., Juelsholt M., Christiansen T.L., Skjærvø S.L., Jørgensen M.R.V., Kantor I., Sørensen D.R., Billinge S.J.L., Selvan R., Jensen K.M.Ø.","Extracting structural motifs from pair distribution function data of nanostructures using explainable machine learning","10.1038/s41524-022-00896-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139230506&doi=10.1038%2fs41524-022-00896-3&partnerID=40&md5=8a75b500535ad336b9ffa2dd5ec08d09","Characterization of material structure with X-ray or neutron scattering using e.g. Pair Distribution Function (PDF) analysis most often rely on refining a structure model against an experimental dataset. However, identifying a suitable model is often a bottleneck. Recently, automated approaches have made it possible to test thousands of models for each dataset, but these methods are computationally expensive and analysing the output, i.e. extracting structural information from the resulting fits in a meaningful way, is challenging. Our Machine Learning based Motif Extractor (ML-MotEx) trains an ML algorithm on thousands of fits, and uses SHAP (SHapley Additive exPlanation) values to identify which model features are important for the fit quality. We use the method for 4 different chemical systems, including disordered nanomaterials and clusters. ML-MotEx opens for a type of modelling where each feature in a model is assigned an importance value for the fit quality based on explainable ML. © 2022, The Author(s).",,"Distribution functions; Neutron scattering; Statistical tests; Automated approach; Fit quality; Machine-learning; Materials structure; Pair distribution functions; Pair-distribution function analysis; Shapley; Structural information; Structural motifs; Structure models; Machine learning"
"Anlauf S., Haghofer A., Dirnberger K., Winkler S.","Data-based herb contamination prediction and harvest recommendation","10.46354/i3m.2020.foodops.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097945706&doi=10.46354%2fi3m.2020.foodops.001&partnerID=40&md5=b05f7f0ab1c1238ad3cbc3110f828080","The quality of freshly harvested herbs is heavily influenced by multiple factors, namely weather conditions, harvesting, transport, drying, storage, and many more. Our main goal here is to identify models that are able to predict spore contaminations on different types of herbs on the basis of these factors as well as to find optimal processing parameters, which shall lead to lower contaminations of herbs as well as lower costs for contamination prevention represents. The here presented workflow utilizes two different approaches, which in combination shall lead to a reliable contamination prediction and prevention mechanism. For the prediction part we learn ensembles of machine learning models using the processing parameters as features to predict the risk for spore contamination a priori of labor analysis data. Using tree-based modelling algorithms we already achieved a spore contamination prediction accuracy of 86.21% for the herb nettle. In Addition to that, we use descriptive statistics to provide information on the relevant parameters which could be responsible for the occurred contamination. Here we already achieve a p-value smaller than 0.01 for a few processing parameters. In the future we want to expand this workflow by improving the modelling process using different modelling algorithms. Additionally, we are working on an online life system, which combine these two methods, to not only present a farmer the information whether a contamination is probably, but also provide him the information which processing parameters lead to a contamination and how they should be affected to lower the risk. © 2020 The Authors. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY-NC-ND) license (https://creativecommons.org/licenses/by-nc-nd/4.0/).","Applied statistics; Contamination classification; Data preprocessing; Machine learning","Digital storage; Forecasting; Harvesting; Image segmentation; Risk assessment; Descriptive statistics; Machine learning models; Modelling process; Multiple factors; Optimal processing; Prediction accuracy; Processing parameters; Tree-based; Contamination"
"Anna M.","Explainable AI","10.3233/FAIA190100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084972619&doi=10.3233%2fFAIA190100&partnerID=40&md5=dcf9185575d08c9b4d0e20c084e7f021",[No abstract available],"Decision systems; Explainable AI; Machine Learning",
"Annajjar W., Alnasrallah A.M., Alrikabi H.A.","Preparation and analytic of intelligence big data for smart systems","10.1166/jctn.2019.8397","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077528018&doi=10.1166%2fjctn.2019.8397&partnerID=40&md5=baeab94920ec0ae69a4a7fcf66ecce5d","It is already true that smart system involving big data has drawn massive attention from researchers in analytic, decision makers, intelligence in smart city or system. As the speed of Information Technology (IT) and internet developing, become necessary to come up smart system meets all requirements of modern life. Smart system make the life of human beings more comfortable and easy. However people can get so much interest and highly useful benefits from using big data in smart system. A proposed new scientific paradigm is born in this study to get the advantage and avoid the disadvantage of existing smart systems. Some Important structure illustrated in this study including triad main issues that control any smart system such as big data that responsible to make avenues to success smart system. Valuable insight comes from big data should analytic and control under process before manipulate in intelligence phase to get right decision in regimes. There is no doubt that competition in the future in field of big data will open the horizon to evolve the smart system. This paper is aimed to illustrate a close up view about using the modern technologies that currently evolve with amazing acceleration like intelligence and big data utilities. Challenging of these three issues big data, intelligence and analytic are adopted to find the opportunities of integrate smart system that dealing with hazardous government data clouding in such system. State-of-the-art discussed in this paper and useful recommendation been put up within conclusion to overcome the problems regarding designing smart system. Copyright © 2019 American Scientific Publishers All rights reserved","Analytic of Big Data; Artificial Intelligence; Big Data; Intelligence; Smart System",
"Annamoradnejad I.","Requirements for automating moderation in community question-answering websites","10.1145/3511430.3511458","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125638860&doi=10.1145%2f3511430.3511458&partnerID=40&md5=e47138d57b8a0ea78dc70822fe93259f","In recent years, community Q&A websites have attracted many users and have become reliable sources among experts from various fields. These platforms have specific rules to maintain their content quality in addition to general user agreements. Due to the vast expanse of these systems in terms of the number of users and posts, manual checking and verification of new contents by the administrators and official moderators are not feasible, and these systems require scalable solutions. In major Q&A networks, the current strategy is to use crowdsourcing with reliance on reporting systems. This strategy has serious problems, including the slow handling of violations, the loss of new and experienced users' time, the low quality of user reports, and discouraging feedback to new users. While this is a great opportunity to utilize machine-learning approaches to provide automated recommender systems and classification models, there are specific non-functional requirements or aspects related to these software systems that need to be introduced and incorporated in the design of a new system. In this short paper, I pinpoint three key aspects: (1) Any good approach should consider specific attributes and features related to context, (2) Automated mechanisms should be proposed according to the highly evolving content of Q&A websites, and (3) Decisions are best to accompany clear and justifiable explanations. Furthermore, a technical conceptual model is proposed by considering these aspects and related approaches. This is a part of a research project where the final goal is to provide accurate, adaptable, efficient, and explainable solutions for automating moderation actions in Q&A websites. © 2022 ACM.","Community question-answering; Content moderation; Expert systems; Q&A website; Recommender systems","Expert systems; Recommender systems; 'current; Community question answering; Content moderation; Contents qualities; Low qualities; Manual checking; Q&A website; Reporting systems; Scalable solution; User agreements; Websites"
"Annapragada A.V., Donaruma M.M., Annapragada A.V., Starosolski Z.A.","A natural language processing and deep learning approach to identify child abuse from pediatric electronic medical records","10.1371/journal.pone.0247404","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102077957&doi=10.1371%2fjournal.pone.0247404&partnerID=40&md5=210f0035692d7e22efa8d3581c14e3b5","Child physical abuse is a leading cause of traumatic injury and death in children. In 2017, child abuse was responsible for 1688 fatalities in the United States, of 3.5 million children referred to Child Protection Services and 674,000 substantiated victims. While large referral hospitals maintain teams trained in Child Abuse Pediatrics, smaller community hospitals often do not have such dedicated resources to evaluate patients for potential abuse. Moreover, identification of abuse has a low margin of error, as false positive identifications lead to unwarranted separations, while false negatives allow dangerous situations to continue. This context makes the consistent detection of and response to abuse difficult, particularly given subtle signs in young, non-verbal patients. Here, we describe the development of artificial intelligence algorithms that use unstructured free-text in the electronic medical record - including notes from physicians, nurses, and social workers - to identify children who are suspected victims of physical abuse. Importantly, only the notes from time of first encounter (e.g.: birth, routine visit, sickness) to the last record before child protection team involvement were used. This allowed us to develop an algorithm using only information available prior to referral to the specialized child protection team. The study was performed in a multi-center referral pediatric hospital on patients screened for abuse within five different locations between 2015 and 2019. Of 1123 patients, 867 records were available after data cleaning and processing, and 55% were abuse-positive as determined by a multi-disciplinary team of clinical professionals. These electronic medical records were encoded with three natural language processing (NLP) algorithms - Bag of Words (BOW), Word Embeddings (WE), and Rules-Based (RB) - and used to train multiple neural network architectures. The BOW and WE encodings utilize the full free-text, while RB selects crucial phrases as identified by physicians. The best architecture was selected by average classification accuracy for the best performing model from each train-test split of a cross-validation experiment. Natural language processing coupled with neural networks detected cases of likely child abuse using only information available to clinicians prior to child protection team referral with average accuracy of 0.90±0.02 and average area under the receiver operator characteristic curve (ROC-AUC) 0.93±0.02 for the best performing Bag of Words models. The best performing rules-based models achieved average accuracy of 0.77±0.04 and average ROCAUC 0.81±0.05, while a Word Embeddings strategy was severely limited by lack of representative embeddings. Importantly, the best performing model had a false positive rate of 8%, as compared to rates of 20% or higher in previously reported studies. This artificial intelligence approach can help screen patients for whom an abuse concern exists and streamline the identification of patients who may benefit from referral to a child protection team. Furthermore, this approach could be applied to develop computer-aided-diagnosis platforms for the challenging and often intractable problem of reliably identifying pediatric patients suffering from physical abuse. © 2021 Annapragada et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"article; artificial intelligence; artificial neural network; child; child abuse; child protection; cleaning; controlled study; cross validation; deep learning; doctor nurse relation; electronic medical record; embedding; false negative result; female; human; major clinical study; male; multidisciplinary team; natural language processing; patient referral; pediatric hospital; pediatric patient; physical abuse; receiver operating characteristic; social worker; validation process; victim; algorithm; child abuse; clinical trial; community hospital; computer assisted diagnosis; electronic health record; epidemiology; multicenter study; natural language processing; procedures; retrospective study; United States; Algorithms; Child; Child Abuse; Deep Learning; Diagnosis, Computer-Assisted; Electronic Health Records; Hospitals, Community; Humans; Natural Language Processing; Referral and Consultation; Retrospective Studies; United States"
"Annapureddy P., Franco Z., Madiraju P., Ahamed S.I., Flower M., Hossain M.F., Haque M.R., Johnson N., Rubya S., Baker N.D., Jain N., Winstead O.","Identifying Precursors to Long-Term Crisis in Veterans Using Associative Classifier","10.1109/BigData52589.2021.9671761","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125314376&doi=10.1109%2fBigData52589.2021.9671761&partnerID=40&md5=eb3df35dd7af5da43662968b5649f4f8","Post-Traumatic Stress Disorder (PTSD) is one of the most common mental health disorders prevalent in the US. Most alarming, PTSD occurs at double the rate for combat veterans compared to the general population. Severity of PTSD is associated with risk taking behaviors such as substance abuse, non-suicidal self-injury, sexual risk behaviors, among other negative behaviors. Psychological disorders are often preceded by crisis events, thus monitoring for crisis events can help prevent risky behavior in veterans. Ecological momentary assessment techniques are effective in capturing possible crisis events for veterans. Mobile apps are commonly used to gather such behavioral changes in participants. Crisis events collected from m-health can be analyzed for the identification of long- term PTSD risk. Early identification of risk can help in planning intervention to mitigate the risk. Many scholars have used traditional statistical and machine learning methods for the prediction of mental health issues in individuals. But these models lack transparency in how decisions are made. Providing justifications for the predictions can increase the reliability of the model. Our research focused on developing an explainable prediction model using class association rules to identify veterans at risk of persistent PTSD. The generated association rules serve as precursors to the long-term crisis in veterans. Results of the analysis showed that having no family support, little or no interest in hobbies, stress and lack of sleep are some of the influencing factors of persistent PTSD in veterans. © 2021 IEEE.","Associative classifier; Mental health crisis; PTSD; Veterans","Association rules; Behavioral research; Health risks; Learning systems; mHealth; Risk management; Associative classifiers; Crisis events; General population; Health crisis; Health disorders; Mental health; Mental health crisis; Posttraumatic stress disorder; Risk-taking behaviors; Veteran; Forecasting"
"Annasamy R.M., Sycara K.","Towards better interpretability in deep Q-networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090807251&partnerID=40&md5=53e57dc1c2be1b4d9bb80e573d3d4899","Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model's behavior using key-value memories, attention and reconstructible embeddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org).",,"Deep learning; Learning systems; Network architecture; Reinforcement learning; Empirical studies; Exploration strategies; Interpretability; Key values; Q-learning; Reinforcement learning techniques; State of the art; Training algorithms; Neural networks"
"Anneken M., Markgraf S., Robert S., Beyerer J.","Learning of utility functions for the behaviour analysis in maritime surveillance tasks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080967016&partnerID=40&md5=50fee57a7f4537d2f726e698f81460a9","For detecting suspicious activities in the maritime domain such as illegal, unreported and unregulated fishing, it is crucial to counter the increasing amount of available information regarding vessels at sea with sophisticated algorithms and user interfaces. Deep learning and other data driven approaches create good results, but for an operator to be able to intervene suspicious activities, the supporting algorithms must be explainable and transparent. One possibility is to use simulations based on utility functions and behaviour models derived from the field of human behaviour modelling like game theory. Here, an expensive and time-consuming way is to model these utility functions by experts. As this is not always feasible or the behaviour patterns might not be easily expressed by experts, this work follows a different approach by utilizing inverse reinforcement learning in order to estimate the utility functions for different ship types. For this study, data based on the automatic identification system (AIS) is used for comparing the behaviour of cargo vessels and fishing boats. Copyright © 2020 for this paper by its authors.","Agents; Decision support; Maritime domain; Maximum entropy inverse reinforcement learning; Spatiotemporal data","Agents; Automatic identification; Behavioral research; Decision support systems; Deep learning; Fisheries; Fishing vessels; Game theory; Inverse problems; User interfaces; Automatic identification system; Data-driven approach; Decision supports; Human behaviour modelling; Inverse reinforcement learning; Maritime domains; Maritime surveillance; Spatio-temporal data; Reinforcement learning"
"Anning S., Goldberg Z.","Assessing The Ethical Implications of Artificial Intelligence In Policing","10.1145/3501247.3539506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133653062&doi=10.1145%2f3501247.3539506&partnerID=40&md5=9929386d0dee69cf41e8e764c10ded7f","This workshop addresses successful approaches and challenges to assessing the ethical implications of artificial intelligence in policing. It is divided into three main streams: A) How to conduct an ethics assessment of AI in policing? B) Applying Explainable AI in a Policing Context; C) The Practicalities of Co-Design Between Police and Developers © 2022 Owner/Author.","Co-Design; Digital Policing; Ethical AI; Ethical Impact Assessment; Explainability","Artificial intelligence; Co-designs; Digital policing; Ethical AI; Ethical impact assessment; Ethical implications; Explainability; Impact assessments; Ethical technology"
"Anon","Diagnostics: how expert systems can pinpoint faults",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024054786&partnerID=40&md5=1cb1812096805bd8afa85027e7e7e97c","Condition monitoring is essential for proper analysis of the performance of engines and other machinery, allowing it to be adjusted for optimum operation and also, of course, for indicating faults and potential component failures. The engineer can then take the necessary steps to correct any malfunction. But what the typical condition monitoring system will not do is tell the engineer a reason for a malfunction or identify precisely what component, or combination of components and systems, is responsible, and why. This article discusses applications of expert systems for fault diagnostics and identification of problems.",,"Ships--Monitoring; Diagnostics; DICARE Monitoring System; Fault Detection; Artificial Intelligence"
"Anon","PROCESS CONTROL TECHNOLOGY: DEMAND FOR FLEXIBILITY.",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023587633&partnerID=40&md5=fbf39b1187116c17d407cd0141d61e9b","Each production process can be regarded as a unit of events during which substance, energy, and information are transmitted, converted, and stored. Process technology covers the area of substance and part of energy (for example steam), while process control technology is responsible for the other sector of energy (for example electric energy) and for the area of information. In the article process control technology is examined under the aspects of transport, conversion and storage of information, i. e. in the sense of information processing.",,"ARTIFICIAL INTELLIGENCE - Expert Systems; COMPUTERS, PERSONAL - Computer Interfaces; CONTROL SYSTEMS, PROGRAMMED - Design; DATA PROCESSING - Manufacturing Applications; PROCESS CONTROL - Instruments; AUTOMATIC CONTROL; COMPACT CONTROLLERS; INFORMATION STORAGE; PROCESS CONTROL TECHNOLOGY; SELF-DIAGNOSIS; STORED PROGRAM CONTROLLERS; CHEMICAL PLANTS"
"Anoop V.S., Asharaf S.","Conceptualized phrase clustering with distributed k -means","10.3233/IDT-180089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066899622&doi=10.3233%2fIDT-180089&partnerID=40&md5=38c212faeae247e3437143a8667ed8e8","A vast majority of text mining and machine learning algorithms such as topic models, classification, clustering are based on statistical methods thus the semantics or meaning of the words or phrases are not considered. Interpretation of outputs generated by such algorithms are difficult for humans because of the absence of sufficient contextual information. Distributional semantics is a relatively new but active research area in natural language processing that quantifies semantic similarities between linguistic elements considering the context in which they occur. Conceptualization algorithms on the other hand enriches short text such as words and phrases. This paper proposes an approach that uses a map-reduce framework for combining these two techniques to generate conceptualized semantic clusters of phrases using distributional representation. Rigorous and systematic experiments on unstructured text datasets show that this approach can generate semantically rich and human interpretable concept clusters from large datasets. Further, the approach is scalable when dealing with high dimensional data since this method uses a map-reduce based framework for clustering. © 2019 - IOS Press and the authors. All rights reserved.","concept extraction; Distributional semantics; map-reduce; semantic clustering; text mining","Classification (of information); Data mining; Large dataset; Learning algorithms; Machine learning; Natural language processing systems; Semantics; Text processing; Concept extraction; Distributional semantics; Map-reduce; Semantic clustering; Text mining; K-means clustering"
"Anoop V.S., Asharaf S.","Distributional semantic phrase clustering and conceptualization using probabilistic knowledgebase","10.1007/978-981-10-8657-1_39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049009668&doi=10.1007%2f978-981-10-8657-1_39&partnerID=40&md5=7b793534978ca57acf163c44d89503ed","Distributional Semantics is an active research area in natural language processing (NLP) that develop methods for quantifying semantic similarities between linguistic elements in large samples of data. Short text conceptualization on the other hand is a technique for enriching short texts so that it become more interpretable. This is needed because most text mining tasks including topic modeling and clustering are based on statistical methods and won’t consider the semantics of text. This paper proposes a novel framework for combining distributional semantics and short text conceptualization for better interpretability of phrases in text data. Experiments on real-world datasets show that this method can better enrich phrases that are represented in distributional semantic spaces. © Springer Nature Singapore Pte Ltd. 2018.","Concept extraction; Distributional semantics; Natural language processing; Phrase2vec; Short text conceptualization; Text mining","Data mining; Natural language processing systems; Text processing; Concept extraction; Distributional semantics; Phrase2vec; Short texts; Text mining; Semantics"
"Anoosha P., Sakthivel R., Gromiha M.M.","Prediction of protein disorder on amino acid substitutions","10.1016/j.ab.2015.08.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944064788&doi=10.1016%2fj.ab.2015.08.028&partnerID=40&md5=1dfa12d27c45326e61c0f39638d2b4eb","Intrinsically disordered regions of proteins are known to have many functional roles in cell signaling and regulatory pathways. The altered expression of these proteins due to mutations is associated with various diseases. Currently, most of the available methods focus on predicting the disordered proteins or the disordered regions in a protein. On the other hand, methods developed for predicting protein disorder on mutation showed a poor performance with a maximum accuracy of 70%. Hence, in this work, we have developed a novel method to classify the disorder-related amino acid substitutions using amino acid properties, substitution matrices, and the effect of neighboring residues that showed an accuracy of 90.0% with a sensitivity and specificity of 94.9 and 80.6%, respectively, in 10-fold cross-validation. The method was evaluated with a test set of 20% data using 10 iterations, which showed an average accuracy of 88.9%. Furthermore, we systematically analyzed the features responsible for the better performance of our method and observed that neighboring residues play an important role in defining the disorder of a given residue in a protein sequence. We have developed a prediction server to identify disorder-related mutations, and it is available at http://www.iitm.ac.in/bioinfo/DIM-Pred/. © 2015 Elsevier Inc.","Disorder; Machine learning; Mutation; Neighboring residue; Stability","Amino acids; Cell signaling; Convergence of numerical methods; Forecasting; Learning systems; 10-fold cross-validation; Amino acid properties; Amino acid substitution; Disorder; Intrinsically disordered regions; Mutation; Neighboring residue; Sensitivity and specificity; Proteins; accuracy; amino acid substitution; Article; machine learning; missense mutation; priority journal; protein stability; protein structure; sensitivity and specificity; support vector machine; amino acid substitution; chemistry; computer program; genetics; metabolism; protein secondary structure; amino acid; protein; solvent; Amino Acid Substitution; Amino Acids; Protein Stability; Protein Structure, Secondary; Proteins; Software; Solvents"
"Ansari F.","Cost-based text understanding to improve maintenance knowledge intelligence in manufacturing enterprises","10.1016/j.cie.2020.106319","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078755103&doi=10.1016%2fj.cie.2020.106319&partnerID=40&md5=fe5a215e7bfaaca144b1e50f00012e9f","Improving maintenance knowledge intelligence using text data has not been largely explored in the literature of production and engineering management. The state-of-the-art approaches and solutions mainly focus on either clustering and classification of maintenance logs, or extracting additional (meta-)data e.g. failure time data from maintenance text reports, operators’ workbooks and digital logbook. Knowledge Discovery from Text (KDT) enables finding undetected causalities, hidden patterns, frequencies, associative relations, and sentiments in maintenance text repositories. Applying KDT may enhance understanding the content of text data syntactically and semantically. However, advanced KDT approaches do not significantly provide meaningful and explainable outcomes, due to certain barriers in manufacturing enterprises, namely availability and quality of (longitudinal) maintenance text data. To overcome these barriers in real world industrial maintenance, generate added value in industrial maintenance, and lay the ground for autonomous maintenance decision-support in the context of Industry 4.0, the first step is to adopt KDT methods and accordingly provide maintenance-specific solutions considering practical challenges and possibilities. This paper discusses the lack of understanding maintenance text data and examines its effect on maintenance knowledge intelligence in manufacturing enterprises. A compositional framework for text understanding (TextPlan) is introduced. TextPlan explores quantification of text data in both syntax and semantic levels, i.e. how to vectorize an annotated maintenance report into numeric values, which represent cost data, hidden associations and sentiments. A prominent feature of TextPlan is cost-based text analysis, which decomposes a maintenance text report into separate cost items, and then (re-)composes the findings to estimate the total maintenance cost associated with the given report. Finally yet importantly, TextPlan consolidates the findings into a Text Understanding Map for assisting maintenance planner, based on three proposed measures of text comprehension, namely Association Measuring Index (AMI), Opinion Index (OI) and Cost Vector (CV). © 2020 Elsevier Ltd","Associative measuring; Cost; Knowledge discovery; Maintenance; NLP; Sentiment analysis; Understanding","Cost benefit analysis; Data mining; Decision support systems; Maintenance; Manufacture; Semantics; Sentiment analysis; Syntactics; Associative measuring; Engineering management; Industrial maintenance; Knowledge discovery from texts; Maintenance decisions; Manufacturing enterprise; State-of-the-art approach; Understanding; Costs"
"Ansari F.","Knowledge Management 4.0: Theoretical and practical considerations in cyber physical production systems","10.1016/j.ifacol.2019.11.428","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078876736&doi=10.1016%2fj.ifacol.2019.11.428&partnerID=40&md5=91298fd13a3dffbe98e30c707846bc0f","Knowledge Management in the era of Industry 4.0 (KM 4.0) in both human- and technology-oriented perspectives is a strategic and operational function comprising exploration and exploitation processes. It is responsible to accomplish two major tasks. First, KM 4.0 should continuously support value generation through enhancing and balancing need- or opportunity-driven knowledge generation and knowledge utilization capacities. Second, KM 4.0 should persistently facilitate developing and protecting human-machine collective intelligence across manufacturing enterprises and in particular smart factories. Hence, KM 4.0 is an enabler to maximize competitive advantages and derive business values in the manufacturing enterprises. The revival of AI and emergence of autonomous and learnable technologies challenge the unique role of human as a knowledge actor, decision-maker, problem-solver and learner. What are the considerations on rethinking KM approaches in relation to the march of technological enhancements? This paper proposes a definition and discusses the theoretical foundation of KM 4.0 as well as related practical aspects that should be taken into consideration, especially in dynamic, data-driven and hybrid human-machine working environments in smart factories. © 2019, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.","Complex adaptive systems; Decision-support for human operators; Emergent synthesis in manufacturing; Industry 4.0; Knowledge management in production; Smart manufacturing systems","Adaptive systems; Artificial intelligence; Balancing; Competition; Cyber Physical System; Decision support systems; Industry 4.0; Knowledge management; Collective intelligences; Complex adaptive systems; Emergent synthesis; Exploration and exploitation; Human operator; Manufacturing enterprise; Technological enhancement; Theoretical foundations; Decision making"
"Ansari M., Huang W., Homayouni S., Niazmardi S., Safari A.","Convolutional Deep Kernel Method for Land Cover Mapping from Hyperspectral Imagery","10.1109/IGARSS46834.2022.9883309","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140384349&doi=10.1109%2fIGARSS46834.2022.9883309&partnerID=40&md5=55eac65782ed743b324de33726b1c2e6","In recent years, kernel-based methods and Deep Learning (DL) models have become the two most successful Remote Sensing (RS) analysis techniques for various Earth observations, particularly hyperspectral images. However, kernel-based methods are generally considered shallow models and intrinsically inconsistent with end-to-end learning. On the other hand, end-to-end learning is one of DL models' essential features as it seems to be responsible for their proven higher performances. Nevertheless, kernel methods are based on rigid mathematical theory and can efficiently cope with high-dimensional data. This paper proposed a hybrid deep kernel model to benefit from both kernel-based methods and DL models. This novel deep kernel model, namely Convolutional Kernel Network (CKN), was applied to two benchmark hyperspectral image datasets. Moreover, the proposed hybrid method was compared to Support Vector Machine (SVM) classifiers with various kernel functions. The experimental results indicated that the CKN's outperforms SVM. © 2022 IEEE.","Classification; Deep Kernel; Deep Learning; Kernel-based",
"Ansarifar J., Wang L., Archontoulis S.V.","An interaction regression model for crop yield prediction","10.1038/s41598-021-97221-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114626102&doi=10.1038%2fs41598-021-97221-7&partnerID=40&md5=487cedeb197c9dcc2fb22770ec0f1098","Crop yield prediction is crucial for global food security yet notoriously challenging due to multitudinous factors that jointly determine the yield, including genotype, environment, management, and their complex interactions. Integrating the power of optimization, machine learning, and agronomic insight, we present a new predictive model (referred to as the interaction regression model) for crop yield prediction, which has three salient properties. First, it achieved a relative root mean square error of 8% or less in three Midwest states (Illinois, Indiana, and Iowa) in the US for both corn and soybean yield prediction, outperforming state-of-the-art machine learning algorithms. Second, it identified about a dozen environment by management interactions for corn and soybean yield, some of which are consistent with conventional agronomic knowledge whereas some others interactions require additional analysis or experiment to prove or disprove. Third, it quantitatively dissected crop yield into contributions from weather, soil, management, and their interactions, allowing agronomists to pinpoint the factors that favorably or unfavorably affect the yield of a given location under a given weather and management scenario. The most significant contribution of the new prediction model is its capability to produce accurate prediction and explainable insights simultaneously. This was achieved by training the algorithm to select features and interactions that are spatially and temporally robust to balance prediction accuracy for the training data and generalizability to the test data. © 2021, The Author(s).",,"algorithm; article; harvest; Illinois; Indiana; Iowa; machine learning; nonhuman; plant yield; prediction; soil management; soybean; weather"
"Anselma L., Piovesan L., Terenziani P.","Temporal reasoning techniques for the analysis of interactions in the treatment of comorbid patients","10.1145/3019612.3019713","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020888454&doi=10.1145%2f3019612.3019713&partnerID=40&md5=6779da6b47981fe49f4293cc9bcc8721","Clinical practice guidelines are assuming a major role in the medical area, to provide physicians with evidence-based recommendations for the treatment of single pathologies. The treatment of comorbid patients (i.e., patients affected by multiple diseases) is one of the main challenges for the modern healthcare. It requires the development of new methodologies, supporting physicians in the treatment of interactions between guidelines. Several Artificial Intelligence approaches have started to face such a challenging problem. However, current approaches have a substantial limitation: they do not take into account the temporal dimension. This is a strong limitation. For instance, the effects of two actions taken from different guidelines may potentially conflict, but practical conflicts happen only if effects of such actions overlaps in time. In this paper, we propose an approach to support the temporal detection of interactions. Artificial intelligence temporal reasoning techniques, based on temporal constraint propagation, are widely exploited to such a purpose. Copyright is held by the owner/author(s).","Comorbidity treatment; Computer-interpretable clinical guidelines; Guideline interaction detection; Medical knowledge representation; Temporal reasoning","Artificial intelligence; Knowledge representation; Clinical guideline; Co morbidities; Interaction detection; Medical knowledge; Temporal reasoning; Patient treatment"
"Anselma L., Piovesan L., Terenziani P.","Temporal detection and analysis of guideline interactions","10.1016/j.artmed.2017.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014080978&doi=10.1016%2fj.artmed.2017.01.001&partnerID=40&md5=2051f86adc6517ed24279dada113f311","Background Clinical practice guidelines (CPGs) are assuming a major role in the medical area, to grant the quality of medical assistance, supporting physicians with evidence-based information of interventions in the treatment of single pathologies. The treatment of patients affected by multiple diseases (comorbid patients) is one of the main challenges for the modern healthcare. It requires the development of new methodologies, supporting physicians in the treatment of interactions between CPGs. Several approaches have started to face such a challenging problem. However, they suffer from a substantial limitation: they do not take into account the temporal dimension. Indeed, practically speaking, interactions occur in time. For instance, the effects of two actions taken from different guidelines may potentially conflict, but practical conflicts happen only if the times of execution of such actions are such that their effects overlap in time. Objectives We aim at devising a methodology to detect and analyse interactions between CPGs that considers the temporal dimension. Methods In this paper, we first extend our previous ontological model to deal with the fact that actions, goals, effects and interactions occur in time, and to model both qualitative and quantitative temporal constraints between them. Then, we identify different application scenarios, and, for each of them, we propose different types of facilities for user physicians, useful to support the temporal detection of interactions. Results We provide a modular approach in which different Artificial Intelligence temporal reasoning techniques, based on temporal constraint propagation, are widely exploited to provide users with such facilities. We applied our methodology to two cases of comorbidities, using simplified versions of CPGs. Conclusion We propose an innovative approach to the detection and analysis of interactions between CPGs considering different sources of temporal information (CPGs, ontological knowledge and execution logs), which is the first one in the literature that takes into account the temporal issues, and accounts for different application scenarios. © 2017 Elsevier B.V.","Comorbidity treatment; Computer-interpretable clinical guidelines; Guideline interaction detection; Medical knowledge representation; Ontology of time and interactions; Temporal reasoning","Knowledge representation; Ontology; Clinical guideline; Co morbidities; Interaction detection; Medical knowledge; Temporal reasoning; Patient treatment; Article; artificial intelligence; comorbidity; data extraction; human; human computer interaction; medical decision making; medical ontology; physician; practice guideline; priority journal; qualitative analysis; quantitative analysis; temporal analysis; decision making; time; Artificial Intelligence; Decision Making; Humans; Practice Guidelines as Topic; Time"
"Anselma L., Bottrighi A., Giordano L., Hommersom A., Molino G., Montani S., Terenziani P., Torchio M.","A hybrid approach to the verification of computer interpretable guidelines","10.1007/978-3-319-28007-3_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984644929&doi=10.1007%2f978-3-319-28007-3_19&partnerID=40&md5=008bfc2d6f5f93116c179abe20e28cae","Computer Interpretable Guidelines (CIGs) are assuming a major role in the medical area, in order to enhance the quality of medical assistance by providing physicians with evidence-based recommendations. However, the complexity of CIGs (which may contain hundreds of related clinical activities) demands for a verification process, aimed at assuring that a CIG satisfies several different types of properties (e.g., verification of the CIG correctness with respect to several criteria). Verification is a demanding task, which may be enhanced through the adoption of advanced Artificial Intelligence techniques. In this paper, we propose a general and hybrid approach to address such a task, suggesting that, given the heterogeneous character of the knowledge in CIGs, different forms of verification should be supported, through the adoption of proper (and different) methodologies. © Springer International Publishing Switzerland 2015.",,"Computer science; Computers; Artificial intelligence techniques; Evidence-based; Hybrid approach; Medical areas; Verification process; Artificial intelligence"
"Ansótegui C., Levy J.","On the modularity of industrial SAT instances","10.3233/978-1-60750-842-7-11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155141580&doi=10.3233%2f978-1-60750-842-7-11&partnerID=40&md5=002125f866144db9f027053a3d12f6b3","Learning, re-starting and other techniques of modern SAT solvers have been shown efficient when solving SAT instances from industrial application. The ability to exploit the structure of these instances has been proposed as the responsible of such success. Here we study the modularity of some of these instances, used in the latest SAT competitions. Using a simple label propagation algorithm we show that the community structure of most of these SAT instances can be identified very efficiently. We also discuss how this structure may be used to speed up SAT solvers. © 2011 The authors and IOS Press. All rights reserved.","graph theory; modularity; Satisfiability","Artificial intelligence; Model checking; Community structures; Label propagation; modularity; SAT instances; SAT solvers; Satisfiability; Speed up; Graph theory"
"Ansuini A., Medvet E., Pellegrino F.A., Zullich M.","On the similarity between hidden layers of pruned and unpruned convolutional neural networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082998374&partnerID=40&md5=4cd0b94b6ab9101500a5ef8ec90ec6ba","During the last few decades, artificial neural networks (ANN) have achieved an enormous success in regression and classification tasks. The empirical success has not been matched with an equally strong theoretical understanding of such models, as some of their working principles (training dynamics, generalization properties, and the structure of inner representations) still remain largely unknown. It is, for example, particularly difficult to reconcile the well known fact that ANNs achieve remarkable levels of generalization also in conditions of severe over-parametrization. In our work, we explore a recent network compression technique, called Iterative Magnitude Pruning (IMP), and apply it to convolutional neural networks (CNN). The pruned and unpruned models are compared layer-wise with Canonical Correlation Analysis (CCA). Our results show a high similarity between layers of pruned and unpruned CNNs in the first convolutional layers and in the fully-connected layer, while for the intermediate convolutional layers the similarity is significantly lower. This suggests that, although in intermediate layers representation in pruned and unpruned networks is markedly different, in the last part the fully-connected layers act as pivots, producing not only similar performances but also similar representations of the data, despite the large difference in the number of parameters involved. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved","Canonical Correlation Analysis; Convolutional Neural Networks; Explainable Knowledge; Lottery Ticket Hypothesis; Machine Learning; Pruning","Convolution; Correlation methods; Iterative methods; Learning systems; Multilayer neural networks; Pattern recognition; Canonical correlation analysis; Classification tasks; Explainable Knowledge; Generalization properties; Intermediate layers; Lottery Ticket Hypothesis; Network compression; Pruning; Convolutional neural networks"
"Antal L., Bodó Z.","Feature axes orthogonalization in semantic face editing","10.1109/ICCP53602.2021.9733549","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127407721&doi=10.1109%2fICCP53602.2021.9733549&partnerID=40&md5=4022513a93ab8a357c4e914f0bed95dc","Human image synthesis is the technology that allows a computer program to create realistic photos of non-existing people. Though it is a relatively novel research topic that is mostly used to synthesize human faces, generating moving human figures is also possible using this method.At first, composing the believable and realistic images was a complex process. To achieve decent results, photo-realistic modelling, animating and mapping of the soft dynamics of the human body was required. Nowadays these methods are replaced by approaches based on machine learning and neural networks.Our system is able to create realistic images, consisting of three main components. The first component is a Generative Adversarial Network (GAN) that can generate a random face from a noise vector. Secondly, a convolutional neural network is responsible to recognize facial features on the input photos. Lastly, a regression model computes the correspondence between the input noise vector and output features of the generated face.Using a well-known face dataset, we report results applying the newly proposed model and we also analyze the accuracy and the plausibility of these results. © 2021 IEEE.","composite sketches; custom loss function; Generative Adversarial Networks; human image synthesis; least squares; orthogonality; semantic face editing","Computer vision; Convolutional neural networks; Generative adversarial networks; Regression analysis; Semantic Segmentation; Semantic Web; Composite sketch; Custom loss function; Human image synthesis; Images synthesis; Least Square; Loss functions; Noise vectors; Orthogonality; Realistic images; Semantic face editing; Semantics"
"Antell G.C., Dampier W., Aiamkitsumrit B., Nonnemacher M.R., Jacobson J.M., Pirrone V., Zhong W., Kercher K., Passic S., Williams J.W., Schwartz G., Hershberg U., Krebs F.C., Wigdahl B.","Utilization of HIV-1 envelope V3 to identify X4- and R5-specific Tat and LTR sequence signatures","10.1186/s12977-016-0266-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964777629&doi=10.1186%2fs12977-016-0266-9&partnerID=40&md5=423091e12da75bb6728f86c5729d06bb","Background: HIV-1 entry is a receptor-mediated process directed by the interaction of the viral envelope with the host cell CD4 molecule and one of two co-receptors, CCR5 or CXCR4. The amino acid sequence of the third variable (V3) loop of the HIV-1 envelope is highly predictive of co-receptor utilization preference during entry, and machine learning predictive algorithms have been developed to characterize sequences as CCR5-utilizing (R5) or CXCR4-utilizing (X4). It was hypothesized that while the V3 loop is predominantly responsible for determining co-receptor binding, additional components of the HIV-1 genome may contribute to overall viral tropism and display sequence signatures associated with co-receptor utilization. Results: The accessory protein Tat and the HlV-1 long terminal repeat (LTR) were analyzed with respect to genetic diversity and compared by Jensen-Shannon divergence which resulted in a correlation with both mean genetic diversity as well as the absolute difference in genetic diversity between R5- and X4-genome specific trends. As expected, the V3 domain of the gp120 protein was enriched with statistically divergent positions. Statistically divergent positions were also identified in Tat amino acid sequences within the transactivation and TAR-binding domains, and in nucleotide positions throughout the LTR. We further analyzed LTR sequences for putative transcription factor binding sites using the JASPAR transcription factor binding profile database and found several putative differences in transcription factor binding sites between R5 and X4 HIV-1 genomes, specifically identifying the C/EBP sites I and II, and Sp site III to differ with respect to sequence configuration for R5 and X4 LTRs. Conclusion: These observations support the hypothesis that co-receptor utilization coincides with specific genetic signatures in HIV-1 Tat and the LTR, likely due to differing transcriptional regulatory mechanisms and selective pressures applied within specific cellular targets during the course of productive HIV-1 infection. © 2016 Antell et al..","Co-receptor; Divergence; Diversity; Gp120; HIV-1; LTR; Tat; Transcription factor; Tropism; V3","chemokine receptor CCR5; chemokine receptor CXCR4; glycoprotein gp 120; transactivator protein; virus envelope protein; CD4 antigen; chemokine receptor CCR5; chemokine receptor CXCR4; glycoprotein gp 120; HIV envelope protein gp120 (305-321); peptide fragment; transactivator protein; transcription factor; amino acid sequence; Article; binding site; envelope gene; genetic correlation; genetic variability; Human immunodeficiency virus 1; long terminal repeat; nonhuman; protein domain; protein protein interaction; transcription regulation; virus entry; virus envelope; virus genome; chemistry; genetic variation; genetics; human; Human immunodeficiency virus 1; long terminal repeat; metabolism; physiology; viral tropism; Antigens, CD4; Binding Sites; Genetic Variation; HIV Envelope Protein gp120; HIV Long Terminal Repeat; HIV-1; Humans; Peptide Fragments; Receptors, CCR5; Receptors, CXCR4; tat Gene Products, Human Immunodeficiency Virus; Transcription Factors; Viral Tropism"
"Antipov E.A., Pokryshevskaya E.B.","Interpretable machine learning for demand modeling with high-dimensional data using Gradient Boosting Machines and Shapley values","10.1057/s41272-020-00236-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082705204&doi=10.1057%2fs41272-020-00236-4&partnerID=40&md5=f6794009d807ba0d365fa1340c0b6d32","Forecasting demand and understanding sales drivers are one of the most important tasks in retail analytics. However, traditionally, linear models and/or models with a small number of predictors have been predominantly used in sales modeling. Taking into account that real-world demand is naturally determined by complex substitution and complementation patterns among a large number of interrelated SKUs, nonlinear effects of prices, promotions, seasonality, as well as many other factors, their lagged values, and interactions, a realistic model has to be able to account for all that. We propose a conceptual model for sales modeling based on standard POS data available to any retailer and generate almost 500 potentially useful predictors of a focal SKU’s sales accordingly. In our comparison of three classes of models, Gradient Boosting Machines outperformed Random Forests and Elastic nets. By using interpretable machine learning methods, we came up with actionable insights related to the importance of various groups of predictors from the conceptual model, as well as demonstrated how helpful it can be for marketing managers to decompose predictions into the effects of individual regressors by using an approximation of Shapley values for feature attribution. © 2020, Springer Nature Limited.","Elastic net; Gradient Boosting Machines; Interpretable machine learning; Random forest; Sales forecasting; Shapley value",
"Antman E.M., Benjamin E.J., Harrington R.A., Houser S.R., Peterson E.D., Bauman M.A., Brown N., Bufalino V., Califf R.M., Creager M.A., Daugherty A., Demets D.L., Dennis B.P., Ebadollahi S., Jessup M., Lauer M.S., Lo B., MacRae C.A., McConnell M.V., McCray A.T., Mello M.M., Mueller E., Newburger J.W., Okun S., Packer M., Philippakis A., Ping P., Prasoon P., Roger V.L., Singer S., Temple R., Turner M.B., Vigilante K., Warner J., Wayte P.","Acquisition, analysis, and sharing of data in 2015 and beyond: A survey of the landscape a conference report from the American heart association data summit 2015","10.1161/JAHA.115.002810","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006213908&doi=10.1161%2fJAHA.115.002810&partnerID=40&md5=92204e94693dce24b82ccedab79b8e46","Background--A 1.5-day interactive forum was convened to discuss critical issues in the acquisition, analysis, and sharing of data in the field of cardiovascular and stroke science. The discussion will serve as the foundation for the American Heart Association's (AHA's) near-term and future strategies in the Big Data area. The concepts evolving from this forum may also inform other fields of medicine and science. Methods and Results--A total of 47 participants representing stakeholders from 7 domains (patients, basic scientists, clinical investigators, population researchers, clinicians and healthcare system administrators, industry, and regulatory authorities) participated in the conference. Presentation topics included updates on data as viewed from conventional medical and nonmedical sources, building and using Big Data repositories, articulation of the goals of data sharing, and principles of responsible data sharing. Facilitated breakout sessions were conducted to examine what each of the 7 stakeholder domains wants from Big Data under ideal circumstances and the possible roles that the AHA might play in meeting their needs. Important areas that are high priorities for further study regarding Big Data include a description of the methodology of how to acquire and analyze findings, validation of the veracity of discoveries from such research, and integration into investigative and clinical care aspects of future cardiovascular and stroke medicine. Potential roles that the AHA might consider include facilitating a standards discussion (eg, tools, methodology, and appropriate data use), providing education (eg, healthcare providers, patients, investigators), and helping build an interoperable digital ecosystem in cardiovascular and stroke science. Conclusion--There was a consensus across stakeholder domains that Big Data holds great promise for revolutionizing the way cardiovascular and stroke research is conducted and clinical care is delivered; however, there is a clear need for the creation of a vision of how to use it to achieve the desired goals. Potential roles for the AHA center around facilitating a discussion of standards, providing education, and helping establish a cardiovascular digital ecosystem. This ecosystem should be interoperable and needs to interface with the rapidly growing digital object environment of the modern-day healthcare system.","AHA Scientific Statements; Clinical trials; Data; Epidemiology; Ethics; Mobile health; Preclinical","Article; clinical data repository; clinical examination; health care management; health care personnel; health care system; human; information processing; landscape; medical education; medical information system; medical society; patient care; population research; priority journal; professional standard; scientist; workshop; access to information; cardiology; Cardiovascular Diseases; consensus; cooperation; data mining; factual database; forecasting; information dissemination; interdisciplinary communication; mass communication; medical research; organization and management; Stroke; trends; United States; Access to Information; American Heart Association; Biomedical Research; Cardiology; Cardiovascular Diseases; Consensus; Cooperative Behavior; Data Mining; Databases, Factual; Diffusion of Innovation; Forecasting; Humans; Information Dissemination; Interdisciplinary Communication; Stroke; United States"
"Antognini D., Musat C., Faltings B.","Multi-Dimensional Explanation of Target Variables from Documents",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130093642&partnerID=40&md5=ee60f108e24f261ee9e68b882cc370cb","Automated predictions require explanations to be interpretable by humans. Past work used attention and rationale mechanisms to find words that predict the target variable of a document. Often though, they result in a tradeoff between noisy explanations or a drop in accuracy. Furthermore, rationale methods cannot capture the multi-faceted nature of justifications for multiple targets, because of the non-probabilistic nature of the mask. In this paper, we propose the Multi-Target Masker (MTM) to address these shortcomings. The novelty lies in the soft multi-dimensional mask that models a relevance probability distribution over the set of target variables to handle ambiguities. Additionally, two regularizers guide MTM to induce long, meaningful explanations. We evaluate MTM on two datasets and show, using standard metrics and human annotations, that the resulting masks are more accurate and coherent than those generated by the state-of-the-art methods. Moreover, MTM is the first to also achieve the highest F1 scores for all the target variables simultaneously. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; F1 scores; Human annotations; Multi dimensional; Multi-targets; Multiple targets; Non-probabilistic; Probability: distributions; Regularizer; Standard metrics; State-of-the-art methods; Probability distributions"
"Antolínez García A., Cáceres Campana J.W.","Identification of pathogens in corn using near-infrared UAV imagery and deep learning","10.1007/s11119-022-09951-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136101738&doi=10.1007%2fs11119-022-09951-x&partnerID=40&md5=ebfcb649d6dabb4d507e899e935df8d2","Maize is the second most plentiful cereal grown for human consumption. It constitutes 36% of total grain production worldwide and it is cultivated in about 160 countries on nearly 150 m ha. Maize faces fungal diseases causing extraordinary reduction in the grain yield. Fungi are responsible for many maize foliar diseases. Fungicides show hazardous effects on human health and also soil and water pollution. Near-infrared (NIR) images can disclose damage patterns not visible to the naked eye or depicted in RGB images. Unmanned aerial vehicles (UAV) are an inexpensive way to collect low altitude images. State-of-the-art Convolutional Neural Networks (CNN) have proven excellent results in image classification in computer vision. This study presents a novel Transfer Learning (TL) based CNN technique and states the hypothesis that NIR images acquired by UAVs contribute to a more precise classification of pathogens in maize. GPS coordinates of the infested areas are also provided for precision spraying with fungicide agents for specific targets, representing an economical mean for yield protection and with the least possible hazard to people and to the ecosystem. The proposed model was evaluated on its performance using different metrics achieving an accuracy of 86.7%, precision 98%, sensitivity 86.9% and F1 Score 92%. According to the state-of-the-art literature consulted, this is the first time that a validated deep learning-based approach has been applied in fungal diseases classification using infrared images. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Computer vision; Convolutional neural network; Deep learning; Fungi; Infrared; UAV",
"Antón F.S.","Artificial Intelligence and Tax Administration: Strategy, Applications and Implications, with Special Reference to the Tax Inspection Procedure",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127462022&partnerID=40&md5=9991af6edd1b58dc99a1a02e0ee0d2f0","This article provides insights into the use of artificial intelligence (AI) by tax administrations as a means of transforming tax procedures by improving tax efficiency and transparency, coming up with ideas and plans to implement new data-driven tax administrations, automating repetitive tax tasks, improving the fight against tax evasion, obtaining tax information to be used by the tax administration and providing other taxpayer services. Although the article focuses mainly on the use of AI in tax audits and the way to identify tax risks and taxpayer segmentation through AI, it also includes some reflections on data governance. Due to the fact that tax administrations are currently at different stages of AI use, the author aims to reveal the state of the art of AI in the tax world and to foster the ethical and responsible use of AI in tax administrations, in balance with taxpayers’ rights and guarantees. Modern tax administrations cannot allow themselves to be anything but efficient and transparent in the fight against tax evasion. © 2021, International Bureau of Fiscal Documentation (IBFD). All rights reserved.",,
"Antoncic M.","Uncovering hidden signals for sustainable investing using big data: Artificial intelligence, machine learning and natural language processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084218276&partnerID=40&md5=33138d4e0bbabff93f47a9cdc820154d","Risk managers and investors have increasingly been seeking high-quality environment, social and governance (ESG) data in order to assess nonfinancial risks as well as allocate capital towards companies that manage themselves in a ‘socially responsible’ way and adhere to their contract with society. The problem is that due to the lack of agreed-upon standards for companies to use for reporting on sustainability issues, there is a paucity of high-quality firm-level data to serve as key inputs in assessing a company’s risks and adherence to ESG criteria. Big Data, developed through cutting-edge statistical models, artificial intelligence (AI) and natural language processing (NLP) covering dozens of languages, provides the solution for ESG rankings and ratings and can help combat self-reported bias and ‘greenwashing’ and provide high-quality data. The ŉext generation’ measures of firms ‘doing good’ are the UN sustainable development goals (SDGs), which are this decade’s benchmarks against which millennials and many investors are beginning to assess companies. The SDGs go beyond the more narrowly focused set of sustainability issues embedded in ESGs, and quality data to measure performance against the SDGs are even more sparse. Using Big Data, Global AI Corporation uncovers data measuring companies’ and counties’ performance on all 17 SDGs, which can enable the integration of SDG factors into investment, risk management and national policy decision-making processes. Big Data is providing statistical indicators and performance metrics data to national governments and the United Nations to benchmark progress towards achieving the SDGs. It is also producing the SDG footprint of the private sector at the regional and global levels for policy purposes as shown in the United Nations Conference on Trade and Development’s (UNCTAD) SDG Pulse publication. Using Big Data, Global AI Corporation eliminates self-reporting biases and uncovers hidden data, which results in negative as well as positive ESG/SDG scores, while the self-reporting data only produces positive scores. © Henry Stewart Publications 1752-8887 (2020).","Artificial; Big data; ESG; Intelligence; Risk management; SDGs; Sustainable investing",
"Antonelli M., Bernardo D., Hagras H., Marcelloni F.","Multiobjective Evolutionary Optimization of Type-2 Fuzzy Rule-Based Systems for Financial Data Classification","10.1109/TFUZZ.2016.2578341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018523722&doi=10.1109%2fTFUZZ.2016.2578341&partnerID=40&md5=fa0173e8748ecd221649c8a50d3fa47e","Classification techniques are becoming essential in the financial world for reducing risks and possible disasters. Managers are interested in not only high accuracy, but in interpretability and transparency as well. It is widely accepted now that the comprehension of how inputs and outputs are related to each other is crucial for taking operative and strategic decisions. Furthermore, inputs are often affected by contextual factors and characterized by a high level of uncertainty. In addition, financial data are usually highly skewed toward the majority class. With the aim of achieving high accuracies, preserving the interpretability, and managing uncertain and unbalanced data, this paper presents a novel method to deal with financial data classification by adopting type-2 fuzzy rule-based classifiers (FRBCs) generated from data by a multiobjective evolutionary algorithm (MOEA). The classifiers employ an approach, denoted as scaled dominance, for defining rule weights in such a way to help minority classes to be correctly classified. In particular, we have extended PAES-RCS, an MOEA-based approach to learn concurrently the rule and data bases of FRBCs, for managing both interval type-2 fuzzy sets and unbalanced datasets. To the best of our knowledge, this is the first work that generates type-2 FRBCs by concurrently maximizing accuracy and minimizing the number of rules and the rule length with the objective of producing interpretable models of real-world skewed and incomplete financial datasets. The rule bases are generated by exploiting a rule and condition selection (RCS) approach, which selects a reduced number of rules from a heuristically generated rule base and a reduced number of conditions for each selected rule during the evolutionary process. The weight associated with each rule is scaled by the scaled dominance approach on the fuzzy frequency of the output class, in order to give a higher weight to the minority class. As regards the data base learning, the membership function parameters of the interval type-2 fuzzy sets used in the rules are learned concurrently to the application of RCS. Unbalanced datasets are managed by using, in addition to complexity, selectivity and specificity as objectives of the MOEA rather than only the classification rate. We tested our approach, named IT2-PAES-RCS, on 11 financial datasets and compared our results with the ones obtained by the original PAES-RCS with three objectives and with and without scaled dominance, the FRBCs, fuzzy association rule-based classification model for high-dimensional dataset (FARC-HD) and fuzzy unordered rules induction algorithm (FURIA), the classical C4.5 decision tree algorithm, and its cost-sensitive version. Using nonparametric statistical tests, we will show that IT2-PAES-RCS generates FRBCs with, on average, accuracy statistically comparable with and complexity lower than the ones generated by the two versions of the original PAES-RCS. Further, the FRBCs generated by FARC-HD and FURIA and the decision trees computed by C4.5 and its cost-sensitive version, despite the highest complexity, result to be less accurate than the FRBCs generated by IT2-PAES-RCS. Finally, we will highlight how these FRBCs are easily interpretable by showing and discussing one of them. © 1993-2012 IEEE.","Financial datasets; multiobjective evolutionary fuzzy systems; type-2 fuzzy rule-based classifiers; unbalanced datasets","Data mining; Decision trees; Evolutionary algorithms; Finance; Fuzzy inference; Fuzzy rules; Fuzzy sets; Membership functions; Optimization; Trees (mathematics); C4.5 decision tree algorithm; Financial datasets; Multi objective evolutionary algorithms; Multi-objective evolutionary fuzzy systems; Multi-objective evolutionary optimizations; Non-parametric statistical tests; Type-2 fuzzy; Unbalanced datasets; Classification (of information)"
"Antoniadi A.M., Galvin M., Heverin M., Wei L., Hardiman O., Mooney C.","A Clinical Decision Support System for the Prediction of Quality of Life in ALS","10.3390/jpm12030435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126473983&doi=10.3390%2fjpm12030435&partnerID=40&md5=b018328d03e58977c6a1d7a343739c2d","Amyotrophic Lateral Sclerosis (ALS), also known as Motor Neuron Disease (MND), is a rare and fatal neurodegenerative disease. As ALS is currently incurable, the aim of the treatment is mainly to alleviate symptoms and improve quality of life (QoL). We designed a prototype Clinical Decision Support System (CDSS) to alert clinicians when a person with ALS is experiencing low QoL in order to inform and personalise the support they receive. Explainability is important for the success of a CDSS and its acceptance by healthcare professionals. The aim of this work isto announce our prototype (C-ALS), supported by a first short evaluation of its explainability. Given the lack of similar studies and systems, this work is a valid proof-of-concept that will lead to future work. We developed a CDSS that was evaluated by members of the team of healthcare professionals that provide care to people with ALS in the ALS/MND Multidisciplinary Clinic in Dublin, Ireland. We conducted a user study where participants were asked to review the CDSS and complete a short survey with a focus on explainability. Healthcare professionals demonstrated some uncertainty in understanding the system’s output. Based on their feedback, we altered the explanation provided in the updated version of our CDSS. C-ALS provides local explanations of its predictions in a post-hoc manner, using SHAP (SHapley Additive exPlanations). The CDSS predicts the risk of low QoL in the form of a probability, a bar plot shows the feature importance for the specific prediction, along with some verbal guidelines on how to interpret the results. Additionally, we provide the option of a global explanation of the system’s function in the form of a bar plot showing the average importance of each feature. C-ALS is available online for academic use. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","ALS: Amyotrophic Lateral Sclerosis; Artificial intelligence; CDSS; Clinical decision support systems; Explainability; Explainable AI; Machine learning; MND; Quality of life; XAI","accuracy; algorithm; amyotrophic lateral sclerosis; Article; caregiver; clinical article; clinical decision making; clinical decision support system; coronavirus disease 2019; employment status; fatigue; female; health care personnel; health service; health survey; human; Likert scale; male; motor neuron disease; pandemic; prediction; probability; proof of concept; quality of life; questionnaire; rating scale; sleep; sleep disorder; social status; social support; uncertainty"
"Antoniadi A.M., Galvin M., Heverin M., Hardiman O., Mooney C.","Prediction of caregiver quality of life in amyotrophic lateral sclerosis using explainable machine learning","10.1038/s41598-021-91632-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107569521&doi=10.1038%2fs41598-021-91632-2&partnerID=40&md5=d4d5df10bc2f9fd2475343d839833865","Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative, fatal and currently incurable disease. People with ALS need support from informal caregivers due to the motor and cognitive decline caused by the disease. This study aims to identify caregivers whose quality of life (QoL) may be impacted as a result of caring for a person with ALS. In this study, we worked towards the identification of the predictors of a caregiver’s QoL in addition to the development of a model for clinical use to alert clinicians when a caregiver is at risk of experiencing low QoL. The data were collected through the Irish ALS Registry and via interviews on several topics with 90 patient and caregiver pairs at three time-points. The McGill QoL questionnaire was used to assess caregiver QoL—the MQoL Single Item Score measures the overall QoL and was selected as the outcome of interest in this work. The caregiver’s existential QoL and burden, as well as the patient’s depression and employment before the onset of symptoms were the features that had the highest impact in predicting caregiver quality of life. A small subset of features that could be easy to collect was used to develop a second model to use it in a clinical setting. The most predictive features for that model were the weekly caregiving duties, age and health of the caregiver, as well as the patient’s physical functioning and age of onset. © 2021, The Author(s).",,"algorithm; amyotrophic lateral sclerosis; caregiver; clinical decision support system; health survey; human; machine learning; psychology; quality of life; theoretical model; Algorithms; Amyotrophic Lateral Sclerosis; Caregivers; Decision Support Systems, Clinical; Humans; Machine Learning; Models, Theoretical; Public Health Surveillance; Quality of Life"
"Antoniadi A.M., Du Y., Guendouz Y., Wei L., Mazo C., Becker B.A., Mooney C.","Current challenges and future opportunities for xai in machine learning-based clinical decision support systems: A systematic review","10.3390/app11115088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107556681&doi=10.3390%2fapp11115088&partnerID=40&md5=32428f478e5780cc8ba68bd71a0055e8","Machine Learning and Artificial Intelligence (AI) more broadly have great immediate and future potential for transforming almost all aspects of medicine. However, in many applications, even outside medicine, a lack of transparency in AI applications has become increasingly problematic. This is particularly pronounced where users need to interpret the output of AI systems. Explainable AI (XAI) provides a rationale that allows users to understand why a system has produced a given output. The output can then be interpreted within a given context. One area that is in great need of XAI is that of Clinical Decision Support Systems (CDSSs). These systems support medical practitioners in their clinic decision-making and in the absence of explainability may lead to issues of under or over-reliance. Providing explanations for how recommendations are arrived at will allow practitioners to make more nuanced, and in some cases, life-saving decisions. The need for XAI in CDSS, and the medical field in general, is amplified by the need for ethical and fair decision-making and the fact that AI trained with historical data can be a reinforcement agent of historical actions and biases that should be uncovered. We performed a systematic literature review of work to-date in the application of XAI in CDSS. Tabular data processing XAI-enabled systems are the most common, while XAI-enabled CDSS for text analysis are the least common in literature. There is more interest in developers for the provision of local explanations, while there was almost a balance between post-hoc and ante-hoc explanations, as well as between model-specific and model-agnostic techniques. Studies reported benefits of the use of XAI such as the fact that it could enhance decision confidence for clinicians, or generate the hypothesis about causality, which ultimately leads to increased trustworthiness and acceptability of the system and potential for its incorporation in the clinical workflow. However, we found an overall distinct lack of application of XAI in the context of CDSS and, in particular, a lack of user studies exploring the needs of clinicians. We propose some guidelines for the implementation of XAI in CDSS and explore some opportunities, challenges, and future research needs. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence; CDSS; Clinical decision support systems; Deep learning; Explainability; Explainable AI; Interpretability; Machine learning; Medicine; Transparency; XAI",
"Antoniadi A.M., Galvin M., Heverin M., Hardiman O., Mooney C.","Development of an explainable clinical decision support system for the prediction of patient quality of life in amyotrophic lateral sclerosis","10.1145/3412841.3441940","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105016478&doi=10.1145%2f3412841.3441940&partnerID=40&md5=13cd44891d258749fea239d05829f0bb","Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative and currently incurable disease. It causes a rapid decline in motor functions and has a fatal trajectory. The aim of the treatment is mostly to alleviate symptoms and improve the patient's quality of life (QoL). The goal of this study is to develop a Clinical Decision Support System (CDSS) in order to alert clinicians when a patient is at risk of experiencing a low QoL, so that they are better supported. The source of the data was the Irish ALS Registry and interviews with the 90 patients and their primary informal caregiver at three time-points. In this dataset, there were two different scores to measure a person's overall QoL, based on the McGill QoL (MQoL) Questionnaire and we worked towards the prediction of both. The method we used for the development of the predictive models was Extreme Gradient Boosting (XGBoost), which was compared to a logistic regression baseline model. We used the SHAP (SHapley Additive exPlanations) values as a technique to provide local and global explanations to the outputs as well as to select the most important features. The total calculated MQoL score was predicted accurately by three features, with a F1-score on the test set equal to 0.81, a recall score of 0.78, and a precision score of 0.84, while, the addition of two features produced similar outcomes (0.79, 0.70 and 0.90 respectively). The three most important features were the age at disease onset, ALSFRS score for orthopnoea and the caregiver's status pre-caregiving. © 2021 Owner/Author.","amyotrophic lateral sclerosis; clinical decision support system; explainable artificial intelligence; machine learning; quality of life","Logistic regression; Neurodegenerative diseases; Patient treatment; Predictive analytics; Research laboratories; Amyotrophic lateral sclerosis; Clinical decision support systems; Gradient boosting; Important features; Incurable disease; Informal caregivers; Neurodegenerative; Predictive models; Decision support systems"
"Antoniadou E., Belo D., D'Silva K., Wang B., Russell B., Soboczenski F., Martin A., Mackintosh G., Shaw T.","Harnessing artificial intelligence to support astronaut medical care with automated and interpretable diagnosis for cardiac abnormalities in space",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100950312&partnerID=40&md5=6068a189277bb32052c541d350750230","Long-duration space missions require advanced medical capabilities, including continuous and automated monitoring of astronaut vital signs, to ensure optimal crew health. The potential latent health effects induced by reduced gravitational loading, high-energy cosmic rays, altered nutritional patterns and social stresses, call for an expanded framework of medical decision support system with real-time diagnostic telemonitoring independent of ground-based support systems. To monitor and diagnose cardiovascular health conditions, space agencies are utilizing wearable devices to collect continuous biosignal data. However, there is a lack of symptomatic wearable electrocardiogram (ECG)data from astronauts. This work focuses on the development of a deep learning framework, the ECG Generator of Representative Encoding of Style and Symptoms (EGRESS), which harnesses traditional machine learning to build a novel generative model for Artificial Intelligence (AI)-enhanced cardiac monitoring. EGRESS produces symptomatic data that resemble the output of a wearable device, which enables training of diagnostic tools for cardiac conditions. Our approach can reproduce ECG signals with 1.3X-3.6X reduction in mean squared error (MSE) compared to standard reconstruction-based approaches. This work is an important first step towards the development of an onboard AI doctor system. Copyright © 2020 by Frontier Development Lab (NASA-FDL). Published by the IAF, with permission and released to the IAF to publish in all forms.","Artificial intelligence; Atrial fibrillation; Bioastronautics; Biosignals; Machine learning; Wearables","Cosmology; Deep learning; Electrocardiography; Manned space flight; Mean square error; Real time systems; Wearable technology; Automated monitoring; Expanded frameworks; Gravitational loading; High-energy cosmic rays; Learning frameworks; Long duration space mission; Medical decision support system; Real-time diagnostics; Decision support systems"
"Antonie M.-L., Zaïane O.R.","Text document categorization by term association",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149287585&partnerID=40&md5=56870ec54d1315a6f028f6fd53f1b249","A good text classifier is a classifier that efficiently categorizes large sets of text documents in a reasonable time frame and with an acceptable accuracy, and that provides classification rules that are human readable for possible fine-tuning. If the training of the classifer is also quick, this could become in some application domains a good asset for the classifier. Many techniques and algorithms for automatic text categorization have been devised. According to published literature, some are more accurate than others. and some provide more interpretable classification models than others. However, none can combine all the beneficial properties enumerated above. In this paper, we present a novel approach for automatic text categorization that borrows from market basket analysis techniques using association rule mining in the data-mining field. We focus on two major problems: (1) finding the best term association rules in a textual database by generating and pruning; and (2) using the rules to build a text classifier: Our text categorization method proves to be efficient and effective, and experiments on well-known collections show that the classifier performs well. In addition, training as well as classification are both fast and the generated rules are human readable. © 2002 IEEE.",,
"Antonio N., de Almeida A., Nunes L.","Big Data in Hotel Revenue Management: Exploring Cancellation Drivers to Gain Insights Into Booking Cancellation Behavior","10.1177/1938965519851466","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066863566&doi=10.1177%2f1938965519851466&partnerID=40&md5=ee10a324bee56367cb7c96b9917191eb","In the hospitality industry, demand forecast accuracy is highly impacted by booking cancellations, which makes demand-management decisions difficult and risky. In attempting to minimize losses, hotels tend to implement restrictive cancellation policies and employ overbooking tactics, which, in turn, reduce the number of bookings and reduce revenue. To tackle the uncertainty arising from booking cancellations, we combined the data from eight hotels’ property management systems with data from several sources (weather, holidays, events, social reputation, and online prices/inventory) and machine learning interpretable algorithms to develop booking cancellation prediction models for the hotels. In a real production environment, improvement of the forecast accuracy due to the use of these models could enable hoteliers to decrease the number of cancellations, thus, increasing confidence in demand-management decisions. Moreover, this work shows that improvement of the demand forecast would allow hoteliers to better understand their net demand, that is, current demand minus predicted cancellations. Simultaneously, by focusing not only on forecast accuracy but also on its explicability, this work illustrates one other advantage of the application of these types of techniques in forecasting: the interpretation of the predictions of the model. By exposing cancellation drivers, models help hoteliers to better understand booking cancellation patterns and enable the adjustment of a hotel’s cancellation policies and overbooking tactics according to the characteristics of its bookings. © The Author(s) 2019.","big data; forecasting; machine learning; prediction; revenue management",
"Antoniou G., Papadakis E., Baryannis G.","Mental Health Diagnosis: A Case for Explainable Artificial Intelligence","10.1142/S0218213022410032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129957649&doi=10.1142%2fS0218213022410032&partnerID=40&md5=79d7a6943312fffcb1118fae72884e62","Mental illnesses are becoming increasingly prevalent, in turn leading to an increased interest in exploring artificial intelligence (AI) solutions to facilitate and enhance healthcare processes ranging from diagnosis to monitoring and treatment. In contrast to application areas where black box systems may be acceptable, explainability in healthcare applications is essential, especially in the case of diagnosing complex and sensitive mental health issues. In this paper, we first summarize recent developments in AI research for mental health, followed by an overview of approaches to explainable AI and their potential benefits in healthcare settings. We then present a recent case study of applying explainable AI for ADHD diagnosis which is used as a basis to identify challenges in realizing explainable AI solutions for mental health diagnosis and potential future research directions to address these challenges. © 2022 World Scientific Publishing Company.","Explainable artificial intelligence; healthcare; mental health","Artificial intelligence; Diseases; Application area; Artificial intelligence research; Black box system; Explainable artificial intelligence; Health care application; Health diagnosis; Health issues; Healthcare process; Mental health; Mental illness; Health care"
"Antoniou G., Sperschneider V.","Operational Concepts of Nonmonotonic Logics Part 2: Autoepistemic Logic","10.1023/A:1006516126932","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032288389&doi=10.1023%2fA%3a1006516126932&partnerID=40&md5=42a4d3bc0779848576d34dc14b9fc911","The subject of nonmonotonic reasoning is reasoning with incomplete information. One of the main approaches is autoepistemic logic in which reasoning is based on introspection. This paper aims at providing a smooth introduction to this logic, stressing its motivation and basic concepts. The meaning (semantics) of autoepistemic logic is given in terms of so-called expansions which are usually defined as solutions of a fixed-point equation. The present paper shows a more understandable, operational method for determining expansions. By improving applicability of the basic concepts to concrete examples, we hope to make a contribution to a wider usage of autoepistemic logic in practical applications.","Autoepistemic logic; Knowledge representation; Nonmonotonic reasoning","Autoepistemic logics; Nonmonotonic logics; Automata theory; Computational linguistics; Formal logic; Knowledge representation; Artificial intelligence"
"Antoniou G., Courtney A.P., Ernsttand J., Williams M.A.","A system for computing constrained default logic extensions",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449495267&partnerID=40&md5=904bbd17b87ec18468669993fbb833a7","The aim of this paper is to describe the algorithmic foundations of the part of the program Exten responsible for the computation of extensions in Constrained Default Logic. Exten is a system that computes extensions for various default logics. The efficiency of the system is increased by pruning techniques for the search tree. We motivate and present these techniques, and demonstrate that they can cut down the size of the search tree significantly. Quite importantly, they complement well the recently developed stratification method. This technique has to be modified to work properly with Constrained Default Logic, and we show how this can be done. Exten supports experimentation with default logic, allowing the user to set various parameters. Also it has been designed to be open to future enhancements, which are supported by its object-oriented design. Exten is part of our long-term effort to develop an integrated toolkit for intelligent information management based on nonmonotonic reasoning and befief revision methods. © Springer-Verlag Berlin Heidelberg 1996.",,"Artificial intelligence; Computation theory; Forestry; Information management; Algorithmic foundations; Default logic; Integrated toolkit; Intelligent information management; Non-monotonic reasoning; Object oriented design; Pruning techniques; Search trees; Computer circuits"
"Antoniou J.","Dealing with emerging AI technologies: Teaching and learning ethics for AI","10.1007/978-3-030-52559-0_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090491733&doi=10.1007%2f978-3-030-52559-0_6&partnerID=40&md5=5a47d5b4e8ec0f0018887828758ac27e","This chapter addresses emerging AI technologies, but not from a technical aspect. It explores the need for ethics to be integrated with the technical design aspects during the development process of AI products. In order for designers and developers to be able to consider and include ethics in to this process, they have to learn ethics as part of their acquired skills. Therefore, the chapter deals with the teaching and learning aspects of ethics for emerging new technologies, especially AI. It is important to recognise that there are ethical issues associated with the development and use of such technologies. Furthermore, the chapter recognises the trend of an increase in the development and use of AI across many application areas, and ethics must be considered. Moving forward, necessary learning and teaching practices need to be integrated into software development courses. AI is not a standalone technology but a technology that is integrated with society and people, and needs to be understood in such an inter-disciplinary manner. The assumption that such courses exist and developers can decide to integrate them into ongoing professional development goals is what the chapter scenario addresses, in an attempt to demonstrate that expectation of ethical design by users can be reflected into resulting user experience from usage of the specific technologies. © Springer Nature Switzerland AG 2021.","Artificial intelligence; Ethics; Responsible technology development","Curricula; Philosophical aspects; Product design; Software design; Teaching; User experience; Application area; Development process; Ethical designs; Learning and teachings; Professional development; Teaching and learning; Technical aspects; Technical design; Engineering education"
"Antonov V.G., Petrenko Y.S.","Development Prospects of SMART Grid in the Energy Sector of Kazakhstan","10.1007/978-3-030-94873-3_38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128428006&doi=10.1007%2f978-3-030-94873-3_38&partnerID=40&md5=c0d6e5bf9f82da47d1efa621214f3bac","The contribution investigates the forecast of the development of the energy industry of Kazakhstan, compiled on the basis of the foresight methodology. The authors summarized the assessments of 136 leading experts in the field of energy and presented the main development trends. The national energy sector is developing under the influence of the general trend of digitalization and the increase in environmental requirements. However, the use of artificial intelligence is determined by the peculiarities of the state of the national energy system. The purpose of the research is to study development prospects of Smart Grid in the energy sector of Kazakhstan based on the analysis of stakeholders’ opinions who are responsible for the innovative development of the industry. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Development; Energy; Forecast; Foresight; Kazakhstan; Smart Grid",
"Antunes M., Folgado D., Barandas M., Carreiro A., Quintão C., de Carvalho M., Gamboa H.","A morphology-based feature set for automated Amyotrophic Lateral Sclerosis diagnosis on surface electromyography","10.1016/j.bspc.2022.104011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136154007&doi=10.1016%2fj.bspc.2022.104011&partnerID=40&md5=ac99f30992d92bd558f155af36fb52cc","Amyotrophic Lateral Sclerosis (ALS) is a fast-progressing disease with no cure. Nowadays, needle electromyography (nEMG) is the standard practice for electrodiagnosis of ALS. Surface electromyography (sEMG) is emerging as a more practical and less painful alternative to nEMG but still has analytical and technical challenges. The objective of this work was to study the feasibility of using a set of morphological features extracted from sEMG to support a machine learning pipeline for ALS diagnosis. We developed a novel feature set to characterize sEMG based on quantitative measurements to surface representation of Motor Unit Action Potentials. We conducted several experiments to study the relevance of the proposed feature set either individually or combined with conventional feature sets from temporal, statistical, spectral, and fractal domains. We validated the proposed machine learning pipeline on a dataset with sEMG upper limb muscle data from 17 ALS patients and 24 control subjects. The results support the utility of the proposed feature set, achieving an F1 score of (81.9 ± 5.7) for the onset classification approach and (83.6 ± 6.9) for the subject classification approach, solely relying on features extracted from the proposed feature set in the right first dorsal interosseous muscle. We concluded that introducing the proposed feature set is relevant for automated ALS diagnosis since it increased the classifier performance during our experiments. The proposed feature set might also help design more interpretable classifiers as the features give additional information related to the nature of the disease, being inspired by the clinical interpretation of sEMG. © 2022","Amyotrophic Lateral Sclerosis; Feature selection; Machine learning; Signal processing; Surface electromyography; Time series","Classification (of information); Computer aided diagnosis; Electrophysiology; Morphology; Muscle; Neurodegenerative diseases; Pipelines; Signal processing; Amyotrophic lateral sclerosis; Analytical challenge; Classification approach; Features selection; Features sets; Machine-learning; Signal-processing; Standard practices; Surface electromyography; Times series; Feature Selection; adult; amyotrophic lateral sclerosis; arm muscle; Article; clinical article; clinical feature; controlled study; feature selection; female; fractal analysis; human; machine learning; male; middle aged; morphology; motor unit potential; needle electromyography; signal processing; surface electromyography; time series analysis"
"Antunes R., Matos S.","Biomedical word sense disambiguation with word embeddings","10.1007/978-3-319-60816-7_33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025169861&doi=10.1007%2f978-3-319-60816-7_33&partnerID=40&md5=123eb3a6ef28cea5812e807cfb236f43","There is a growing need for automatic extraction of information and knowledge from the increasing amount of biomedical and clinical data produced, namely in textual form. Natural language processing comes in this direction, helping in tasks such as information extraction and information retrieval. Word sense disambiguation is an important part of this process, being responsible for assigning the proper concept to an ambiguous term. In this paper, we present results from machine learning and knowledge-based algorithms applied to biomedical word sense disambiguation. For the supervised machine learning algorithms we used word embeddings, calculated from the full MEDLINE literature database, as global features and compare the results to the use of local unigram and bigram features. For the knowledge-based method we represented the textual definitions of biomedical concepts from the UMLS database as word embedding vectors, and combined this with concept associations derived from the MeSH term co-occurrences. Both the machine learning and the knowledge-based results indicate that word embeddings are informative and improve the biomedical word disambiguation accuracy. Applied to the reference MSH WSD data set, our knowledge-based approach achieves 85.1% disambiguation accuracy, which is higher than some previously proposed approaches that do not use machine-learning strategies. © Springer International Publishing AG 2017.","Biomedical word sense disambiguation; Word embeddings","Artificial intelligence; Bioinformatics; Data mining; Education; Information analysis; Knowledge based systems; Learning systems; Natural language processing systems; Automatic extraction; Embeddings; Knowledge-based algorithms; Knowledge-based approach; Knowledge-based methods; Literature database; Supervised machine learning; Word Sense Disambiguation; Learning algorithms"
"Antwarg L., Miller R.M., Shapira B., Rokach L.","Explaining anomalies detected by autoencoders using Shapley Additive Explanations[Formula presented]","10.1016/j.eswa.2021.115736","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112853792&doi=10.1016%2fj.eswa.2021.115736&partnerID=40&md5=af0622d3ade358d09e89f6603b6349b5","Deep learning algorithms for anomaly detection, such as autoencoders, point out the outliers, saving experts the time-consuming task of examining normal cases in order to find anomalies. Most outlier detection algorithms output a score for each instance in the database. The top-k most intense outliers are returned to the user for further inspection; however, the manual validation of results becomes challenging without justification or additional clues. An explanation of why an instance is anomalous enables the experts to focus their investigation on the most important anomalies and may increase their trust in the algorithm. Recently, a game theory-based framework known as SHapley Additive exPlanations (SHAP) was shown to be effective in explaining various supervised learning models. In this paper, we propose a method that uses Kernel SHAP to explain anomalies detected by an autoencoder, which is an unsupervised model. The proposed explanation method aims to provide a comprehensive explanation to the experts by focusing on the connection between the features with high reconstruction error and the features that are most important in terms of their affect on the reconstruction error. We propose a black-box explanation method, because it has the advantage of being able to explain any autoencoder without being aware of the exact architecture of the autoencoder model. The proposed explanation method extracts and visually depicts both features that contribute the most to the anomaly and those that offset it. An expert evaluation using real-world data demonstrates the usefulness of the proposed method in helping domain experts better understand the anomalies. Our evaluation of the explanation method, in which a “perfect” autoencoder is used as the ground truth, shows that the proposed method explains anomalies correctly, using the exact features, and evaluation on real-data demonstrates that (1) our explanation model, which uses SHAP, is more robust than the Local Interpretable Model-agnostic Explanations (LIME) method, and (2) the explanations our method provides are more effective at reducing the anomaly score than other methods. © 2021 Elsevier Ltd","Anomaly detection; Autoencoder; Explainable black-box models; SHAP; Shapley values; XAI","Additives; Deep learning; Game theory; Learning algorithms; Lime; Statistics; Anomaly detection; Autoencoders; Explainable black-box model; Outlier detection algorithm; Reconstruction error; Shapley; Shapley additive explanation; Shapley value; Time-consuming tasks; XAI; Anomaly detection"
"Antzoulatos G., Kouloglou I.-O., Bakratsas M., Moumtzidou A., Gialampoukidis I., Karakostas A., Lombardo F., Fiorin R., Norbiato D., Ferri M., Symeonidis A., Vrochidis S., Kompatsiaris I.","Flood Hazard and Risk Mapping by Applying an Explainable Machine Learning Framework Using Satellite Imagery and GIS Data","10.3390/su14063251","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126276355&doi=10.3390%2fsu14063251&partnerID=40&md5=5cfae8fda33a31b49712b971d30265e0","Flooding is one of the most destructive natural phenomena that happen worldwide, leading to the damage of property and infrastructure or even the loss of lives. The escalation in the intensity and number of flooding events as a result of the combination of climate change and anthropogenic factors motivates the need to adopt real-time solutions for mapping flood hazards and risks. In this study, a methodological framework is proposed that enables the assessment of flood hazard and risk levels of severity dynamically by fusing optical remote sensing (Sentinel-1) and GIS-based data from the region of the Trieste, Monfalcone and Muggia Municipalities. Explainable machine learning techniques were utilised, aiming to interpret the results for the assessment of flood hazard. The flood inventory was randomly divided into 70%, used for training, and 30%, employed for testing. Various combinations of the models were evaluated for the assessment of flood hazard. The results revealed that the Random Forest model achieved the highest F1-score (approx. 0.99), among others utilised for generating flood hazard maps. Furthermore, the estimation of the flood risk was achieved by a combination of a rule-based approach to estimate the exposure and vulnerability with the dynamic assessment of flood hazard. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Crisis maps; Flood hazard; Flood risk maps; Flood susceptibility; Machine learning; Satellite imagery analysis","flood; flooding; GIS; hazard assessment; machine learning; mapping method; risk assessment; satellite data; satellite imagery; spatiotemporal analysis; Friuli-Venezia Giulia; Italy; Trieste [Friuli-Venezia Giulia]; Trieste [Trieste (PRV)]"
"Anurag N.V., Burra Y., Sharanya S., Gireeshan M.G.","Air quality index prediction using meteorological data using featured based weighted xgboost","10.35940/ijitee.K1211.09811S19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073732497&doi=10.35940%2fijitee.K1211.09811S19&partnerID=40&md5=aa550b0fc309fda79d4bcec8207194ff","Over the recent years, air pollution or air contamination has become a concerning threat, being responsible for over 7 million deaths annually according to a survey conducted by “WHO”(World Health Organisation). The four air pollutants which are becoming a concerning threat to human health are namely respirable particulate matter, nitrogen oxides, particulate matter and sulphur dioxide. Hence to tackle this problem, efficient air quality prediction will enable us to foresee these undesirable changes made in the environment keeping the pollutant emission under check and control. Also inclusion of meteorological data for isolating the factors that contributes more to the Air Quality Index (AIQ) prediction is the need of the hour. A feature based weighted XGBoost model is built to predict the AIQ of Velachery, a fast developing commercial station in South India. The model resulted in low RMSE value when compared with other state of art techniques. © BEIESP.","Air Quality Index; Artificial Intelligence; Machine Learning",
"Anutariya C., Wuwongse V., Akama K.","XML declarative description with first-order logical constraints","10.1111/j.0824-7935.2005.00268.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644391379&doi=10.1111%2fj.0824-7935.2005.00268.x&partnerID=40&md5=e4d88c4dfd7f95731c5d5ad617866afe","The expressive power of XML Declarative Description (XDD), a unified XML-based representation language for the Knowledge Grid, is enhanced by a well-defined mechanism for modeling arbitrary XML first-order logical constraints (FLCs) - a special kind of constraints comprising XML expressions and logical symbols. The resulting knowledge representation can uniformly express explicit and implicit information, ontologies, axioms as well as integrity, structural and FLCs. It facilitates direct use of ordinary XML elements as its basic language component and semantic units, and formally defines XML clauses for modeling advanced complex statements. It achieves sound, efficient, and flexible computation or inference by means of the Equivalent Transformation (ET) paradigm - a new computational model based on semantic preserving transformations. Basic ET computational rules for reasoning with XDD descriptions with FLCs are also presented. Due to its well-founded mechanism and expressiveness, employment of the proposed representation and computation framework to model a knowledge grid and its services not only enables direct representation of knowledge bases described by such emerging Semantic Web ontology languages as RDF(S) and OWL, but also offers additional descriptive facilities by allowing expression of and reasoning with rules, relationships, and constraints. Moreover, in order to provide machine-interpretable descriptions of knowledge grid services, standard service description languages, e.g., WSDL, UDDI, OWL-S and WSMO are employed and extended with facilities to define additional service relationships, constraints, and composition rules. © 2005 Blackwell Publishing.","Knowledge grid; Knowledge representation; XML declarative description; XML first-order logical constraints, equivalent transformation rules","Artificial intelligence; Codes (symbols); Computation theory; Computer architecture; Computer programming languages; Constraint theory; Knowledge based systems; Knowledge representation; Logic programming; Mathematical models; Semantics; Equivalent transformation rules; Knowledge grids; XML declarative description (XDD); XML first-order logical constraint; XML"
"Anuwongcharoen N., Shoombuatong W., Tantimongcolwat T., Prachayasittikul V., Nantasenamat C.","Exploring the chemical space of influenza neuraminidase inhibitors","10.7717/peerj.1958","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966312406&doi=10.7717%2fpeerj.1958&partnerID=40&md5=ec2c2b8e15e00b2f00dc7541c7e889bf","The fight against the emergence of mutant influenza strains has led to the screening of an increasing number of compounds for inhibitory activity against influenza neuraminidase. This study explores the chemical space of neuraminidase inhibitors (NAIs), which provides an opportunity to obtain further molecular insights regarding the underlying basis of their bioactivity. In particular, a large set of 347 and 175 NAIs against influenza A and B, respectively, was compiled from the literature. Molecular and quantum chemical descriptors were obtained from low-energy conformational structures geometrically optimized at the PM6 level. The bioactivities of NAIs were classified as active or inactive according to their half maximum inhibitory concentration (IC50) value in which IC50 &lt; 1 μM and ≥ 10 μM were defined as active and inactive compounds, respectively. Interpretable decision rules were derived from a quantitative structure-activity relationship (QSAR) model established using a set of substructure descriptors via decision tree analysis. Univariate analysis, feature importance analysis from decision tree modeling and molecular scaffold analysis were performed on both data sets for discriminating important structural features amongst active and inactive NAIs. Good predictive performance was achieved as deduced from accuracy and Matthews correlation coefficient values in excess of 81% and 0.58, respectively, for both influenza A and B NAIs. Furthermore, molecular docking was employed to investigate the binding modes and their moiety preferences of active NAIs against both influenza A and B neuraminidases. Moreover, novel NAIs with robust binding fitness towards influenzaA and B neuraminidase were generated via combinatorial library enumeration and their binding fitness was on par or better than FDA-approved drugs. The results from this study are anticipated to be beneficial for guiding the rational drug design of novel NAIs for treating influenza infections. © 2016 Anuwongcharoen et al.","Chemical space; Combinatorial library enumeration; Data mining; Fragment analysis; Influenza; Molecular docking; Neuraminidase; Neuraminidase inhibitor; QSAR; Scaffold analysis","molecular scaffold; oseltamivir; peramivir; sialidase; sialidase inhibitor; zanamivir; Article; biological activity; chemical structure; dipole; IC50; influenza; influenza A; influenza B; molecular docking; quantitative structure activity relation; scoring system"
"Anwar A., Franklin S.","Sparse distributed memory for 'conscious' software agents","10.1016/S1389-0417(03)00015-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442466692&doi=10.1016%2fS1389-0417%2803%2900015-9&partnerID=40&md5=fa0f809f5d143fee4d41759ae1bbd46f","In this work we are reporting a case study on the use of SDM as the associative memory for a software agent, CMattie, whose architecture is modeled on human cognition. Sparse distributed memory (SDM) is a content-addressable memory technique that relies on close memory items tending to be clustered together. In this work, we used an enhanced version of SDM augmented with the use of genetic algorithms as an associative memory in our 'conscious' software agent, CMattie, who is responsible for emailing seminar announcements in an academic department. Interacting with seminar organizers via email in natural language, CMattie can replace the secretary who normally handles such announcements. SDM is a key ingredient in a complex agent architecture that implements global workspace theory, a psychological theory of consciousness and cognition. In this architecture, SDM, as the primary memory for the agent, provides associations with incoming percepts. These include disambiguation of the percept by removing noise, correcting misspellings, and adding missing pieces of information. It also retrieves behaviors and emotions associated with the percept. These associations are based on previous similar percepts, and their consequences, that have been recorded earlier. SDM also possesses several key psychological features. Some enhancements to SDM including multiple writes of important items, use of error detection and correction, and the use of hashing to map the original information into fixed size keys were used. Test results indicate that SDM can be used successfully as an associative memory in such complex agent architectures. The results show that SDM is capable of recovering a percept based on a part of that percept, and finding defaults for empty perception registers. The evaluation of suggested actions and emotional states is satisfactory. We think that this work opens the door to more scientific and empirical uses for SDM. © 2003 Elsevier B.V. All rights reserved.","Artificial intelligence; Cognition; Consciousness; Genetic algorithms; Software agents; Sparse distributed memory","ambiguity; article; associative memory; behavior; cognition; computer language; computer program; consciousness; e-mail; emotion; error; genetic algorithm; human computer interaction; information processing; noise; nonbiological model; perception; priority journal; psychological theory; register; satisfaction"
"Anysz H., Brzozowski L., Kretowicz W., Narloch P.","Feature importance of stabilised rammed earth components affecting the compressive strength calculated with explainable artificial intelligence tools","10.3390/ma13102317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085628409&doi=10.3390%2fma13102317&partnerID=40&md5=50f3bb076bf615d52b969952bee06efa","Cement-stabilized rammed earth (CSRE) is a sustainable construction material. The use of it allows for economizing on the cost of a structure. These two properties of CSRE are based on the fact that the soil used for the rammed mixture is usually dug close to the construction site, so it has random characteristics. That is the reason for the lack of widely accepted prescriptions for CSRE mixture, which could ascertain high enough compressive strength. Therefore, assessing which components of CSRE have the highest impact on its compressive strength becomes an important issue. There are three machine learning regression tools, i.e., artificial neural networks, decision tree, and random forest, used for predicting the compressive strength based on the relative content of CSRE composites (clay, silt, sand, gravel, cement, and water content). The database consisted of 434 samples of CSRE, which were prepared and crushed for testing purposes. Relatively low prediction errors of aforementioned models allowed for the use of explainable artificial intelligence tools (drop-out loss, mean squared error reduction, accumulated local effect) to rank the influence of the ingredients on the dependent variable-the compressive strength. Consistent results from all above-mentioned methods are discussed and compared to some statistical analysis of selected features. This innovative approach, helpful in designing the construction material is a solid base for reliable conclusions. © 2020 by the authors.","Artificial inteligence; Cement stabilized rammed earth; Features importance ranking; Multivariate regression; Rammed earth; Random forest","Cements; Decision trees; Learning algorithms; Mean square error; Mixtures; Neural networks; Sandwich structures; Artificial intelligence tools; Construction sites; Dependent variables; Innovative approaches; Mean squared error; Random characteristics; Stabilised rammed earths; Sustainable construction; Compressive strength"
"Aono A.H., Nagai J.S., Dickel G.D.S.M., Marinho R.C., de Oliveira P.E.A.M., Papa J.P., Faria F.A.","A stomata classification and detection system in microscope images of maize cultivars","10.1371/journal.pone.0258679","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117935727&doi=10.1371%2fjournal.pone.0258679&partnerID=40&md5=37a335049201ff616bd7c40f4880079f","Plant stomata are essential structures (pores) that control the exchange of gases between plant leaves and the atmosphere, and also they influence plant adaptation to climate through photosynthesis and transpiration stream. Many works in literature aim for a better understanding of these structures and their role in the evolution process and the behavior of plants. Although stomata studies in dicots species have advanced considerably in the past years, even there is not much knowledge about the stomata of cereal grasses. Due to the high morphological variation of stomata traits intra- and inter-species, detecting and classifying stomata automatically becomes challenging. For this reason, in this work, we propose a new system for automatic stomata classification and detection in microscope images for maize cultivars based on transfer learning strategy of different deep convolution neural net-woks (DCNN). Our performed experiments show that our system achieves an approximated accuracy of 97.1% in identifying stomata regions using classifiers based on deep learning features, which figures out as a nearly perfect classification system. As the stomata are responsible for several plant functionalities, this work represents an important advance for maize research, providing an accurate system in replacing the current manual task of categorizing these pores on microscope images. Furthermore, this system can also be a reference for studies using images from different cereal grasses. Copyright: © 2021 Aono et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"article; classifier; cultivar; deep learning; maize; microscope image; nonhuman; transfer of learning; anatomy and histology; classification; evapotranspiration; image processing; maize; microscopy; photosynthesis; physiology; plant leaf; plant physiology; plant stoma; procedures; Image Processing, Computer-Assisted; Microscopy; Photosynthesis; Plant Leaves; Plant Physiological Phenomena; Plant Stomata; Plant Transpiration; Zea mays"
"Aoudia F.A., Gautier M., Berder O.","RLMan: An Energy Manager Based on Reinforcement Learning for Energy Harvesting Wireless Sensor Networks","10.1109/TGCN.2018.2801725","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061389718&doi=10.1109%2fTGCN.2018.2801725&partnerID=40&md5=5eb18954b52a8ce514a16a01721e87bd","A promising solution for achieving autonomous wireless sensor networks is to enable each node to harvest energy in its environment. To address the time-varying behavior of energy sources, each node embeds an energy manager responsible for dynamically adapting the power consumption of the node in order to maximize the quality of service while avoiding power failures. A novel energy management algorithm based on reinforcement learning (RLMan) is proposed in this paper. By continuously exploring the environment, RLMan adapts its energy management policy to time-varying environment, regarding both the harvested energy and the energy consumption of the node. Linear function approximations are used to achieve very low computational and memory footprint, making RLMan suitable for resource-constrained systems, such as wireless sensor nodes. Moreover, RLMan only requires the state of charge of the energy storage device to operate, which makes it practical to implement. Exhaustive simulations using real measurements of indoor light and outdoor wind show that RLMan outperforms current state-of-the-art approaches, by enabling almost 70% gain regarding the average packet rate. Moreover, RLMan is more robust to variability of the node energy consumption. © 2018 IEEE.","autonomous systems; energy harvesting; energy management; learning systems; Wireless sensor networks","Energy harvesting; Energy management; Energy policy; Energy storage; Energy utilization; Managers; Quality of service; Reinforcement learning; Wireless sensor networks; Energy management algorithms; Exhaustive simulation; Learning (artificial intelligence); State of charge; State-of-the-art approach; Time varying behavior; Time-varying environments; Wireless sensor node; Sensor nodes"
"Aouf M., Liyanage L.","Analysis of high dimensionality yeast gene expression data using data mining","10.4028/www.scientific.net/AMM.197.515","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869234285&doi=10.4028%2fwww.scientific.net%2fAMM.197.515&partnerID=40&md5=ae59c1ae77c062067061a1d908e9991e","Data Mining is the process of discovering interesting knowledge from large amounts of data stored either in databases, data warehouses, or other information repositories. From biological studies, the Yeast Proteome Database (YPD) is a model for the organization and presentation of genome-wide functional data. Accordingly, a yeast gene expression which is a unicellular DNA is selected which contains 6103 genes and the database combined with a number of related dataset to create a general dataset. DNA-binding transcriptional regulators interpret the genome's regulatory code by binding to specific sequences to induce or repress gene expression. The gene products including RNA and protein are responsible for the development and functioning of all living membranes by 2 steps process, transcription and translation. Various transcription factors control gene transcription by binding to the promoter regions. Translation is the production of proteins from mRNA produced in transcription. In this study, out of the 169 transcription factors known to access yeast, we are considering those thought to be involved in the response of Hydrogen Peroxide (H2O2). They are 22 transcription factors. Each one is partitioned to 3 parts: TF with No H2O2, TF with Low H2O2 and TF with High H2O2. The aim of this paper was to enhance the effectiveness of the integration of hydrogen peroxide response data related to yeast gene expression. © (2012) Trans Tech Publications, Switzerland.","SAS enterprise miner and decision tree; Yeast gene expression dataset","Biological studies; Data sets; DNA-binding; Functional datas; Gene Expression Data; Gene products; Gene transcriptions; High dimensionality; Information repositories; Large amounts of data; Promoter region; Proteome database; Response data; Specific sequences; Transcriptional regulator; Data mining; Data warehouses; Decision trees; DNA sequences; Gene encoding; Hydrogen peroxide; RNA; Transcription factors; Yeast; Transcription"
"Aoun C.G., Lagadec L., Champeau J., Moussa J., Hanna E.","A High Abstraction Level Constraint for Object Localization in Marine Observatories","10.1109/CSCI.2017.105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060602432&doi=10.1109%2fCSCI.2017.105&partnerID=40&md5=4e4a1290e71f91022b897ffd103d2318","A sensor network is a specific type of network that consists of a set of distributed sensors with a main objective to observe and analyze its environment. Therefore, an underwater sensor network can be defined as a sensor network deployed underwater that monitors underwater activity. The sensors deployed, such as Hydrophones, are responsible for registering underwater activity and transfer it to more advanced components. The process of data exchange between the aforementioned components perfectly define the Marine Observatory (MO) concept. The first step towards the implementation of this concept is defining the environmental constraints and the required tools and components (Marine Cables, Specific Servers, etc). The logical and physical components that are used in these observatories supply interchange procedures between the various devices of the environment (Smart Sensors, Data Fusion Servers). In this paper, we present an extension to our already extended Meta-Model that is used to generate a new design tool (ArchiMO). Thus, we propose new constraints to be taken under consideration at design time. We illustrate our proposal with an example from the MO domain. Additionally, we generate the corresponding simulation code using our self-developed domain-specific model compiler. Our approach helps to reduce the complexity and time of the design activity. © 2017 IEEE.","Marine Observatories; Underwater Object Localization","Artificial intelligence; Data fusion; Electronic data interchange; Object recognition; Sensor networks; Distributed sensor; Domain specific modeling; Environmental constraints; Marine observatories; Object localization; Physical components; Underwater objects; Underwater sensor networks; Observatories"
"Aparicio P.R., Marcinkevics R., Wolfertstetter P.R., Wellmann S., Knorr C., Vogt J.E., Marcinkevics R.","Learning Medical Risk Scores for Pediatric Appendicitis","10.1109/ICMLA52953.2021.00243","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125832152&doi=10.1109%2fICMLA52953.2021.00243&partnerID=40&md5=4bae2bc180966fd5efc99f110eee26b9","Appendicitis is a common childhood disease, the management of which still lacks consolidated international criteria. In clinical practice, heuristic scoring systems are often used to assess the urgency of patients with suspected appendicitis. Previous work on machine learning for appendicitis has focused on conventional classification models, such as logistic regression and tree-based ensembles. In this study, we investigate the use of risk supersparse linear integer models (risk SLIM) for learning data-driven risk scores to predict the diagnosis, management, and complications in pediatric patients with suspected appendicitis on a dataset consisting of 430 children from a tertiary care hospital. We demonstrate the efficacy of our approach and compare the performance of learnt risk scores to previous analyses with random forests. Risk SLIM is able to detect medically meaningful features and outperforms the traditional appendicitis scores, while at the same time is better suited for the clinical setting than tree-based ensembles. © 2021 IEEE.","Decision support; Diagnosis; Interpretable machine learning; Pediatric appendicitis; Treatment","Decision support systems; Decision trees; Diagnosis; Logistic regression; Machine learning; Random forests; Childhood disease; Classification models; Clinical practices; Decision supports; Interpretable machine learning; Pediatric appendiciti; Risk score; Scoring systems; Treatment; Tree-based ensembles; Pediatrics"
"Aphinyanaphongs Y., Aliferis C.","Learning boolean queries for article quality filtering","10.3233/978-1-60750-949-3-263","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887117785&doi=10.3233%2f978-1-60750-949-3-263&partnerID=40&md5=de393aa4eac9ce69b4aa9aeda68471e5","Prior research has shown that Support Vector Machine models have the ability to identify high quality content-specific articles in the domain of internal medicine. These models, though powerful, cannot be used in Boolean search engines nor can the content of the models be verified via human inspection. In this paper, we use decision trees combined with several feature selection methods to generate Boolean query filters for the same domain and task. The resulting trees are generated automatically and exhibit high performance. The trees are understandable, manageable, and able to be validated by humans. The subsequent Boolean queries are sensible and can be readily used as filters by Boolean search engines. © 2004 IMIA. All rights reserved.","Artificial Intelligence; Information Storage and Retrieval; Medical Informatics; PubMed; Text Categorization","Artificial intelligence; Decision trees; Forestry; Medicine; Search engines; Text processing; Article qualities; Feature selection methods; Information storage and retrieval; Internal medicine; Medical informatics; PubMed; Support vector machine models; Text categorization; Boolean functions"
"Apicella A., Giugliano S., Isgrò F., Prevete R.","Exploiting auto-encoders and segmentation methods for middle-level explanations of image classification systems","10.1016/j.knosys.2022.109725","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137162220&doi=10.1016%2fj.knosys.2022.109725&partnerID=40&md5=ee9f31d711f70bfca02cdcce3dbcf8b5","A central issue addressed by the rapidly growing research area of eXplainable Artificial Intelligence (XAI) is to provide methods to give explanations for the behaviours of Machine Learning (ML) non-interpretable models after the training. Recently, it is becoming more and more evident that new directions to create better explanations should take into account what a good explanation is to a human user. This paper suggests taking advantage of developing an XAI framework that allows producing multiple explanations for the response of image a classification system in terms of potentially different middle-level input features. To this end, we propose an XAI framework able to construct explanations in terms of input features extracted by auto-encoders. We start from the hypothesis that some auto-encoders, relying on standard data representation approaches, could extract more salient and understandable input properties, which we call here Middle-Level input Features (MLFs), for a user with respect to raw low-level features. Furthermore, extracting different types of MLFs through different type of auto-encoders, different types of explanations for the same ML system behaviour can be returned. We experimentally tested our method on two different image datasets and using three different types of MLFs. The results are encouraging. Although our novel approach was tested in the context of image classification, it can potentially be used on other data types to the extent that auto-encoders to extract humanly understandable representations can be applied. © 2022 Elsevier B.V.","Explainable AI; Hierarchical; Interpretable models; Middle-level; XAI","Artificial intelligence; Classification (of information); Data mining; Hierarchical systems; Image segmentation; Signal encoding; Auto encoders; Auto segmentation; Explainable AI; Hierarchical; Image classification systems; Input features; Interpretable model; Middle-level; Segmentation methods; XAI; Image classification"
"Apicella A., Giugliano S., Isgró F., Prevete R.","Explanations in terms of Hierarchically organised Middle Level Features",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121596294&partnerID=40&md5=af51152dd0a5d4060f89d8608dd47f1d","The rapidly growing research area of eXplainable Artificial Intelligence (XAI) focuses on making Machine Learning systems' decisions more transparent and humanly understandable. One of the most successful XAI strategies is to provide explanations in terms of visualisations and, more specifically, low-level input features such as relevance scores or heat maps of the input, like sensitivity analysis or layer-wise relevance propagation methods. The main problem with such methods is that starting from the relevance of low-level features, the human user needs to identify the overall input properties that are salient. Thus, a current line of XAI research attempts to alleviate this weakness of low-level approaches, constructing explanations in terms of input features that represent more salient and understandable input properties for a user, which we call here Middle-Level input Features (MLF). In addition, another interesting and very recent approach is that of considering hierarchically organised explanations. Thus, in this paper, we investigate the possibility to combine both MLFs and hierarchical organisations. The potential advantages of providing explanations in terms of hierarchically organised MLFs are grounded on the possibility of exhibiting explanations to a different granularity of MLFs interacting with each other. We experimentally tested our approach on 300 Birds Species and Cars dataset. The results seem encouraging. © 2021 Copyright for this paper by its authors.","Explainable AI; Hierarchical; Interpretable models; Middle-level; XAI","Artificial intelligence; Learning systems; Explainable AI; Hierarchical; Input features; Interpretable model; Machine learning systems; Middle-level; Property; Relevance score; Research areas; XAI; Sensitivity analysis"
"Apicella A., Giugliano S., Isgrò F., Prevete R.","A General Approach to Compute the Relevance of Middle-Level Input Features","10.1007/978-3-030-68796-0_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104338436&doi=10.1007%2f978-3-030-68796-0_14&partnerID=40&md5=ca55b103ecef8d2807a1be14a668c92a","This work proposes a novel general framework, in the context of eXplainable Artificial Intelligence (XAI), to construct explanations for the behaviour of Machine Learning (ML) models in terms of middle-level features which represent perceptually salient input parts. One can isolate two different ways to provide explanations in the context of XAI: low and middle-level explanations. Middle-level explanations have been introduced for alleviating some deficiencies of low-level explanations such as, in the context of image classification, the fact that human users are left with a significant interpretive burden: starting from low-level explanations, one has to identify properties of the overall input that are perceptually salient for the human visual system. However, a general approach to correctly evaluate the elements of middle-level explanations with respect ML model responses has never been proposed in the literature. We experimentally evaluate the proposed approach to explain the decisions made by an Imagenet pre-trained VGG16 model on STL-10 images and by a customised model trained on the JAFFE dataset, using two different computational definitions of middle-level features and compare it with two different XAI middle-level methods. The results show that our approach can be used successfully in different computational definitions of middle-level explanations. © 2021, Springer Nature Switzerland AG.","Machine Learning; Middle-level features; XAI","Artificial intelligence; Human users; Human Visual System; Input features; Level method; Model response; Pattern recognition"
"Apicella A., Isgrò F., Prevete R., Tamburrini G.","Middle-Level Features for the Explanation of Classification Systems by Sparse Dictionary Methods","10.1142/S0129065720500409","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088853041&doi=10.1142%2fS0129065720500409&partnerID=40&md5=f2cd767b6b7c618adc5dd287fed1ee9b","Machine learning (ML) systems are affected by a pervasive lack of transparency. The eXplainable Artificial Intelligence (XAI) research area addresses this problem and the related issue of explaining the behavior of ML systems in terms that are understandable to human beings. In many explanation of XAI approaches, the output of ML systems are explained in terms of low-level features of their inputs. However, these approaches leave a substantive explanatory burden with human users, insofar as the latter are required to map low-level properties into more salient and readily understandable parts of the input. To alleviate this cognitive burden, an alternative model-agnostic framework is proposed here. This framework is instantiated to address explanation problems in the context of ML image classification systems, without relying on pixel relevance maps and other low-level features of the input. More specifically, one obtains sets of middle-level properties of classification inputs that are perceptually salient by applying sparse dictionary learning techniques. These middle-level properties are used as building blocks for explanations of image classifications. The achieved explanations are parsimonious, for their reliance on a limited set of middle-level image properties. And they can be contrastive, because the set of middle-level image properties can be used to explain why the system advanced the proposed classification over other antagonist classifications. In view of its model-agnostic character, the proposed framework is adaptable to a variety of other ML systems and explanation problems. © 2020 The Author(s).","machine learning; sparse coding; XAI and explainable artificial intelligence","Artificial intelligence; Image classification; Printing machinery; Building blockes; Classification system; Human being; Human users; Image classification systems; Image properties; Low-level features; Sparse dictionaries; Learning systems; book; human; image processing; machine learning; theoretical model; Dictionaries as Topic; Humans; Image Processing, Computer-Assisted; Machine Learning; Models, Theoretical"
"Apicella A., Isgrò F., Prevete R., Tamburrini G.","Contrastive explanations to classification systems using sparse dictionaries","10.1007/978-3-030-30642-7_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072963885&doi=10.1007%2f978-3-030-30642-7_19&partnerID=40&md5=a9518e09f9a124184772558529a97991","Providing algorithmic explanations for the decisions of machine learning systems to end users, data protection officers, and other stakeholders in the design, production, commercialisation and use of machine learning systems pipeline is an important and challenging research problem. Much work in this area focuses on image classification, where the required explanations can be given in terms of images, therefore making explanations relatively easy to communicate to end-users. For a classification problem, a contrastive explanation tries to understand why the classifier has not answered a particular class, say B, instead of the returned class A. Sparse dictionaries have been recently used to identify local image properties as main ingredients for a system producing humanly understandable explanations for the decisions of a classifier developed based on machine learning methods. In this paper, we show how the system mentioned above can be extended to produce contrastive explanations. © 2019, Springer Nature Switzerland AG.","Contrastive explanations; Explainable artificial intelligence; Machine learning; Sparse coding; XAI","Image analysis; Learning systems; Classification system; Commercialisation; Contrastive explanations; Image properties; On-machines; Research problems; Sparse coding; Sparse dictionaries; Machine learning"
"Apicella A., Isgrò F., Prevete R., Sorrentino A., Tamburrini G.","Explaining classification systems using sparse dictionaries",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071292279&partnerID=40&md5=2b6ba634f3e63ffa4a724afc7e2dea03","A pressing research topic is to find ways to explain the decisions of machine learning systems to end users, data officers, and other stakeholders. These explanations must be understandable to human beings. Much work in this field focuses on image classification, as the required explanations can rely on images, therefore making communication relatively easy, and may take into account the image as a whole. Here, we propose to exploit the representational power of sparse dictionaries to determine image local properties that can be used as crucial ingredients of humanly understandable explanations of classification decisions. © 2019 ESANN (i6doc.com). All rights reserved.",,"Neural networks; Classification decision; Classification system; End users; Human being; Local property; Research topics; Sparse dictionaries; Machine learning"
"Apicella A., Isgrò F., Prevete R., Tamburrini G., Vietri A.","Sparse dictionaries for the explanation of classification systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071137506&partnerID=40&md5=13db9f1f128fa256dff4ee8b0e0d969d","Providing algorithmic explanations for the decisions of machine learning systems to end users, data protection officers, and other stakeholders in the design, production, commercialization and use of machine learning systems pipeline is an important and challenging research problem. Crucial motivations to address this research problem can be advanced on both ethical and legal grounds. Notably, explanations of the decisions of machine learning systems appear to be needed to protect the dignity, autonomy and legitimate interests of people who are subject to automatic decision-making. Much work in this area focuses on image classification, where the required explanations can be given in terms of images, therefore making explanations relatively easy to communicate to end users. In this paper we discuss how the representational power of sparse dictionaries can be used to identify local image properties as main ingredients for producing humanly understandable explanations for the decisions of a classifier developed on the basis of machine learning methods. © 2019 The Author(s).","Explainable artificial intelligence; Machine learning; Sparse coding; XAI","Behavioral research; Data privacy; Decision making; Learning systems; Automatic decision; Classification system; End users; Image properties; Machine learning methods; Research problems; Sparse coding; Sparse dictionaries; Machine learning"
"Apolloni B.","Inferring statistical trends of the COVID19 pandemic from current data. Where probability meets fuzziness","10.1016/j.ins.2021.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108301996&doi=10.1016%2fj.ins.2021.06.011&partnerID=40&md5=2d7a036b63d0cbf5e28439c332a0e02c","We introduce unprecedented tools to infer approximate evolution features of the COVID19 outbreak when these features are altered by containment measures. In this framework we present: (1) a basic tool to deal with samples that are both truncated and non independently drawn, and (2) a two-phase random variable to capture a game changer along a process evolution. To overcome these challenges we lie in an intermediate domain between probability models and fuzzy sets, still maintaining probabilistic features of the employed statistics as the reference KPI of the tools. This research uses as a benchmark the daily cumulative death numbers of COVID19 in two countries, with no any ancillary data. Numerical results show: (i) the model capability of capturing the inflection point and forecasting the end-of-infection time and related outbreak size, and (ii) the out-performance of the model inference method according to conventional indicators. © 2021 Elsevier Inc.","COVID19 pandemic; Explainable Artificial Intelligence; Shifted-Pareto distribution; Statistics from non-iid samples; Two-phase processes","Fuzzy sets; Numerical methods; COVID19 pandemic; Current data; Explainable artificial intelligence; Probabilistics; Probability modelling; Process evolution; Shifted-pareto distribution; Statistic from non-iid sample; Two phase; Two-phase process; Pareto principle"
"Apolloni B., Damiani E.","Learning simplified functions to understand",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098913232&partnerID=40&md5=878d81531160965cbf27559628294ff9","We propose an unprecedented approach to post-hoc interpretable machine learning. Facing a complex phenomenon, rather than fully capturing its mechanisms through a universal learner, albeit structured in modular building blocks, we train a robust neural network, no matter its complexity, to use as an oracle. Then we approximate its behavior via a linear combination of simple, explicit functions of its input. Simplicity is achieved by (i) marginal functions mapping individual inputs to the network output, (ii) the same consisting of univariate polynomials with a low degree,(iii) a small number of polynomials being involved in the linear combination, whose input is properly granulated. With this contrivance, we handle various real-world learning scenarios arising from expertise and experimental frameworks’ composition. They range from cooperative training instances to transfer learning. Concise theoretical considerations and comparative numerical experiments further detail and support the proposed approach . © 2020 CEUR-WS. All rights reserved.","Compatible explanation; Explainable AI; Minimum description length; Post-hoc Intepretable ML; Ridge polynomials; Transfer learning","Complex networks; Modular construction; Linear combinations; Low degree; Marginal function; Modular buildings; Numerical experiments; Real-world learning; Simplified functions; Univariate; Transfer learning"
"Apolloni B., Esposito A., Malchiodi D., Orovas C., Palmas G., Taylor J.G.","A general framework for learning rules from data","10.1109/TNN.2004.836249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-9244240796&doi=10.1109%2fTNN.2004.836249&partnerID=40&md5=852ab47b69649ca792b485977843bd1e","With the aim of getting understandable symbolic rules to explain a given phenomenon, we split the task of learning these rules from sensory data in two phases: a multilayer perceptron maps features into propositional variables and a set of subsequent layers operated by a PAC-like algorithm learns Boolean expressions on these variables. The special features of this procedure are that: i) the neural network is trained to produce a Boolean output having the principal task of discriminating between classes of inputs; ii) the symbolic part is directed to compute rules within a family that is not known a priori; iii) the welding point between the two learning systems is represented by a feedback based on a suitability evaluation of the computed rules. The procedure we propose is based on a computational learning paradigm set up recently in some papers in the fields of theoretical computer science, artificial intelligence and cognitive systems. The present article focuses on information management aspects of the procedure. We deal with the lack of prior information about the rules through learning strategies that affect both the meaning of the variables and the description length of the rules into which they combine. The paper uses the task of learning to formally discriminate among several emotional states as both a working example and a test bench for a comparison with previous symbolic and subsymbolic methods in the field. © 2004 IEEE.","Edge pulling functions; Fuzzy relaxations; Hybrid systems; Learning fitness; Ockham razor; PAC-learning; Symbolic feedbacks; Understandable learning","Artificial intelligence; Boolean functions; Cognitive systems; Fuzzy sets; Learning algorithms; Multilayer neural networks; Self organizing maps; Edge pulling functions; Fuzzy relaxations; Hybrid systems; Learning fitness; Ockham razor; Probably approximately correct learning algorithms; Symbolic feedbacks; Understandable learning; Learning systems; algorithm; article; artificial intelligence; artificial neural network; automated pattern recognition; biomimetics; classification; computer assisted diagnosis; computer simulation; decision support system; emotion; evaluation; human; mental stress; methodology; speech perception; statistical analysis; statistical model; Algorithms; Artificial Intelligence; Biomimetics; Computer Simulation; Data Interpretation, Statistical; Decision Support Techniques; Diagnosis, Computer-Assisted; Emotions; Humans; Logistic Models; Neural Networks (Computer); Pattern Recognition, Automated; Speech Perception; Stress, Psychological"
"Apolloni B., Bassis S., Brega A., Gaito S., Malchiodi D., Valcamonica N., Zanaboni A.M.","Monitoring of car driving awareness from biosignals","10.1007/978-3-540-45216-4_30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142156564&doi=10.1007%2f978-3-540-45216-4_30&partnerID=40&md5=8d6f2d90456b0f58147784acd58229dc","We infer symbolic rules for deciding the awareness state of a driver on the basis of physiological signals traced on his body through non invasive techniques. We use a standard device for collecting signals and a three-level procedure for: 1) extracting features from them, 2) computing Boolean independent components of the features acting as propositional variables, and 3) inferring Boolean normal forms on these variables deciding the driver awareness. In spite of their symbolic form, these formulas are not easily interpretable, rather they represent a sort of Boolean wavelets for describing the driver emotional state. A set of experiments are shown on a benchmark expressly drawn from a car driver simulator by the Psychology Department of Queen University of Belfast. © Springer-Verlag Berlin Heidelberg 2003.",,"Computers; Boolean normal form; Extracting features; Independent components; Noninvasive technique; Physiological signals; Propositional variables; Standard devices; University of Belfast; Artificial intelligence"
"Apon T.S., Hasan M.M., Islam A., Alam M.G.R.","Demystifying Deep Learning Models for Retinal OCT Disease Classification using Explainable AI","10.1109/CSDE53843.2021.9718400","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127871426&doi=10.1109%2fCSDE53843.2021.9718400&partnerID=40&md5=847b03fa648471ed048e7eab6547bb23","In the world of medical diagnostics, the adoption of various deep learning techniques is quite common as well as effective, and its statement is equally true when it comes to implementing it into the retina Optical Coherence Tomography (OCT) sector. However, firstly, these techniques have the black box characteristics that prevent the medical professionals from completely trusting the results generated from them. Secondly, the lack of precision of these methods restricts their implementation in clinical and complex cases, and finally, the existing works and models on the OCT classification are substantially large and complicated and they require a considerable amount of memory and computational power, reducing the quality of classifiers in real-time applications. To meet these problems, in this paper a self-developed CNN model has been proposed which is comparatively smaller and simpler along with the use of Lime that introduces Explainable AI to the study and helps to increase the interpretability of the model. This addition will be an asset to the medical experts for getting major and detailed information and will help them in making final decisions and will also reduce the opacity and vulnerability of the conventional deep learning models. © IEEE 2022.","AI in Healthcare; Deep Neural Network; Explainable AI; Image Classification; Lime; Medical Image Processing; Retinal OCT","Diagnosis; Image classification; Lime; Medical imaging; Ophthalmology; Optical data processing; Optical tomography; AI in healthcare; Black boxes; Disease classification; Explainable AI; Images classification; Learning models; Learning techniques; Medical diagnostics; Medical images processing; Retinal optical coherence tomography; Deep neural networks"
"Aporna A.A., Azad I., Amlan N.S., Mehedi M.H.K., Mahbub M.J.A., Rasel A.A.","Classifying Offensive Speech of Bangla Text and Analysis Using Explainable AI","10.1007/978-3-031-12638-3_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135930200&doi=10.1007%2f978-3-031-12638-3_12&partnerID=40&md5=8b8852c043cda1b9e1e56584774c6065","The rapid rise of social networking websites and blogging sites not only provides freedom of expression or speech, but also allows people to express society-prohibited behaviors such as online harassment and cyberbullying, which are known as offensive speech or hate speech. Despite the fact that various research work has been done on detecting hate or abusive speech on social networking websites in the English language, the opportunities for research for detecting offensive or abusive speech in the Bengali language remain open due to the computational resource constraints or the lack of standard-labeled datasets for accurate or effective Natural Language Processing (NLP) of Bangla language. In this paper, an Explainable AI approach is used for analysis as well as for detecting offensive comments or speech in the Bengali language is proposed. Moreover, Convolutional Neural Network (CNN) model is used to extract and classify features. Since the Neural Network is time-consuming for extracting features from the dataset, our proposed approach allows people to save time and effort. In the dataset, we classified all user’s comments from social media comment sections into four categories: religious, personal, geopolitical, and political. Our proposed model successfully detects Bangla offensive speeches from the dataset (Bengali Hate Speech Dataset) by evaluating Machine Learning algorithms like linear and tree-based models and Neural Networks like CNN, Bi-LSTM, Conv-LSTM, and SVM models. Moreover, we calculate scores for completeness and sufficiency to assess the quality of explanations in terms of fidelity, achieving the results with the accuracy of 78% score, significantly outperforming ML and DNN baselines. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Bangla offensive speech classification; CNN; DNN; Explainable AI; NLP","Convolutional neural networks; Long short-term memory; Natural language processing systems; Social networking (online); Speech recognition; Support vector machines; Text processing; Bangla offensive speech classification; Bengali language; Convolutional neural network; DNN; Explainable AI; Language processing; Natural language processing; Natural languages; Social-networking; Speech classification; Learning algorithms"
"Apostolo G.H., Sampaio I.G.B., Viterbo J.","Feature selection on database optimization for Wi-Fi fingerprint indoor positioning","10.1016/j.procs.2019.09.180","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076258979&doi=10.1016%2fj.procs.2019.09.180&partnerID=40&md5=939c090fe821d499a2995ec75deb1e91","Indoor location-based services have become very popular, principally, because of its wide and valuable applications. On that context, Wi-fi fingerprinting based on the received signal strength indicator (RSSI) has become very popular, due the fact that RSSI values are easily acquired. On the Wi-fi fingerprint method, machine learning algorithms are trained on the constructed fingerprint database and then used on a new entry to give the indoor location based on its estimations. Choosing the correct machine learning algorithm is one of the main problems in the literature. However the database sizes used during the training phase is also one of the main concerns. In this paper, a proposed feature selection method used on the original UJIIndoorLoc database created a smaller version of it, with the 30 highest RSSIs after the APIDs responsible for then in descending order, and created even smaller database subsets. Both databases, the original UJIIndoor Loc database and ours, were split into smaller subsets that were used on the classification problem according the DESIP method proposed in [1]. Six machine learning algorithms were deployed for training and testing the two database subsets with the classification attributes modified for symbolic localization. The J48 with the AdaBoost iterative algorithm gave the best results on both database subsets. The minimized database subsets showed smaller elapsed time results for all the classifications that were done. The accuracy results show similar results for both database subsets, on building and floor classification. Although, on the region attribute, the database subset with 520 attributes got better accuracy results than the reduced one. © 2019 The Author(s). Published by Elsevier B.V.","Database Optimization; Feature Selection; Machine Learning Algorithms; Wi-fi Indoor Localization","Adaptive boosting; Database systems; Feature extraction; Genetic algorithms; Indoor positioning systems; Iterative methods; Knowledge based systems; Learning systems; Location based services; Machine learning; Mobile computing; Set theory; Telecommunication services; Wireless local area networks (WLAN); Database optimization; Feature selection methods; Fingerprint database; Indoor localization; Iterative algorithm; Received signal strength indicators; Training and testing; Wi-Fi fingerprinting; Classification (of information)"
"Apostolova L.G., Hwang K.S., Kohannim O., Avila D., Elashoff D., Jack Jr. C.R., Shaw L., Trojanowski J.Q., Weiner M.W., Thompson P.M.","ApoE4 effects on automated diagnostic classifiers for mild cognitive impairment and Alzheimer's disease","10.1016/j.nicl.2013.12.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896512293&doi=10.1016%2fj.nicl.2013.12.012&partnerID=40&md5=2ef9ca6d97a431a1eb8f73e24c768ef3","Biomarkers are the only feasible way to detect and monitor presymptomatic Alzheimer's disease (AD). No single biomarker can predict future cognitive decline with an acceptable level of accuracy. In addition to designing powerful multimodal diagnostic platforms, a careful investigation of the major sources of disease heterogeneity and their influence on biomarker changes is needed. Here we investigated the accuracy of a novel multimodal biomarker classifier for differentiating cognitively normal (NC), mild cognitive impairment (MCI) and AD subjects with and without stratification by ApoE4 genotype. 111 NC, 182 MCI and 95 AD ADNI participants provided both structural MRI and CSF data at baseline. We used an automated machine-learning classifier to test the ability of hippocampal volume and CSF Aβ, t-tau and p-tau levels, both separately and in combination, to differentiate NC, MCI and AD subjects, and predict conversion. We hypothesized that the combined hippocampal/CSF biomarker classifier model would achieve the highest accuracy in differentiating between the three diagnostic groups and that ApoE4 genotype will affect both diagnostic accuracy and biomarker selection. The combined hippocampal/CSF classifier performed better than hippocampus-only classifier in differentiating NC from MCI and NC from AD. It also outperformed the CSF-only classifier in differentiating NC vs. AD. Our amyloid marker played a role in discriminating NC from MCI or AD but not for MCI vs. AD. Neurodegenerative markers contributed to accurate discrimination of AD from NC and MCI but not NC from MCI. Classifiers predicting MCI conversion performed well only after ApoE4 stratification. Hippocampal volume and sex achieved AUC = 0.68 for predicting conversion in the ApoE4-positive MCI, while CSF p-tau, education and sex achieved AUC = 0.89 for predicting conversion in ApoE4-negative MCI. These observations support the proposed biomarker trajectory in AD, which postulates that amyloid markers become abnormal early in the disease course while markers of neurodegeneration become abnormal later in the disease course and suggests that ApoE4 could be at least partially responsible for some of the observed disease heterogeneity. © 2013 The Authors.","Abeta; ADNI; Alzheimer's disease; Diagnosis; Hippocampus atrophy; Tau","amyloid beta protein; apolipoprotein E4; biological marker; tau protein; threonine; apolipoprotein E4; biological marker; nerve protein; biological marker; adult; aged; Alzheimer disease; area under the curve; article; automation; brain size; cognition; controlled study; diagnostic accuracy; diagnostic test accuracy study; differential diagnosis; disease course; educational status; female; genotype; heterozygote; hippocampus; human; major clinical study; male; middle aged; mild cognitive impairment; Mini Mental State Examination; neuroimaging; nuclear magnetic resonance imaging; nuclear magnetic resonance scanner; predictive value; priority journal; protein cerebrospinal fluid level; protein phosphorylation; support vector machine; very elderly; algorithm; Alzheimer disease; atrophy; cerebrospinal fluid; Cognitive Dysfunction; computer assisted diagnosis; machine learning; metabolism; organ size; pathology; procedures; reproducibility; sensitivity and specificity; tissue distribution; Article; brain atrophy; cerebrospinal fluid; degenerative disease; gender; nerve degeneration; protein analysis; protein function; Aged; Aged, 80 and over; Algorithms; Alzheimer Disease; Apolipoprotein E4; Atrophy; Biomarkers; Cognitive Dysfunction; Diagnosis, Computer-Assisted; Diagnosis, Differential; Female; Hippocampus; Humans; Machine Learning; Male; Middle Aged; Nerve Tissue Proteins; Organ Size; Reproducibility of Results; Sensitivity and Specificity; Tissue Distribution"
"Appriou A., Cichocki A., Lotte F.","Towards robust neuroadaptive HCI: Exploring modern machine learning methods to estimate mental workload from EEG signals","10.1145/3170427.3188617","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052023457&doi=10.1145%2f3170427.3188617&partnerID=40&md5=39e30d630a3174355a24386d2fb6a99a","Estimating mental workload from brain signals such as Electroencephalography (EEG) has proven very promising in multiple Human-Computer Interaction (HCI) applications, e.g., to design games or educational applications with adaptive difficulty, or to assess how cognitively difficult to use an interface can be. However, current EEG-based workload estimation may not be robust enough for some practical applications. Indeed, the currently obtained workload classification accuracies are relatively low, making the resulting estimations not fully trustable. This paper thus studies promising modern machine learning algorithms, including Riemannian geometry-based methods and Deep Learning, to estimate workload from EEG signals. We study them with both user-specific and user-independent calibration, to go towards calibration-free systems. Our results suggested that a shallow Convolutional Neural Network obtained the best performance in both conditions, outperforming state-of-the-art methods on the used data sets. This suggests that Deep Learning can bring new possibilities in HCI. Copyright held by the owner/author(s).","Brain-Computer Interfaces; Deep Learning; EEG; Machine Learning; Mental Workload; Neuroadaptive technology; Neuroergonomics","Calibration; Computer games; Convolutional neural networks; Deep learning; Electroencephalography; Electrophysiology; Geometry; Human computer interaction; Human engineering; Learning systems; Calibration-free systems; Classification accuracy; Educational Applications; Human computer interaction (HCI); Mental workload; Riemannian geometry; State-of-the-art methods; User independents; Learning algorithms"
"Apté C., Weiss S.","Data mining with decision trees and decision rules","10.1016/s0167-739x(97)00021-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031274825&doi=10.1016%2fs0167-739x%2897%2900021-6&partnerID=40&md5=2217c14dd4d32bae7cd7af77e93a36db","This paper describes the use of decision tree and rule induction in data-mining applications. Of methods for classification and regression that have been developed in the fields of pattern recognition, statistics, and machine learning, these are of particular interest for data mining since they utilize symbolic and interpretable representations. Symbolic solutions can provide a high degree of insight into the decision boundaries that exist in the data, and the logic underlying them. This aspect makes these predictive-mining techniques particularly attractive in commercial and industrial data-mining applications. We present here a synopsis of some major state-of-the-art tree and rule mining methodologies, as well as some recent advances.","Data mining; Decision tree; Rule induction","Decision theory; Learning systems; Pattern recognition; Regression analysis; Trees (mathematics); Data mining; Decision rules; Decision trees; Rule induction; Data acquisition"
"Aquino G., Costa M.G.F., Costa Filho C.F.F.","Explaining One-Dimensional Convolutional Models in Human Activity Recognition and Biometric Identification Tasks","10.3390/s22155644","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136341007&doi=10.3390%2fs22155644&partnerID=40&md5=3a530a49715e9d49d72972ab521644d0","Due to wearables’ popularity, human activity recognition (HAR) plays a significant role in people’s routines. Many deep learning (DL) approaches have studied HAR to classify human activities. Previous studies employ two HAR validation approaches: subject-dependent (SD) and subject-independent (SI). Using accelerometer data, this paper shows how to generate visual explanations about the trained models’ decision making on both HAR and biometric user identification (BUI) tasks and the correlation between them. We adapted gradient-weighted class activation mapping (grad-CAM) to one-dimensional convolutional neural networks (CNN) architectures to produce visual explanations of HAR and BUI models. Our proposed networks achieved 0.978 and 0.755 accuracy, employing both SD and SI. The proposed BUI network achieved 0.937 average accuracy. We demonstrate that HAR’s high performance with SD comes not only from physical activity learning but also from learning an individual’s signature, as in BUI models. Our experiments show that CNN focuses on larger signal sections in BUI, while HAR focuses on smaller signal segments. We also use the grad-CAM technique to identify database bias problems, such as signal discontinuities. Combining explainable techniques with deep learning can help models design, avoid results overestimation, find bias problems, and improve generalization capability. © 2022 by the authors.","accelerometer data; biometric user identification; convolutional neural networks; deep learning; explainable AI; grad-cam; human activity recognition","Accelerometers; Behavioral research; Convolution; Convolutional neural networks; Decision making; Deep neural networks; Learning systems; Accelerometer data; Activation mapping; Biometric user identification; Convolutional neural network; Deep learning; Explainable AI; Grad-cam; Human activity recognition; One-dimensional; User identification; Biometrics; biometry; factual database; human; human activities; Biometric Identification; Databases, Factual; Human Activities; Humans; Neural Networks, Computer"
"Aquino M.S., De Souza F.F., Frery A.C., De Souza D.A.C.M., Fujioka R.C.","Supporting adaptive virtual environments with intelligent agents","10.1109/ISDA.2007.4389611","https://www.scopus.com/inward/record.uri?eid=2-s2.0-48349105478&doi=10.1109%2fISDA.2007.4389611&partnerID=40&md5=a5949cd499b35367cd1d8e03de3623b0","Adaptive Virtual Environments have been developed aiming at generating 3D environments adapted to user's necessities and interests. Navigation aid techniques and content adaptation improve user interaction in such environments. This article presents a system for supporting real time adaptive virtual environments. A multiagents architecture is used to manage contents updating in the virtual world according to user profile evolution. Its agents utilize the User Model, the Environment Model and a 3D objects database for decision-making on the necessary updating. A manager agent is responsible to accompany the user's actions through the user interface to collect information that may be used to update his/her profile. This agent also controls the communication between agents. © 2007 IEEE.",,"Agents; Artificial intelligence; Computer software; Decision making; Intelligent agents; Intelligent control; Problem solving; Systems analysis; Three dimensional; User interfaces; Virtual reality; 3 D objects; 3-D environments; Content adaptation; Environment modeling; Evolution (CO); International conferences; Navigation aids; Real time; Systems design; User interactions; User modelling; User profiling; Virtual environments (VE); Virtual world (VW); Intelligent systems"
"Arab K., Bouida Z., Ibnkahla M.","Artificial Intelligence for Diabetes Mellitus Type II: Forecasting and Anomaly Detection","10.1109/WCNC.2019.8885802","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074791110&doi=10.1109%2fWCNC.2019.8885802&partnerID=40&md5=be7b2ee6784591dc2adcb74eb08c3190","Diabetes Mellitus Type II (T2D) is a Chronic Disease and is the most common type of Diabetes in the world, responsible for 95% of all Diabetes patients. T2D is a very complex disease and requires a large amount of self-management from the patient in order to maintain a healthy and threat-free lifestyle. Therefore, we develop in this paper a data analytics solution to assist in the self-management of T2D patients through several methods consisting of a rule-based system, anomaly detection, and threat forecasting. © 2019 IEEE.","Anomaly Detection; Artificial Intelligence; Chronic Diseases; Diabetes; Forecasting","Artificial intelligence; Data Analytics; Diseases; Forecasting; Information management; Medical problems; Chronic disease; Complex disease; Diabetes mellitus; Diabetes patients; Large amounts; Self management; Type II; Anomaly detection"
"Arabameri A., Rezaie F., Pal S.C., Cerda A., Saha A., Chakrabortty R., Lee S.","Modelling of piping collapses and gully headcut landforms: Evaluating topographic variables from different types of DEM","10.1016/j.gsf.2021.101230","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108431485&doi=10.1016%2fj.gsf.2021.101230&partnerID=40&md5=bcd448a0af2335baa49e89180575fd31","The geomorphic studies are extremely dependent on the quality and spatial resolution of digital elevation model (DEM) data. The unique terrain characteristics of a particular landscape are derived from DEM, which are responsible for initiation and development of ephemeral gullies. As the topographic features of an area significantly influences on the erosive power of the water flow, it is an important task the extraction of terrain features from DEM to properly research gully erosion. Alongside, topography is highly correlated with other geo-environmental factors i.e. geology, climate, soil types, vegetation density and floristic composition, runoff generation, which ultimately influences on gully occurrences. Therefore, terrain morphometric attributes derived from DEM data are used in spatial prediction of gully erosion susceptibility (GES) mapping. In this study, remote sensing-Geographic information system (GIS) techniques coupled with machine learning (ML) methods has been used for GES mapping in the parts of Semnan province, Iran. Current research focuses on the comparison of predicted GES result by using three types of DEM i.e. Advanced Land Observation satellite (ALOS), ALOS World 3D-30 m (AW3D30) and Advanced Space borne Thermal Emission and Reflection Radiometer (ASTER) in different resolutions. For further progress of our research work, here we have used thirteen suitable geo-environmental gully erosion conditioning factors (GECFs) based on the multi-collinearity analysis. ML methods of conditional inference forests (Cforest), Cubist model and Elastic net model have been chosen for modelling GES accordingly. Variable's importance of GECFs was measured through sensitivity analysis and result show that elevation is the most important factor for occurrences of gullies in the three aforementioned ML methods (Cforest = 21.4, Cubist = 19.65 and Elastic net = 17.08), followed by lithology and slope. Validation of the model's result was performed through area under curve (AUC) and other statistical indices. The validation result of AUC has shown that Cforest is the most appropriate model for predicting the GES assessment in three different DEMs (AUC value of Cforest in ALOS DEM is 0.994, AW3D30 DEM is 0.989 and ASTER DEM is 0.982) used in this study, followed by elastic net and cubist model. The output result of GES maps will be used by decision-makers for sustainable development of degraded land in this study area. © 2021 China University of Geosciences (Beijing) and Peking University","Advanced land observation satellite (ALOS); Cforest; Cubist; Digital elevation model (DEM); Elastic net; Gully erosion susceptibility (GES)","ALOS; digital elevation model; erosion rate; fluvial geomorphology; gully erosion; land degradation; modeling; morphometry; piping; satellite data; sediment transport; topographic effect; Iran; Semnan"
"Arabshahi F., Huang F., Anandkumar A., Butts C.T., Fitzhugh S.M.","Are you going to the party: Depends, who else is coming?: [Learning hidden group dynamics via conditional latent tree models]","10.1109/ICDM.2015.146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963631836&doi=10.1109%2fICDM.2015.146&partnerID=40&md5=8e705c6844d3a9e4beecb317a535c48f","Scalable probabilistic modeling and prediction in high dimensional multivariate time-series, such as dynamic social networks with co-evolving nodes and edges, is a challenging problem, particularly for systems with hidden sources of dependence and/or homogeneity. Here, we address this problem through the discovery of hierarchical latent groups. We introduce a family of Conditional Latent Tree Models (CLTM), in which tree-structured latent variables incorporate the unknown groups. The latent tree itself is conditioned on observed covariates such as seasonality, historical activity, and node attributes. We propose a statistically efficient framework for learning both the hierarchical tree structure and the parameters of the CLTM. We demonstrate competitive performance on two real world datasets, one from the students' attempts at answering questions in a psychology MOOC and the other from Twitter users participating in an emergency management discussion and interacting with one another. In addition, our modeling framework provides valuable and interpretable information about the hidden group structures and their effect on the evolution of the time series. © 2015 IEEE.","Conditional latent tree models; Dynamic networks; Hierarchical latent groups; Multivariate time series","Forestry; Risk management; Time series; Trees (mathematics); Competitive performance; Dynamic network; Dynamic social networks; Emergency management; Hierarchical latent groups; Multivariate time series; Probabilistic modeling; Tree models; Data mining"
"Arai Y.","Study of cognitive promotion stage in the visual system and examples of the element technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017356418&partnerID=40&md5=474a0693cf1dbfa6b9f6b5fe4eb9fc59","One of the major purposes of artificial intelligence and intellectual processing by a computer (machine) is an agency of the human function. As a result, the accuracy can be improved compared to the work done by human kind, and the processing time is expected to be shortened. Therefore, such technology has been applied in various fields. On the other hand, a lot of attempts to engineering investigated, simulated and application of human sensibility have been studied. Man and machine, should work together in responsible for the parts being good at in each. Therefore, the machine is expected to support the human ability for taking its full advantage. In addition, the machine is expected to have the effect of the stimulus even for unknown functions such as human sensibility and subjective functions, known as Kansei. In this paper, an attempt to support the intelligent processing ability possessed by human beings is picked up and studied. At first, we generally consider the stage of cognitive promotion in the visual world. Then, we will introduce Consecutive Fuzzy Reasoning with Preconception as a simulated example of recognition of the moving image. We also introduce the emotional contour extraction of graphic by inference as an application example of the sensibility in image processing. Moreover, we will introduce the emotional image classification as an example of the promotion of awareness by stimulating the human intellectual processing. Finally, we refer to the future prospects of these technologies.","Cognitive promotion; Kansei; Support; Visual","Electronics engineering; Supports; Application examples; Cognitive promotion; Contour Extraction; Element technology; Future prospects; Intelligent processing; Kansei; Visual; Image processing"
"Araki B., Vodrahalli K., Leech T., Vasile C.-I., Donahue M., Rus D.","Deep bayesian nonparametric learning of rules and plans from demonstrations with a learned automaton prior",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102540258&partnerID=40&md5=75ddb37fff68518ee71e3eecb46aed2b","We introduce a method to learn imitative policies from expert demonstrations that are interpretable and manipulable. We achieve interpretability by modeling the interactions between high-level actions as an automaton with connections to formal logic. We achieve manipulability by integrating this automaton into planning, so that changes to the automaton have predictable effects on the learned behavior. These qualities allow a human user to first understand what the model has learned, and then either correct the learned behavior or zeroshot generalize to new, similar tasks. We build upon previous work by no longer requiring additional supervised information which is hard to collect in practice. We achieve this by using a deep Bayesian nonparametric hierarchical model. We test our model on several domains and also show results for a real-world implementation on a mobile robotic arm platform. Copyright 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Formal logic; Hierarchical systems; Robots; Bayesian; Hierarchical model; Human users; Interpretability; Manipulability; Mobile robotic; Non-parametric; Real-world implementation; Deep learning"
"Aral S., Ishida T.","Learning for human-agent collaboration on the semantic web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-10444273106&partnerID=40&md5=558b706f49977606f4c590be74260616","Semantic Web is a challenging framework to make Web information machine readable or understandable, but it seems not enough to make human's requirements for collecting and utilizing information automatically. The Agent technology becomes hopeful approach to bridge the gap between humans and machines. Agents may be autonomous and intelligent entities that may travel among agents and human. They get the requirements from human or other agents, and offer an appropriate solution through consulting among them. The main difference between agent and ordinary software development is the issue of coordination, cooperation and learning. This issue is very important for utilizing the web information. In this paper, we attempt to give an overview and research challenges with respect to the combination of machine learning and agent technologies with Semantic Web from the perspective of interaction as well as interoperability among agents and humans.",,"Approximation theory; Computer supported cooperative work; Electronic commerce; Learning systems; Multi agent systems; Problem solving; Semantics; Agent technology; Computer games; Human-agent collaboration; Interface design; World Wide Web"
"Aramyan S., McGregor K., Sandeep S., Haczku A.","SP-A binding to the SARS-CoV-2 spike protein using hybrid quantum and classical in silico modeling and molecular pruning by Quantum Approximate Optimization Algorithm (QAOA) Based MaxCut with ZDOCK","10.3389/fimmu.2022.945317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139082230&doi=10.3389%2ffimmu.2022.945317&partnerID=40&md5=01bb44ea1ff115b41afd97dd7986c126","The pulmonary surfactant protein A (SP-A) is a constitutively expressed immune-protective collagenous lectin (collectin) in the lung. It binds to the cell membrane of immune cells and opsonizes infectious agents such as bacteria, fungi, and viruses through glycoprotein binding. SARS-CoV-2 enters airway epithelial cells by ligating the Angiotensin Converting Enzyme 2 (ACE2) receptor on the cell surface using its Spike glycoprotein (S protein). We hypothesized that SP-A binds to the SARS-CoV-2 S protein and this binding interferes with ACE2 ligation. To study this hypothesis, we used a hybrid quantum and classical in silico modeling technique that utilized protein graph pruning. This graph pruning technique determines the best binding sites between amino acid chains by utilizing the Quantum Approximate Optimization Algorithm (QAOA)-based MaxCut (QAOA-MaxCut) program on a Near Intermediate Scale Quantum (NISQ) device. In this, the angles between every neighboring three atoms were Fourier-transformed into microwave frequencies and sent to a quantum chip that identified the chemically irrelevant atoms to eliminate based on their chemical topology. We confirmed that the remaining residues contained all the potential binding sites in the molecules by the Universal Protein Resource (UniProt) database. QAOA-MaxCut was compared with GROMACS with T-REMD using AMBER, OPLS, and CHARMM force fields to determine the differences in preparing a protein structure docking, as well as with Goemans-Williamson, the best classical algorithm for MaxCut. The relative binding affinity of potential interactions between the pruned protein chain residues of SP-A and SARS-CoV-2 S proteins was assessed by the ZDOCK program. Our data indicate that SP-A could ligate the S protein with a similar affinity to the ACE2-Spike binding. Interestingly, however, the results suggest that the most tightly-bound SP-A binding site is localized to the S2 chain, in the fusion region of the SARS-CoV-2 S protein, that is responsible for cell entry Based on these findings we speculate that SP-A may not directly compete with ACE2 for the binding site on the S protein, but interferes with viral entry to the cell by hindering necessary conformational changes or the fusion process. Copyright © 2022 Aramyan, McGregor, Sandeep and Haczku.","glycosylation; immunoprotection; in silico; MaxCut; QAOA; quantum computation (QC); SARS-CoV-2; SP-A","angiotensin converting enzyme 2; coronavirus spike glycoprotein; amino acid; coronavirus spike glycoprotein; dipeptidyl carboxypeptidase; lung surfactant; spike protein, SARS-CoV-2; surfactant protein A; amino acid sequence; Article; binding affinity; biochemical analysis; bioinformatics; computer model; Fourier transform spectroscopy; glycosylation; machine learning; microwave radiation; molecular docking; molecular pruning; nonhuman; protein database; protein structure; quantum approximate optimization algorithm based MaxCut; quantum mechanics; Severe acute respiratory syndrome coronavirus 2; structural bioinformatics; algorithm; computer simulation; human; metabolism; Algorithms; Amino Acids; Angiotensin-Converting Enzyme 2; Computer Simulation; COVID-19; Humans; Peptidyl-Dipeptidase A; Pulmonary Surfactant-Associated Protein A; Pulmonary Surfactants; SARS-CoV-2; Spike Glycoprotein, Coronavirus"
"Arancibia G.V., Bustamante O.P., Vigneau G.H., Allende-Cid H., Fuentelaba G.S., Nieto V.A.","Estimation of Moisture Content in Thickened Tailings Dams: Machine Learning Techniques Applied to Remote Sensing Images","10.1109/ACCESS.2021.3053767","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100262902&doi=10.1109%2fACCESS.2021.3053767&partnerID=40&md5=fdebc381832a5ed9c115a3ff90340961","Chile is one of the major producers of copper in the world, and as such is responsible for 1.7 million tons of tailings per day. While the most commonly used deposit to store this type of mining waste is historically tailings sand dams, the mining industry has over the last two decades been inclined toward thickened tailings dams (TTD) because of their advantages in water resource recovery, lower environmental impact, and better physical and chemical stability over conventional deposits. Within the geotechnical area, one key requirement of TDD, is the need to monitor moisture content (w%) during operation, which is today mostly performed in situ - via conventional geotechnical or simple visual means by TTD operators - or off site, via remote sensing. In this work, an intelligent system is proposed that allows estimation of different classes of in-situ states and w% in TTD using Machine learning algorithms based on Artificial Neural Networks (ANN), Support Vector Machine (SVM) and Random Forest (RF). The results show an accuracy of between 94% and 97% in the classification task of the Dry, Semisolid, Plastic and Saturated classes, and between 0.356 and 0.378 of the MAE metric in the regression task, which is sufficient to estimate the w% with ML methods. © 2013 IEEE.","Artificial Neural Networks; Physical Stability; Remote Sensing; Thickened Tailings Dams","Chemical stability; Decision trees; Deposits; Embankment dams; Environmental impact; Intelligent systems; Learning systems; Moisture control; Moisture determination; Neural networks; Remote sensing; Support vector machines; Water resources; Classification tasks; Different class; Estimation of moisture content; Machine learning techniques; Major producers; Mining waste; Remote sensing images; Resource recovery; Learning algorithms"
"Arano-Martinez J.A., Martínez-González C.L., Salazar M.I., Torres-Torres C.","A Framework for Biosensors Assisted by Multiphoton Effects and Machine Learning","10.3390/bios12090710","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138324550&doi=10.3390%2fbios12090710&partnerID=40&md5=d1b6750f046c9c80a01c1a1d92f975b3","The ability to interpret information through automatic sensors is one of the most important pillars of modern technology. In particular, the potential of biosensors has been used to evaluate biological information of living organisms, and to detect danger or predict urgent situations in a battlefield, as in the invasion of SARS-CoV-2 in this era. This work is devoted to describing a panoramic overview of optical biosensors that can be improved by the assistance of nonlinear optics and machine learning methods. Optical biosensors have demonstrated their effectiveness in detecting a diverse range of viruses. Specifically, the SARS-CoV-2 virus has generated disturbance all over the world, and biosensors have emerged as a key for providing an analysis based on physical and chemical phenomena. In this perspective, we highlight how multiphoton interactions can be responsible for an enhancement in sensibility exhibited by biosensors. The nonlinear optical effects open up a series of options to expand the applications of optical biosensors. Nonlinearities together with computer tools are suitable for the identification of complex low-dimensional agents. Machine learning methods can approximate functions to reveal patterns in the detection of dynamic objects in the human body and determine viruses, harmful entities, or strange kinetics in cells. © 2022 by the authors.","machine learning; nonlinear optics; optical biosensors; photonics; SARS-CoV-2","absorption; artificial neural network; chemical interaction; clinical effectiveness; human; machine learning; modulation; multiphoton interaction; nonhuman; nonlinear system; optical Kerr; optics; Review; second harmonic generation nonlinear optics; sensibility; Severe acute respiratory syndrome coronavirus 2; sum frequency generation nonlinear optics; surface analysis; total quality management; virus detection; diagnosis; genetic procedures; machine learning; procedures; virus; Biosensing Techniques; COVID-19; Humans; Machine Learning; SARS-CoV-2; Viruses"
"Aras S., Hanifi Van M.","An interpretable forecasting framework for energy consumption and CO2 emissions","10.1016/j.apenergy.2022.120163","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140306206&doi=10.1016%2fj.apenergy.2022.120163&partnerID=40&md5=243dae6de497a331efa3e72dd368d0aa","It is a well-established fact that energy consumption and production, as the primary sources of greenhouse gases, contribute to climate change and global warming issues. The analysis and estimation of the factors that contribute to these harmful gases will be of great assistance in the development of policies to reduce carbon dioxide emissions. In addition to identifying the factors related to energy consumption and CO2 emissions, forecasting the variable of interest as accurately as possible has a key role in increasing the efficiency of energy strategies to be implemented. Unlike studies in the literature, this study not only forecasts the future value of energy consumption and CO2 emissions but also determines the relationship between the predictions and the influential variables by revealing the contribution of each variable to the prediction. For this purpose, the study proposes an interpretable forecasting framework based on values of the Shapley additive explanation (SHAP) to provide a simpler explanation of machine learning (ML) models in forecasting energy consumption and CO2 emissions. The results obtained show that the total electricity generation from different energy sources is found to be the most important variable interacting positively with both energy consumption and CO2 emissions. Also, the influence of the predictors on projections made before and after COVID-19 has changed dramatically. The proposed method may assist policymakers in making future energy investments and establishing energy laws more accurately and efficiently as it explains the drivers of the forecasts. © 2022 Elsevier Ltd","CO2 emissions; Energy consumption; Machine learning; Model Interpretability; SHAP","Carbon dioxide; Energy efficiency; Forecasting; Global warming; Greenhouse gases; Investments; Machine learning; CO 2 emission; Energy consumption and production; Energy-consumption; Established facts; Interpretability; Machine-learning; Model interpretability; Primary sources; Shapley; Shapley additive explanation; Energy utilization; carbon dioxide; carbon emission; climate change; COVID-19; electricity generation; global warming; machine learning; policy making"
"Aras S., Lisboa P.J.G.","Explainable inflation forecasts by machine learning models","10.1016/j.eswa.2022.117982","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134582165&doi=10.1016%2fj.eswa.2022.117982&partnerID=40&md5=0a6645f90957447802a602a37090de8f","Forecasting inflation accurately in a data-rich environment is a challenging task and an active research field which still contains various unanswered methodological questions. One of them is how to find and extract the information with the most predictive power for a variable of interest when there are many highly correlated predictors, as in the inflation forecasting problem. Traditionally, factor models have been used to tackle this problem. However, a few recent studies have revealed that machine learning (ML) models such as random forests may offer some valuable solutions to the problem. This study encourages greater use of ML models with or without factor models by replacing the functional form of the forecast equation in a factor model with ML models or directly employing them with several feature selection techniques. This study adds new tree-based models to the analysis in the light of the recent findings in the literature. Moreover, it proposes the integration of feature selection techniques with Shapley values to find out concise explanations of the inflation predictions. The results obtained by a comprehensive set of experiments in an emerging country, Turkey, facing a high degree of volatility and uncertainty, indicate that tree-based ensemble models can be advantageous by providing better accuracy together with explainable predictions. © 2022 Elsevier Ltd","Factor models; Inflation forecasting; Machine learning; Model interpretability; Shapley values; Tree-based models","Decision trees; Feature Selection; Random forests; Factor model; Features selection; Inflation forecasting; Interpretability; Machine learning models; Machine-learning; Model interpretability; Shapley value; Tree-based model; Trees-based models; Forecasting"
"Arastoopour Irgens G., Vega H., Adisa I., Bailey C.","Characterizing children's conceptual knowledge and computational practices in a critical machine learning educational program","10.1016/j.ijcci.2022.100541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139987392&doi=10.1016%2fj.ijcci.2022.100541&partnerID=40&md5=d697cb5c1fd1be7ead9d71aa9c5ff800","In this study, we describe the design and implementation of a CML (critical machine learning) education program for children between the ages of 9 and 13 at an after-school center. In this participatory design-based research, we collected learner artifacts, recordings of interactions, and pre/post drawings and written responses to model children's developing knowledge and practices related to critical machine learning. Drawing from constructionist and critical pedagogical perspectives, our research questions are: (1) How do children develop machine learning knowledge grounded in social, ethical, and political orientations in a CML education program? and (2) What computational practices do children engage in when developing robots for social good in a CML education program? We found that (1) children made more sophisticated connections with socio-political orientations and ML content as they progressed through the program, and (2) they engaged in computational practices, such as experimenting and iterating, testing and debugging, reusing and remixing, and abstracting and modularizing. Further, our findings indicate that a critical lens to ML education can be characterized by posing and answering questions about the roles of AI technologies producers and consumers and identifying how these technologies are designed to apply this knowledge to build applications for marginalized populations. This study suggests that a critical lens is an effective approach towards engaging young children in designing their own machine learning tools in socially responsible ways. © 2022 Elsevier B.V.","Critical pedagogies; Design-based research; Elementary education; Informal education; Machine learning education; Robotics",
"Araújo F.F., Pinheiro Å.M.A., Farias K.M., Lóscio B.F., Oliveira D.M.","FlagelLink: A decision support system for distributed flagellar data using data warehouse","10.1145/1363686.1363980","https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749101856&doi=10.1145%2f1363686.1363980&partnerID=40&md5=b9a723f1ac0cd0967a2bfb61038f4c8c","Combining different types of data from multiple databases (DBs) is a key feature in bioinformati.es, particularly due to the problem that each of these DB resources usually contains different subsets of biological knowledge and only answers questions in its domain, nether helping with questions that span domain boundaries nor considering them. As bioinformatics DBs grow in size and as biological questions grow in scope, better solutions will inevitably consist in preserving the autonomy and diversity of DBs and developing new systems to offer an integrated and transparent access to existing distributed data sources (DS). In this paper, we present a decision support system (DSS), called FlagelLink, to provide access to a set of distributed information about a particular domain (the flagellum, a cellular organelle responsible for motility). It employs useful bioinformatics tools (such as BLAST, MUSCLE, HMMER, etc) in an exclusive data warehouse (DW) through terminology and ontology resources (semantic-driven) to maintain an actual DSS for a specific knowledge domain. FlagelLink (available at http://flagellink.n.ugen.uece.br/flagellink'l has a unified, on-demand integration approach that merges the identified ontological knowledge (which means a defined number of test cases and scenarios of genes and proteins all involved in flagellar activities) with traditional and ontology-based information integration techniques. Copyright 2008 ACM.","Bioinformatics; Data integration; Data warehouse; Decision support system; Flagellar ontology","Administrative data processing; Artificial intelligence; Bioinformatics; Data warehouses; Database systems; Decision making; Decision theory; Information theory; Integration; Knowledge based systems; Management information systems; Military operations; Ontology; Rough set theory; Terminology; Warehouses; Bioinformatics tools; Cellular organelles; Data integration; Decision support system; Distributed data sources; Distributed informations; Do-mains; Domain boundaries; Flagellar ontology; Information integrations; Key features; New systems; On demands; Ontological knowledges; Specific knowledges; TEst cases; Decision support systems"
"Araújo T., Aresta G., Mendonça L., Penas S., Maia C., Carneiro Â., Mendonça A.M., Campilho A.","DR|GRADUATE: Uncertainty-aware deep learning-based diabetic retinopathy grading in eye fundus images","10.1016/j.media.2020.101715","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084682948&doi=10.1016%2fj.media.2020.101715&partnerID=40&md5=34b2f44dc6eb206f86fbc081c7c6d16f","Diabetic retinopathy (DR) grading is crucial in determining the adequate treatment and follow up of patient, but the screening process can be tiresome and prone to errors. Deep learning approaches have shown promising performance as computer-aided diagnosis (CAD) systems, but their black-box behaviour hinders clinical application. We propose DR|GRADUATE, a novel deep learning-based DR grading CAD system that supports its decision by providing a medically interpretable explanation and an estimation of how uncertain that prediction is, allowing the ophthalmologist to measure how much that decision should be trusted. We designed DR|GRADUATE taking into account the ordinal nature of the DR grading problem. A novel Gaussian-sampling approach built upon a Multiple Instance Learning framework allow DR|GRADUATE to infer an image grade associated with an explanation map and a prediction uncertainty while being trained only with image-wise labels. DR|GRADUATE was trained on the Kaggle DR detection training set and evaluated across multiple datasets. In DR grading, a quadratic-weighted Cohen's kappa (κ) between 0.71 and 0.84 was achieved in five different datasets. We show that high κ values occur for images with low prediction uncertainty, thus indicating that this uncertainty is a valid measure of the predictions’ quality. Further, bad quality images are generally associated with higher uncertainties, showing that images not suitable for diagnosis indeed lead to less trustworthy predictions. Additionally, tests on unfamiliar medical image data types suggest that DR|GRADUATE allows outlier detection. The attention maps generally highlight regions of interest for diagnosis. These results show the great potential of DR|GRADUATE as a second-opinion system in DR severity grading. © 2020 Elsevier B.V.","Deep learning; Diabetic retinopathy grading; Explainability; Uncertainty","Computer aided diagnosis; Computer aided instruction; Eye protection; Forecasting; Grading; Image quality; Learning systems; Medical imaging; Patient treatment; Reactor cores; Clinical application; Computer Aided Diagnosis(CAD); Diabetic retinopathy; Learning approach; Multiple data sets; Multiple instance learning; Prediction uncertainty; Regions of interest; Deep learning; Article; deep learning; diabetic retinopathy; diagnostic error; eye fundus; human; image analysis; image quality; outlier detection; priority journal; quality control; uncertainty; computer assisted diagnosis; diabetes mellitus; diabetic retinopathy; diagnostic imaging; eye fundus; Deep Learning; Diabetes Mellitus; Diabetic Retinopathy; Diagnosis, Computer-Assisted; Fundus Oculi; Humans; Uncertainty"
"Araya R., Jiménez A., Bahamondez M., Dartnell P., Soto-Andrade J., González P., Calfucura P.","Strategies used by students on a massively multiplayer online mathematics game","10.1007/978-3-642-25813-8_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857336950&doi=10.1007%2f978-3-642-25813-8_1&partnerID=40&md5=3b63648c2b51d5a88e2e5e5fd83d3ab6","We analyze the logs of an online mathematics game tournament, played simultaneously by thousands of students. Nearly 10,000 students, coming from 356 schools from all regions in Chile, registered to the fourth tournament instance. The children play in teams of 12 students from the same class, and send their personal bets to a central server every 2 minutes. Each competition lasts about one clock hour and takes place within school hours. Students are pre-registered and trained by their school teacher. The teacher is responsible for reviewing curriculum contents useful for improving performance at the game and coaches students participating in trial tournaments taking place a few weeks before the national tournament. All bets are recorded in a database that enables us to analyze later the sequence of bets made by each student. Using cluster analysis with this information, we have identified three types of players, each with a well-defined strategy. © 2011 Springer-Verlag.","algebra; data mining; learning; on-line tournaments; statistics","Central servers; Improving performance; learning; Massively multiplayer; on-line tournaments; School teachers; Algebra; Cluster analysis; Curricula; Data mining; Statistics; Students"
"Arbelaez Ossa L., Starke G., Lorenzini G., Vogt J.E., Shaw D.M., Elger B.S.","Re-focusing explainability in medicine","10.1177/20552076221074488","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125767643&doi=10.1177%2f20552076221074488&partnerID=40&md5=ac9b559eed036f93d6060b72ad85d62d","Using artificial intelligence to improve patient care is a cutting-edge methodology, but its implementation in clinical routine has been limited due to significant concerns about understanding its behavior. One major barrier is the explainability dilemma and how much explanation is required to use artificial intelligence safely in healthcare. A key issue is the lack of consensus on the definition of explainability by experts, regulators, and healthcare professionals, resulting in a wide variety of terminology and expectations. This paper aims to fill the gap by defining minimal explainability standards to serve the views and needs of essential stakeholders in healthcare. In that sense, we propose to define minimal explainability criteria that can support doctors’ understanding, meet patients’ needs, and fulfill legal requirements. Therefore, explainability need not to be exhaustive but sufficient for doctors and patients to comprehend the artificial intelligence models’ clinical implications and be integrated safely into clinical practice. Thus, minimally acceptable standards for explainability are context-dependent and should respond to the specific need and potential risks of each clinical scenario for a responsible and ethical implementation of artificial intelligence. © The Author(s) 2022.","digital health; Explainability; explainable AI; human-center AI; medicine",
"Arbix G.","A TRANSPARÊNCIA NO CENTRO DA CONSTRUÇÃO DE UMA IA ÉTICA","10.25091/s01013300202000020008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098222118&doi=10.25091%2fs01013300202000020008&partnerID=40&md5=6168bfb7667c4a56d0efab854f85a810","AI is a constellation of technologies able to generate other technologies, new methodologies and applications. Its characteristics are different in nature from other innovations that emerge in society. This essay approaches AI as the brain and engine of the current technological wave. And it addresses issues related to the ethics of algorithms, whose decision processes are not always transparent, in contrast to the rights of individuals and the values of societies. © 2020. All Rights Reserved.","Artificial Intelligence; Black Box; ethics; responsible AI; transparency",
"Archenaa J., Mary Anita E.A.","Predictive analytic framework for efficient healthcare system","10.5373/JARDCS/V12SP6/SP20201031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087168926&doi=10.5373%2fJARDCS%2fV12SP6%2fSP20201031&partnerID=40&md5=3867046c49c97fa26a4fbcf562446ef0","The discovery of knowledge from medical databases is important in order to make effective medical diagnosis. The aim of data mining is to extract knowledge from information collected from various datastore and generate clear and understandable description of patterns. Applying machine learning and data mining methods in DM research is a key approach to utilizing large volumes of available diabetes-related data for extracting knowledge. The severe social impact of the specific disease renders DM one of the main priorities in medical science research, which inevitably generates huge amounts of data. The different machine learning algorithms namely SVM, SVM Kernel Linear, Logistic Regression, Decision tree, KNN, random forest algorithm and Bayesian classifier were used to make diabetes prediction. From the results obtained, it is very clear that random forest algorithm performs much better in terms of all the different performance measures when compared to all other machine learning algorithms.Thus, the results show that the machine learning algorithms can able to produce highly accurate diabetes predictive healthcare systems. © 2020, Institute of Advanced Scientific Research, Inc.. All rights reserved.","Big Data; Data mining; Diabetes mellitus; Machine learning",
"Archip N., Erard P.-J., Egmont-Petersen M., Haefliger J.-M., Germond J.-F.","A knowledge-based approach to automatic detection of the spinal cord in CT images","10.1109/TMI.2002.806578","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036999642&doi=10.1109%2fTMI.2002.806578&partnerID=40&md5=1f850ba39bf0f2877f5733e1c74768f2","Accurate planning of radiation therapy entails the definition of treatment volumes and a clear delimitation of normal tissue of which unnecessary exposure should be prevented. The spinal cord is a radiosensitive organ, which should be precisely identified because an overexposure to radiation may lead to undesired complications for the patient such as neuronal disfunction or paralysis. In this paper, a knowledge-based approach to identifying the spinal cord in computed tomography images of the thorax is presented. The approach relies on a knowledge-base which consists of a so-called anatomical structures map (ASM) and a task-oriented architecture called the plan solver. The ASM contains a frame-like knowledge representation of the macro-anatomy in the human thorax. The plan solver is responsible for determining the position, orientation and size of the structures of interest to radiation therapy. The plan solver relies on a number of image processing operators. Some are so-called atomic (e.g., thresholding and snakes) whereas others are composite. The whole system has been implemented on a standard PC. Experiments performed on the image material from 23 patients show that the approach results in a reliable recognition of the spinal cord (92% accuracy) and the spinal canal (85% accuracy). The lamina is more problematic to locate correctly (accuracy 72%). The position of the outer thorax is always determined correctly.","Image interpretation; Knowledge representation; Medical imaging; Radiotherapy; Spinal cord","Computerized tomography; Image analysis; Image understanding; Knowledge representation; Neurology; Radiotherapy; Tissue; Anatomical structures map; Automatic detection; Neuronal disfunction; Spinal cord; Medical imaging; adult; aged; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer assisted radiotherapy; computer assisted tomography; evaluation; factual database; female; histology; human; image quality; male; methodology; middle aged; radiography; radiometry; reproducibility; sensitivity and specificity; spinal cord; thorax radiography; thorax tumor; three dimensional imaging; validation study; Adult; Aged; Algorithms; Artificial Intelligence; Databases, Factual; Female; Humans; Imaging, Three-Dimensional; Male; Middle Aged; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Radiometry; Radiotherapy Planning, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Spinal Cord; Thoracic Neoplasms; Tomography, X-Ray Computed"
"Arcones E., Alvarez F., Ortego J., Khamlichi A., Camunas A., Rosete A., Garnacho F., Gomez P.","Development, testing and aging of reference insulation defects for the improvement in partial discharges diagnosis","10.1109/EIC49891.2021.9612350","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123355198&doi=10.1109%2fEIC49891.2021.9612350&partnerID=40&md5=9e4e9d0ce6c267eba831b29bc1537cd9","When partial discharge (PD) activity is detected in a high-voltage (HV) installation, those responsible for their reliable functioning want to know the criticality of the defect or defects with the aim of taking the appropriate corrective actions. In order to assess the criticality of a defect and decide when the red light on the alarm should be activated, a thorough study of the defects evolution over time is required. Although in recent years various companies have acquired valuable knowledge from on-site monitoring applications, the study over time of defects in controlled laboratory test cells can significantly strengthen this knowledge. In the research presented in this paper reproducible test cells, each containing a reference insulation defect, have been developed, tested and aged for the improvement in PD diagnosis. The temporary study of the insulation defects evolution provides valuable information for adopting adequate criteria when evaluating their criticality in the electrical assets. In addition, the information gathered in this research is useful to train the learning processes of artificial intelligence systems, which are developed to perform assisted or automatic PD diagnoses. Finally, it is worth mentioning that as a result of this research the reproducible test cells developed have proved to be useful to perform long lasting laboratory tests with real insulation defects, which results interesting not only for scientific purposes but also for didactic issues. © 2021 IEEE.","condition monitoring; insulation testing; partial discharges; pattern recognition",
"Ardakanian O., Koochakzadeh N., Singh R.P., Golab L., Keshav S.","Computing electricity consumption profiles from household smart meter data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923647990&partnerID=40&md5=b8820ee4d21f9061ba824d7b2078dbd5","In this paper, we investigate a critical problem in smart meter data mining: computing electricity consumption pro-files. We present a simple, interpretable and practical pro-filing framework for residential consumers, which accounts for variations in electricity consumption at di.erent times of day and at di.erent external temperatures. Our approach is to isolate the e.ect of external temperature on electricity consumption and apply a time-series autoregressive model to the remaining signal. The proposed profiles may be used for making personalized energy-saving recommendations, detecting outliers, and generating very large realistic data sets for testing the scalability of smart meter data management systems. Using predictive power as a metric for the accuracy of consumption profiles, we show, using a real data set of 1000 homes, that our approach results in improved root-mean-squared prediction error compared to existing approaches.",,"Computation theory; Data mining; Electric power measurement; Energy conservation; Information management; Auto regressive models; Critical problems; Electricity-consumption; External temperature; Meter data management; Predictive power; Residential consumers; Root mean squared; Smart meters"
"Arden-Close E., Bolat E., Vuillier L., Ali R.","Perceptions of Interactive, Real-Time Persuasive Technology for Managing Online Gambling","10.1007/978-3-030-98438-0_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127038771&doi=10.1007%2f978-3-030-98438-0_3&partnerID=40&md5=e7ba15366564d325e608eb2448836c93","Background: Interactive persuasive techniques, supported by the ability to retrieve real-time behaviour and other contextual data, offer an unprecedented opportunity to manage online activity. An example is Responsible Gambling (RG) tools. Currently, despite vast potential, they do not make use of real time gambling behaviour data, whether captured by operators (device, location, bets, limits set) or self-reported (finance, emotion, online browsing history). To design useful interactive persuasive tools, it is important to understand users’ perceptions to ensure maximum acceptance. Aims: Explore gamblers’ perceptions of the potential of future online platforms in providing data-driven, real-time, persuasive interventions for supporting responsible online gambling. Method: Qualitative semi-structured interviews conducted with 22 gamblers (80% men; 15 ex-problem, 7 current), regarding perceptions of the potential of persuasive techniques. Results: Thematic analysis showed participants were positive about data-driven, real-time, interactive technology for (i) providing information (educational, personal and comparative), (ii) limiting gambling (time and money spent, access to gambling operators) and (iii) providing support to gamblers (advice, feedback and context sensing). The technology was identified as most appropriate for low to moderate gamblers. Conclusions: Participants were positive about the new data access, techniques and modalities of interactions for supporting responsible online gambling. To ensure maximum reach and acceptability, such technology should be customised to fit individual profiles. Personalisation and tailoring of content, interactivity, framing and timing are necessary to enhance acceptance of such technology and avoid reactance, unintended harm, inconvenience, and information overload. © 2022, Springer Nature Switzerland AG.","Persuasive technology; Responsible gambling; Technology acceptance","Computers; Browsing history; Data driven; Limit sets; Online activities; Online gambling; Persuasive technology; Real time behavior; Real- time; Responsible gambling; Technology acceptance; Artificial intelligence"
"Ardichvili A.","The Impact of Artificial Intelligence on Expertise Development: Implications for HRD","10.1177/15234223221077304","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127221165&doi=10.1177%2f15234223221077304&partnerID=40&md5=c3da312e04f656837d443e2c2457c034","Problem: The implementation of artificial intelligence (AI) is assumed to lead to increased productivity of knowledge workers. However, AI could also have negative effects on the development of professional expertise. Solution: A review of the literature on expertise development is provided, followed by examples of AI implementation in a knowledge-intensive profession, accounting. The analysis of these examples suggests that automation can result in the loss of expertise due to reduced opportunities for learning from deliberate practice and experienced colleagues, and from working on progressively more complex tasks. Implications for human resource development (HRD) include creating alternative individual development opportunities and promoting organizational cultures conducive to expertise development in human-machine interaction modes. Stakeholders: The results of this study will be of interest to scholars of HRD, accounting education, and human-machine interaction. Practical implications will be of relevance to HRD professionals and managers responsible for the implementation of artificial intelligence solutions. © The Author(s) 2022.","artificial Intelligence; automation; deep learning; expertise; HRD; human-machine interaction; machine learning",
"Ardito C., Deldjoo Y., Noia T.D., Sciascio E.D., Nazary F.","Visual inspection of fault type and zone prediction in electrical grids using interpretable spectrogram-based CNN modeling","10.1016/j.eswa.2022.118368","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135955509&doi=10.1016%2fj.eswa.2022.118368&partnerID=40&md5=1b6aefa9093a430a5cffe6673b3448b2","In electrical grids, fault diagnosis (fault type and fault location classifications) are critical due to their economic and important implications. Numerous smart grid applications have embraced data-driven methodologies. While the majority of the work in this topic has been on increasing the predicted accuracy of machine-learning model for fault diagnosis, one important aspect that has received less attention is the interpretability of these systems. We advocate for a complementary perspective. To represent faulty signals, we propose a spectrogram–convolutional neural network based representation of the electrical signals where pre-trained models such as GoogleNet and SqueezeNet are trivially used. We then perform multiple fault classification tasks and offer a visual interpretation of the collected findings. The suggested approach makes the model more transparent through the use of Gradient-weighted Class Activation Mapping (Grad-CAM), which visualizes regions in the input spectrogram that are more relevant for predictions, assisting the end-user in the understanding and interpreting the results. We explore the merits of the suggested technique in terms of increasing the transparency of the black-box machine learning system, which is a critical requirement for designing modernized smart grids. © 2022 Elsevier Ltd","Fault diagnosis; Interpretability; Smart grids; Visual explanation","Convolutional neural networks; Electric power transmission networks; Fault detection; Machine learning; Smart power grids; Spectrographs; Electrical grids; Fault types; Fault zone; Faults diagnosis; Grid fault; Interpretability; Smart grid; Spectrograms; Visual explanation; Visual inspection; Failure analysis"
"Ardito C., Deldjoo Y., Di Sciascio E., Nazary F., Sapienza G.","ISCADA: Towards a Framework for Interpretable Fault Prediction in Smart Electrical Grids","10.1007/978-3-030-85607-6_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115270578&doi=10.1007%2f978-3-030-85607-6_20&partnerID=40&md5=a9e47616e9b8a86d190c723a619d9ac1","This paper reports ongoing research for the definition of a data-driven self-healing system using machine learning (ML) techniques that can perform automatic and timely detection of fault types and locations. Specifically, the proposed method makes use of spectrogram-based CNN modeling of the 3-phase voltage signals. Furthermore, to keep human operators informed about why certain decisions were made, i.e., to facilitate the interpretability of the black-box ML model, we propose a novel explanation approach that highlight regions in the input spectrogram that contributed the most for the prediction task at hand (e.g., fault type or location) - or visual explanation. © 2021, IFIP International Federation for Information Processing.","Fault prediction; Interpretability; Self-healing system","Spectrographs; Electrical grids; Fault prediction; Human operator; Interpretability; Prediction tasks; Self-healing systems; Spectrograms; Voltage signals; Human computer interaction"
"Arefin R., Samad M.D., Akyelken F.A., Davanian A.","Non-transfer Deep Learning of Optical Coherence Tomography for Post-hoc Explanation of Macular Disease Classification","10.1109/ICHI52183.2021.00020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118194683&doi=10.1109%2fICHI52183.2021.00020&partnerID=40&md5=f23cb50f8581879ace686171b1589234","Deep transfer learning is widely used for medical image classification by leveraging models that are pretrained by natural images. This choice may introduce unnecessary model complexity that can limit explanations of such model outcomes in clinical practice. To investigate this hypothesis, we develop a configurable deep convolutional neural network (CNN) to classify four macular disease types using retinal optical coherence tomography (OCT) images. Our proposed non-transfer deep CNN model (acc: 97.9%) outperforms existing transfer learning models such as ResNet-50 (acc: 89.0%), ResNet-101 (acc: 96.7%), VGG-19 (acc: 93.3%), and Inception-V3 (acc: 95.8%) in the same retinal OCT image classification task. Our post-hoc analysis of the model extracted image features reveals that only eight out of 256 CNN filter kernels are active at the final convolutional layer. The convolutional responses of these eight selective filters yield image features that efficiently separate four macular disease classes even when projected onto two-dimensional principal component space. Our findings suggest that a large portion of deep learning parameters and computations are redundant for retinal OCT image classification, which intensifies when using transfer learning. Additionally, we provide clinical interpretations of our misclassified test images identifying manifest artifacts, shadowing of useful texture, false texture representing fluids, and other confounding factors. These clinical explanations along with model optimization via filter selection can improve the classification accuracy, computational costs, and explainability of deep model outcomes. © 2021 IEEE.","Convolutional neural network; Explainable AI; Feature extraction; Filter kernels; Macular disease; Medical imaging; Ophthalmology","Convolution; Convolutional neural networks; Deep neural networks; Image classification; Ophthalmology; Optical tomography; Textures; Convolutional neural network; Disease classification; Explainable AI; Features extraction; Filter kernel; Image features; Images classification; Macular disease; Retinal optical coherence tomography; Transfer learning; Medical imaging"
"Arellano D., Varona J., Helzle V.","Ontologies for authoring, or authoring ontologies?",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877939400&partnerID=40&md5=a7ffcd107f452880d755d012ee736738","In the last years the use of ontologies has broaden to areas that until some time ago were unthinkable, like storytelling or context/content representation. The main problem with the use of ontologies is that the user responsible of authoring the story needs to input every single element that is required for the story to make sense. Depending on the case, this might be a tedious task. However, once it is done, different stories can be developed by reusing the already defined concepts. The objective of the paper is to provide examples of applications where the use of ontologies conveyed ""authoring effort"" with satisfactory results. We also state our opinion of why is it better to use ontologies for such tasks, explain our own experience with an use case and propose ideas of what could be enhanced, or taken from other areas, to improve the authoring process.","Content authoring; Knowledge representation; Ontologies; Storytelling","Authoring process; Content authoring; Single element; Storytelling; Knowledge representation; Ontology; Artificial intelligence"
"Arellano-Espitia F., Delgado-Prieto M., Martinez-Viol V., Saucedo-Dorantes J.-J., Osornio-Rios R.A.","Diagnosis Electromechanical System by Means CNN and SAE: An Interpretable-Learning Study","10.1109/ICPS51978.2022.9816942","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135613885&doi=10.1109%2fICPS51978.2022.9816942&partnerID=40&md5=580779a00783f3b5fb26330b7e3f95ef","Cyber-physical systems are the response to the adaptability, scalability and accurate demands of the new era of manufacturing called Industry 4.0. They will become the core technology of control and monitoring in smart manufacturing processes. In this regard, the complexity of industrial systems implies a challenge for the implementation of monitoring and diagnosis schemes. Moreover, the challenges that is presented in technological aspects regarding connectivity, data management and computing are being resolved through different IT-OT (information technology and operational technology) convergence proposals. These solutions are making it possible to have large computing capacities and low response latency. However, regarding the logical part of information processing and analysis, this still requires additional studies to identify the options with a better complexity-performance trade-off. The emergence of techniques based on artificial intelligence, especially those based on deep-learning, has provided monitoring schemes with the capacity for characterization and recognition in front of complex electromechanical systems. However, most deep learning-based schemes suffer from critical lack of interpretability lying to low generalization capabilities and overfitted responses. This paper proposes a study of two of the main deep learning-based techniques applied to fault diagnosis in electromechanical systems. An analysis of the interpretability of the learning processes is carried out, and the approaches are evaluated under common performance metrics. © 2022 IEEE.","autoencoder; condition monitoring; convolutional neural networks; cyber-physical systems; deep-learning","Complex networks; Condition monitoring; Convolutional neural networks; Deep learning; Economic and social effects; Embedded systems; Engineering education; Information management; Learning systems; Manufacture; Auto encoders; Control and monitoring; Convolutional neural network; Core technology; Cybe-physical systems; Cyber-physical systems; Deep-learning; Electromechanical systems; Interpretability; Learning studies; Cyber Physical System"
"Arena P., Fortuna L., Frasca M., Sicurella G.","An adaptive, self-organizing dynamical system for hierarchical control of bio-inspired locomotion","10.1109/TSMCB.2004.828593","https://www.scopus.com/inward/record.uri?eid=2-s2.0-3442897921&doi=10.1109%2fTSMCB.2004.828593&partnerID=40&md5=5b64ca3cf0f6d470b14526abd10db94f","In this paper, dynamical systems made up of locally coupled nonlinear units are used to control the locomotion of bio-inspired robots and, in particular, a simulation of an insect-like hexapod robot. These controllers are inspired by the biological paradigm of central pattern generators and are responsible for generating a locomotion gait. A general structure, which is able to change the locomotion gait according to environmental conditions, is introduced. This structure is based on an adaptive system, implemented by motor maps, and is able to learn the correct locomotion gait on the basis of a reward function. The proposed control system is validated by a large number of simulations carried out in a dynamic environment for simulating legged robots. © 2004 IEEE.",,"Adaptive control systems; Cellular neural networks; Computer simulation; Control system analysis; Gait analysis; Hierarchical systems; Matrix algebra; Motion control; Robot learning; Robotic arms; Self organizing maps; Speed control; Bioinspired robots; Central pattern generators; Dynamical nonlinear systems; Locomotion control; Nonlinear control systems; adaptation; algorithm; animal; article; artificial intelligence; artificial neural network; biological model; biological rhythm; biomimetics; computer simulation; evaluation; feedback system; human; locomotion; methodology; nerve cell network; physiology; robotics; validation study; Adaptation, Physiological; Algorithms; Animals; Artificial Intelligence; Biological Clocks; Biomimetics; Computer Simulation; Feedback; Humans; Locomotion; Models, Neurological; Nerve Net; Neural Networks (Computer); Robotics"
"Arendt D., Shaw Z., Shrestha P., Ayton E., Glenski M., Volkova S.","CrossCheck: Rapid, Reproducible, and Interpretable Model Evaluation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123187157&partnerID=40&md5=066ddcc0a76686d2e39d3dcb9d2f340a","Evaluation beyond aggregate performance metrics, e.g. F1-score, is crucial to both establish an appropriate level of trust in machine learning models and identify avenues for future model improvements. In this paper we demonstrate CrossCheck, an interactive capability for rapid cross-model comparison and reproducible error analysis. We describe the tool, discuss design and implementation details, and present three NLP use cases – named entity recognition, reading comprehension, and clickbait detection that show the benefits of using the tool for model evaluation. CrossCheck enables users to make informed decisions when choosing between multiple models, identify when the models are correct and for which examples, investigate whether the models are making the same mistakes as humans, evaluate models’ generalizability and highlight models’ limitations, strengths and weaknesses. Furthermore, CrossCheck is implemented as a Jupyter widget, which allows for rapid and convenient integration into existing model development workflows. © 2021 Association for Computational Linguistics",,"Aggregate performance; Cross model; Design and implementations; F1 scores; Future models; Machine learning models; Model evaluation; Models comparisons; Named entity recognition; Performance metrices"
"Areosa I., Torgo L.","Visual Interpretation of Regression Error","10.1007/978-3-030-30244-3_39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072850152&doi=10.1007%2f978-3-030-30244-3_39&partnerID=40&md5=62ce56ffeb995c7c7c77c003c1bddb4c","Numerous sophisticated machine learning tools (e.g. ensembles or deep networks) have shown outstanding performance in terms of accuracy on different numeric forecasting tasks. In many real world application domains the numeric predictions of the models drive important and costly decisions. Frequently, decision makers require more than a black box model to be able to “trust” the predictions up to the point that they base their decisions on them. In this context, understanding these black boxes has become one of the hot topics in Machine Learning and Data Mining research. This paper proposes a series of visualisation tools that help in understanding the predictive performance of non-interpretable regression models. More specifically, these tools allow the user to relate the expected error of any model to the values of the predictor variables. This type of information allows end-users to correctly assess the risks associated with the use of the models, by showing how concrete values of the predictors may affect the performance of the models. Our illustrations with different real world data sets and learning algorithms provide insights on the type of usage and information these tools bring to both the data analyst and the end-user. © 2019, Springer Nature Switzerland AG.",,"Data mining; Decision making; Digital storage; Forecasting; Machine learning; Regression analysis; Risk assessment; Black-box model; Decision makers; Predictive performance; Predictor variables; Regression errors; Regression model; Sophisticated machines; Visual interpretation; Learning algorithms"
"Argall M.R., Small C.R., Piatt S., Breen L., Petrik M., Kokkonen K., Barnum J., Larsen K., Wilder F.D., Oka M., Paterson W.R., Torbert R.B., Ergun R.E., Phan T., Giles B.L., Burch J.L.","MMS SITL Ground Loop: Automating the Burst Data Selection Process","10.3389/fspas.2020.00054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106907448&doi=10.3389%2ffspas.2020.00054&partnerID=40&md5=603a387a55809ba443965ffd30a6dee0","Global-scale energy flow throughout Earth's magnetosphere is catalyzed by processes that occur at Earth's magnetopause (MP). Magnetic reconnection is one process responsible for solar wind entry into and global convection within the magnetosphere, and the MP location, orientation, and motion have an impact on the dynamics. Statistical studies that focus on these and other MP phenomena and characteristics inherently require MP identification in their event search criteria, a task that can be automated using machine learning so that more man hours can be spent on research and analysis. We introduce a Long-Short Term Memory (LSTM) Recurrent Neural Network model to detect MP crossings and assist studies of energy transfer into the magnetosphere. As its first application, the LSTM has been implemented into the operational data stream of the Magnetospheric Multiscale (MMS) mission. MMS focuses on the electron diffusion region of reconnection, where electron dynamics break magnetic field lines and plasma is energized. MMS employs automated burst triggers onboard the spacecraft and a Scientist-in-the-Loop (SITL) on the ground to select intervals likely to contain diffusion regions. Only low-resolution survey data is available to the SITL, which is insufficient to resolve electron dynamics. A strategy for the SITL, then, is to select all MP crossings. Of all 219 SITL selections classified as MP crossings during the first five months of model operations, the model predicted 166 (76%) of them, and of all 360 model predictions, 257 (71%) were selected by the SITL. Most predictions that were not classified as MP crossings by the SITL were still MP-like, in that the intervals contained mixed magnetosheath and magnetospheric plasmas. The LSTM model and its predictions are public to ease the burden of arduous event searches involving the MP, including those for EDRs. For MMS, this helps free up mission operation costs by consolidating manual classification processes into automated routines. © Copyright © 2020 Argall, Small, Piatt, Breen, Petrik, Kokkonen, Barnum, Larsen, Wilder, Oka, Paterson, Torbert, Ergun, Phan, Giles and Burch.","burst data management; ground loop; long-short term memory (LSTM); magnetopause; magnetospheric multiscale (MMS); mission operations; scientist in the loop (SITL)",
"Arguello-Casteleiro M., Henson C., Maroto N., Li S., Des-Diz J., Fernandez-Prieto M.J., Peters S., Furmston T., Sevillano Torrado C., Maseda Fernandez D., Kulshrestha M., Keane J., Stevens R., Wroe C.","MetaMap versus BERT models with explainable active learning: Ontology-based experiments with prior knowledge for COVID-19",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128899680&partnerID=40&md5=1c31de031ed995c8019e4f2dcca31a30","Emergence of the Coronavirus 2019 Disease has highlighted further the need for timely support for clinicians as they manage severely ill patients. We combine Semantic Web technologies with Deep Learning for Natural Language Processing with the aim of converting human-readable best evidence/ practice for COVID-19 into that which is computer-interpretable. We present the results of experiments with 1212 clinical ideas (medical terms and expressions) from two UK national healthcare services specialty guides for COVID-19 and three versions of two BMJ Best Practice documents for COVID-19. The paper seeks to recognise and categorise clinical ideas, performing a Named Entity Recognition (NER) task, with an ontology providing extra terms as context and describing the intended meaning of categories understandable by clinicians. The paper investigates: 1) the performance of classical NER using MetaMap versus NER with fine-tuned BERT models; 2) the integration of both NER approaches using a lightweight ontology developed in close collaboration with senior doctors; and 3) the easy interpretation by junior doctors of the main classes from the ontology once populated with NER results. We report the NER performance and the observed agreement for human audits. Copyright © 2022 for this paper by its authors.","COVID-19; Deep Learning for Natural Language Processing; Ontologies; static embeddings; transformer-based language models","Coronavirus; Deep learning; Natural language processing systems; Active Learning; COVID-19; Deep learning for natural language processing; Embeddings; Language model; Named entity recognition; Ontology's; Performance; Static embedding; Transformer-based language model; Ontology"
"Arguello-Casteleiro M., Maroto N., Wroe C., Torrado C.S., Henson C., Des-Diz J., Fernandez-Prieto M.J., Furmston T., Fernandez D.M., Kulshrestha M., Stevens R., Keane J., Peters S.","Named Entity Recognition and Relation Extraction for COVID-19: Explainable Active Learning with Word2vec Embeddings and Transformer-Based BERT Models","10.1007/978-3-030-91100-3_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121934611&doi=10.1007%2f978-3-030-91100-3_14&partnerID=40&md5=9fc69b05b03812758ff0c25b86a225e8","Deep learning for natural language processing acquires dense vector representations for n-grams from large-scale unstructured corpora. Converting static embeddings of n-grams into a dataset of interlinked concepts with explicit contextual semantic dependencies provides the foundation to acquire reusable knowledge. However, the validation of this knowledge requires cross-checking with ground-truths that may be unavailable in an actionable or computable form. This paper presents a novel approach from the new field of explainable active learning that combines methods for learning static embeddings (word2vec models) with methods for learning dynamic contextual embeddings (transformer-based BERT models). We created a dataset for named entity recognition (NER) and relation extraction (REX) for the Coronavirus Disease 2019 (COVID-19). The COVID-19 dataset has 2,212 associations captured by 11 word2vec models with additional examples of use from the biomedical literature. We propose interpreting the NER and REX tasks for COVID-19 as Question Answering (QA) incorporating general medical knowledge within the question, e.g. “does ‘cough’ (n-gram) belong to ‘clinical presentation/symptoms’ for COVID-19?”. We evaluated biomedical-specific pre-trained language models (BioBERT, SciBERT, ClinicalBERT, BlueBERT, and PubMedBERT) versus general-domain pre-trained language models (BERT, and RoBERTa) for transfer learning with COVID-19 dataset, i.e. task-specific fine-tuning considering NER as a sequence-level task. Using 2,060 QA for training (associations from 10 word2vec models) and 152 QA for validation (associations from 1 word2vec model), BERT obtained an F-measure of 87.38%, with precision = 93.75% and recall = 81.82%. SciBERT achieved the highest F-measure of 94.34%, with precision = 98.04% and recall = 90.91%. © 2021, Springer Nature Switzerland AG.","Deep learning for natural language processing; Embeddings; Explainable active learning; Transfer learning; Transformer-based models","Computational linguistics; Deep learning; Embeddings; Extraction; Natural language processing systems; Semantics; Active Learning; Coronaviruses; Deep learning for natural language processing; Embeddings; Explainable active learning; N-grams; Named entity recognition; Relation extraction; Transfer learning; Transformer-based model; Coronavirus"
"Ariane Christie S., Conroy A.S., Callcut R.A., Hubbard A.E., Cohen M.J.","Dynamic multi-outcome prediction after injury: Applying adaptive machine learning for precision medicine in trauma","10.1371/journal.pone.0213836","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064149668&doi=10.1371%2fjournal.pone.0213836&partnerID=40&md5=63cf9540f784663034d31e64ab3da9b5","Objective Machine learning techniques have demonstrated superior discrimination compared to conventional statistical approaches in predicting trauma death. The objective of this study is to evaluate whether machine learning algorithms can be used to assess risk and dynamically identify patient-specific modifiable factors critical to patient trajectory for multiple key outcomes after severe injury. Methods SuperLearner, an ensemble machine-learning algorithm, was applied to prospective observational cohort data from 1494 critically-injured patients. Over 1000 agnostic predictors were used to generate prediction models from multiple candidate learners for outcomes of interest at serial time points post-injury. Model accuracy was estimated using cross-validation and area under the curve was compared to select among predictors. Clinical variables responsible for driving outcomes were estimated at each time point. Results SuperLearner fits demonstrated excellent cross-validated prediction of death (overall AUC 0.94–0.97), multi-organ failure (overall AUC 0.84–0.90), and transfusion (overall AUC 0.87–0.9) across multiple post-injury time points, and good prediction of Acute Respiratory Distress Syndrome (overall AUC 0.84–0.89) and venous thromboembolism (overall AUC 0.73–0.83). Outcomes with inferior data quality included coagulopathic trajectory (AUC 0.48–0.88). Key clinical predictors evolved over the post-injury timecourse and included both anticipated and unexpected variables. Non-random missingness of data was identified as a predictor of multiple outcomes over time. Conclusions Machine learning algorithms can be used to generate dynamic prediction after injury while avoiding the risk of over- and under-fitting inherent in ad hoc statistical approaches. SuperLearner prediction after injury demonstrates promise as an adaptable means of helping clinicians integrate voluminous, evolving data on severely-injured patients into real-time, dynamic decision-making support. © 2019 Christie et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"accuracy; adult; adult respiratory distress syndrome; algorithm; area under the curve; Article; blood transfusion; death; disease severity; female; human; injury; machine learning; major clinical study; male; multiple organ failure; outcome assessment; personalized medicine; prediction; validation process; venous thromboembolism; adult respiratory distress syndrome; biological model; clinical decision making; complication; decision support system; injury; middle aged; mortality; multiple organ failure; procedures; prognosis; prospective study; receiver operating characteristic; risk assessment; time factor; venous thromboembolism; Adult; Blood Transfusion; Clinical Decision-Making; Decision Support Techniques; Female; Humans; Machine Learning; Male; Middle Aged; Models, Biological; Multiple Organ Failure; Prognosis; Prospective Studies; Respiratory Distress Syndrome, Adult; Risk Assessment; ROC Curve; Time Factors; Venous Thromboembolism; Wounds and Injuries"
"Arias D.G., Cirne M.V.M., Chire J.E., Pedrini H.","Classification of pollen grain images based on an ensemble of classifiers","10.1109/ICMLA.2017.0-153","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048470383&doi=10.1109%2fICMLA.2017.0-153&partnerID=40&md5=acbd3ce54b83090634102b44571e0049","The recognition of pollen grains is a challenging task since they are three-dimensional structures with complex morphological characteristics. Palynologists are responsible for studying pollen, spores and similar microscopic plant structures. In this work, we develop and analyze an automatic method for classification of pollen grain images based on a set of features and classifiers. Predictions of different classifiers are fused into an ensemble rule of majority voting. Experiments conducted on two datasets containing different types of pollen grains are used to demonstrate the effectiveness of the proposed approach. © 2017 IEEE.","classifier ensemble; image descriptors; Palynology; pollen classification","Machine learning; Plants (botany); Classifier ensembles; Ensemble of classifiers; Image descriptors; Morphological characteristic; Palynology; Plant structures; Pollen classification; Three-dimensional structure; Image classification"
"Arias R., Mejia J.","Varicella zoster early detection with deep learning","10.1109/EIRCON51178.2020.9254033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097851299&doi=10.1109%2fEIRCON51178.2020.9254033&partnerID=40&md5=521b189130370964806fc255372d4886","When Varicella-zoster virus affects the trigeminal nerve, early detection is important to prevent eye damage. Our study introduces a new methodology using convolutional neural networks, via a deep learning process, for the early detection of Herpes Zoster virus (HZV). The methodology was constructed with preprocessing, segmentation, extraction, and classification stages. The varicella-zoster virus (VZV) is responsible for causing two diseases: Varicella (also known as chickenpox) and Herpes Zoster (HZ). HZ is associated with the trigeminal nerve on one side of the face. In this paper are determined the limitations, devices, and software failures associated with developing early HZ disease detection system. A main factor is the classification process, which should enable the categorization of data training. Herein, we incorporated the techniques of K-Nearest Neighborhood (KNN), neural networks, and logistic regression for this purpose, the effectiveness of the paradigm on average was 97% for early detection with minimal information. © 2020 IEEE.","Deep Learning; detection; Herpes Zoster; neural network algorithm; Varicella",
"Arias V., Salazar J., Garicano C., Contreras J., Chacón G., Chacín-González M., Añez R., Rojas J., Bermúdez-Pirela V.","An introduction to artificial intelligence applications in medicine: Historical aspects [Una introducción a las aplicaciones de la inteligencia artificial en Medicina: Aspectos históricos]",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076057897&partnerID=40&md5=4155d23862f1de882f8e85bd7787505b","in a broad sense, artificial intelligence and machine learning have been applied to medical data since the beginning of computing given the deep roots of this area in innovation, but recent years have witnessed an increasing generation of data related to health sciences, an issue that has given birth to a new field of computer science called big data. Large-scale medical data (in the form of structured and unstructured databases) if properly acquired and interpreted can generate great benefits by reducing costs and times of health service, but could also serve to predict epidemics, improve therapeutic schemes, advise doctors in remote places and improve the quality of life. The deep learning algorithms are especially useful to deal with this large amount of complex, poorly documented and generally unstructured data, all this because deep learning can break when creating models that automatically discover the predictive characteristics of a large amount of complex data. In the future, the human-machine relationship in the medical evaluation will be narrower and complex; while the machine would be responsible for extraction, cleaning and assisted searches, the physician will be concentrate on both, data interpretation and the best treatment option, improving the patient´s attention and ultimately, quality of life. © 2019 Revista Latinoamericana de Hipertension. All rights reserved.","Artificial intelligence; Databases; Innovation; Medical records","artificial intelligence; deep learning; health service; human; machine learning; medical assessment; Review"
"Arias-Londono J.D., Gomez-Garcia J.A., Moro-Velazquez L., Godino-Llorente J.I.","Artificial Intelligence applied to chest X-Ray images for the automatic detection of COVID-19. A thoughtful evaluation approach","10.1109/ACCESS.2020.3044858","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098800887&doi=10.1109%2fACCESS.2020.3044858&partnerID=40&md5=5b2948fb34f8edf4b9e5c90abe8bd00e","Current standard protocols used in the clinic for diagnosing COVID-19 include molecular or antigen tests, generally complemented by a plain chest X-Ray. The combined analysis aims to reduce the significant number of false negatives of these tests and provide complementary evidence about the presence and severity of the disease. However, the procedure is not free of errors, and the interpretation of the chest X-Ray is only restricted to radiologists due to its complexity. With the long term goal to provide new evidence for the diagnosis, this paper presents an evaluation of different methods based on a deep neural network. These are the first steps to develop an automatic COVID-19 diagnosis tool using chest X-Ray images to differentiate between controls, pneumonia, or COVID-19 groups. The paper describes the process followed to train a Convolutional Neural Network with a dataset of more than 79, 500 X-Ray images compiled from different sources, including more than 8, 500 COVID-19 examples. Three different experiments following three preprocessing schemes are carried out to evaluate and compare the developed models. The aim is to evaluate how preprocessing the data affects the results and improves its explainability. Likewise, a critical analysis of different variability issues that might compromise the system and its effects is performed. With the employed methodology, a 91.5% classification accuracy is obtained, with an 87.4% average recall for the worst but most explainable experiment, which requires a previous automatic segmentation of the lung region. CCBY","chest X-Ray; Computed tomography; COVID-19; COVID-19; Deep learning; Deep Learning; Diseases; Lung; Pneumonia; radiological imaging; Sensitivity; X-ray imaging","Convolutional neural networks; Deep neural networks; Automatic Detection; Automatic segmentations; Classification accuracy; Combined analysis; Critical analysis; Evaluation approach; Preprocessing scheme; Standard protocols; Diagnosis"
"Aridor Yariv, Lange Danny B.","Agent design patterns: Elements of agent application design",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031700834&partnerID=40&md5=b0b97611de174bbc5d385ff51617d72c","Agent technology is an emerging field and agent-based application design is still a pioneering discipline. We are all pioneers, inventing and re-inventing sometimes smart but perhaps more not-so-smart solutions to recurrent problems. It is here that agent design patterns can help by capturing good solutions to common problems in agent design. No special skills, language features, or other tricks are required for you to benefit from these patterns. Simply speaking, agent design patterns can make your applications more flexible, understandable, and reusable, which is probably why you were interested in agent technology in the first place. In this paper we report on several design patterns we have found in mobile agent applications.",,"Agent design patterns; Mobile agents; Computer aided software engineering; Artificial intelligence"
"Arief L., Tantowi A.Z., Novani N.P., Sundara T.A.","Implementation of YOLO and smoke sensor for automating public service announcement of cigarette's hazard in public facilities","10.1109/ICITSI50517.2020.9264972","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099587232&doi=10.1109%2fICITSI50517.2020.9264972&partnerID=40&md5=a01c479fb426576289f3a6b1fa9f6d90","Smoking in public facilities is one of the most unwanted behaviours that shall be given serious and continuous approaches. Using automation-based approaches, nowadays, a common technique used is using detectors which trigger an alarm upon detecting smoke. While this approach is better than none, it is more detective than preventive and is still lacking awareness-stimulation. This research aims to propose a more comprehensive solution. A real-time alert is not only triggered when the system detects a certain level of smoke, but also when it recognizes cigarette objects, to add the preventive purpose. Not only a buzzer is triggered, upon detection of either smoke or cigarette or both, a public service announcement (PSA) is also automatically displayed to provide education about smoking's hazard to conduct responsible behavior in public facilities. The system combines the concepts of embedded systems and deep learning. The process consists of a continuous monitoring of smoke level from several sensors reading and also a continuous cigarette object detection using YOLO. The outputs are two binary digits that will role as the inputs for an OR operation which results in the bit required to trigger an anti-smoking PSA and a buzzer. The system consists of a Raspberry Pi 4B 4GB, smoke sensors, a camera module as the visual input device, a monitor, and also a buzzer. The results show that the system is able to perform in a semi indoor environment for detecting both smoke and cigarette then automating an alert and a PSA in a soft real-time manner. © 2020 IEEE.","Embedded System; MQ-2; Object detection; Public service announcements; Real time; Smoke; YOLO","Deep learning; Embedded systems; Hazards; Object detection; Public relations; Tobacco; Camera modules; Continuous approach; Continuous monitoring; Indoor environment; Input devices; Public facilities; Public services; Soft real time; Smoke"
"Arieli O., Borg A., Hesse M., Strasser C.","Explainable logic-based argumentation","10.3233/FAIA220139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139475689&doi=10.3233%2fFAIA220139&partnerID=40&md5=d205f9cde7a308428ab4b37738ba3977","Explainable artificial intelligence (XAI) has gained increasing interest in recent years in the argumentation community. In this paper we consider this topic in the context of logic-based argumentation, showing that the latter is a particularly promising paradigm for facilitating explainable AI. In particular, we provide two representations of abductive reasoning by sequent-based argumentation frameworks and show that such frameworks successfully cope with related challenges, such as the handling of synonyms, justifications, and logical equivalences. © 2022 The authors and IOS Press. All rights reserved.","abductive logics; Explainable AI; sequent-based argumentation","Artificial intelligence; Computation theory; Abductive logic; Abductive reasoning; Argumentation frameworks; Explainable AI; Logic-based argumentations; Logical equivalence; Sequent-based argumentation; Computer circuits"
"Arieli O., Borg A., Hesse M., Straßer C.","Abductive Reasoning with Sequent-Based Argumentation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137602697&partnerID=40&md5=f7c7fd5e53c57618ff9b759ea34197d6","We show that logic-based argumentation, and in particular sequent-based frameworks, is a robust argumentative setting for abductive reasoning and explainable artificial intelligence. © 2022 Copyright for this paper by its authors.",,"Abductive reasoning; Logic-based argumentations; Artificial intelligence"
"Arif Wani M., Kantardzic M., Sayed-Mouchaweh M.","Trends in Deep Learning Applications","10.1007/978-981-15-1816-4_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080957749&doi=10.1007%2f978-981-15-1816-4_1&partnerID=40&md5=3cdddcffa7942b6fd9b2c6366fe1f5ef","Deep Learning is a new area of Machine Learning which has gained popularity in the recent past. It has surpassed the conventional algorithms in accuracy as the features are learned from the data using a general purpose learning procedure instead of being designed by human engineers [1]. Deep learning is responsible for today’s explosion of AI. Deep networks have demonstrated dramatic improvements in computer vision and machine translation tasks. It has the ability to recognize spoken words nearly as good as humans can. It has demonstrated good generalization power and has achieved high accuracy in machine learning modeling, which has even attracted non-computer scientists. It is now being used as a guide to make key decisions in fields like medicine, finance, manufacturing, and beyond. Deep learning has succeeded in previously unsolved problems which were quite difficult to resolve using machine learning as well as other shallow networks. However, deep learning is still in its infancy, but it is likely that deep learning will have many successes in the near future as it requires little hand engineering and thus can take advantage of the vast amount of data and computation power. Various supervised and unsupervised deep architectures have been reported in [1]. This chapter outlines the use of deep learning technology in applications like game playing, medical applications, video analytics, regression/classification, object detection/recognition, and robotic automation. © 2020 Springer Nature Singapore Pte Ltd 2020.",,"Computer vision; Learning systems; Medical applications; Object detection; Computer scientists; Conventional algorithms; Deep architectures; Learning procedures; Learning technology; Machine learning models; Machine translations; Robotic automation; Deep learning"
"Arık S.Ö., Pfister T.","TabNet: Attentive Interpretable Tabular Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122723700&partnerID=40&md5=7ae0f72431278091e705c730d211aab8","We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into its global behavior. Finally, we demonstrate self-supervised learning for tabular data, significantly improving performance when unlabeled data is abundant. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved",,"Efficient learning; Global behaviors; Improving performance; Interpretability; Learning architectures; Learning capacity; Performance; Salient features; Tabular data; Unlabeled data; Deep learning"
"Arik S.O., Pfister T.","Protoattend: Attention-based prototypical learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094913326&partnerID=40&md5=f7432167e1e1e6a753d804188a0a2012","We propose a novel inherently interpretable machine learning method that bases decisions on few relevant examples that we call prototypes. Our method, ProtoAttend, can be integrated into a wide range of neural network architectures including pre-trained models. It utilizes an attention mechanism that relates the encoded representations to samples in order to determine prototypes. Protoattend yields superior results in three high impact problems without sacrificing accuracy of the original model: (1) it enables high-quality interpretability that outputs samples most relevant to the decision-making (i.e. a samplebased interpretability method); (2) it achieves state of the art confidence estimation by quantifying the mismatch across prototype labels; and (3) it obtains state of the art in distribution mismatch detection. All these can be achieved with minimal additional test time and a practically viable training time computational cost. © 2020 Sercan O . Arik and Tomas Pfister.","Attention; Confidence; Explainable deep learning; Prototypical; Sample-based interpretability","Machine learning; Network architecture; Attention mechanisms; Computational costs; Confidence estimation; Interpretability; Machine learning methods; Original model; State of the art; Training time; Decision making"
"Arik S.Ö., Li C.-L., Yoon J., Sinha R., Epshteyn A., Le L.T., Menon V., Singh S., Zhang L., Nikoltchev M., Sonthalia Y., Nakhost H., Kanal E., Pfister T.","Interpretable sequence learning for COVID-19 forecasting",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108455015&partnerID=40&md5=b51da1e625a2cee18e44588e9ea7b34a","We propose a novel approach that integrates machine learning into compartmental disease modeling (e.g., SEIR) to predict the progression of COVID-19. Our model is explainable by design as it explicitly shows how different compartments evolve and it uses interpretable encoders to incorporate covariates and improve performance. Explainability is valuable to ensure that the model’s forecasts are credible to epidemiologists and to instill confidence in end-users such as policy makers and healthcare institutions. Our model can be applied at different geographic resolutions, and we demonstrate it for states and counties in the United States. We show that our model provides more accurate forecasts compared to the alternatives, and that it provides qualitatively meaningful explanatory insights. © 2020 Neural information processing systems foundation. All rights reserved.",,"Covariates; Disease modeling; End users; Healthcare institutions; Improve performance; Policy makers; Sequence learning; Forecasting"
"Arimoto R.","Computational models for predicting interactions with cytochrome p450 enzyme","10.2174/156802606778108951","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748105936&doi=10.2174%2f156802606778108951&partnerID=40&md5=a0065949e9f0017ed9ab97f30956b530","Cytochrome p450 (CYP) enzymes are predominantly involved in Phase 1 metabolism of xenobiotics. As only 6 isoenzymes are responsible for ∼90 % of known oxidative drug metabolism, a number of frequently prescribed drugs share the CYP-mediated metabolic pathways. Competing for a single enzyme by the co-administered therapeutic agents can substantially alter the plasma concentration and clearance of the agents. Furthermore, many drugs are known to inhibit certain p450 enzymes which they are not substrates for. Because some drug-drug interactions could cause serious adverse events leading to a costly failure of drug development, early detection of potential drug-drug interactions is highly desirable. The ultimate goal is to be able to predict the CYP specificity and the interactions for a novel compound from its chemical structure. Current computational modeling approaches, such as two-dimensional and three-dimensional quantitative structure-activity relationship (QSAR), pharmacophore mapping and machine learning methods have resulted in statistically valid predictions. Homology models have been often combined with 3D-QSAR models to impose additional steric restrictions and/or to identify the interaction site on the proteins. This article summarizes the available models, methods, and key findings for CYPIA2, 2A6, 2C9, 2D6 and 3A4 isoenzyme. © 2006 Bentham Science Publishers Ltd.","3D-QSAR; CYP1A subfamily; Neural network; Pharmacophore modeling; Support vector machine (SVM)","analgesic agent; antiarrhythmic agent; beta adrenergic receptor blocking agent; clozapine; coumarin derivative; cytochrome P450 1A2; cytochrome P450 2A6; cytochrome P450 2C9; cytochrome P450 3A4; diazepam; flavanoid; imipramine; isoenzyme; lactone derivative; methoxsalen; mexiletine; midazolam; naphthalene derivative; naproxen; phenprocoumon; quinoline derivative; quinolone derivative; riluzole; sulfaphenazole; theophylline; tranylcypromine; tricyclic antidepressant agent; unindexed drug; uricosuric agent; warfarin; analytic method; chemical structure; drug binding site; drug clearance; drug design; drug metabolism; drug synthesis; enzyme inhibition; enzyme metabolism; enzyme specificity; human; mathematical computing; nonhuman; pharmacophore; prediction; protein interaction; quantitative structure activity relation; review; statistical model; structural homology; xenobiotic metabolism; Computational Biology; Computer Simulation; Cytochrome P-450 Enzyme System; Enzyme Inhibitors; Humans; Structure-Activity Relationship"
"Aringhieri R., Dell’Anna D., Duma D., Sonnessa M.","Evaluating the dispatching policies for a regional network of emergency departments exploiting health care big data","10.1007/978-3-319-72926-8_46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039413975&doi=10.1007%2f978-3-319-72926-8_46&partnerID=40&md5=6d6d537148b44680b0de9bbea8e2f5c8","The Emergency Department (ED) is responsible to provide medical and surgical care to patients arriving at the hospital in need of immediate care. At the regional level, the EDs system can be seen as a network of EDs cooperating to maximise the outputs (number of patients served, average waiting time,..) and outcomes in terms of the provided care quality. In this paper we discuss how quantitative analysis based on health care big data can provide a tool to evaluate the dispatching policies for the network of emergency departments operating in Piedmont, Italy: the basic idea is to exploit clusters of EDs in such a way to fairly distribute the workload. Further, we discuss how big data can enable a novel methodological approach to the health system analysis. © Springer International Publishing AG 2018.","Big data; Emergency care pathway; Health systems","Artificial intelligence; Electric load dispatching; Emergency rooms; Health care; Hospitals; Learning systems; Average waiting-time; Emergency care; Emergency departments; Health systems; Methodological approach; Regional levels; Regional networks; Big data"
"Arisdakessian S., Wahab O.A., Mourad A., Otrok H., Guizani M.","A Survey on IoT Intrusion Detection: Federated Learning, Game Theory, Social Psychology and Explainable AI as Future Directions","10.1109/JIOT.2022.3203249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137608785&doi=10.1109%2fJIOT.2022.3203249&partnerID=40&md5=4ad2f5be011033dd34fb63f2f88dd7f1","In the past several years, the world has witnessed an acute surge in the production and usage of smart devices which are referred to as the Internet of Things (IoT). These devices interact with each other as well as with their surrounding environments to sense, gather and process data of various kinds. Such devices are now part of our everyday&#x2019;s life and are being actively used in several verticals such as transportation, healthcare, and smart homes. IoT devices, which usually are resource-constrained, often need to communicate with other devices such as fog nodes and/or cloud computing servers to accomplish certain tasks that demand large resource requirements. These communications entail unprecedented security vulnerabilities, where malicious parties find in this heterogeneous and multi-party architecture a compelling platform to launch their attacks. In this work, we conduct an in-depth survey on the existing intrusion detection solutions proposed for the IoT ecosystem which includes the IoT devices as well as the communications between the IoT, fog computing and cloud computing layers. Although some survey articles already exist, the originality of this work stems from the three following points: (1) discuss the security issues of the IoT ecosystem not only from the perspective of IoT devices but also taking into account the communications between the IoT, fog and cloud computing layers; (2) propose a novel two-level classification scheme that first categorizes the literature based on the approach used to detect attacks and then classify each approach into a set of sub-techniques; and (3) propose a comprehensive cybersecurity framework that combines the concepts of Explainable Artificial Intelligence (XAI), federated learning, game theory and social psychology to offer future IoT systems a strong protection against cyberattacks. IEEE","Cloud computing; Collaborative work; Cybersecurity; Edge computing; Explainable Artificial Intelligence; Federated Learning; Game theory; Game Theory; Internet of Things; Internet of Things; Intrusion detection; Intrusion Detection Systems; Taxonomy",
"Ariza-Colpas P.P., Vicario E., Oviedo-Carrascal A.I., Aziz S.B., Piñeres-Melo M.A., Quintero-Linero A., Patara F.","Human Activity Recognition Data Analysis: History, Evolutions, and New Trends","10.3390/s22093401","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129013868&doi=10.3390%2fs22093401&partnerID=40&md5=4b3fd5c08eba0fd7d6d22f6e877e7e0d","The Assisted Living Environments Research Area–AAL (Ambient Assisted Living), focuses on generating innovative technology, products, and services to assist, medical care and rehabilitation to older adults, to increase the time in which these people can live. independently, whether they suffer from neurodegenerative diseases or some disability. This important area is responsible for the development of activity recognition systems—ARS (Activity Recognition Systems), which is a valuable tool when it comes to identifying the type of activity carried out by older adults, to provide them with assistance. that allows you to carry out your daily activities with complete normality. This article aims to show the review of the literature and the evolution of the different techniques for processing this type of data from supervised, unsupervised, ensembled learning, deep learning, reinforcement learning, transfer learning, and metaheuristics approach applied to this sector of science. health, showing the metrics of recent experiments for researchers in this area of knowledge. As a result of this article, it can be identified that models based on reinforcement or transfer learning constitute a good line of work for the processing and analysis of human recognition activities. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","activities of daily living—ADL; activity recognition systems—ARS; ambient assisted living—AAL; clustering; deep learning; ensemble learning; human activity recognition—HAR; reinforcement learning; supervised learning; unsupervised activity recognition; unsupervised learning","Assisted living; Neurodegenerative diseases; Pattern recognition; Reinforcement learning; Activities of Daily Living; Activity of daily living—ADL; Activity recognition; Activity recognition system—ARS; Ambient assisted living; Ambient assisted living—AAL; Clusterings; Deep learning; Ensemble learning; Human activity recognition; Human activity recognition—HAR; Recognition systems; Unsupervised activity recognition; Deep learning; aged; daily life activity; disabled person; human; human activities; technology; Activities of Daily Living; Aged; Ambient Intelligence; Disabled Persons; Human Activities; Humans; Technology"
"Ariza-Garzon M.J., Arroyo J., Caparrini A., Segovia-Vargas M.-J.","Explainability of a Machine Learning Granting Scoring Model in Peer-to-Peer Lending","10.1109/ACCESS.2020.2984412","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083695710&doi=10.1109%2fACCESS.2020.2984412&partnerID=40&md5=1c6f225ace3727d23e957385d8a39555","Peer-to-peer (P2P) lending demands effective and explainable credit risk models. Typical machine learning algorithms offer high prediction performance, but most of them lack explanatory power. However, this deficiency can be solved with the help of the explainability tools proposed in the last few years, such as the SHAP values. In this work, we assess the well-known logistic regression model and several machine learning algorithms for granting scoring in P2P lending. The comparison reveals that the machine learning alternative is superior in terms of not only classification performance but also explainability. More precisely, the SHAP values reveal that machine learning algorithms can reflect dispersion, nonlinearity and structural breaks in the relationships between each feature and the target variable. Our results demonstrate that is possible to have machine learning credit scoring models be both accurate and transparent. Such models provide the trust that the industry, regulators and end-users demand in P2P lending and may lead to a wider adoption of machine learning in this and other risk assessment applications where explainability is required. © 2013 IEEE.","boosting; Credit risk; explainability; logistic regression; P2P lending; Shapley values","Finance; Logistic regression; Machine learning; Peer to peer networks; Risk assessment; Classification performance; Credit scoring model; Explanatory power; Logistic Regression modeling; Peer-to-peer lending; Prediction performance; Scoring models; Structural break; Learning algorithms"
"Arjaria S.K., Rathore A.S., Chaubey G.","Developing an Explainable Machine Learning-Based Thyroid Disease Prediction Model","10.4018/IJBAN.292058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130959587&doi=10.4018%2fIJBAN.292058&partnerID=40&md5=e4e0328fc0218fde37929cd69ce76394","Healthcare and medicine are key areas where machine learning algorithms are widely used. The medical decision support systems thus created are accurate enough; however, they suffer from the lack of transparency in decision making and shows a black box behavior. However, transparency and trust are significant in the field of health and medicine, and hence, a black box system is sub optimal in terms of widespread applicability and reach. Hence, the explainablility of the research makes the system reliable and understandable, thereby enhancing its social acceptability. The presented work explores a thyroid disease diagnosis system. SHAP, a popular method based on coalition game theory, is used for interpretability of results. The work explains the system behavior both locally and globally and shows how machine leaning can be used to ascertain the causality of the disease and support doctors to suggest the most effective treatment of the disease. The work not only demonstrates the results of machine learning algorithms but also explains related feature importance and model insights. © 2021 American Society of Mechanical Engineers (ASME). All rights reserved.","Explainable AI; Features; Healthcare; Interpretability; Logistic Regression; Machine Learning; SHAP; Thyroid Disease",
"Arjaria S.K., Rathore A.S., Cherian J.S.","Kidney disease prediction using a machine learning approach: A comparative and comprehensive analysis","10.1016/B978-0-12-821633-0.00006-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123662352&doi=10.1016%2fB978-0-12-821633-0.00006-4&partnerID=40&md5=a54c27d4720f38ca824c3c753ecc2008","In our competitive world, the lifestyles of human beings are very irregular. Irregularities in eating habits, daily routines, and workloads create a lot of tension that in turn leads to many chronic diseases. The four most prominent chronic diseases are heart disease, cancer, diabetes, and kidney disease. In this chapter, the potential of individual machine learning algorithms is analyzed for the prediction of severe kidney diseases. The kidney is known to be one of the most important organs of the human body. It is solely accountable for the purification of blood. The kidneys are responsible for the percolation of excess fluids and wastes from the blood so they can be excreted from the human body through urine. In the advanced stage of kidney disease, the kidney is unable to filter the surplus levels of fluid, wastes, and electrolytes from the blood, so they remain in the body. The kidney is damaged by a variety of reasons, including excess alcohol consumption, excess antibiotic doses, smoking, family history, high blood pressure, and obesity. A damaged kidney makes a person miserable. To predict the likelihood of kidney-related diseases, the doctor analyzes the key symptoms of a patient’s body and also suggests various tests such as blood tests, urine tests, and imaging tests to further confirm the disease. Based on tests and examinations, the doctor confirms whether the individual is suffering from kidney-related problems. In the present era of machine learning, the mathematical models have prediction and classification capacities. These capacities can be used to assist doctors in cross-checking their results. Therefore, they increase the prediction accuracy for disease diagnosis. The correct diagnosis of a disease can only help in providing better treatment and thus increasing the chances of survival. The dataset used in the work is taken from the University of California Irvine Knowledge Discovery in Databases (UCI KDD) site. The data have 25 attributes and 400 records divided into two classes: chronic kidney disease (CKD) and non-CKD. Preprocessing steps are applied to make the data more suitable for further analytics. The preprocessing steps here deal with the missing value, removal of noise, and normalization of attributes. This chapter then ranks the features based on different feature selection techniques such as information gain, information gain ratio, Gini Index, relief, FCBF, and chi-square. It presents a comparative study of these methods. The ranking of features is helpful in feature selection and in feature reduction. It improves the performance in terms of time and accuracy by achieving better results in the least time. In the next step, the performance of different machine learning algorithms such as k-nearest neighbor, artificial neutral network (ANN), support vector machine (SVM), AdaBoost, random forest, and naïve Bayes classifier is noted. Regression analysis is recorded and evaluated using k-fold cross-validation. Then after varying the different hyperparameters of the algorithms, the number of features and combination of features, the evaluation is done using measures such as accuracy, precision, recall, F1 measure, etc. Different standard machine learning algorithm results have been compared for the early diagnosis of CKD based on different parameters and the number of features, which will help researchers tune parameters in their respective models. Considering the information gain ratio, Gini Index, and chi-square as feature selections, all the models give accuracy greater than 96%; some models reaches 100% accuracy. This much accuracy is sufficient to assist doctors in the correct diagnosis so they can provide better treatment. © 2021 Elsevier Inc. All rights reserved.","AdaBoost; CKD; k-NN; Logistic regression; Random forest; SVM",
"Arjmand A., Angelis C.T., Christou V., Tzallas A.T., Tsipouras M.G., Glavas E., Forlano R., Manousou P., Giannakeas N.","Training of deep convolutional neural networks to identify critical liver alterations in histopathology image samples","10.3390/app10010042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078876253&doi=10.3390%2fapp10010042&partnerID=40&md5=e74458449d71a140832f509b8c0c98bf","Nonalcoholic fatty liver disease (NAFLD) is responsible for a wide range of pathological disorders. It is characterized by the prevalence of steatosis, which results in excessive accumulation of triglyceride in the liver tissue. At high rates, it can lead to a partial or total occlusion of the organ. In contrast, nonalcoholic steatohepatitis (NASH) is a progressive form of NAFLD, with the inclusion of hepatocellular injury and inflammation histological diseases. Since there is no approved pharmacotherapeutic solution for both conditions, physicians and engineers are constantly in search for fast and accurate diagnostic methods. The proposed work introduces a fully automated classification approach, taking into consideration the high discrimination capability of four histological tissue alterations. The proposed work utilizes a deep supervised learning method, with a convolutional neural network (CNN) architecture achieving a classification accuracy of 95%. The classification capability of the new CNN model is compared with a pre-trained AlexNet model, a visual geometry group (VGG)-16 deep architecture and a conventional multilayer perceptron (MLP) artificial neural network. The results show that the constructed model can achieve better classification accuracy than VGG-16 (94%) and MLP (90.3%), while AlexNet emerges as the most efficient classifier (97%). © 2019 by the authors.","Computer vision; Convolutional neural networks; Deep learning; Fatty liver; Hepatocyte ballooning; Liver biopsies",
"Arjunan P., Poolla K., Miller C.","BEEM: Data-driven building energy benchmarking for Singapore","10.1016/j.enbuild.2022.111869","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124701277&doi=10.1016%2fj.enbuild.2022.111869&partnerID=40&md5=129f5481bb929155ac7e9fe9203cee7c","Building energy use benchmarking is the process of measuring the energy performance of buildings relative to their peer group for creating awareness and identifying energy-saving opportunities. In this paper, we present the design and implementation of BEEM, a data-driven energy use benchmarking system for buildings in Singapore. The peer groups for comparison are established using a public energy disclosure data set. We use an ensemble tree algorithm for accurately modeling building energy use and for identifying the most influential factors. Our models reduce the prediction error from 24.39% to 6.04%, on average, when compared to the baseline linear regression models, which were used in the previous energy efficiency labeling program in Singapore, and outperforms ten other recent models. Using the prototype implementation of BEEM, we benchmarked three building types, office (290), hotel (203), and retail (125), and compared their rating. The code repository and the accompanying data set are released as an open-source project for community use. © 2022 Elsevier B.V.","Building energy benchmarking; Building energy labeling; Feature interaction; Gradient boosting trees; Interpretable machine learning; Regression analysis","Adaptive boosting; Benchmarking; Buildings; Decision trees; Energy efficiency; Energy utilization; Forestry; Machine learning; Open source software; Open systems; Boosting trees; Building energy; Building energy benchmarking; Building energy labelling; Energy benchmarking; Feature interactions; Gradient boosting; Gradient boosting tree; Interpretable machine learning; Singapore; Regression analysis"
"Arjunan P., Poolla K., Miller C.","EnergyStar++: Towards more accurate and explanatory building energy benchmarking","10.1016/j.apenergy.2020.115413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087717811&doi=10.1016%2fj.apenergy.2020.115413&partnerID=40&md5=6893e878dbb88b088c9dba8b23290d89","Building energy performance benchmarking has been adopted widely in the USA and Canada through the Energy Star Portfolio Manager platform. Building operations and energy management professionals have long used this simple 1–100 score to understand how their building compares to its peers. This single number is easy to use but is created by potentially inaccurate multiple linear regression (MLR) models and lacks much further information about why a building achieves that score. This paper proposes a methodology that enhances the existing Energy Star calculation method by increasing accuracy and providing additional model output processing to help explain why a building is achieving a particular score. Two new prediction models were proposed and tested: multiple linear regression with feature interactions (MLRi) and gradient boosted trees (GBT). Both models performed better than a baseline Energy Star MLR model as well as four baseline models from previous benchmarking studies. This paper shows that for six building types, on average, the third-order MLRi models achieved a 4.9% increase in adjusted R2 and a 7.0% decrease in normalized root mean squared error (NRMSE) over the baseline MLR model. More substantially, the most accurate GBT models, on average, achieved a 24.9% increase in adjusted R2 and a 13.7% decrease in NMRSE against the baseline MLR model. In addition, a set of techniques was developed to help determine which factors most influence a building's energy use versus its peers using SHapley Additive exPlanation (SHAP) values. The SHAP force visualization, in particular, offered an accessible overview of the aspects of the building that influenced the score that even non-technical users can interpret. This methodology was tested on the 2012 Commercial Building Energy Consumption Survey (CBECS)(1,812 buildings) and public data sets from the energy disclosure programs of New York City (11,131 buildings) and Seattle (2,073 buildings). © 2020 Elsevier Ltd","Building energy benchmarking; Building performance rating; Feature interaction; Gradient boosting trees; Interpretable machine learning; Multiple linear regression","Benchmarking; Energy utilization; Linear regression; Mean square error; Office buildings; Stars; Building energy performance; Building operations; Commercial building; Feature interactions; Management professionals; Multiple linear regression models; Multiple linear regressions; Root mean squared errors; Predictive analytics"
"Arkhangelski J., Abdou-Tankari M., Lefebvre G.","Day-Ahead Optimal Power Flow for Efficient Energy Management of Urban Microgrid","10.1109/TIA.2020.3049117","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099259500&doi=10.1109%2fTIA.2020.3049117&partnerID=40&md5=c1e3b4534e229e68d94d0911e88929d3","This study deals with the urban microgrid energy management that is dedicated to individual and collective self-consumption by providing flexibility to the distribution grid (DG). The proposed urban community microgrid is interconnected to a DG and it consists of an association of centralized storage units (called community energy storage system), community intermittent renewable generation, and intelligent energy management system (EMS). One of the main advantages of urban microgrid is that, in case of faults in the DG, it can cut existing interconnections and continue to supply the responsible community in the island mode. In this study, the developed urban microgrid EMS is based on the predictive control management through the day-ahead optimal power flow (DA-OPF) strategy. The main contributions of this work can be defined by two points. The first point is related to a development of the DA-OPF strategy for the urban microgrid based on the intelligent deep learning data forecasting and the mixed-integer nonlinear programming optimization methods. The second point concerns a development of an optimization function integrating the concept of ancillary services of DG flexibility. Experimental results and economical evaluation are presented in this article. By using the proposed strategies, it results in an important electricity price reduction for the considered urban microgrid, compared to a conventional distribution system and basic operation schemes. © 1972-2012 IEEE.","Ancillary services (AS); day-ahead optimal power flow (DA-OPF); deep learning (DL); flexibility; mixed-integer nonlinear programming (MINLP); renewable energy; self-consumption; urban microgrid","Acoustic generators; Deep learning; Digital storage; Electric energy storage; Electric load flow; Energy efficiency; Energy management; Integer programming; Microgrids; Nonlinear programming; Distribution systems; Economical evaluation; Intelligent energy management systems; Mixed-integer nonlinear programming; Optimal power flows; Optimization function; Optimization method; Renewable generation; Energy management systems"
"Arkin R.C.","Ethics and Autonomous Systems: Perils and Promises","10.1109/JPROC.2016.2601162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991354985&doi=10.1109%2fJPROC.2016.2601162&partnerID=40&md5=509ec4b557302d5206d9ee7a954cf7f9","Examines the technology of autonomous systems and explores its ethical and social implications. Artificial intelligence (AI) and its role in autonomous systems have promised everything from utopian freedom to existential dystopia. The unfilled hyperbole surrounding past and present promises regarding AI futures has left many people skeptical, afraid, or just confused. Rational discussion is often left in the wake due to the fears and fantasy evoked by the press and Hollywood. Fortunately, as a byproduct, this has resulted in a blossoming of worldwide discourse on the ethical implications of the intelligent machines we are creating. Many near and mid-term ethical concerns have arisen with the advent of autonomous systems: particularly regarding driverless cars, privacy and drones, companionand intimate robotics, the displacement of jobs by intelligent machines, and warfighting robots among others. The IEEE Global Initiative on the Ethics of Autonomous Systems, the United Nations, the International Committee of the Red Cross, the White House, and the Future of Life Institute are among many responsible organizations that are now considering the ramifications of the real-world consequences of machine autonomy as we continue to stumble about trying to find a way forward. © 1963-2012 IEEE.",,"Intelligent robots; Autonomous systems; Ethical concerns; Ethical implications; Intelligent machine; International committee of the red cross; Machine autonomy; Past and present; Social implication; Philosophical aspects"
"Armagan S., Fallon E., Qiao Y.","PAL: A path selection algorithm for Life Critical Data","10.1109/CICSyN.2010.49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649875511&doi=10.1109%2fCICSyN.2010.49&partnerID=40&md5=898c3c0972605da6b84c3f0b78c15dfc","The real time data transmission facilities provided by Personal Area Networks (PAN) can improve both the life expectancy and living conditions of patients following cardiac surgery. The limited signal range of such devices however, coupled with the life critical nature of the data transferred places strict performance limits on the mobility protocol. The Stream Control Transmission Protocol (SCTP) can support mobility through its multi-homing feature. SCTP has 2 core features (1) analyzing performance metrics which trigger network migration (2) implementing the physical migration from one network path to another. We illustrate that as a transport layer protocol which adheres to protocol boundaries SCTP's network evaluation is performance limited, as Bluetooth MAC retransmissions distort end-to-end calculations. We propose a division of functionality in which SCTP is responsible for network migration while we propose a new Path Selection Algorithm for Life Critical Data (PAL) which defines a Media Independent Handover (MIH) oriented trigger to control path switchover. PAL illustrates the importance of Bluetooth MAC retransmissions as a performance metric over traditional metrics such as Received Signal Strength (RSS). © 2010 IEEE.","Body sensor network; MIH; SCTP; Switchover","Body sensor network; Cardiac surgery; Control path; Core features; Critical data; IS performance; Life expectancies; Living conditions; Media independent handover; MIH; Mobility protocols; Multi-homing; Network evaluation; Network paths; Path selection algorithms; Performance limits; Performance metrices; Performance metrics; Personal area networks; Real time data transmission; Received signal strength; Retransmissions; SCTP; Signal range; Stream control transmission protocols; Switchover; Transport layer protocols; Algorithms; Artificial intelligence; Cardiovascular surgery; Communication systems; Internet protocols; Personal communication systems; Sensor networks; Telecommunication networks; Bluetooth"
"Armengol E.","Classification of melanomas in situ using knowledge discovery with explained case-based reasoning","10.1016/j.artmed.2010.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952509940&doi=10.1016%2fj.artmed.2010.09.001&partnerID=40&md5=cfde6260d531c99c975a0747270f3a24","Objective: Early diagnosis of melanoma is based on the ABCD rule which considers asymmetry, border irregularity, color variegation, and a diameter larger than 5. mm as the characteristic features of melanomas. When a skin lesion presents these features it is excised as prevention. Using a non-invasive technique called dermoscopy, dermatologists can give a more accurate evaluation of skin lesions, and can therefore avoid the excision of lesions that are benign. However, dermatologists need to achieve a good dermatoscopic classification of lesions prior to extraction. In this paper we propose a procedure called LazyCL to support dermatologists in assessing the classification of skin lesions. Our goal is to use LazyCL for generating a domain theory to classify melanomas in situ. Methods: To generate a domain theory, the LazyCL procedure uses a combination of two artificial intelligence techniques: case-based reasoning and clustering. First LazyCL randomly creates clusters and then uses a lazy learning method called lazy induction of descriptions (LID) with leave-one-out on them. By means of LID, LazyCL collects explanations of why the cases in the database should belong to a class. Then the analysis of relationships among explanations produces an understandable clustering of the dataset. After a process of elimination of redundancies and merging of clusters, the set of explanations is reduced to a subset of it describing classes that are "" almost"" discriminant. The remaining explanations form a preliminary domain theory that is the basis on which experts can perform knowledge discovery. Results: We performed two kinds of experiments. First ones consisted on using LazyCL on a database containing the description of 76 melanomas. The domain theory obtained from these experiments was compared on previous experiments performed using a different clustering method called self-organizing maps (SOM).Results of both methods, LazyCL and SOM, were similar. The second kind of experiments consisted on using LazyCL on well known domains coming from the machine learning repository of the Irvine University. Thus, since these domains have known solution classes, we can prove that the clusters build by LazyCL are correct. Conclusions: We can conclude that LazyCL that uses explained case-based reasoning for knowledge discovery is feasible for constructing a domain theory. On one hand, experiments on the melanoma database show that the domain theory build by LazyCL is easy to understand. Explanations provided by LID are easily understood by domain experts since these descriptions involve the same attributes than they used to represent domain objects. On the other hand, experiments on standard machine learning data sets show that LazyCL is a good method of clustering since all clusters produced are correct. © 2010 Elsevier B.V.","Case-based reasoning; Clustering methods; Dermatology; Explanations in case-based reasoning; Knowledge discovery; Lazy learning methods; Melanomas in situ","Clustering methods; Explanations in case-based reasoning; Knowledge discovery; Lazy learning; Melanomas in situ; Artificial intelligence; Cluster analysis; Conformal mapping; Database systems; Dermatology; Diagnosis; Experiments; Learning systems; Oncology; Self organizing maps; Case based reasoning; article; artificial intelligence; cancer classification; carcinoma in situ; case based reasoning; clinical data repository; cluster analysis; controlled study; data base; dermatological procedures; experimental study; intermethod comparison; knowledge discovery; lazy induction of description; LazyCL; learning style; melanoma; melanoma in situ; physician; priority journal; redundancy analysis; self organizing map; skin defect; standardization; theoretical model; Algorithms; Artificial Intelligence; Cluster Analysis; Decision Support Systems, Clinical; Decision Support Techniques; Dermoscopy; Diagnosis, Computer-Assisted; Early Detection of Cancer; Humans; Knowledge Bases; Medical Informatics; Melanoma; Predictive Value of Tests; Systems Integration; Unnecessary Procedures"
"Armengol E.","Using explanations for determining carcinogenecity in chemical compounds","10.1016/j.engappai.2008.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58249084767&doi=10.1016%2fj.engappai.2008.04.004&partnerID=40&md5=f51278d36b73cab8ef981746bb7f6c85","The goal of predictive toxicology is the automatic construction of carcinogenecity models. Most common artificial intelligence techniques used to construct these models are inductive learning methods. In a previous work we presented an approach that uses lazy learning methods for solving the problem of predicting carcinogenecity. Lazy learning methods solve new problems based on their similarity to already solved problems. Nevertheless, a weakness of these kind of methods is that sometimes the result is not completely understandable by the user. In this paper we propose an explanation scheme for a concrete lazy learning method. This scheme is particularly interesting to justify the predictions about the carcinogenesis of chemical compounds. In addition we propose that these explanations could be used to build a partial domain knowledge. In our particular case, we use the explanations for building general knowledge about carcinogenesis. © 2008 Elsevier Ltd. All rights reserved.","Explanations; Feature terms; Lazy induction of descriptions; Lazy learning; Partial domain models; Predictive toxicology","Artificial intelligence; Chemical compounds; Chemicals; Concretes; Pathology; Explanations; Feature terms; Lazy induction of descriptions; Lazy learning; Partial domain models; Predictive toxicology; Education"
"Armengol E.","Knowledge discovery with explained case-based reasoning","10.3233/978-1-58603-925-7-151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875947828&doi=10.3233%2f978-1-58603-925-7-151&partnerID=40&md5=c0c6b96494d60801c53241d7588e4754","The goal of Knowledge Discovery is to extract knowledge from a set of data. Most common techniques used in knowledge discovery are clustering methods, whose goal is to analyze a set of objects and obtain clusters based on the similarity among these objects. A desirable characteristic of clustering results is that these should be easily understandable by domain experts. In fact, these are characteristics that exhibit the results of eager learning methods (such as ID3) and lazy learning methods when used for building lazy domain theories. In this paper we propose LazyCL, a procedure using a lazy learning method to produce explanations on clusters of unlabeled cases. The analysis of the relations among these explanations converges to a correct clustering of the data set. © 2008 The authors and IOS Press. All rights reserved.","CBR; Clustering; Explanations; Knowledge Discovery","Artificial intelligence; Clustering algorithms; Data mining; Learning systems; CBr; Clustering; Clustering methods; Clustering results; Domain experts; Explanations; Lazy learning; Learning methods; Case based reasoning"
"Armengol E.","Discovering plausible explanations of carcinogenecity in chemical compounds","10.1007/978-3-540-73499-4_57","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249035929&doi=10.1007%2f978-3-540-73499-4_57&partnerID=40&md5=69ed40c1e8277bf8da2d8358f602066d","The goal of predictive toxicology is the automatic construction of carcinogenecity models. Most common artificial intelligence techniques used to construct these models are inductive learning methods. In a previous work we presented an approach that uses lazy learning methods for solving the problem of predicting carcinogenecity. Lazy learning methods solve new problems based on their similarity to already solved problems. Nevertheless, a weakness of these kind of methods is that sometimes the result is not completely understandable by the user. In this paper we propose an explanation scheme for a concrete lazy learning method. This scheme is particularly interesting to justify the predictions about the carcinogenesis of chemical compounds. © Springer-Verlag Berlin Heidelberg 2007.",,"Artificial intelligence; Learning systems; Toxicity; User interfaces; Carcinogenecity; Inductive learning methods; Lazy learning methods; Toxicology; Genetic engineering"
"Armenta-Medina D., Brambila-Tapia A.J.L., Miranda-Jiménez S., Rodea-Montero E.R.","A Web Application for Biomedical Text Mining of Scientific Literature Associated with Coronavirus-Related Syndromes: Coronavirus Finder","10.3390/diagnostics12040887","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128287319&doi=10.3390%2fdiagnostics12040887&partnerID=40&md5=5b00b31d70d738f9b6999bd158330a52","In this study, a web application was developed that comprises scientific literature associated with the Coronaviridae family, specifically for those viruses that are members of the Genus Betacoronavirus, responsible for emerging diseases with a great impact on human health: Middle East Respiratory Syndrome-Related Coronavirus (MERS-CoV) and Severe Acute Respiratory Syndrome-Related Coronavirus (SARS-CoV, SARS-CoV-2). The information compiled on this webserver aims to understand the basics of these viruses’ infection, and the nature of their pathogenesis, enabling the identification of molecular and cellular components that may function as potential targets on the design and development of successful treatments for the diseases associated with the Coronaviridae family. Some of the web application’s primary functions are searching for keywords within the scientific literature, natural language processing for the extraction of genes and words, the generation and visualization of gene networks associated with viral diseases derived from the analysis of latent semantic space, and cosine similarity measures. Interestingly, our gene association analysis reveals drug targets in understudies, and new targets suggested in the scientific literature to treat coronavirus. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","coronavirus; latent semantic analysis; MERS; natural language processing; SARS","Article; Betacoronavirus; Coronaviridae; Coronavirinae; coronavirus disease 2019; data mining; gene regulatory network; genetic association study; human; Middle East respiratory syndrome; Middle East respiratory syndrome coronavirus; mobile application; natural language processing; nonhuman; SARS coronavirus; scientific literature; severe acute respiratory syndrome; Severe acute respiratory syndrome coronavirus 2"
"Armstrong O., Gilad-Bachrach R.","Robust Model Compression Using Deep Hypotheses",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130094161&partnerID=40&md5=5e1d27d44a8790021a37f98fafb575bb","Machine Learning models should ideally be compact and robust. Compactness provides efficiency and comprehensibility whereas robustness provides resilience. Both topics have been studied in recent years but in isolation. Here we present a robust model compression scheme which is independent of model types: it can compress ensembles, neural networks and other types of models into diverse types of small models. The main building block is the notion of depth derived from robust statistics. Originally, depth was introduced as a measure of the centrality of a point in a sample such that the median is the deepest point. This concept was extended to classification functions which makes it possible to define the depth of a hypothesis and the median hypothesis. Algorithms have been suggested to approximate the median but they have been limited to binary classification. In this study, we present a new algorithm, the Multiclass Empirical Median Optimization (MEMO) algorithm that finds a deep hypothesis in multi-class tasks, and prove its correctness. This leads to our Compact Robust Estimated Median Belief Optimization (CREMBO) algorithm for robust model compression. We demonstrate the success of this algorithm empirically by compressing neural networks and random forests into small decision trees, which are interpretable models, and show that they are more accurate and robust than other comparable methods. In addition, our empirical study shows that our method outperforms Knowledge Distillation on DNN to DNN compression. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved",,"Artificial intelligence; Distillation; Building blockes; Classification functions; Compression scheme; Ensemble neural network; Machine learning models; Model compression; Modeling type; Optimization algorithms; Robust modeling; Robust statistics; Decision trees"
"Armstrong R., Symons M., Scott J.G., Arnott W.L., Copland D.A., McMahon K.L., Whitehouse A.J.O.","Predicting language difficulties in middle childhood from early developmental milestones: A comparison of traditional regression and machine learning techniques","10.1044/2018_JSLHR-L-17-0210","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051464790&doi=10.1044%2f2018_JSLHR-L-17-0210&partnerID=40&md5=4ebc04e7e0762b32cb49935bf2a00c74","Purpose: The current study aimed to compare traditional logistic regression models with machine learning algorithms to investigate the predictive ability of (a) communication performance at 3 years old on language outcomes at 10 years old and (b) broader developmental skills (motor, social, and adaptive) at 3 years old on language outcomes at 10 years old. Method: Participants (N = 1,322) were drawn from the Western Australian Pregnancy Cohort (Raine) Study (Straker et al., 2017). A general developmental screener, the Infant Monitoring Questionnaire (Squires, Bricker, & Potter, 1990), was completed by caregivers at the 3-year follow-up. Language ability at 10 years old was assessed using the Clinical Evaluation of Language Fundamentals–Third Edition (Semel, Wiig, & Secord, 1995). Logistic regression models and interpretable machine learning algorithms were used to assess predictive abilities of early developmental milestones for later language outcomes. Results: Overall, the findings showed that prediction accuracies were comparable between logistic regression and machine learning models using communication-only performance as well as performance on communication and broader developmental domains to predict language performance at 10 years old. Decision trees are incorporated to visually present these findings but must be interpreted with caution because of the poor accuracy of the models overall. Conclusions: The current study provides preliminary evidence that machine learning algorithms provide equivalent predictive accuracy to traditional methods. Furthermore, the inclusion of broader developmental skills did not improve predictive capability. Assessment of language at more than 1 time point is necessary to ensure children whose language delays emerge later are identified and supported. © 2018 American Speech-Language-Hearing Association.",,"algorithm; child; developmental language disorder; evaluation study; female; human; language development; language test; machine learning; male; pathophysiology; predictive value; preschool child; statistical model; Algorithms; Child; Child Language; Child, Preschool; Female; Humans; Language Development Disorders; Language Tests; Logistic Models; Machine Learning; Male; Predictive Value of Tests"
"Arnaiz A., Ferreiro S., Buderath M.","New decision support system based on operational risk assessment to improve aircraft operability","10.1243/1748006XJRR282","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957603189&doi=10.1243%2f1748006XJRR282&partnerID=40&md5=d54eec9bbd1709af76e7c8e27ad4087c","This paper details a new decision support that adds a proactive function to the actual line aircraft maintenance turn-around-time (TAT) process, where next-flight decisions are assisted by the health assessment function of the integrated vehicle health management (IVHM) of an aircraft. The 'operational risk assessment' concept appears an extended function supported on the IVHM information for calculating and evaluating the operational risk for aircraft and fleet operations. It creates or reshapes maintenance plans based on predictions of the future maintenance relevant events (e.g. component-degradation-driven repair or replacement events) and its impact on the operational planning of the aircraft/fleet. 'Operational risk assessment' makes it possible to turn the scheduled line maintenance into a proactively defined maintenance. The paper also illustrates the first step involved in this approach: 'condition view' function, which is responsible for the provision of the remaining useful life (RUL) prediction of a health managed component. This function provides the basis for the operational risk estimation on the aircraft in order to identify maintenance actions that can be deferred.","availability; Bayesian networks; decision support; operability; operational risk assessment; prognostics; reliability","Bayesian; Decision supports; operability; Operational risks; prognostics; Aircraft; Artificial intelligence; Bayesian networks; Decision support systems; Decision theory; Distributed parameter networks; Fleet operations; Health; Health risks; Inference engines; Intelligent networks; Maintainability; Repair; Risk perception; Risk assessment"
"Arnaldo I., Lam M., Veeramachaneni K.","eX2: A framework for interactive anomaly detection",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063213917&partnerID=40&md5=d7493f64c13680e8b18049fe8c7b3a45","We introduce eX2 (coined after explain and explore), a framework based on explainable outlier analysis and interactive recommendations that enables cybersecurity researchers to efficiently search for new attacks. We demonstrate the framework with both publicly available and real-world cybersecurity datasets, showing that eX2 improves the detection capability of stand-alone outlier analysis methods, therefore improving the efficiency of so-called threat hunting activities. © 2019 for the individual papers by the papers’ authors. Copying permitted for private and academic purposes. This volume is published and copyrighted by its editors.","Anomaly detection; Cybersecurity; Explainable machine learning; Interactive machine learning; Recommender systems","Machine learning; Recommender systems; Statistics; User interfaces; Cyber security; Detection capability; Interactive machine learning; Outlier analysis; Real-world; Stand -alone; Anomaly detection"
"Arnold J., Schäfer F., Žonda M., Lode A.U.J.","Interpretable and unsupervised phase classification","10.1103/PhysRevResearch.3.033052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112475727&doi=10.1103%2fPhysRevResearch.3.033052&partnerID=40&md5=067669e7b3c65371b85739b7ddf8b149","Fully automated classification methods that provide direct physical insights into phase diagrams are of current interest. Interpretable, i.e., fully explainable, methods are desired for which we understand why they yield a given phase classification. Ideally, phase classification methods should also be unsupervised. That is, they should not require prior labeling or knowledge of the phases of matter to be characterized. Here, we demonstrate an unsupervised machine-learning method for phase classification, which is rendered interpretable via an analytical derivation of the functional relationship between its optimal predictions and the input data. Based on these findings, we propose and apply an alternative, physically-motivated, data-driven scheme, which relies on the difference between mean input features. This mean-based method does not rely on any predictive model and is thus computationally cheap and directly explainable. As an example, we consider the physically rich ground-state phase diagram of the spinless Falicov-Kimball model. © 2021 Published by the American Physical Society",,"Ground state; Phase diagrams; Predictive analytics; Falicov-Kimball model; Fully automated; Functional relationship; Ground state phase diagram; Optimal predictions; Phase classification; Predictive modeling; Unsupervised machine learning; Learning systems"
"Aromolaran O., Aromolaran D., Isewon I., Oyelade J.","Machine learning approach to gene essentiality prediction: A review","10.1093/bib/bbab128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114927272&doi=10.1093%2fbib%2fbbab128&partnerID=40&md5=e8bba6fd0c92a9fae2a7110415e5909f","Essential genes are critical for the growth and survival of any organism. The machine learning approach complements the experimental methods to minimize the resources required for essentiality assays. Previous studies revealed the need to discover relevant features that significantly classify essential genes, improve on the generalizability of prediction models across organisms, and construct a robust gold standard as the class label for the train data to enhance prediction. Findings also show that a significant limitation of the machine learning approach is predicting conditionally essential genes. The essentiality status of a gene can change due to a specific condition of the organism. This review examines various methods applied to essential gene prediction task, their strengths, limitations and the factors responsible for effective computational prediction of essential genes. We discussed categories of features and how they contribute to the classification performance of essentiality prediction models. Five categories of features, namely, gene sequence, protein sequence, network topology, homology and gene ontology-based features, were generated for Caenorhabditis elegans to perform a comparative analysis of their essentiality prediction capacity. Gene ontology-based feature category outperformed other categories of features majorly due to its high correlation with the genes' biological functions. However, the topology feature category provided the highest discriminatory power making it more suitable for essentiality prediction. The major limiting factor of machine learning to predict essential genes conditionality is the unavailability of labeled data for interest conditions that can train a classifier. Therefore, cooperative machine learning could further exploit models that can perform well in conditional essentiality predictions. Short abstract: Identification of essential genes is imperative because it provides an understanding of the core structure and function, accelerating drug targets' discovery, among other functions. Recent studies have applied machine learning to complement the experimental identification of essential genes. However, several factors are limiting the performance of machine learning approaches. This review aims to present the standard procedure and resources available for predicting essential genes in organisms, and also highlight the factors responsible for the current limitation in using machine learning for conditional gene essentiality prediction. The choice of features and ML technique was identified as an important factor to predict essential genes effectively. © 2021 The Author(s) 2021. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.","conditional essentiality; conditionally essential genes; essential genes; essential proteins; feature selection; supervised learning","algorithm; animal; biology; Caenorhabditis elegans; essential gene; gene ontology; gene regulatory network; genetics; human; machine learning; procedures; support vector machine; Algorithms; Animals; Caenorhabditis elegans; Computational Biology; Gene Ontology; Gene Regulatory Networks; Genes, Essential; Humans; Machine Learning; Support Vector Machine"
"Aromolaran O., Oyelade J., Adebiyi E.","Performance evaluation of features for gene essentiality prediction","10.1088/1755-1315/655/1/012019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102353267&doi=10.1088%2f1755-1315%2f655%2f1%2f012019&partnerID=40&md5=3584b8be1722e29e925e075d07c56d62","Essential genes are subset of genes required by an organism for growth and sustenance of life and as well responsible for phenotypic changes when their activities are altered. They have been utilized as drug targets, disease control agent, etc. Essential genes have been widely identified especially in microorganisms, due to the extensive experimental studies on some of them such as Escherichia coli and Saccharomyces cerevisiae. Experimental approach has been a reliable method to identify essential genes. However, it is complex, costly, labour and time intensive. Therefore, computational approach has been developed to complement the experimental approach in order to minimize resources required for essentiality identification experiments. Machine learning approaches have been widely used to predict essential genes in model organisms using different categories of features with varying degrees of accuracy and performance. However, previous studies have not established the most important categories of features that provide the distinguishing power in machine learning essentiality predictions. Therefore, this study evaluates the discriminating strength of major categories of features used in essential gene prediction task as well as the factors responsible for effective computational prediction. Four categories of features were considered and k- fold cross-validation machine learning technique was used to build the classification model. Our results show that ontology features with an AUROC score of 0.936 has the most discriminating power to classify essential and non-essential genes. This studyconcludes that more ontology related features will further improve the performance of machine learning approach and also sensitivity, precision and AUPRC are realistic measures of performance in essentiality prediction. © Published under licence by IOP Publishing Ltd.","Classification features; Essential genes; Essential proteins; Machine-learning","Disease control; Escherichia coli; Forecasting; Machine learning; Ontology; Planning; Sustainable development; Yeast; Classification models; Computational approach; Computational predictions; Experimental approaches; K fold cross validations; Machine learning approaches; Machine learning techniques; Measures of performance; Genes"
"Aronsky D., Ransom J., Robinson K.","Accuracy of references in five biomedical informatics journals","10.1197/jamia.M1683","https://www.scopus.com/inward/record.uri?eid=2-s2.0-14544271421&doi=10.1197%2fjamia.M1683&partnerID=40&md5=5babefe1d5f833d8257b99305fe89e9a","To determine the rate and type of errors in biomedical informatics journal article references. References in articles from the first 2004 issues of five biomedical informatics journals, Journal of the American Medical Informatics Association, Journal of Biomedical Informatics, International Journal of Medical Informatics, Methods of Information in Medicine, and Artificial Intelligence in Medicine were compared with MEDLINE for journal, authors, title, year, volume, and page number accuracy. If discrepancies were identified, the reference was compared with the original publication. Two reviewers independently evaluated each reference. The five journal issues contained 37 articles. Among the 656 eligible references, 225 (34.3%) included at least one error. Among the 225 references, 311 errors were identified. One or more errors were found in the bibliography of 31 (84%) of the 37 articles. The reference error rates by journal ranged from 22.1% to 40.7%. Most errors (39.0%) occurred in the author element, followed by the journal (31.2%), title (17.7%), page (7.4%), year (3.5%), and volume (1.3%) information. The study identified a considerable error rate in the references of five biomedical informatics journals. Authors are responsible for the accuracy of references and should more carefully check them, possibly using informatics-based assistance.",,"bioinformatics; biomedicine; medical literature; MEDLINE; review"
"Arora A., Upadhyaya S., Jain R.","Integrated approach of reduct and clustering for mining patterns from clusters","10.3923/itj.2009.173.180","https://www.scopus.com/inward/record.uri?eid=2-s2.0-62749109500&doi=10.3923%2fitj.2009.173.180&partnerID=40&md5=d22402721c29083df7166094a9bc574d","In this study, a method is presented for selection and ranking of significant attributes for individual clusters which lead to formulation of concise and user understandable patterns. Cluster is set of similar data objects and similarity is measured on attribute values. Attributes which have same value for majority of objects in a cluster are considered significant and rest non significant for that cluster. Reduct from rough set theory is defined as the set of attributes which distinguishes the objects in a homogenous cluster, therefore these can be clear cut removed from the same. Non reduct attributes are ranked for their contribution in the cluster. Pattern is then formed by conjunction of most contributing attributes of that cluster. © 2009 Asian Network for Scientific Information.","Cluster description; Clustering; Data mining; Indiscernibility; Pattern; Reduct; Rough set theory","Fuzzy sets; Information management; Cluster description; Clustering; Indiscernibility; Pattern; Reduct; Rough set theory"
"Arora I., Tollefsbol T.O.","Computational methods and next-generation sequencing approaches to analyze epigenetics data: Profiling of methods and applications","10.1016/j.ymeth.2020.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091534926&doi=10.1016%2fj.ymeth.2020.09.008&partnerID=40&md5=a88f5ebaf49de5065b912188f3ca782b","Epigenetics is mainly comprised of features that regulate genomic interactions thereby playing a crucial role in a vast array of biological processes. Epigenetic mechanisms such as DNA methylation and histone modifications influence gene expression by modulating the packaging of DNA in the nucleus. A plethora of studies have emphasized the importance of analyzing epigenetics data through genome-wide studies and high-throughput approaches, thereby providing key insights towards epigenetics-based diseases such as cancer. Recent advancements have been made towards translating epigenetics research into a high throughput approach such as genome-scale profiling. Amongst all, bioinformatics plays a pivotal role in achieving epigenetics-related computational studies. Despite significant advancements towards epigenomic profiling, it is challenging to understand how various epigenetic modifications such as chromatin modifications and DNA methylation regulate gene expression. Next-generation sequencing (NGS) provides accurate and parallel sequencing thereby allowing researchers to comprehend epigenomic profiling. In this review, we summarize different computational methods such as machine learning and other bioinformatics tools, publicly available databases and resources to identify key modifications associated with epigenetic machinery. Additionally, the review also focuses on understanding recent methodologies related to epigenome profiling using NGS methods ranging from library preparation, different sequencing platforms and analytical techniques to evaluate various epigenetic modifications such as DNA methylation and histone modifications. We also provide detailed information on bioinformatics tools and computational strategies responsible for analyzing large scale data in epigenetics. © 2020 Elsevier Inc.","Computational epigenetics; DNA methylation; Epigenetics; Epigenome; Histone modifications; Machine learning; Next-generation sequencing; Transcriptional regulation","restriction endonuclease; bioinformatics; computer model; DNA methylation; epigenetic modification; epigenetics; epigenome; gene expression profiling; gene interaction; high throughput sequencing; histone modification; human; immunoprecipitation; machine learning; nonhuman; Review; transcription regulation; animal; data analysis; epigenetics; genetic epigenesis; histone code; machine learning; mouse; procedures; Animals; Data Analysis; DNA Methylation; Epigenesis, Genetic; Epigenomics; High-Throughput Nucleotide Sequencing; Histone Code; Humans; Machine Learning; Mice"
"Arora M., Khan R.M.K.","Empirical or concept based mineral exploration to iron ore: A diligent choice for explorer",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955367946&partnerID=40&md5=37600b0bb8de6ac651fda74e038eaf6c","There was the time when empirical or observation based exploration strategies were quite in use, and were used to be successful as well. Those were the days when exposed iron ore body and deposits used to be explored, developed and exploited eventually. But days are over, as almost all such exposed iron ore bodies have already contributed towards the glorified evaluation of human societies for their socio-economic development and global industrialization. Mining industries being one of the oldest trade, may be over 5000 years, have left very little for generation to come, which may be explored and developed by empirical manner, as exposed iron ore bodies are either exhausted or being exploited at the fullest swing. Thus what is left for the time to come is nothing but the blind or unexposed mineral wealth. This is the juncture, where exploration companies have to carry rather serious and responsible duty to explore and develop blind deposits for future exploitation and continuation to uphold the economic growth rate. These responsibilities may not be fulfilled unless explorers choose to adopt concept based exploration strategies with the help of diligent utilization of modern and artificial intelligence added technologies. Further, the choice of such exploration methods would become Hobson s choice, especially under the added social and legal complications and challenges being faced by modern societies. Again, thin margins and tough global competitions would hardly extend any scope for freedom from both time and finance. Therefore, modern day explorers have actually no choice but to adopt concept based exploration designs with wise applications of technologies, which may assure swift, economical and without physically being on ground, till investors confidence level is enhanced significantly for detailed and conventional ways of exploration. In their presentation, authors explain, emphasis and support the doctrine of concept based exploration and high tech means of exploration, be it satellite driven gravity and topography survey, or computer added 3D IP imaging along with wide spectrum of geological, geophysical and geochemical exploration methods. Authors believe that these are the only way to identify, explore, develop and exploit natural mineral resources to cater immediate and ever-growing need for the industries.",,"Concept-based; Confidence levels; Economic growth rate; Exploration company; Exploration methods; Exploration strategies; Geochemical exploration; Global competition; High tech; Human society; Mining industry; Natural minerals; Socio-economic development; Wide spectrum; Artificial intelligence; Deposits; Economics; Industry; Iron ores; Mineral exploration; Mineral resources; Minerals; Silicate minerals; Iron deposits"
"Arora T., Soni R.","A review of techniques to detect the GAN-generated fake images","10.1016/B978-0-12-823519-5.00004-X","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117183294&doi=10.1016%2fB978-0-12-823519-5.00004-X&partnerID=40&md5=665e1ea4767ea9b7f24b70aee6d68189","Recent advancements in the domain of artificial intelligence and computer vision has given the computing devices the capabilities to mimic the human traits. These days, artificial intelligence has given capabilities to machines to book flight tickets by just talking over the phone, do medical diagnosis, Siri and Alexa can act like our virtual assistants and many things can be done by machines by just giving a set of instructions. Although the innovations carried out in this domain have been quite beneficial, these advancements have also created ways and means by which mischiefs can also be done. The technology has advanced to such an extent that we can even create fake images that are almost similar to the real ones, the alteration is done in such a way that the fake images go unnoticed by the human eye, for example, forgery. One of the most famous and reliable approach based on artificial intelligence to generate fake images is generative adversarial networks (GANs). It has the capability to change and manipulate the pixels in a novel way to create images that never existed. These techniques have popularized over the years and are generally used to create fake images. The GAN techniques are backed by two neural networks: one of them is the generator network and the other one is the discriminator network. The generator network is responsible for creating the images and the discriminator network takes care of the training images, thus it tries to find out which images are the real images and which images have been created using the generator network. Both the generator and the discriminator networks play a minimax game, in which one tries to increase the probability of being correct whereas the other tries to minimize the probability of being right. In recent days, the fake images thus created have flooded the internet with misinformation and have also created privacy concerns. These images are generally uploaded over the social media to promote the fake news and to increase its trustworthiness. Many frauds can also be carried out by using this unlawful and unethical means. Therefore, many researchers have contributed in recent years to develop methods that can detect the authenticity of the images and can easily distinguish between the authentic and the fake images, thus created with the help of technology. The contributions made in this context will be quite helpful to curb this menace of falsehood and hence enable us to protect the privacy of the individual. In this chapter, an effort is being made to compare the various state-of-the-art techniques that have been proposed so far to detect the fake images generated by the GAN technology and to find out the research gaps that can be further improved upon. © 2021 Elsevier Inc. All rights reserved.","Artificial intelligence; Deep fake; Detection; Fake images; GAN; Review",
"Arotaritei D., Rotariu C.","Automatic prediction of paroxysmal atrial fibrillation in patients with heart arrhythmia","10.1109/ICEPE.2014.6969969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919663464&doi=10.1109%2fICEPE.2014.6969969&partnerID=40&md5=762b99c8746123c65b8ef1828c5d7274","A new predictor that takes into accounts the randomness of RR interval before PAFib is proposed. Using data mining techniques, temporal patterns are identified based their presence in ECG that precedes paroxysmal atrial fibrillation (PAF) and not present in patients with normal ECG. The algorithm used the supposition that the premature atrial complexes (PAC) are responsible for most of PAF. Other statistical parameters that are related to randomness of signal are used to improve the accuracy of proposed algorithm. © 2014 IEEE.","atrial fibrillation; heart arrhythmia; parosymal atrial fibrillation; pattern mining; premature atrial complexes; RR-tachogram; Teager-Kaiser operator","Cardiology; Diseases; Electrocardiography; Random processes; Atrial fibrillation; Heart arrhythmias; Pattern mining; Premature Atrial Complexes; RR-tachogram; Teager-Kaiser operator; Data mining"
"Arrais S., Urquiza-Aguiar L., Tripp-Barba C.","Analysis of Information Availability for Seismic and Volcanic Monitoring Systems: A Review","10.3390/s22145186","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135111590&doi=10.3390%2fs22145186&partnerID=40&md5=3495d6707aab3c71a429c4eb649648b6","Organizations responsible for seismic and volcanic monitoring worldwide mainly gather information from instrumental networks composed of specialized sensors, data-loggers, and transmission equipment. This information must be available in seismological data centers to improve early warning diffusion. Furthermore, this information is necessary for research purposes to improve the understanding of the phenomena. However, the acquisition data systems could have some information gaps due to unstable connections with instrumental networks and repeater nodes or exceeded waiting times in data acquisition processes. In this work, we performed a systematic review around information availability issues and solutions in data acquisition systems, instrumental networks, and their interplay with transmission media for seismic and volcanic monitoring. Based on the SLR methodology proposed by Kitchenham, B., a search string strategy was considered where 1938 articles were found until December 2021. Subsequently, through selection processes, 282 articles were obtained and 51 relevant articles were extracted using filters based on the content of articles mainly referring to seismic–volcanic data acquisition, data formats, monitoring networks, and early warnings. As a result, we identified two independent partial solutions that could complement each other. One focused on extracting information in the acquisition systems corresponding to continuous data generated by the monitoring points through the development of mechanisms for identifying sequential files. The other solution focused on the detection and assessment of the alternative transmission media capabilities available in the seismic–volcanic monitoring network. Moreover, we point out the advantage of a unified solution by identifying data files/plots corresponding to information gaps. These could be recovered through alternate/backup transmission channels to the monitoring points to improve the availability of the information that contributes to real-time access to information from seismic–volcanic monitoring networks, which speeds up data recovery processes. © 2022 by the authors.","algorithms; availability; information security; seismic–volcanic monitoring; seismic–volcanic networks; seismological data centers","Data mining; Monitoring; Network security; Seismology; Transmissions; Volcanoes; Datacenter; Information availability; Monitoring network; Seismic monitoring; Seismic–volcanic monitoring; Seismic–volcanic network; Seismological data; Seismological data center; Volcanic monitoring; Volcanics; Data acquisition"
"Arras L., Osman A., Samek W.","CLEVR-XAI: A benchmark dataset for the ground truth evaluation of neural network explanations","10.1016/j.inffus.2021.11.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120632168&doi=10.1016%2fj.inffus.2021.11.008&partnerID=40&md5=8910eebd973ff586ddb94a12d8940ad6","The rise of deep learning in today's applications entailed an increasing need in explaining the model's decisions beyond prediction performances in order to foster trust and accountability. Recently, the field of explainable AI (XAI) has developed methods that provide such explanations for already trained neural networks. In computer vision tasks such explanations, termed heatmaps, visualize the contributions of individual pixels to the prediction. So far XAI methods along with their heatmaps were mainly validated qualitatively via human-based assessment, or evaluated through auxiliary proxy tasks such as pixel perturbation, weak object localization or randomization tests. Due to the lack of an objective and commonly accepted quality measure for heatmaps, it was debatable which XAI method performs best and whether explanations can be trusted at all. In the present work, we tackle the problem by proposing a ground truth based evaluation framework for XAI methods based on the CLEVR visual question answering task. Our framework provides a (1) selective, (2) controlled and (3) realistic testbed for the evaluation of neural network explanations. We compare ten different explanation methods, resulting in new insights about the quality and properties of XAI methods, sometimes contradicting with conclusions from previous comparative studies. The CLEVR-XAI dataset and the benchmarking code can be found at https://github.com/ahmedmagdiosman/clevr-xai. © 2021 The Authors","Benchmark; Computer vision; Convolutional neural network; Evaluation; Explainable AI; Relation network; Visual question answering","Benchmarking; Convolutional neural networks; Deep learning; Pixels; Benchmark; Convolutional neural network; Evaluation; Explainable AI; Ground truth; Heatmaps; Neural-networks; Question Answering; Relation network; Visual question answering; Computer vision"
"Arras L., Arjona-Medina J., Widrich M., Montavon G., Gillhofer M., Müller K.-R., Hochreiter S., Samek W.","Explaining and Interpreting LSTMs","10.1007/978-3-030-28954-6_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072819669&doi=10.1007%2f978-3-030-28954-6_11&partnerID=40&md5=8de227ef43f2120c3bcd83b3e5c4cf74","While neural networks have acted as a strong unifying force in the design of modern AI systems, the neural network architectures themselves remain highly heterogeneous due to the variety of tasks to be solved. In this chapter, we explore how to adapt the Layer-wise Relevance Propagation (LRP) technique used for explaining the predictions of feed-forward networks to the LSTM architecture used for sequential data modeling and forecasting. The special accumulators and gated interactions present in the LSTM require both a new propagation scheme and an extension of the underlying theoretical framework to deliver faithful explanations. © Springer Nature Switzerland AG 2019.","Explainable artificial intelligence; Interpretability; LSTM; Model transparency; Recurrent neural networks","Backpropagation; Network architecture; Recurrent neural networks; Feed-forward network; Interpretability; Layer-wise; LSTM; Model transparency; Modeling and forecasting; Sequential data; Theoretical framework; Long short-term memory"
"Arras L., Horn F., Montavon G., Müller K.-R., Samek W.","""What is relevant in a text document?"": An interpretable machine learning approach","10.1371/journal.pone.0181142","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027142265&doi=10.1371%2fjournal.pone.0181142&partnerID=40&md5=19fa867fc5ae52ec2f41281f56cfdcab","Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text’s category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications. © 2017 Arras et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"classification; classifier; decomposition; extraction; human; model; nervous system; prediction; artificial neural network; documentation; linguistics; machine learning; principal component analysis; support vector machine; Documentation; Machine Learning; Neural Networks (Computer); Principal Component Analysis; Support Vector Machine; Vocabulary"
"Arrigoni M., Madsen G.K.H.","Evolutionary computing and machine learning for discovering of low-energy defect configurations","10.1038/s41524-021-00537-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106311171&doi=10.1038%2fs41524-021-00537-1&partnerID=40&md5=398232b3ffb7eab6626a2e679b4eda7e","Density functional theory (DFT) has become a standard tool for the study of point defects in materials. However, finding the most stable defective structures remains a very challenging task as it involves the solution of a multimodal optimization problem with a high-dimensional objective function. Hitherto, the approaches most commonly used to tackle this problem have been mostly empirical, heuristic, and/or based on domain knowledge. In this contribution, we describe an approach for exploring the potential energy surface (PES) based on the covariance matrix adaptation evolution strategy (CMA-ES) and supervised and unsupervised machine learning models. The resulting algorithm depends only on a limited set of physically interpretable hyperparameters and the approach offers a systematic way for finding low-energy configurations of isolated point defects in solids. We demonstrate its applicability on different systems and show its ability to find known low-energy structures and discover additional ones as well. © 2021, The Author(s).",,"Computation theory; Covariance matrix; Density functional theory; Optimization; Point defects; Potential energy; Quantum chemistry; Covariance matrix adaptation evolution strategies; Defective structures; Evolutionary computing; Low energy configurations; Low energy structures; Multimodal optimization problems; Objective functions; Unsupervised machine learning; Machine learning"
"Arrotta L., Civitarese G., Bettini C.","Dexar: Deep explainable sensor-based activity recognition in smart-home environments","10.1145/3517224","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127886586&doi=10.1145%2f3517224&partnerID=40&md5=032397a9a2bd7b5cedaafd8c8fdb23c3","The sensor-based recognition of Activities of Daily Living (ADLs) in smart-home environments is an active research area, with relevant applications in healthcare and ambient assisted living. The application of Explainable Artificial Intelligence (XAI) to ADLs recognition has the potential of making this process trusted, transparent and understandable. The few works that investigated this problem considered only interpretable machine learning models. In this work, we propose DeXAR, a novel methodology to transform sensor data into semantic images to take advantage of XAI methods based on Convolutional Neural Networks (CNN). We apply different XAI approaches for deep learning and, from the resulting heat maps, we generate explanations in natural language. In order to identify the most effective XAI method, we performed extensive experiments on two different datasets, with both a common-knowledge and a user-based evaluation. The results of a user study show that the white-box XAI method based on prototypes is the most effective. © 2022 ACM.","activity recognition; deep learning; explainable artificial intelligence; smart-home","Convolutional neural networks; Deep learning; Intelligent buildings; Pattern recognition; Semantics; Activities of Daily Living; Activity recognition; Ambient assisted living; Deep learning; Explainable artificial intelligence; Home environment; Machine learning models; Novel methodology; Research areas; Smart homes; Automation"
"Arrotta L.","Multi-inhabitant and explainable Activity Recognition in Smart Homes","10.1109/MDM52706.2021.00054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112363997&doi=10.1109%2fMDM52706.2021.00054&partnerID=40&md5=5b5bc3f50d20a4a6e4cd2f83f1252be8","The sensor-based detection of Activities of Daily Living (ADLs) in smart home environments can be exploited to provide healthcare applications, like remotely monitoring fragile subjects living in their habitations. However, ADLs recognition methods have been mainly investigated with a focus on singleinhabitant scenarios. The major problem in multi-inhabitant settings is data association: Assigning to each resident the environmental sensors' events that he/she triggered. Furthermore, Deep Learning (DL) solutions have been recently explored for ADLs recognition, with promising results. Nevertheless, the main drawbacks of these methods are their need for large amounts of training data, and their lack of interpretability. This paper summarizes some contributions of my Ph.D. research, in which we are designing explainable multi-inhabitant approaches for ADLs recognition. We have already investigated a hybrid knowledge-and data-driven solution that exploits the high-level context of each resident to perform data association. Currently, we are studying semi-supervised techniques to mitigate the data scarcity issue, and explainable Artificial Intelligence (XAI) methods to make DL classifiers for ADLs more transparent. © 2021 IEEE.",,"Ambient intelligence; Automation; Deep learning; Information management; Intelligent buildings; Activities of daily living (ADLs); Activity recognition; Data association; Environmental sensor; Health care application; Hybrid knowledge; Interpretability; Recognition methods; Remote patient monitoring"
"Arroyo J., Corea F., Jimenez-Diaz G., Recio-Garcia J.A.","Assessment of machine learning performance for decision support in venture capital investments","10.1109/ACCESS.2019.2938659","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077993530&doi=10.1109%2fACCESS.2019.2938659&partnerID=40&md5=c4e467edfa5b6ef0d1bd990ca51e300d","The venture capital (VC) industry offers opportunities for investment in early-stage companies where uncertainty is very high. Unfortunately, the tools investors currently have available are not robust enough to reduce risk and help them managing uncertainty better. Machine learning data-driven approaches can bridge this gap, as they already do in the hedge fund industry. These approaches are now possible because data from thousands of companies over the world is available through platforms such as Crunchbase. Previous academic efforts have focused only on predicting two classes of exits, i.e., being acquired by other company or offering shares to the public, using only one or a few subsets of explanatory variables. These events are typically related to high returns, but also higher risk, making hard for a venture fund to get repeatable and sustainable returns. On the contrary, we will try to predict more possible outcomes including a subsequent funding round or the closure of the company using a large set of signals. In this way, our approach would provide VC investors with more information to set up a portfolio with lower risk that may eventually achieve higher returns than those based on finding unicorns (i.e., companies with a valuation higher than one billion dollars). We will analyze the performance of several machine learning methods in a dataset of over 120,000 early-stage companies in a realistic setting that tries to predict their progress in a 3-year time window. Results show that machine learning can support venture investors in their decision-making processes to find opportunities and better assessing the risk of potential investments. © 2013 IEEE.","Crunchbase; decision support systems; explainable artificial intelligence; investment; machine learning; risk assessment; venture capital","Decision making; Decision support systems; Financial markets; Forecasting; Learning systems; Machine learning; Plant shutdowns; Risk assessment; Crunchbase; Decision making process; Decision supports; Explanatory variables; Machine learning methods; Managing uncertainty; Venture Capital; Venture capital investments; Investments"
"Arsene D., Predescu A., Truica C.-O., Apostol E.-S., Mocanu M., Chiru C.","Clustering Consumption Activities in a Water Monitoring System","10.1109/AQTR55203.2022.9801952","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134019006&doi=10.1109%2fAQTR55203.2022.9801952&partnerID=40&md5=c55eb0a23695549a9aff8ebe466806d7","Monitoring water consumption in a household is essential to understanding where and how much is consumed. To improve the management of water resources, the consumer demand must be observed, the inadequate consumption must be limited, and the leaks must be reduced. This study compares the most common water consuming activities using the data collected from various households. Consumption events were extracted for clustering consumption activities using the K-Means algorithm. The results show that the toilet accounts for the highest water consumption, while having a lower variability in terms of duration and volume when compared to the sink where the most variable behavior is observed. The clusters obtained are analyzed in terms of distribution using consumption and duration as variables. Some clusters are characterized by lower consumption and duration, while others are more widespread, and showing higher consumption and duration. Therefore, water consumption can be reduced, while the consumers should be encouraged to monitor water demand towards achieving a more responsible behavior. © 2022 IEEE.","Decision Support System; K-Means Clustering; Water Distribution System","Artificial intelligence; Consumer behavior; K-means clustering; Water distribution systems; Water management; Clusterings; Consumer demands; High water; K-mean algorithms; K-means++ clustering; Low consumption; Water consumption; Water demand; Water monitoring systems; Waters resources; Decision support systems"
"Arsene D.-A., Pahontu B.-I., Vladuta A.V.","Modeling Economic Influence on Technical Decisions in Urban, IoT-based Water Management Systems","10.1109/ECAI46879.2019.9042109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084563145&doi=10.1109%2fECAI46879.2019.9042109&partnerID=40&md5=6f1f235dde2847ab3c8b9ead91334643","Water is one of the most important resources for survival. Nowadays, there is a continuous preoccupation for optimizing the use of water resources. The paper presents the model of a decision support system for the optimization of water consumption, which starts from the definition of different consumer profiles. In the water resource management system, several stakeholders are involved: the supplier, the state authority and the consumer, which are linked by well-defined contractual relationships. Depending on the consumer profile, the supplier may establish different types of contracts that favor responsible water use. The system proposes consumer profile oriented scenarios that reflect the influence of consumer habits on water consumption. © 2019 IEEE.","Economic Modeling; Management; Water Systems; Wireless Sensors","Artificial intelligence; Internet of things; Water management; Water supply; Contractual relationships; Technical decision; Water consumption; Water management systems; Water use; Waterresource management; Decision support systems"
"Arshadipour A., Thorand B., Linkohr B., Rospleszcz S., Ladwig K.-H., Heier M., Peters A.","Impact of prenatal and childhood adversity effects around World War II on multimorbidity: results from the KORA-Age study","10.1186/s12877-022-02793-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124577580&doi=10.1186%2fs12877-022-02793-2&partnerID=40&md5=a2e75fd3b9794a92b932cdb9c0d1a1bd","Background: While risk factors for age-related diseases may increase multimorbidity (MM), early life deprivation may also accelerate the development of chronic diseases and MM. Methods: This study explores the prevalence and pattern of MM in 65–71 year-old individuals born before, during, and after World War II in Southern Germany based on two large cross-sectional KORA (Cooperative Health Research in the Region of Augsburg) -Age studies in 2008/9 and 2016. MM was defined as having at least two chronic diseases, and birth periods were classified into five phases: pre-war, early war, late war, famine, and after the famine period. Logistic regression models were used to analyze the effect of the birth phases on MM with adjustment for sociodemographic and lifestyle risk factors. Furthermore, we used agglomerative hierarchical clustering to investigate the co-occurrence of diseases. Results: Participants born during the late war phase had the highest prevalence of MM (62.2%) and single chronic diseases compared to participants born during the other phases. Being born in the late war phase was significantly associated with a higher odds of MM (OR = 1.83, 95% CI: 1.15–2.91) after adjustment for sociodemographic and lifestyle factors. In women, the prevalence of joint, gastrointestinal, eye diseases, and anxiety was higher, while heart disease, stroke, and diabetes were more common in men. Moreover, three main chronic disease clusters responsible for the observed associations were identified as: joint and psychosomatic, cardiometabolic and, other internal organ diseases. Conclusions: Our findings imply that adverse early-life exposure may increase the risk of MM in adults aged 65–71 years. Moreover, identified disease clusters are not coincidental and require more investigation. © 2022, The Author(s).","Agglomerative hierarchical clustering; Chronic disease; Geriatrics; Logistic regression; Machine learning; Multimorbidity","aged; chronic disease; cross-sectional study; female; human; male; multiple chronic conditions; pregnancy; prevalence; risk factor; war; Adverse Childhood Experiences; Aged; Chronic Disease; Cross-Sectional Studies; Female; Humans; Male; Multimorbidity; Pregnancy; Prevalence; Risk Factors; World War II"
"Arslan Y., Lebichot B., Allix K., Veiber L., Lefebvre C., Boytsov A., Goujon A., Bissyandé T.F., Klein J.","Towards Refined Classifications Driven by SHAP Explanations","10.1007/978-3-031-14463-9_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136984853&doi=10.1007%2f978-3-031-14463-9_5&partnerID=40&md5=c069be68daf3c1bd98ae16d98e791a75","Machine Learning (ML) models are inherently approximate; as a result, the predictions of an ML model can be wrong. In applications where errors can jeopardize a company’s reputation, human experts often have to manually check the alarms raised by the ML models by hand, as wrong or delayed decisions can have a significant business impact. These experts often use interpretable ML tools for the verification of predictions. However, post-prediction verification is also costly. In this paper, we hypothesize that the outputs of interpretable ML tools, such as SHAP explanations, can be exploited by machine learning techniques to improve classifier performance. By doing so, the cost of the post-prediction analysis can be reduced. To confirm our intuition, we conduct several experiments where we use SHAP explanations directly as new features. In particular, by considering nine datasets, we first compare the performance of these “SHAP features” against traditional “base features” on binary classification tasks. Then, we add a second-step classifier relying on SHAP features, with the goal of reducing false-positive and false-negative results of typical classifiers. We show that SHAP explanations used as SHAP features can help to improve classification performance, especially for false-negative reduction. © 2022, IFIP International Federation for Information Processing.","Interpretable machine learning; Second-step classification; SHAP Explanations","Forecasting; Machine learning; Business impact; Classifier performance; Human expert; Interpretable machine learning; Learning tool; Machine learning models; Machine learning techniques; Machine-learning; Second-step classification; SHAP explanation; Classification (of information)"
"Arteaga C., Paz A., Park J.","Injury severity on traffic crashes: A text mining with an interpretable machine-learning approach","10.1016/j.ssci.2020.104988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091095829&doi=10.1016%2fj.ssci.2020.104988&partnerID=40&md5=18fbc73cb24b5dec1d8a8240d898e3bb","The analysis of traffic crash severities provides significant information for the development of safety countermeasures. Most available traffic crash datasets contain rich information including linguistic narratives with details about crash events and contexts, which can reveal new insights regarding severity and associated causality factors. Previous research has paid insufficient attention to this source of information. This study proposes an approach to analyze traffic crash narratives to identify factors associated with high injury-severity levels. The proposed approach explicitly seeks global interpretability of the results by expanding the capabilities of the Local Interpretable Model-Agnostic Explanations (LIME) method. Our proposed new approach, Global Cross-Validation LIME (GCV-LIME), aggregates individual LIME explanations using cross-validation. Thus, this study combines machine learning-based text mining with GCV-LIME to identify likely causality factors for injury severities while providing interpretability as required by traffic safety analysts. Data for heavy vehicle crashes collected from 2007 to 2017 in Queensland, Australia, were used to evaluate the proposed approach. Six different machine-learning models were tested, and global explanations were generated using GCV-LIME. The results indicated a strong association among a set of terms, such as “collided_headon,” “side_collided,” “motorcycle,” “cab,” and “pedestrian” with fatal crashes. Results from GCV-LIME were compared with those obtained using the corresponding available tabular data and classic regression analysis. The comparison suggest that the proposed approach has great potential to provide additional insights as well as enables to confirm results obtained with classic analysis on tabular data. Results from GCV-LIME combined with knowledge and experience from safety analysts can help establish effective safety countermeasures based on factors likely causing crashes and/or increasing their severity. © 2020 Elsevier Ltd","Crash severity; Interpretable machine learning; Machine learning; Text mining","Accidents; Lime; Regression analysis; Crash severity; Cross validation; Injury severity; Interpretability; Interpretable machine learning; Machine-learning; Safety countermeasures; Text-mining; Traffic crashes; Machine learning; Article; Australia; data mining; fatality; global cross validation local interpretable model agnostic explanations; human; injury severity; intermethod comparison; machine learning; motor vehicle; pedestrian; priority journal; regression analysis; risk factor; traffic accident; traffic safety"
"Arthur J.W., Sanchez-Perez A., Cook D.I.","Scoring of predicted GRK2 phosphorylation sites in Nedd4-2","10.1093/bioinformatics/btl381","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748705340&doi=10.1093%2fbioinformatics%2fbtl381&partnerID=40&md5=fcd1e9d95c007643fd40f95b00bddd08","Motivation: Epithelial Na+ channels (ENaC) mediate the transport of sodium (Na) across epithelia in the kidney, gut and lungs and are required for blood pressure regulation. They are inhibited by ubiquitin protein ligases, such as Nedd4-2. These ligases bind to proline-rich motifs (PY motifs) present in the C-termini of ENaC subunits. Loss of this inhibition leads to hypertension. We have previously reported that ENaC channels are maintained in the active state by the G protein coupled receptor kinase, GRK2. The enzyme has been implicated in the development of essential hypertension [R. D. Feldman (2002) Mol. Pharmacol., 61, 707-709]. Additional findings in our lab pointed towards a possible role for GRK2 in the phosphorylation and inactivation of Nedd4-2. Results: We have predicted GRK2 phosphorylation sites on Nedd4-2 by combining sequence analysis, homology modeling and surface accessibility calculations. A total of 24 potential phosphorylation sites were predicted by sequence analysis. Of these, 16 could be modeled using homology modeling and 6 of these were found to have sufficient surface exposure to be accessible to the GRK2 enzyme responsible for the phosphorylation of Nedd4-2. The method provides an ordered list of the most probable GRK2 phosphorylation sites on Nedd4-2 providing invaluable guidance to future experimental studies aimed at mutating certain Nedd4-2 residues in order to prevent phosphorylation by GRK2. The method developed could be applied in a wide variety of biological applications involving the binding of one molecule to a protein. The relative effectiveness of the technique is determined mainly by the quality of the homology model built for the protein of interest. © 2006 Oxford University Press.",,"G protein coupled receptor kinase 2; ubiquitin protein ligase; ubiquitin protein ligase NEDD2; ubiquitin protein ligase NEDD4; unclassified drug; article; artificial neural network; controlled study; enzyme activity; enzyme assay; enzyme inactivation; enzyme localization; enzyme phosphorylation; molecular model; practice guideline; prediction; priority journal; protein binding; scoring system; sequence analysis; sequence homology; Algorithms; Amino Acid Sequence; Artificial Intelligence; beta-Adrenergic Receptor Kinase; Binding Sites; Computer Simulation; Models, Chemical; Models, Molecular; Molecular Sequence Data; Phosphorylation; Protein Binding; Protein Interaction Mapping; Sequence Alignment; Sequence Analysis, Protein; Sequence Homology, Amino Acid; Ubiquitin-Protein Ligases"
"Artini M., Papa R., Sapienza F., Božović M., Vrenna G., Assanti V.T.G., Sabatino M., Garzoli S., Fiscarelli E.V., Ragno R., Selan L.","Essential Oils Biofilm Modulation Activity and Machine Learning Analysis on Pseudomonas aeruginosa Isolates from Cystic Fibrosis Patients","10.3390/microorganisms10050887","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128846958&doi=10.3390%2fmicroorganisms10050887&partnerID=40&md5=09b12441dce59b9bd7a1a2d8739009f9","The opportunistic pathogen Pseudomonas aeruginosa is often involved in airway infections of cystic fibrosis (CF) patients. It persists in the hostile CF lung environment, inducing chronic infections due to the production of several virulence factors. In this regard, the ability to form a biofilm plays a pivotal role in CF airway colonization by P. aeruginosa. Bacterial virulence mitigation and bacterial cell adhesion hampering and/or biofilm reduced formation could represent a major target for the development of new therapeutic treatments for infection control. Essential oils (EOs) are being considered as a potential alternative in clinical settings for the prevention, treatment, and control of infections sustained by microbial biofilms. EOs are complex mixtures of different classes of organic compounds, usually used for the treatment of upper respiratory tract infections in traditional medicine. Recently, a wide series of EOs were investigated for their ability to modulate biofilm production by different pathogens comprising S. aureus, S. epidermidis, and P. aeruginosa strains. Machine learning (ML) algorithms were applied to develop classification models in order to suggest a possible antibiofilm action for each chemical component of the studied EOs. In the present study, we assessed the biofilm growth modulation exerted by 61 commercial EOs on a selected number of P. aeruginosa strains isolated from CF patients. Furthermore, ML has been used to shed light on the EO chemical components likely responsible for the positive or negative modulation of bacterial biofilm formation. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","biofilm modulation; cystic fibrosis; essential oil; machine learning; Pseudomonas aeruginosa",
"Arul M., Kareem A.","Applications of shapelet transform to time series classification of earthquake, wind and wave data","10.1016/j.engstruct.2020.111564","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097351306&doi=10.1016%2fj.engstruct.2020.111564&partnerID=40&md5=c05dd83c241b6755d7780c7a1f395717","Autonomous detection of desired events from large databases using time series classification is becoming increasingly important in civil engineering as a result of continued long-term health monitoring of a large number of engineering structures encompassing buildings, bridges, towers, and offshore platforms. In this context, this paper proposes the application of a relatively new time series representation named “Shapelet transform”, which is based on local similarity in the shape of the time series subsequences. In consideration of the individual attributes distinctive to time series signals in earthquake, wind and ocean engineering, the application of this transform yields a new shape-based feature representation. Combining this shape-based representation with a standard machine learning algorithm, a truly “white-box” machine learning model is proposed with understandable features and a transparent algorithm. This model automates event detection without the intervention of domain practitioners, yielding a practical event detection procedure. The efficacy of this proposed shapelet transform-based autonomous detection procedure is demonstrated by examples, to identify known and unknown earthquake events from continuously recorded ground-motion measurements, to detect pulses in the velocity time history of ground motions to distinguish between near-field and far-field ground motions, to identify thunderstorms from continuous wind speed measurements, to detect large-amplitude wind-induced vibrations from the bridge monitoring data, and to identify plunging breaking waves that have a significant impact on offshore structures. © 2020 Elsevier Ltd","Breaking wave detection; Earthquake detection; Machine learning; Shapelet transform; Thunderstorm classification; Time series classification; Time series shapelets","Bridges; Classification (of information); Earthquakes; Machine learning; Ocean engineering; Offshore oil well production; Offshore structures; Seismic prospecting; Structural health monitoring; Time series; Time series analysis; Wind; Engineering structures; Far-field ground motion; Machine learning models; Plunging breaking waves; Time series classifications; Time series subsequences; Wind induced vibrations; Wind speed measurement; Learning algorithms; algorithm; breaking wave; bridge; classification; database; earthquake engineering; ground motion; machine learning; thunderstorm; time series analysis; vibration; wind stress"
"Arun P.V., Karnieli A.","Learning of physically significant features from earth observation data: an illustration for crop classification and irrigation scheme detection","10.1007/s00521-022-07019-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126222781&doi=10.1007%2fs00521-022-07019-5&partnerID=40&md5=7082dfb11012a0b9b44df96501e4be30","Earth observation data processing requires interpretable deep learning (DL) models that learn physically significant and meaningful features. The current study proposes approaches to make the network to learn meaningful features. In addition, a set of interpretability- and explanation-based evaluation strategies are proposed to evaluate the DL models. Adversarial variational encoding along with constraints to regulate latent representations and embed label information are employed to learn interpretable manifold. The proposed architecture, called interpretable adversarial encoding network (IAENet), significantly improves the results compared to other main existing DL models. The proposed IAENet learns the features which are essential in distinguishing the different classes thereby improving the interpretability of the model. The explanations for the different models are generated through analysis of the concepts learned by each model using activation maximization. Besides, the relevance assigned by the model to input features is also estimated using the layer-wise relevance propagation approach. Experiments on the phenological curve-based crop classification illustrate that IAENet learn relevant features (giving importance to the non-rainy season) to distinguish different irrigation schemes. The performance can be attributed to the learned interpretable manifold, and the refinement of architectural units and convolutions considering the point-nature and irregular sampling of the input data. Experiments on learning crop-specific features from multispectral images for crop-type classification indicate that IAENet learns red and green edge features crucial in distinguishing the studied crops. The improvement in interpretability of the DL models is found to reduce the sensitivity toward network parameters. The proposed evaluation measures facilitate ascertaining the physical significance of the learned manifold. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Classification; Crop-specific features; Deep learning; Interpretability; Phenological curves; VENµS","Activation analysis; Classification (of information); Data handling; Deep learning; Encoding (symbols); Irrigation; Signal encoding; Crop classification; Crop-specific feature; Deep learning; Earth observation data; Interpretability; Irrigation schemes; Learn+; Learning models; Phenological curve; VENµS; Crops"
"Arun S.V.K., Subramaniam U., Padmanaban S., Bhaskar M.S., Almakhles D.","Investigation for performances comparison PI, adaptive PI, fuzzy speed control induction motor for centrifugal pumping application","10.1109/CPE.2019.8862351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074134941&doi=10.1109%2fCPE.2019.8862351&partnerID=40&md5=001914e818784aeebb983f8cc0e7fab7","Among the total energy consumed by presently installed utilities, pumping systems contributes nearly 30% of them that are controlled through induction motor. The flow rate of centrifugal pump is directly proportional to the speed of induction motor. Induction motor based electric drives are a complex nonlinear system to control due to their varying nature of speed with respect to changes in load. Thus, controllers are required for the speed control and performance enhancement of the induction motors. In this paper, a comparison of various controllers is discussed for the flow rate control of the pumping system. Among all the conventional controllers the proportional integral derivative (PID) is the mostly preferred for induction motor speed control. However, high overshoot, fixed gain constants and sluggish response are the disadvantages of the PID controller. So, automatic gain tuned PI controllers (i.e., adaptive PI controllers) are introduced to overcome the drawbacks of the conventional PID controllers. Further advancements in the field of artificial intelligence lead to the development of fuzzy logic based controllers. These controllers are simple, compatible and can be modelled without detailed mathematical model of the system. Thus, they are gaining more popularity than other controllers. The rules and membership functions (MFs) are responsible for performance of fuzzy logic controllers are designed using trial and error method. The result comparison of all the controllers is presented and the performance of each controller is analyzed. From the observations, fuzzy logic controller provides better performance than the conventional controllers. © 2019 IEEE.","Flow control; Fuzzy; Optimal Controllers - PID; Performance Comparison; Pumping System; Variable Frequency Drives (VFD)","Computer circuits; Controllers; Electric control equipment; Electric drives; Flow control; Fuzzy logic; Induction motors; Membership functions; Power electronics; Proportional control systems; Pumping plants; Pumps; Speed; Speed control; Speed regulators; Three term control systems; Traffic signals; Two term control systems; Variable speed drives; Fuzzy; Optimal controller; Performance comparison; Pumping systems; Variable frequency drives; Electric machine control"
"Arvind C.S., Totla A., Jain T., Sinha N., Jyothi R., Aditya K., Keerthan, Farhan M., Sumukh G., Ak G.","Deep Learning Based Plant Disease Classification with Explainable AI and Mitigation Recommendation","10.1109/SSCI50451.2021.9659869","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125790384&doi=10.1109%2fSSCI50451.2021.9659869&partnerID=40&md5=f5143a3bee6f73a05fbd0ef85c71ddc0","Plants show visible symptoms of getting infected with a disease. Presently an experienced plant pathologist can diagnose the condition through visual inspection of disease-affected plants. However, manual visualization is time-consuming and depends on the plant pathologist's expertise in identifying plant disease. Hence this problem can be solved by a computer-aided diagnostic system with artificial intelligence (CADS-AI). This system will aid in improving and protecting the yield of the plant, but it lacks trust as the existing system is not flawless. Hence, in this research work, a plant disease classification with an explainable AI pipeline is developed which ensures trust in the CADS solution. Furthermore, an expert recommendation system will act as an alternative to expert plant pathologists. Tomato leaf diseases data from the PlantVillage dataset is used in the proposed solution. Transfer learning technique was adopted in training deep neural network models with original and augmented data of 16, 684 and 53, 476 images respectively. The best model for the dataset was efficientNet B5 with best F1 score accuracy of 0.9842 and 0.9930. The predicted output of B5 was interpreted with explainable AI techniques and validated using YOLOv4. Inference of the proposed solution was a client-server interface where end-users can upload infected leaf images via mobile phones or web browsers. This entire system was tested in real-time with 250 volunteers with 4G mobile network or 100 MBPS wifi. The average throughput time of the system is around 4.3 seconds. © 2021 IEEE.","Deeplearning; Deployment; Explainable AI; Plant Disease; Recommendation; Validation","Deep neural networks; Web browsers; Computer aided diagnostics; Condition; Deeplearning; Deployment; Disease classification; Explainable AI; Plant disease; Recommendation; Validation; Visual inspection; Diagnosis"
"Arya H., Coumar M.S.","Target identification and validation","10.1016/B978-0-12-821471-8.00002-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126419332&doi=10.1016%2fB978-0-12-821471-8.00002-7&partnerID=40&md5=04611a5e0f189d693cc45e3d007977d2","Target is a cellular structure that is involved in a molecular-level understanding of a specific disease, which could be an enzyme, receptor, nucleic acid, hormone, ion channel, membrane protein/s, etc. Target identification is the first step of the computer-aided drug design process of the drug development program and it is essential to determine that the selected targets are responsible for the cause of disease. © 2021 Elsevier Inc. All rights reserved.","3D structure; Data mining; Enzyme; In silico study; Network analysis; Nucleic acid",
"Arya N., Mathur A., Saha S., Saha S.","Proposal of SVM Utility Kernel for Breast Cancer Survival Estimation","10.1109/TCBB.2022.3198879","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137594043&doi=10.1109%2fTCBB.2022.3198879&partnerID=40&md5=e2b5ff589ec1f119fcdd0bab096cab9a","The advancement of medical research in the field of cancer prognosis and diagnosis using various modalities has put oncologists under tremendous stress. The complexity and heterogeneity involved in multiple modalities and their significantly varied clinical outcomes make it difficult to analyze the disease and provide the correct treatment. Breast cancer is the major concern among all cancers worldwide, specifically for females. To help oncologists and cancer patients, research for breast cancer survival estimation has been proposed. It ranges from complex deep neural networks to simple and interpretable architectures. We propose a utility kernel for a support vector machine (SVM) in this study. It is a simple yet powerful function, which performs better than other popular machine learning algorithms and deep neural networks in the task of breast cancer survival prediction using the TCGA-BRCA dataset. This study validates the proposed utility kernel using four different modalities (gene expression, copy number variation, clinical, and histopathological tissue images) and their multi-modal combinations. The SVM based on our utility kernel empirically proves its efficacy by achieving the highest value on various performance measures, whereas advanced deep neural networks fail to train on small and highly imbalanced breast cancer data. IEEE","Breast cancer; Breast cancer survival estimation; Cancer; copy number variation; Deep learning; deep neural networks; gene expression; Gene expression; histopathological whole slide images; Kernel; machine learning; Prognostics and health management; support vector machine; Support vector machines; utility kernel","Complex networks; Deep neural networks; Diagnosis; Diseases; Learning algorithms; Medical imaging; Support vector machines; Breast Cancer; Breast cancer survival estimation; Cancer; Copy number variations; Deep learning; Genes expression; Histopathological whole slide image; Kernel; Machine-learning; Prognostic and health management; Support vectors machine; Utility kernel; Whole slide images; Gene expression"
"Aryal S., Nadarajah D., Rupasinghe P.L., Jayawardena C., Kasthurirathna D.","Comparative Analysis of Deep Learning Models for Multi-Step Prediction of Financial Time Series","10.3844/jcssp.2020.1401.1416","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096673953&doi=10.3844%2fjcssp.2020.1401.1416&partnerID=40&md5=fcff937d5bb39c151bf3082a3f4c44c8","Financial time series prediction has been a key topic of interest among researchers considering the complexity of the domain and also due to its significant impact on a wide range of applications. In contrast to one-step ahead prediction, multi-step forecasting is more desirable in the industry but the task is more challenging. In recent days, advancement in deep learning has shown impressive accomplishments across various tasks including sequence learning and time series forecasting. Although most previous studies are focused on applications of deep learning models for single-step ahead prediction, multi-step financial time series forecasting has not been explored exhaustively. This paper aims at extensively evaluating the performance of various state-of-the-art deep learning models for multiple multi-steps ahead prediction horizons on real-world stock and forex markets dataset. Specifically, we focus on Long-Short Term Memory (LSTM) network and its variations, Encoder-Decoder based sequence to sequence models, Temporal Convolution Network (TCN), hybrid Exponential Smoothing- Recurrent Neural Networks (ES-RNN) and Neural Basis Expansion Analysis for interpretable Time Series forecasting (N-BEATS). Experimental results show that the latest deep learning models such as N-BEATS, ES-LSTM and TCN produced better results for all stock market related datasets by obtaining around 50% less Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) scores for each prediction horizon as compared to other models. However, the conventional LSTM-based models still prove to be dominant in the forex domain by comparatively achieving around 2% less error values. © 2020 © 2020 Saugat Aryal, Dheynoshan Nadarajah, Prabath Lakmal Rupasinghe, Chandimal Jayawardena and Dharshana Kasthurirathna. This open access article is distributed under a Creative Commons Attribution (CC-BY) 3.0 license.","Deep Learning; Financial Time Series; Forecasting; Multi-Step Prediction",
"Aryan P.R.","Knowledge graph for explainable cyber physical systems: A case study in smart energy grids",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121149016&partnerID=40&md5=88f307fce264f67ded2620f9ffc8fa93","The rapid development of computing technology and automation widens the scope of the task delegated to cyber-physical systems (CPS) such as smart grids or smart buildings. Explainability, i.e., the ability to provide explanations about system states or behaviors becomes one of the requirements for future cyber-physical systems as more complex computer-made decisions affect our daily lives. The work on the explainability in CPS is scarce despite recent attention on the explainability of algorithms in artificial intelligence. This doctorate research aims to comprehensively understand the scope of explainability in CPS, identify the critical components of an explainable CPS, and methods and metrics to evaluate them. Specifically, our main research question is how and to what extent Knowledge Graphs can be applied in enabling the explainability of CPS. Using the design science approach, we attempt to answer these questions in a set of iterations, starting with a simulation-based approach and constructing a baseline system followed by more focused studies and more realistic settings using data from real-world CPS. The selected application domain in this work is industrial energy systems such as smart grids and smart buildings. The expected outcome of this work is a theoretical foundation and methods for developing an explainable CPS applicable in various domains. Copyright © 2021 for this paper by its authors.","Cyber-physical systems; Explainability; Knowledge graph","Cyber Physical System; Electric power transmission networks; Intelligent buildings; Knowledge graph; Smart power grids; Case-studies; Computing technology; Critical component; Cybe-physical systems; Cyber-physical systems; Daily lives; Explainability; Knowledge graphs; System behaviors; System state; Embedded systems"
"Arzani B., Hsieh K., Chen H.","Interpretable Feedback for AutoML and a Proposal for Domain-customized AutoML for Networking","10.1145/3484266.3487373","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119453658&doi=10.1145%2f3484266.3487373&partnerID=40&md5=5ae2211a632df9ae0c020f3453f52c43","The barrier to entry for network operators to use machine learning (ML) is high for operators who are not ML experts. Automated machine learning (AutoML) promises operators the ability to train ML models without requiring the expertise of data scientists or the need to learn ML. However, AutoML today: (a) is black-box; and (b) does not allow operators to leverage domain expertise. We start this paper by describing our broader vision for a domain-customized AutoML platform for networking and propose a set of potential solutions to realize that vision. As the first step, we introduce our feedback solution for AutoML that allows domain experts (who are not experts in ML) to better understand how to improve the input data to AutoML in order to achieve better accuracy. © 2021 ACM.",,"Black boxes; Domain expertise; Domain experts; Feedback solution; Input datas; Learn+; Machine learning models; Network operator"
"Asad M., Moustafa A., Rabhi F.A., Aslam M.","THF: 3-Way Hierarchical Framework for Efficient Client Selection and Resource Management in Federated Learning","10.1109/JIOT.2021.3126828","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133285262&doi=10.1109%2fJIOT.2021.3126828&partnerID=40&md5=45da14860918f7e39a1baff175e1dab7","Federated learning (FL) is a promising technique for collaboratively training machine-learning models on massively distributed clients data under privacy constraints. However, the existing FL literature focuses on speeding up the learning process and ignores minimizing the communication cost which is critical for resource-constrained clients. To this end, in this article, we propose a novel 3-way hierarchical framework (THF) to promote communication efficiency in FL. Using the proposed framework, only a cluster head (CH) communicates with the cloud server through edge aggregation in order to minimize the communication cost of clients. In particular, the clients upload their local models to their respective CHs, which are responsible to forward them to the corresponding edge server. The edge server averages the local models and iterates until it achieves the edge accuracy. Afterward, each edge server uploads the edge models to the cloud server for global aggregation. In this way, model downloading and uploading requires less bandwidth due to the short distance from source to destination that makes an efficient 3-way hierarchical network structure. In addition, we formulate a joint communication and computation resource management scheme through efficient client selection in order to achieve global cost minimization in FL. We conduct extensive empirical evaluations on diverse data learning tasks on multiple data sets to signify that THF achieves global cost savings and converges within fewer communication rounds compared to other FL approaches. © 2014 IEEE.","Client selection; edge aggregation; federated learning (FL); resource management","Cloud computing; Costs; Learning systems; Natural resources management; Client selection; Cloud servers; Communication cost; Edge aggregations; Edge server; Federated learning; Local model; Machine learning models; Resource management; Training machines; Resource allocation"
"Asad M., Mahmood A., Usman M.","A machine learning-based framework for Predicting Treatment Failure in tuberculosis: A case study of six countries","10.1016/j.tube.2020.101944","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088906883&doi=10.1016%2fj.tube.2020.101944&partnerID=40&md5=5ec38dee36acf9f2f4e87f50f0deab16","Tuberculosis is ranked as the 2nd deadliest disease in the world and is responsible for ten million deaths in 2017. Treatment failure is one of a main reason behind these deaths. Reasons of treatment failure are still unknown and the death rate due to TB is increasing. Machine learning and data analytics approaches are proved to be useful in healthcare domain in finding the associations among different attributes that can affect the outcome of any disease. Timely identification of reasons can save a patient's life. This study aims to find features that are strongly correlated with treatment failure using feature selection techniques. The validation of features is demonstrated using different classification algorithms. Moreover, this study provides a demographic based feature association of six highly burdened treatment failure countries. A verified real-life patient's dataset gathered from different countries including Azerbaijan, Belarus, Georgia, India, Moldova, and Romania is utilized to address the problem. Two types of experimentation are performed on combined dataset by achieving an average accuracy of 78% and an accuracy of 92% on Romania's data. Results shows the importance of features obtained through this study are highly influential in leading a patient towards treatment failure. Copyright © 2020 Elsevier Ltd. All rights reserved.","Classification; Features; Machine learning; Preprocessing; Pulmonary tuberculosis; Treatment failure.","tuberculostatic agent; clinical trial; decision support system; drug effect; human; machine learning; microbiology; multicenter study; Mycobacterium tuberculosis; predictive value; reproducibility; risk assessment; risk factor; support vector machine; treatment failure; tuberculosis; Antitubercular Agents; Decision Support Techniques; Humans; Machine Learning; Mycobacterium tuberculosis; Neural Networks, Computer; Predictive Value of Tests; Reproducibility of Results; Risk Assessment; Risk Factors; Support Vector Machine; Treatment Failure; Tuberculosis"
"Asada K., Takasawa K., Machino H., Takahashi S., Shinkai N., Bolatkan A., Kobayashi K., Komatsu M., Kaneko S., Okamoto K., Hamamoto R.","Single-cell analysis using machine learning techniques and its application to medical research","10.3390/biomedicines9111513","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118352332&doi=10.3390%2fbiomedicines9111513&partnerID=40&md5=372f3d5e45ec7e012bbfdc2d02258037","In recent years, the diversity of cancer cells in tumor tissues as a result of intratumor heterogeneity has attracted attention. In particular, the development of single-cell analysis technology has made a significant contribution to the field; technologies that are centered on single-cell RNA sequencing (scRNA-seq) have been reported to analyze cancer constituent cells, identify cell groups responsible for therapeutic resistance, and analyze gene signatures of resistant cell groups. However, although single-cell analysis is a powerful tool, various issues have been reported, including batch effects and transcriptional noise due to gene expression variation and mRNA degradation. To overcome these issues, machine learning techniques are currently being introduced for single-cell analysis, and promising results are being reported. In addition, machine learning has also been used in various ways for single-cell analysis, such as single-cell assay of transposase accessible chromatin sequencing (ATAC-seq), chromatin immunoprecipitation sequencing (ChIP-seq) analysis, and multi-omics analysis; thus, it contributes to a deeper understanding of the characteristics of human diseases, especially cancer, and supports clinical applications. In this review, we present a comprehensive introduction to the implementation of machine learning techniques in medical research for single-cell analysis, and discuss their usefulness and future potential. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Machine learning; Multi-omics analysis; Next-generation sequencing; Single-cell analysis",
"Asadi N., Badie K., Mahmoudi M.T.","Automatic zone identification in scientific papers via fusion techniques","10.1007/s11192-019-03060-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062786159&doi=10.1007%2fs11192-019-03060-9&partnerID=40&md5=d0d1f7fbeaf26edc3f53df06ad8e8337","Zone identification is a topic in the area of text mining which helps researchers be benefited by the content of scientific papers in a satisfactory manner. The major aim of zone identification is to classify the sentences of scientific texts into some predefined zone categories which can be useful for summarization as well as information extraction. In this paper, we propose a two-level approach to zone identification within which the first level is in charge of classifying the sentences in a given paper based on some semantic and lexical features. In this respect, several machine learning algorithms such as Simple Logistics, Logistic Model Trees and Sequential Minimal Optimization are applied. The second level is responsible for applying fusion to the classification results obtained for consecutive sentences of the first level in order to make the final decision. The proposed method is evaluated on ART and DRI corpora as two well-known data sets. Results obtained for the accuracy of zone identification for these corpora are respectively 65.75% and 84.15%, which seem to be quite promising compared to those obtained by previous approaches. © 2019, Akadémiai Kiadó, Budapest, Hungary.","Fusion techniques; Logistic regression; Scientific paper; Semantic features; Zone identification",
"Asahara M., Fujimaki R.","An Empirical Study on Distributed Bayesian Approximation Inference of Piecewise Sparse Linear Models","10.1109/TPDS.2019.2892972","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067352213&doi=10.1109%2fTPDS.2019.2892972&partnerID=40&md5=511c948577a68b434f07ef687fac229d","The importance of interpretability of machine learning models has been increasing due to emerging enterprise predictive analytics. Piecewise linear models have been actively studied to achieve both accuracy and interpretability. They often produce competitive accuracy against state-of-the-art non-linear methods. In addition, their representations (i.e., rule-based segmentation plus sparse linear formula) are often preferred by domain experts. A disadvantage of such models, however, is high computational cost for simultaneous determinations of the number of pieces and cardinality of each linear predictor, which has restricted their applicability to middle-scale data sets. This paper discusses an empirical study on the derivation of a distributed factorized asymptotic Bayesian (FAB) inference of learning piece-wise sparse linear models on distributed memory architectures from the original FAB inference algorithm. The distributed FAB inference solves the simultaneous model selection issue without communicating O(N) data where N is the number of training samples and achieves linear scale-out against the number of CPU cores. Experimental results demonstrate that the distributed FAB inference achieves high prediction accuracy and performance scalability with both synthetic and public benchmark data. © 1990-2012 IEEE.","distributed applications/systems; interpretable models; Machine learning; sparse models","Benchmarking; Learning systems; Machine learning; Memory architecture; Nonvolatile storage; Piecewise linear techniques; Predictive analytics; Approximation inference; Distributed applications; Distributed memory architecture; Machine learning models; Performance scalability; Piecewise linear models; Simultaneous determinations; Sparse models; Inference engines"
"Asai M.","Unsupervised grounding of plannable first-order logic representation from images",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085615598&partnerID=40&md5=ef912a06b2cddf5d29097c98a1cfb30b","Recently, there is an increasing interest in obtaining the relational structures of the environment in the Reinforcement Learning community. However, the resulting ""relations"" are not the discrete, logical predicates compatible with the symbolic reasoning such as classical planning or goal recognition. Meanwhile, Latplan (Asai and Fukunaga 2018) bridged the gap between deep-learning perceptual systems and symbolic classical planners. One key component of the system is a Neural Network called State AutoEncoder (SAE), which encodes an image-based input into a propositional representation compatible with classical planning. To get the best of both worlds, we propose First-Order State AutoEncoder, an unsupervised architecture for grounding the first-order logic predicates and facts. Each predicate models a relationship between objects by taking the interpretable arguments and returning a propositional value. In the experiment using 8- Puzzle and a photo-realistic Blocksworld environment, we show that (1) the resulting predicates capture the interpretable relations (e.g., spatial), (2) they help to obtain the compact, abstract model of the environment, and finally, (3) the resulting model is compatible with symbolic classical planning. © 2019 Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Computer circuits; Deep learning; Formal logic; Reinforcement learning; Scheduling; Abstract modeling; Classical planning; First order logic; Logical predicate; Perceptual system; Photo-realistic; Relational structures; Symbolic reasoning; Learning systems"
"Asai Y., Hiratsuka T., Ueda M., Kawamura Y., Asamizu S., Onaka H., Arioka M., Nishimura S., Yoshida M.","Differential Biosynthesis and Roles of Two Ferrichrome-Type Siderophores, ASP2397/AS2488053 and Ferricrocin, in Acremonium persicinum","10.1021/acschembio.1c00867","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123369637&doi=10.1021%2facschembio.1c00867&partnerID=40&md5=6aafb2057867ce8e2f1b85da1e5a2822","Ferrichromes are a family of fungal siderophores with cyclic hexapeptide structures. Most fungi produce one or two ferrichrome-type siderophores. Acremonium persicinum MF-347833 produces ferrichrome-like potent Trojan horse antifungal antibiotics ASP2397 and AS2488053, the aluminum- and iron-chelating forms of AS2488059, respectively. Here, we show by gene sequencing followed by gene deletion experiments that A. persicinum MF-347833 possesses two nonribosomal peptide synthetase genes responsible for AS2488059 and ferricrocin assembly. AS2488059 was produced under iron starvation conditions and excreted into the media to serve as a defense metabolite and probably an iron courier. In contrast, ferricrocin was produced under iron-replete conditions and retained inside the cells, likely serving as an iron-sequestering molecule. Notably, the phylogenetic analyses suggest the different evolutionary origin of AS2488059 from that of conventional ferrichrome-type siderophores. Harnessing two ferrichrome-type siderophores with distinct biological properties may give A. persicinum a competitive advantage for surviving the natural environment. © 2022 American Chemical Society",,"adenosine phosphate; antifungal agent; AS2488053; ASP2397; ferrichrome; ferricrocin; polyketide synthase; siderophore; unclassified drug; coordination compound; cyclopeptide; ferrichrome; ferricrocin; fungal protein; siderophore; VL-2397; Acremonium; Acremonium persicinum; amino acid sequence; Article; Aspergillus fumigatus; Aspergillus oryzae; biosynthesis; controlled study; data mining; fungal gene; gene deletion; gene sequence; high performance liquid chromatography; IC50; iron transport; liquid chromatography-mass spectrometry; minimum effective concentration; mRNA expression level; nonhuman; open reading frame; phylogeny; polymerase chain reaction; real time polymerase chain reaction; real time reverse transcription polymerase chain reaction; sequence analysis; sid1 gene; sid2 gene; chemistry; fungal genome; gene expression regulation; genetics; metabolism; Acremonium; Coordination Complexes; Data Mining; Ferrichrome; Fungal Proteins; Gene Expression Regulation, Fungal; Genome, Fungal; Peptides, Cyclic; Phylogeny; Siderophores"
"Asano K., Chun J.","Post-hoc explanation using a mimic rule for numerical data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103846870&partnerID=40&md5=0e4a98ef905500fca784a546cddd14b8","We propose a novel rule-based explanation method for an arbitrary pre-trained machine learning model. Generally, machine learning models make black-box decisions that are not easy to explain the logical reasons to derive them. Therefore, it is important to develop a tool that gives reasons for the model's decision. Some studies have tackled the solution of this problem by approximating an explained model with an interpretable model. Although these methods provide logical reasons for a model's decision, a wrong explanation sometimes occurs. To resolve the issue, we define a rule model for the explanation, called a mimic rule, which behaves similarly in the model in its region. We obtain a mimic rule that can explain the large area of the numerical input space by maximizing the region. Through experimentation, we compare our method to earlier methods. Then we show that our method often improves local fidelity. © 2021 by SCITEPRESS - Science and Technology Publications, Lda.","Explanations; Rules; Transparency","Black boxes; Machine learning models; Numerical data; Numerical inputs; Rule based; Rule modeling; Machine learning"
"Asaro P.","Autonomous weapons and the ethics of artificial intelligence","10.1093/oso/9780190905033.003.0008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111846194&doi=10.1093%2foso%2f9780190905033.003.0008&partnerID=40&md5=b527ab9347436d56ba346f3200b147f3","As the militaries of technologically advanced nations seek to apply increasingly sophisticated AI and automation to weapons technologies, a host of ethical, legal, social, and political questions arise. Central among these is whether it is ethical to delegate the decision to use lethal force to an autonomous system that is not under meaningful human control. Further questions arise as to who or what could or should be held responsible when lethal force is used improperly by such systems. This chapter argues that current autonomous weapons are not legal or moral agents that can be held morally responsible or legally accountable for their choices and actions, and that therefore humans need to maintain control over such weapons systems. © Oxford University Press 2020.","Accountability; Autonomous weapons; Lethal force; Meaningful human control; Moral agents; Moral responsibility",
"Asatiani A., Malo P., Nagbøl P.R., Penttinen E., Rinta-Kahila T., Salovaara A.","Sociotechnical envelopment of artificial intelligence: an approach to organizational deployment of inscrutable artificial intelligence systems","10.17705/1jais.00664","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103244579&doi=10.17705%2f1jais.00664&partnerID=40&md5=5ed6305e4ecfe2cc72747f14f8cbab99","The paper presents an approach for implementing inscrutable (i.e., nonexplainable) artificial intelligence (AI) such as neural networks in an accountable and safe manner in organizational settings. Drawing on an exploratory case study and the recently proposed concept of envelopment, it describes a case of an organization successfully “enveloping” its AI solutions to balance the performance benefits of flexible AI models with the risks that inscrutable models can entail. The authors present several envelopment methods—establishing clear boundaries within which the AI is to interact with its surroundings, choosing and curating the training data well, and appropriately managing input and output sources—alongside their influence on the choice of AI models within the organization. This work makes two key contributions: It introduces the concept of sociotechnical envelopment by demonstrating the ways in which an organization’s successful AI envelopment depends on the interaction of social and technical factors, thus extending the literature’s focus beyond mere technical issues. Secondly, the empirical examples illustrate how operationalizing a sociotechnical envelopment enables an organization to manage the trade-off between low explainability and high performance presented by inscrutable models. These contributions pave the way for more responsible, accountable AI implementations in organizations, whereby humans can gain better control of even inscrutable machine-learning models. © 2021 by the Association for Information Systems.","Artificial Intelligence; Envelopment; Explainable AI; Machine Learning; Public Sector; Sociotechnical Systems; XAI","Economic and social effects; Artificial intelligence systems; Exploratory case studies; Input and outputs; Machine learning models; Organizational setting; Performance benefits; Sociotechnical; Technical factors; Artificial intelligence"
"Aschenbrenner D., Tol D.V., Rusak Z., Werker C.","Using Virtual Reality for scenario-based Responsible Research and Innovation approach for Human Robot Co-production","10.1109/AIVR50618.2020.00033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100004688&doi=10.1109%2fAIVR50618.2020.00033&partnerID=40&md5=4c95c721f1d62dfda07c6b982e8cb2ca","This paper proposes to use Virtual Reality scenarios to explore the reaction of stakeholders within an innovation process in the context of the introduction of robots working in close collaboration with users. The goal is to design the system upfront in such a way, that it is not perceived as a threat to the worker or his/her job. Within the responsible research and innovation approach, the introduction of new technology needs to be accompanied by a careful investigation of the thoughts and feelings of all stakeholders. Especially workers who are currently not working with robots but their workspace is currently undergoing an Industry 4.0 driven transformation, experience fear, that this new technology will make their jobs redundant. On the other hand, it can be observed, that successful robot interaction processes, on the one hand, increase the overall productivity, but also can enhance human well-being. The feeling of 'teamwork' with the artificial intelligence entity can develop to be equally positive and motivating. To be able to design future workspaces which will result in a 'teamwork' perception instead of the 'fear' perception, the use of VR can be applied. © 2020 IEEE.","Human Robot Co Production; Human Robot Interaction; Industry 4.0; Responsible Research and Innovation; Virtual Reality","Artificial intelligence; Empowerment of personnel; Industrial robots; Virtual reality; Co-production; Human robots; Innovation approach; Innovation process; Robot interactions; Robots working; Scenario-based; Well being; Human robot interaction"
"Ascher S., Wang X., Watson I., Sloan W., You S.","Interpretable machine learning to model biomass and waste gasification","10.1016/j.biortech.2022.128062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140043104&doi=10.1016%2fj.biortech.2022.128062&partnerID=40&md5=ec0cc042a44b615767c874d7e44aec99","Machine learning has been regarded as a promising method to better model thermochemical processes such as gasification. However, their black box nature can limit how much one can trust and learn from the developed models. Here seven different machine learning methods have been adopted to model the gasification of biomass and waste across a wide range of operating conditions. Gradient boosting regression has been found to outperform the other model types with a coefficient of determination (R2) of 0.90 when averaged across ten key gasification outputs. Global and local model interpretability methods have been used to illuminate the developed black box models. The studied models were most strongly influenced by the feedstock's particle size and the type of gasifying agent employed. By combining global and local interpretability methods, the understanding of black box models has been improved. This allows policy makers and investors to make more educated decisions about gasification process design. © 2022 The Author(s)","Bioenergy; Gradient boosting; SHAP (SHapley Additive exPlanations); Waste-to-energy","Adaptive boosting; Biomass; Machine learning; Particle size; Bio-energy; Biomass Gasification; Black box modelling; Gradient boosting; Interpretability; Machine-learning; Shapley; Shapley additive explanation; Waste gasification; Waste to energy; Gasification; bioenergy; biomass; chemical reaction; machine learning; thermochemistry; article; bioenergy; biomass; gasification; machine learning; particle size; process design; waste-to-energy"
"Ash J.R., Kuenemann M.A., Rotroff D., Motsinger-Reif A., Fourches D.","Cheminformatics approach to exploring and modeling trait-associated metabolite profiles","10.1186/s13321-019-0366-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083204633&doi=10.1186%2fs13321-019-0366-3&partnerID=40&md5=0ea4504b362f354e835021ab131b84f3","Developing predictive and transparent approaches to the analysis of metabolite profiles across patient cohorts is of critical importance for understanding the events that trigger or modulate traits of interest (e.g., disease progres- sion, drug metabolism, chemical risk assessment). However, metabolites’ chemical structures are still rarely used in the statistical modeling workflows that establish these trait-metabolite relationships. Herein, we present a novel cheminformatics-based approach capable of identifying predictive, interpretable, and reproducible trait-metabolite relationships. As a proof-of-concept, we utilize a previously published case study consisting of metabolite profiles from non-small-cell lung cancer (NSCLC) adenocarcinoma patients and healthy controls. By characterizing each structurally annotated metabolite using both computed molecular descriptors and patient metabolite concentration profiles, we show that these complementary features enhance the identification and understanding of key metabolites associ- ated with cancer. Ultimately, we built multi-metabolite classification models for assessing patients’ cancer status using specific groups of metabolites identified based on high structural similarity through chemical clustering. We subse- quently performed a metabolic pathway enrichment analysis to identify potential mechanistic relationships between metabolites and NSCLC adenocarcinoma. This cheminformatics-inspired approach relies on the metabolites’ structural features and chemical properties to provide critical information about metabolite-trait associations. This method could ultimately facilitate biological understanding and advance research based on metabolomics data, especially with respect to the identification of novel biomarkers. © The Author(s) 2019.","Chemical structure; Cheminformatics; Data mining; Metabolomics; Molecular fragmentation; Statistics; Visualization",
"Ashish B.P., Palecha N.","Adaptive Feedback Mechanism for Silicon Bug Detection","10.1007/978-981-16-0275-7_46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115443430&doi=10.1007%2f978-981-16-0275-7_46&partnerID=40&md5=3ef1db73700926ed65d018bbb14fe60c","Silicon validation is the most time-consuming process in the VLSI design flow. It mainly involves pre-silicon validation and post silicon validation. Post-silicon validation has limited visibility and control of internal signals and is the most time-consuming task. To help in narrowing down the features responsible for failure and reproduce the bug, the power of machine learning is leveraged. Based on the data provided, machine learning is used to provide the feature weights that control the failure of the device, thereby leading to faster reproduction of the failure. The framework that stresses the CPU requires manual intervention for providing test vectors and hence to have a faster response, the framework is automated with a reset mechanism so that it can include tests that are expected to fail on the device. Based on sample data, the machine learning model using supervised learning algorithms is developed. Results show that the model is accurately able to predict the features responsible for failure when applied on real time chip data. Due to automation, there is about 66% saving in time when compared to the manual process, thereby giving more time for more experiments to be run on the device and faster time to market. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Automation; Machine learning; Post silicon validation; SoC","Automation; Cell proliferation; Feedback; Learning algorithms; Programmable logic controllers; Silicon; System-on-chip; Adaptive feedback; Bug detection; Design flows; Feedback mechanisms; Internal signals; Limited visibility; Post-silicon validations; Silicon validations; Time-consuming tasks; VLSI design; Machine learning"
"Ashita C.T., Kala T.S.","Prediction of Heart Diseases using Deep Learning: A Review","10.1109/ICCMC53470.2022.9753747","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129155336&doi=10.1109%2fICCMC53470.2022.9753747&partnerID=40&md5=ede9874dd0fbf92aca81a3a6a249bbc8","The WHO studies show that cardiovascular diseases (CVD) are the major cause of 31% of all global deaths. CVD is also responsible for 45 percent of deaths in people aged 40 to 69. An accurate prediction system of heart disease is necessary and important to reduce deaths, globally. Today with the advancement of technology, prediction of heart disease using deep learning models, applying vast data can give an accurate prediction model. Using a deep learning method 94% of accuracy can be obtained and the data sets with different attributes can be used for analysis. The objective is to apply various algorithms to the problem and make a comparative study on the effectiveness of these algorithms in predicting the presence of coronary illness in a person. © 2022 IEEE.","and RNN; Cleveland dataset; CNN; Deep Learning; DNN; KNN; Naïve Bayes; Random Forest; SVM","Barium compounds; Decision trees; Deep learning; Diseases; Forecasting; Heart; Support vector machines; And RNN; Cleveland; Cleveland dataset; CNN; Deep learning; DNN; KNN; Naive bayes; Random forests; SVM; Cardiology"
"Ashley B.K., Hassan U.","Point-of-critical-care diagnostics for sepsis enabled by multiplexed micro and nanosensing technologies","10.1002/wnan.1701","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101853621&doi=10.1002%2fwnan.1701&partnerID=40&md5=6c1cbeeb32a3bd2012331b90499ec4e4","Sepsis is responsible for the highest economic and mortality burden in critical care settings around the world, prompting the World Health Organization in 2018 to designate it as a global health priority. Despite its high universal prevalence and mortality rate, a disproportionately low amount of sponsored research funding is directed toward diagnosis and treatment of sepsis, when early treatment has been shown to significantly improve survival. Additionally, current technologies and methods are inadequate to provide an accurate and timely diagnosis of septic patients in multiple clinical environments. For improved patient outcomes, a comprehensive immunological evaluation is critical which is comprised of both traditional testing and quantifying recently proposed biomarkers for sepsis. There is an urgent need to develop novel point-of-care, low-cost systems which can accurately stratify patients. These point-of-critical-care sensors should adopt a multiplexed approach utilizing multimodal sensing for heterogenous biomarker detection. For effective multiplexing, the sensors must satisfy criteria including rapid sample to result delivery, low sample volumes for clinical sample sparring, and reduced costs per test. A compendium of currently developed multiplexed micro and nano (M/N)-based diagnostic technologies for potential applications toward sepsis are presented. We have also explored the various biomarkers targeted for sepsis including immune cell morphology changes, circulating proteins, small molecules, and presence of infectious pathogens. An overview of different M/N detection mechanisms are also provided, along with recent advances in related nanotechnologies which have shown improved patient outcomes and perspectives on what future successful technologies may encompass. This article is categorized under: Diagnostic Tools > Biosensing. © 2021 Wiley Periodicals LLC.","diagnostics and management; disease; micronanotechnology; point-of-critical-care biosensors","Biomarkers; Clinical environments; Current technology; Detection mechanism; Diagnostic technologies; Diagnostic tools; Infectious pathogens; Multi-modal sensing; World Health Organization; Diagnosis; biological marker; membrane protein; quantum dot; bacterium detection; bacterium identification; cell motility; chemotaxis; colorimetry; fluorescence analysis; fungal detection; fungus identification; human; impedance spectroscopy; infectious agent; machine learning; nanotechnology; nonhuman; point of care testing; protein blood level; quantitative analysis; Review; sepsis; treatment outcome; virus detection; virus identification; genetic procedures; intensive care; microtechnology; nanotechnology; point of care system; sepsis; Biosensing Techniques; Critical Care; Humans; Microtechnology; Nanotechnology; Point-of-Care Systems; Sepsis"
"Ashley W.S., Haberlie A.M., Strohm J.","A climatology of quasi-linear convective systems and their hazards in the united states","10.1175/WAF-D-19-0014.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075159688&doi=10.1175%2fWAF-D-19-0014.1&partnerID=40&md5=ce48e5adcc36570455ace2af8d115940","This research uses image classification and machine learning methods on radar reflectivity mosaics to segment, classify, and track quasi-linear convective systems (QLCSs) in the United States for a 22-yr period. An algorithm is trained and validated using radar-derived spatial and intensity information from thousands of manually labeled QLCS and non-QLCS event slices. The algorithm is then used to automate the identification and tracking of over 3000 QLCSs with high accuracy, affording the first, systematic, long-term climatology of QLCSs. Convective regions determined by the procedure to be QLCSs are used as foci for spatiotemporal filtering of observed severe thunderstorm reports; this permits an estimation of the number of severe storm hazards due to this morphology. Results reveal that nearly 32% of MCSs are classified as QLCSs. On average, 139 QLCSs occur annually, with most of these events clustered from April through August in the eastern Great Plains and central/lower Mississippi and Ohio River Valleys. QLCSs are responsible for a spatiotemporally variable proportion of severe hazard reports, with a maximum in QLCS-report attribution (30%–42%) in the western Ohio and central Mississippi River Valleys. Over 21% of tornadoes, 28% of severe winds, and 10% of severe hail reports are due to QLCSs across the central and eastern United States. The proportion of QLCS-affiliated tornado and severe wind reports maximize during the overnight and cool season, with more than 50% of tornadoes and wind reports in some locations due to QLCSs. This research illustrates the utility of automated storm-mode classification systems in generating extensive, systematic climatologies of phenomena, reducing the need for time-consuming and spatiotemporal-limiting methods where investigators manually assign morphological classifications. © 2019 American Meteorological Society.",,"Climatology; Hazards; Image segmentation; Precipitation (meteorology); Radar; Storms; Tornadoes; Intensity information; Machine learning methods; Mode classification; Morphological classifications; Ohio River Valley; Quasi-linear convective systems; Radar reflectivities; Spatio temporal filtering; Learning systems; accuracy assessment; climatology; convective system; hazard assessment; image classification; machine learning; meteorological hazard; thunderstorm; tornado; United States"
"Ashmawy M.N., Khairy A.M., Hamdy M.W., El-Shazly A., El-Rashidy K., Salah M., Mansour Z., Khattab A.","SmartAmb: An integrated platform for ambulance routing and patient monitoring","10.1109/ICM48031.2019.9021900","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082122206&doi=10.1109%2fICM48031.2019.9021900&partnerID=40&md5=de22396f7b46e16a88bb371c65a62db3","This paper presents an integrated platform for smart ambulance routing and patient status monitoring during the ride. The main target of the platform is to increase the likelihood of the patient's survival by having the ambulance arriving to the hospital as soon as possible while allowing the responsible doctor to monitor the patient's biomedical data. Hence, the doctor can provide the paramedics riding with the patient with helpful instructions or prepare the needed medical services to be received by the patient upon arrival. Furthermore, the platform applies machine learning techniques on the collected data to help the doctor identify possible medical threats. We adopt a layered approach in the design of the system. A prototype of the integrated system is implemented, and its performance is evaluated. © 2019 IEEE.","Green Wave; Internet of Things (IoT); Machine Learning; Smart Ambulance","Internet of things; Learning systems; Machine learning; Microelectronics; Patient monitoring; Ambulance routing; Green waves; Integrated platform; Integrated systems; Internet of Things (IOT); Layered approaches; Machine learning techniques; Status monitoring; Ambulances"
"Ashok P., Jackermeier M., Jagtap P., KÅetínský J., Weininger M., Zamani M.","Demo: dtControl: Decision tree learning algorithms for controller representation","10.1145/3365365.3383468","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086528231&doi=10.1145%2f3365365.3383468&partnerID=40&md5=93fc49d7bbbe90a11d3033399d28360c","Decision tree learning is a popular classification technique most commonly used in machine learning applications. Recent work has shown that decision trees can be used to represent provably-correct controllers concisely. Compared to representations using lookup tables or binary decision diagrams, decision tree representations are smaller and more explainable. We present dtControl, an easily extensible tool offering a wide variety of algorithms for representing memoryless controllers as decision trees. We highlight that the trees produced by dtControl are often very concise with a single-digit number of decision nodes. This demo is based on our tool paper [1]. © 2020 Owner/Author.","controller representation; decision tree; explainability; machine learning; non-uniform quantizer; symbolic control","Binary trees; Controllers; Decision trees; Embedded systems; Hybrid systems; Internet of things; Learning algorithms; Table lookup; Classification technique; Decision tree learning; Decision tree learning algorithm; Machine learning applications; Memoryless; Tree representation; Binary decision diagrams"
"Ashok P., Jackermeier M., Jagtap P., KÅetínský J., Weininger M., Zamani M.","DtControl: Decision tree learning algorithms for controller representation","10.1145/3365365.3382220","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086526558&doi=10.1145%2f3365365.3382220&partnerID=40&md5=b87f00b5e90e2af1e278da75d1fbccae","Decision tree learning is a popular classification technique most commonly used in machine learning applications. Recent work has shown that decision trees can be used to represent provably-correct controllers concisely. Compared to representations using lookup tables or binary decision diagrams, decision trees are smaller and more explainable. We present dtControl, an easily extensible tool for representing memoryless controllers as decision trees. We give a comprehensive evaluation of various decision tree learning algorithms applied to 10 case studies arising out of correct-by-construction controller synthesis. These algorithms include two new techniques, one for using arbitrary linear binary classifiers in the decision tree learning, and one novel approach for determinizing controllers during the decision tree construction. In particular the latter turns out to be extremely efficient, yielding decision trees with a single-digit number of decision nodes on 5 of the case studies. © 2020 Owner/Author.","controller representation; decision tree; explainability; invariance entropy; machine learning; non-uniform quantizer; symbolic control","Binary decision diagrams; Binary trees; Controllers; Decision trees; Embedded systems; Hybrid systems; Internet of things; Table lookup; Classification technique; Comprehensive evaluation; Correct-by-construction; Decision tree construction; Decision tree learning; Decision tree learning algorithm; Linear binary classifiers; Machine learning applications; Learning algorithms"
"Ashrafian A., Panahi E., Salehi S., Taheri Amiri M.J.","On the implementation of the interpretable data-intelligence model for designing service life of structural concrete in a marine environment","10.1016/j.oceaneng.2022.111523","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130343457&doi=10.1016%2fj.oceaneng.2022.111523&partnerID=40&md5=f396b68bfe852e90d0a10ef9ac6db13c","Modeling the durability design of reinforced concrete structures is a particularly crucial challenge for identifying the behavior of marine concrete. In this research, novel interpretable equation-based models based on Fick's second law of diffusion for designing the service life of structural concrete were presented. To do so, a Machine Learning (ML) approach based on linear and non-linear methods including Multivariate Adaptive Regression Splines (MARS), Gene Expression Programming (GEP), and M5p Model Tree (MT) was performed to predict the apparent surface chloride concentration (Cs) in the zones of the marine environment. A comprehensive database is comprising of 642 field exposure experimental and environmental records was utilized to develop ML methods. The proposed ML models were investigated in terms of performance evaluation, error measurements, and visual consideration, MARS model (r=0.890,WI=93.9%andRMSE=0.804%bindercontent, (tidal zone) and r=0.901,WI=93.5%andRMSE=0.818%bindercontent (splash zone) and r=0.875,WI=86.8%andRMSE=0.883%bindercontent (submerged zone)) outperformed the proposed GEP, MT and empirical equations. Furthermore, simulation of Monte-Carlo analysis was extended to verify the proposed ML predictive models. Evaluation of the sensitivity analysis presented that the water to binder ratio, annual mean temperature, and exposure time were the most influential factors in predicting the Cs of reinforced concrete structures. Also, a parametric assessment has been performed for the presentation of the robustness of the proposed ML model. The modeling results presented new insight into the designing of the service life of reinforced concrete with superiority promotion of ML methods. © 2022 Elsevier Ltd","Durability design; Machine learning; Marine concrete; Reinforced concrete structure; Service life prediction","Chlorine compounds; Concrete buildings; Concrete construction; Durability; Forecasting; Gene expression; Monte Carlo methods; Reinforced concrete; Sensitivity analysis; Service life; Durability design; Gene-expression programming; Machine learning methods; Machine learning models; Marine concrete; Marine environment; Model trees; Reinforced concrete structures; Service life prediction; Structural concretes; Machine learning; concrete structure; data set; durability; machine learning; prediction; reinforced concrete"
"Ashtari M.A., Ansari R., Hassannayebi E., Jeong J.","Cost Overrun Risk Assessment and Prediction in Construction Projects: A Bayesian Network Classifier Approach","10.3390/buildings12101660","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140632619&doi=10.3390%2fbuildings12101660&partnerID=40&md5=bbdcd966ea034cbaff2e293aa701618b","Cost overrun risks are declared to be dynamic and interdependent. Ignoring the relationship between cost overrun risks during the risk assessment process is one of the primary reasons construction projects go over budget. Conversely, recent studies have failed to account for potential interrelationships between risk factors in their machine learning (ML) models. Additionally, the presented ML models are not interpretable. Thus, this study contributes to the entire ML process using a Bayesian network (BN) classifier model by considering the possible interactions between predictors, which are cost overrun risks, to predict cost overrun and assess cost overrun risks. Furthermore, this study compared the BN classifier model’s performance accuracy to that of the Naive Bayes (NB) and decision tree (DT) models to determine the effect of considering possible correlations between cost overrun risks on prediction accuracy. Moreover, the most critical risks and their relationships are identified by interpreting the learned BN model. The results indicated that the 18 BN models demonstrated an average prediction accuracy of 78.86%, significantly higher than the NB and DT. The present study identified the most significant risks as an increase in the cost of materials, lack of knowledge and experience among human resources, and inflation. © 2022 by the authors.","Bayesian network classifier; cost overrun; decision tree; machine learning; naive Bayes; risk assessment",
"Ashuha A., Loukachevitch N.","Bigram anchor words topic model","10.1007/978-3-319-52920-2_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014138672&doi=10.1007%2f978-3-319-52920-2_12&partnerID=40&md5=a2142c14d9d0647347c23cc6f6467ea5","A probabilistic topic model is a modern statistical tool for document collection analysis that allows extracting a number of topics in the collection and describes each document as a discrete probability distribution over topics. Classical approaches to statistical topic modeling can be quite effective in various tasks, but the generated topics may be too similar to each other or poorly interpretable. We supposed that it is possible to improve the interpretability and differentiation of topics by using linguistic information such as collocations while building the topic model. In this paper we offer an approach to accounting bigrams (two-word phrases) for the construction of Anchor Words Topic Model. © Springer International Publishing AG 2017.","Anchor words; Bigram; Topic model","Image analysis; Probability distributions; Statistical mechanics; Bigram; Classical approach; Discrete probability distribution; Document collection; Linguistic information; Probabilistic topic models; Statistical tools; Topic Modeling; Data mining"
"Ashwardhan A., Apparao K.C., Babu B.M., Sai S.D.","Artificial intelligent UAVs for precision agriculture","10.1063/5.0057902","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113165181&doi=10.1063%2f5.0057902&partnerID=40&md5=1b754de88258d1e3c6a1f1137c3e494c","India is a country of diversity. The primary occupation of the country is agriculture. In our country there is perfect weather conditions. There are significant challenges due to climatic changes that effects multiple sectors, which includes agriculture. According to food and agriculture organisation (FAO), the food production has to be increased by 70% by 2050 to meet food requirement of the population. The answer to this challenge is by introducing the new technology in the sector. In detailed introduction of IOT, UAVs and many technologies. Precision agriculture's tasks like smart spraying and aerial crop monitoring etc. Use of software development kit (SDKs) in UAVs will connect it to the mobile phones which makes farmer's ease of use. Introducing GPS in the UAV's makes discrete time broadcasts of its global coordinates, which also makes UAVs to fly at same altitude, autopilot, auto home return. By using sensors like collision avoidance sensors (CAS), gyroscope, accelerometer, magnetometer, barometer, range finder, helps the UAVs for anti-collision, balancing, determining speed, range respectively. By using thermal imaging camera, farmers can see which part of the field was affected by pests. By using the cameras farmers can monitor their crops, they can even survey their field. In this paper, we introduce the AI technology in the UAVs to help the farmers. Finally, we outline UAV based solutions for the challenges in precision agriculture. © 2021 Author(s).","crop monitoring; Machine learning through AI; Precision agriculture; responsible decision making; SDK; smart spraying; Thermal imaging camera; Un-manned Aerial Vehicle (UAV)",
"Asilar E., Hemmerich J., Ecker G.F.","Image Based Liver Toxicity Prediction","10.1021/acs.jcim.9b00713","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079671841&doi=10.1021%2facs.jcim.9b00713&partnerID=40&md5=e7aee864919e58ae968c9689fb98b1f7","The drugs we use to cure our diseases can cause damage to the liver as it is the primary organ responsible for metabolism of environmental chemicals and drugs. To identify and eliminate potentially problematic drug candidates in the early stages of drug discovery, in silico techniques provide quick and practical solutions for toxicity determination. Deep learning has emerged as one of the solutions in recent years in the field of pharmaceutical chemistry. Generally, in the case of small data sets as used in toxicology, these data-hungry algorithms are prone to overfitting. We approach the problem from two sides. First, we use images of the three-dimensional conformations and benefit from convolutional neural networks which have fewer parameters than the standard deep neural networks with similar depth. Using images allows connecting various chemical features to the geometry of the compounds. Second, we employ the method COVER to up-sample the data set. It is used not only for increasing the size of the data set, but also for balancing the two classes, i.e., toxic and not toxic. The proof of concept is performed on the p53 end point from the Tox21 data set. The results, which are compatible with the winners of the data challenge, encouraged us to use our methods to predict liver toxicity. We use the most extensive publicly available liver toxicity data set by Mulliner et al. and obtain a sensitivity of 0.79 and a specificity of 0.52. These results demonstrate the applicability of image based toxicity prediction using deep neural networks. © 2020 American Chemical Society.",,"Convolutional neural networks; Deep learning; Forecasting; Toxicity; Chemical features; Drug candidates; Environmental chemicals; Hungry algorithms; Pharmaceutical chemistry; Practical solutions; Proof of concept; Toxicity predictions; Deep neural networks; algorithm; drug development; liver; Algorithms; Drug Discovery; Liver; Neural Networks, Computer"
"Asim A., Kiani Y.S., Saeed M.T., Jabeen I.","Decoding the Role of Epigenetics in Breast Cancer Using Formal Modeling and Machine-Learning Methods","10.3389/fmolb.2022.882738","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134689005&doi=10.3389%2ffmolb.2022.882738&partnerID=40&md5=8312ece6e38a42e8cee2e5b8ca5bd1df","Breast carcinogenesis is known to be instigated by genetic and epigenetic modifications impacting multiple cellular signaling cascades, thus making its prevention and treatments a challenging endeavor. However, epigenetic modification, particularly DNA methylation-mediated silencing of key TSGs, is a hallmark of cancer progression. One such tumor suppressor gene (TSG) RUNX3 (Runt-related transcription factor 3) has been a new insight in breast cancer known to be suppressed due to local promoter hypermethylation mediated by DNA methyltransferase 1 (DNMT1). However, the precise mechanism of epigenetic-influenced silencing of the RUNX3 signaling resulting in cancer invasion and metastasis remains inadequately characterized. In this study, a biological regulatory network (BRN) has been designed to model the dynamics of the DNMT1–RUNX3 network augmented by other regulators such as p21, c-myc, and p53. For this purpose, the René Thomas qualitative modeling was applied to compute the unknown parameters and the subsequent trajectories signified important behaviors of the DNMT1–RUNX3 network (i.e., recovery cycle, homeostasis, and bifurcation state). As a result, the biological system was observed to invade cancer metastasis due to persistent activation of oncogene c-myc accompanied by consistent downregulation of TSG RUNX3. Conversely, homeostasis was achieved in the absence of c-myc and activated TSG RUNX3. Furthermore, DNMT1 was endorsed as a potential epigenetic drug target to be subjected to the implementation of machine-learning techniques for the classification of the active and inactive DNMT1 modulators. The best-performing ML model successfully classified the active and least-active DNMT1 inhibitors exhibiting 97% classification accuracy. Collectively, this study reveals the underlined epigenetic events responsible for RUNX3-implicated breast cancer metastasis along with the classification of DNMT1 modulators that can potentially drive the perception of epigenetic-based tumor therapy. Copyright © 2022 Asim, Kiani, Saeed and Jabeen.","c-myc; Dnmt1; machine learning; qualitative modeling; RUNX3 signaling pathway; SMBioNet",
"Asim M.N., Ibrahim M.A., Malik M.I., Zehe C., Cloarec O., Trygg J., Dengel A., Ahmed S.","EL-RMLocNet: An explainable LSTM network for RNA-associated multi-compartment localization prediction","10.1016/j.csbj.2022.07.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135296076&doi=10.1016%2fj.csbj.2022.07.031&partnerID=40&md5=73c70cab0b81d8594568f18f924ae8c2","Subcellular localization of Ribonucleic Acid (RNA) molecules provide significant insights into the functionality of RNAs and helps to explore their association with various diseases. Predominantly developed single-compartment localization predictors (SCLPs) lack to demystify RNA association with diverse biochemical and pathological processes mainly happen through RNA co-localization in multiple compartments. Limited multi-compartment localization predictors (MCLPs) manage to produce decent performance only for target RNA class of particular sub-type. Further, existing computational approaches have limited practical significance and potential to optimize therapeutics due to the poor degree of model explainability. The paper in hand presents an explainable Long Short-Term Memory (LSTM) network “EL-RMLocNet”, predictive performance and interpretability of which are optimized using a novel GeneticSeq2Vec statistical representation learning scheme and attention mechanism for accurate multi-compartment localization prediction of different RNAs solely using raw RNA sequences. GeneticSeq2Vec generates optimized statistical vectors of raw RNA sequences by capturing short and long range relations of nucleotide k-mers. Using sequence vectors generated by GeneticSeq2Vec scheme, Long Short Term Memory layers extract most informative features, weighting of which on the basis of discriminative potential for accurate multi-compartment localization prediction is performed using attention layer. Through reverse engineering, weights of statistical feature space are mapped to nucleotide k-mers patterns to make multi-compartment localization prediction decision making transparent and explainable for different RNA classes and species. Empirical evaluation indicates that EL-RMLocNet outperforms state-of-the-art predictor for subcellular localization prediction of 4 different RNA classes by an average accuracy figure of 8% for Homo Sapiens species and 6% for Mus Musculus species. EL-RMLocNet is freely available as a web server at (https://sds_genetic_analysis.opendfki.de/subcellular_loc/). © 2022 The Author(s)","Attention mechanism; Deep learning; Explainable; GeneticSeq2Vec; Human; LSTM; Mouse; Multi-class; Multi-label; Neural tricks; RNA subcellular localization prediction; Single or multi compartment","Brain; Decision making; Forecasting; HTTP; Nucleotides; RNA; Attention mechanisms; Deep learning; Explainable; Geneticseq2vec; Human; Mouse; Multi-class; Multi-labels; Neural trick; Ribonucleic acid subcellular localization prediction; Single or multi compartment; Subcellular localizations; Long short-term memory; RNA; Article; cellular distribution; comparative study; decision making; deep learning; discriminant validity; el rmlocnet; feature extraction; geneticseq2vec analysis; human; long short term memory network; machine learning; Mus musculus; nonhuman; nucleotide sequence; predictive value; RNA sequence; statistical analysis"
"Asirvatham M., Vijayaraj M., Manikandan T., Vignesh A., Nithila E.E., Jothi J.N.","Hybrid Deep Learning Network to Classify Eye Diseases","10.1109/ICOEI53556.2022.9776916","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131944124&doi=10.1109%2fICOEI53556.2022.9776916&partnerID=40&md5=4f03ad898a0cc6a0f702d9183923bda7","Early identification and ongoing monitoring of individuals with eye illnesses have been key concerns in computer- assisted detection. In computer aid screens, detecting one or more specific types of eye disease has achieved a big advance. Glaucoma is a collection of eye illnesses that will damage theoptic nerve, which is sole responsible for information sharing between eye to the brain, and can lead to blindness if left untreated. Glaucoma must be detected as early as possible for proper treatment. Diabetic retinopathy is also an eye illness due to diabetes which in turn harm the blood vessels of retina. This system proposed a Hybrid Ensemble Deep Learning system for early detection of Glaucoma and diabetic retinopathy. Eye images are first preprocessed to make them ready for subsequent processing. Based on the features retrieved during training, the system classifies incoming input images as normal eye, glaucoma afflicted eye, or diabetic retinopathy affected eye. This system produces 95% of accurate and stable output when compared to existing system. © 2022 IEEE.","Convolutional Neural Networks; Deep learning; Diabetic Retinopathy; Glaucoma; Hybrid Deep learning","Blood vessels; Convolutional neural networks; Deep learning; Diseases; Eye protection; Computer assisted detection; Convolutional neural network; Deep learning; Diabetic retinopathy; Eye disease; Eye images; Hybrid deep learning; Information sharing; Learning network; On-going monitoring; Ophthalmology"
"Aslaksen I.E., Svanberg E., Fagerholt K., Johnsen L.C., Meisel F.","Ferry Service Network Design for Kiel fjord","10.1007/978-3-030-59747-4_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092261403&doi=10.1007%2f978-3-030-59747-4_3&partnerID=40&md5=067cc28f662860e437a956bc475f1ce9","This paper considers a ferry service network design problem using autonomous ferries for the practical case of the Kiel fjord. Among others, the city of Kiel, Germany, currently runs a number of initiatives for developing an autonomous ferry system to open up new mobility opportunities. The city is divided by the Kiel fjord into an eastern and a western part and the current infrastructure is mainly built to accommodate car transportation on roads around the fjord. We provide a new optimization model for the generation of schedules for an autonomous ferry service, including route design and determination of departure frequencies. The model captures practically relevant aspects of minimum required departure frequencies between specific port pairs and understandable ferry schedules, whilst maximizing customer service quality (i.e., excess transit times and departure frequencies). We provide a two-step optimization approach where candidate combinations of routes and departure frequencies are heuristically generated a priori and fed into an integer programming model. Experiments on real world data provide managerial insights in regard to ferry fleet size, port network design and ferry schedules. © 2020, Springer Nature Switzerland AG.","Autonomous ships; Kiel fjord; Network design problem","Artificial intelligence; Computer science; Computers; Customer services; Departure frequency; Ferry service network design; Ferry services; Integer programming models; Network design; Optimization modeling; Two-step optimizations; Integer programming"
"Aslam M.A.","LOPDF: A framework for extracting and producing open data of scientific documents for smart digital libraries","10.7717/PEERJ-CS.445","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105119856&doi=10.7717%2fPEERJ-CS.445&partnerID=40&md5=b7ec5ba3008f0a62031d824c1663d15f","Background. Results of scientific experiments and research work, either conducted by individuals or organizations, are published and shared with scientific community in different types of scientific publications such as books, chapters, journals, articles, reference works and reference works entries. One aspect of these documents is their contents and the other is metadata. Metadata of scientific documents could be used to increase mutual cooperation, find people with common interest and research work, and to find scientific documents in the matching domains. The major issue in getting these benefits from metadata of scientific publications is availability of these data in unstructured (or semi-structured) format so that it can not be used to ask smart queries that can help in computing and performing different types of analysis on scientific publications data. Also, acquisition and smart processing of publications data is a complicated as well as time and resource consuming task. Methods. To address this problem we have developed a generic framework named as Linked Open Publications Data Framework (LOPDF). The LOPDF framework can be used to crawl, process, extract and produce machine understandable data (i.e., LOD) about scientific publications from different publisher specific sources such as portals, XML export and websites. In this paper we present the architecture, process and algorithm that we developed to process textual publications data and to produce semantically enriched data as RDF datasets (i.e., open data). Results. The resulting datasets can be used to make smart queries by making use of SPARQL protocol. We also present the quantitative as well as qualitative analysis of our resulting datasets which ultimately can be used to compute the research behavior of organizations in rapidly growing knowledge society. Finally, we present the potential usage of producing and processing such open data of scientific publications and how results of performing smart queries on resulting open datasets can be used to compute the impact and perform different types of analysis on scientific publications data. Copyright 2021 Aslam","Algorithms analysis; Digital libraries; Ontological reasoning; Open data","Behavioral research; Data handling; Data mining; Digital libraries; Metadata; Publishing; Common interests; Generic frameworks; Knowledge society; Qualitative analysis; Scientific community; Scientific documents; Scientific experiments; Scientific publications; Open Data"
"Aslam N., Khan I.U., Mirza S., Alowayed A., Anis F.M., Aljuaid R.M., Baageel R.","Interpretable Machine Learning Models for Malicious Domains Detection Using Explainable Artificial Intelligence (XAI)","10.3390/su14127375","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132593107&doi=10.3390%2fsu14127375&partnerID=40&md5=681a98d5941f871d285d56732415275a","With the expansion of the internet, a major threat has emerged involving the spread of malicious domains intended by attackers to perform illegal activities aiming to target governments, violating privacy of organizations, and even manipulating everyday users. Therefore, detecting these harmful domains is necessary to combat the growing network attacks. Machine Learning (ML) models have shown significant outcomes towards the detection of malicious domains. However, the “black box” nature of the complex ML models obstructs their wide-ranging acceptance in some of the fields. The emergence of Explainable Artificial Intelligence (XAI) has successfully incorporated the interpretability and explicability in the complex models. Furthermore, the post hoc XAI model has enabled the interpretability without affecting the performance of the models. This study aimed to propose an Explainable Artificial Intelligence (XAI) model to detect malicious domains on a recent dataset containing 45,000 samples of malicious and non-malicious domains. In the current study, initially several interpretable ML models, such as Decision Tree (DT) and Naïve Bayes (NB), and black box ensemble models, such as Random Forest (RF), Extreme Gradient Boosting (XGB), AdaBoost (AB), and Cat Boost (CB) algorithms, were implemented and found that XGB outperformed the other classifiers. Furthermore, the post hoc XAI global surrogate model (Shapley additive explanations) and local surrogate LIME were used to generate the explanation of the XGB prediction. Two sets of experiments were performed; initially the model was executed using a preprocessed dataset and later with selected features using the Sequential Forward Feature selection algorithm. The results demonstrate that ML algorithms were able to distinguish benign and malicious domains with overall accuracy ranging from 0.8479 to 0.9856. The ensemble classifier XGB achieved the highest result, with an AUC and accuracy of 0.9991 and 0.9856, respectively, before the feature selection algorithm, while there was an AUC of 0.999 and accuracy of 0.9818 after the feature selection algorithm. The proposed model outperformed the benchmark study. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","ensemble models; explainable artificial intelligence; machine learning; malicious domains; network security","accuracy assessment; artificial intelligence; data set; detection method; machine learning; numerical model"
"Aslam N.","Explainable Artificial Intelligence Approach for the Early Prediction of Ventilator Support and Mortality in COVID-19 Patients","10.3390/computation10030036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125812341&doi=10.3390%2fcomputation10030036&partnerID=40&md5=94b9e19c736512ca6e3eef7f197b4df1","Early prediction of mortality and risk of deterioration in COVID-19 patients can reduce mortality and increase the opportunity for better and more timely treatment. In the current study, the DL model and explainable artificial intelligence (EAI) were combined to identify the impact of certain attributes on the prediction of mortality and ventilatory support in COVID-19 patients. Nevertheless, the DL model does not suffer from the curse of dimensionality, but in order to identify significant attributes, the EAI feature importance method was used. The DL model produced significant results; however, it lacks interpretability. The study was performed using COVID-19-hospitalized patients in King Abdulaziz Medical City, Riyadh. The dataset contains the patients’ demographic information, laboratory investigations, and chest X-ray (CXR) findings. The dataset used suffers from an imbalance; therefore, balanced accuracy, sensitivity, specificity, Youden index, and AUC measures were used to investigate the effectiveness of the proposed model. Furthermore, the experiments were conducted using original and SMOTE (over and under sampled) datasets. The proposed model outperforms the baseline study, with a balanced accuracy of 0.98 and an AUC of 0.998 for predicting mortality using the full-feature set. Meanwhile, for predicting ventilator support a highest balanced accuracy of 0.979 and an AUC of 0.981 was achieved. The proposed explainable prediction model will assist doctors in the early prediction of COVID-19 patients that are at risk of mortality or ventilatory support and improve the management of hospital resources. © 2022 by the author. Licensee MDPI, Basel, Switzerland.","Deep learning; Explainable artificial intelligence; Machine learning; Mortality; Prediction; Ventilator support",
"Aslam N., Khan I.U., Aljishi R.F., Alnamer Z.M., Alzawad Z.M., Almomen F.A., Alramadan F.A.","Explainable Computational Intelligence Model for Antepartum Fetal Monitoring to Predict the Risk of IUGR","10.3390/electronics11040593","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124490374&doi=10.3390%2felectronics11040593&partnerID=40&md5=625a074c7f40b03d3afcc27183acc6ed","Intrauterine Growth Restriction (IUGR) is a restriction of the fetus that involves the ab-normal growth rate of the fetus, and it has a huge impact on the new-born’s health. Machine learning (ML) algorithms can help in early prediction and discrimination of the abnormality of the fetus’ health to assist in reducing the risk during the antepartum period. Therefore, in this study, Random Forest (RF), Support Vector Machine (SVM), K Nearest Neighbor (KNN) and Gradient Boosting (GB) was utilized to discriminate whether a fetus was healthy or suffering from IUGR based on the fetal heart rate (FHR). The Recursive Feature Elimination (RFE) method was used to select the sig-nificant feature for the classification of fetus. Furthermore, the study Explainable Artificial Intelligence (EAI) was implemented using LIME and SHAP to generate the explanation and to add com-prehensibility in the proposed models. The experimental results indicate that RF achieved the high-est accuracy (0.97) and F1-score (0.98) with the reduced set of features. However, the SVM outperformed it in terms of Positive Predictive Value (PPV) and specificity (SP). The performance of the model was further validated using another dataset and found that it outperformed the baseline studies for both the datasets. The proposed model can aid doctors in monitoring fetal health and enhancing the prediction process. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Antepartum; Fetal Heart Rate (FHR); IUGR; Machine Learning (ML); Prediction; Preterm birth",
"Aslam N., Khan I.U., Alansari A., Alrammah M., Alghwairy A., Alqahtani R., Alqahtani R., Almushikes M., Hashim M.A.L.","Anomaly Detection Using Explainable Random Forest for the Prediction of Undesirable Events in Oil Wells","10.1155/2022/1558381","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136022159&doi=10.1155%2f2022%2f1558381&partnerID=40&md5=de707ecd1503f548db4eeefe6ffd89b1","The worldwide demand for oil has been rising rapidly for many decades, being the first indicator of economic development. Oil is extracted from underneath reservoirs found below land or ocean using oil wells. An offshore oil well is an oil well type where a wellbore is drilled underneath the ocean bed to obtain oil to the surface that demands more stability than other oil wells. The sensors of oil wells generate massive amounts of multivariate time-series data for surveillance engineers to analyze manually and have continuous insight into drilling operations. The manual analysis of data is challenging and time-consuming. Additionally, it can lead to several faulty events that could increase costs and production losses since the engineers tend to focus on the analysis rather than detecting the faulty events. Recently, machine learning (ML) techniques have significantly solved enormous real-time data anomaly problems by decreasing the data engineers' interaction processes. Accordingly, this study aimed to utilize ML techniques to reduce the time spent manually to establish rules that detect abnormalities in oil wells, leading to rapid and more precise detection. Four ML algorithms were utilized, including random forest (RF), logistic regression (LR), k-nearest neighbor (K-NN), and decision tree (DT). The dataset used in this study suffers from the class imbalance issue; therefore, experiments were conducted using the original and sampled datasets. The empirical results demonstrated promising outcomes, where RF achieved the highest accuracy, recall, precision, F1-score, and AUC of 99.60%, 99.64%, 99.91%, 99.77%, and 1.00, respectively, using the sampled data, and 99.84%, 99.91%, 99.91%, 99.91%, and 1.00, respectively, using the original data. Besides, the study employed Explainable Artificial Intelligence (XAI) to enable surveillance engineers to interpret black box models to understand the causes of abnormalities. The proposed models can be used to successfully identify anomalous events in the oil wells. © 2022 Nida Aslam et al.",,
"Aslan Z., Akin M.","A deep learning approach in automated detection of schizophrenia using scalogram images of EEG signals","10.1007/s13246-021-01083-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119823850&doi=10.1007%2fs13246-021-01083-2&partnerID=40&md5=57ea083b352060834e0110c880179b19","This study presents a method with high accuracy performance that aims to automatically detect schizophrenia (SZ) from electroencephalography (EEG) records. Unlike related literature studies using traditional machine learning algorithms, the features required for the training of the network are automatically extracted from the EEG records in our method. In order to obtain the time frequency features of the EEG signals, the signal was converted into 2D by using the Continuous Wavelet Transform method. This study has the highest accuracy performance in the relevant literature by using 2D time frequency features in automatic detection of SZ disease. It is trained with Visual Geometry Group-16 (VGG16), an advanced convolutional neural networks (CNN) deep learning network architecture, to extract key features found on scalogram images and train the network. The study shows a high success in classifying SZ patients and healthy individuals with a very satisfactory accuracy of 98% and 99.5%, respectively, using two different datasets consisting of individuals from different age groups. Using different techniques [Activization Maximization, Saliency Map, and Gradient-weighted Class Activation Mapping (Grad-CAM)] to visualize the learning outcomes of the CNN network, the relationship of frequency components between SZ and the healthy individual is clearly shown. Moreover, with these interpretable outcomes, the difference between SZ patients and healthy individuals can be distinguished very easily help for expert opinion. © 2021, Australasian College of Physical Scientists and Engineers in Medicine.","CWT; Deep learning; EEG; Scalogram; Schizophrenia","Classification (of information); Convolutional neural networks; Deep learning; Electroencephalography; Electrophysiology; Learning algorithms; Network architecture; Wavelet transforms; Automated detection; CWT; Deep learning; Healthy individuals; High-accuracy; Learning approach; Performance; Scalogram; Schizophrenia patients; Time frequency features; Diseases; accuracy; activization maximization; adolescent; adult; age distribution; Article; child; computer aided design; continuous wavelet transform; controlled study; convolutional neural network; deep learning; electroencephalography; female; gradient weighted class activation mapping; human; imaging and display; machine learning; major clinical study; male; saliency map; scalogram image; schizophrenia; Visual Geometry Group 16; diagnostic imaging; electroencephalography; procedures; Deep Learning; Electroencephalography; Humans; Machine Learning; Neural Networks, Computer; Schizophrenia"
"Asmare M.M., Nitin N., Yun S.-I.L., Mahapatra R.K.","QSAR and deep learning model for virtual screening of potential inhibitors against Inosine 5’ Monophosphate dehydrogenase (IMPDH) of Cryptosporidium parvum","10.1016/j.jmgm.2021.108108","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121130390&doi=10.1016%2fj.jmgm.2021.108108&partnerID=40&md5=16b6fed9dcdacebe831b8e369318a704","Cryptosporidium parvum (Cp) causes a gastro-intestinal disease called Cryptosporidiosis. C. parvum Inosine 5’ monophosphate dehydrogenase (CpIMPDH) is responsible for the production of guanine nucleotides. In the present study, 37 known urea-based congeneric compounds were used to build a 2D and 3D QSAR model against CpIMPDH. The built models were validated based on OECD principles. A deep learning model was adopted from a framework called Deep Purpose. The model was trained with 288 known active compounds and validated using a test set. From the training set of the 3D QSAR, a pharmacophore model was built and the best pharmacophore hypotheses were scored and sorted using a phase-hypo score. A phytochemical database was screened using both the pharmacophore model and a deep learning model. The screened compounds were considered for glide XP docking, followed by quantum polarized ligand docking. Finally, the best compound among them was considered for molecular dynamics simulation study. © 2021 Elsevier Inc.","2D and 3D QSAR; CpIMPDH; Deep learning; MD simulation; Phramcophore","3D modeling; Deep learning; Diagnosis; E-learning; Hydrogen bonds; Molecular dynamics; Molecular graphics; Pharmacodynamics; Urea; 2d and 3d QSAR; 3D-QSAR; C parva inosine 5’ monophosphate dehydrogenase; Cryptosporidium parvum; Deep learning; Inosine; Learning models; MD simulation; Monophosphates; Phramcophore; Computational chemistry; enzyme inhibitor; inosinate dehydrogenase; inosine; cryptosporidiosis; Cryptosporidium; Cryptosporidium parvum; human; metabolism; molecular docking; quantitative structure activity relation; Cryptosporidiosis; Cryptosporidium; Cryptosporidium parvum; Deep Learning; Enzyme Inhibitors; Humans; IMP Dehydrogenase; Inosine; Molecular Docking Simulation; Quantitative Structure-Activity Relationship"
"Asokan D.R., Huq F.A., Smith C.M., Stevenson M.","Socially responsible operations in the Industry 4.0 era: post-COVID-19 technology adoption and perspectives on future research","10.1108/IJOPM-01-2022-0069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133087983&doi=10.1108%2fIJOPM-01-2022-0069&partnerID=40&md5=6e8bda9ee618c9175a62deb58641715f","Purpose: As focal firms in supply networks reflect on their experiences of the pandemic and begin to rethink their operations and supply chains, there is a significant opportunity to leverage digital technological advances to enhance socially responsible operations performance (SROP). This paper develops a novel framework for exploring the adoption of Industry 4.0 technologies for improving SROP. It highlights current best-practice examples and presents future research pathways. Design/methodology/approach: This viewpoint paper argues how Industry 4.0 technology adoption can enable effective SROP in the post-COVID-19 era. Academic articles, relevant grey literature, and insights from industry experts are used to support the development of the framework. Findings: Seven technologies are identified that bring transformational capabilities to SROP, i.e. big data analytics, digital twins, augmented reality, blockchain, 3D printing, artificial intelligence, and the Internet of Things. It is demonstrated how these technologies can help to improve three sub-themes of organisational social performance (employment practices, health and safety, and business practices) and three sub-themes of community social performance (quality of life and social welfare, social governance, and economic welfare and growth). Research limitations/implications: A research agenda is outlined at the intersection of Industry 4.0 and SROP through the six sub-themes of organisational and community social performance. Further, these are connected through three overarching research agendas: “Trust through Technology”, “Responsible Relationships” and “Freedom through Flexibility”. Practical implications: Organisational agendas for Industry 4.0 and social responsibility can be complementary. The framework provides insights into how Industry 4.0 technologies can help firms achieve long-term post-COVID-19 recovery, with an emphasis on SROP. This can offer firms competitive advantage in the “new normal” by helping them build back better. Social implications: People and communities should be at the heart of decisions about rethinking operations and supply chains. This paper expresses a view on what it entails for organisations to be responsible for the supply chain-wide social wellbeing of employees and the wider community they operate in, and how they can use technology to embed social responsibility in their operations and supply chains. Originality/value: Contributes to the limited understanding of how Industry 4.0 technologies can lead to socially responsible transformations. A novel framework integrating SROP and Industry 4.0 is presented. © 2022, Deepak Ram Asokan, Fahian Anisul Huq, Christopher M. Smith and Mark Stevenson.","COVID-19; Digital; Industry 4.0; Operations management; Social responsibility; Sustainability",
"Asoodeh S., Calmon F.P.","Bottleneck problems: An information and estimation-theoretic view","10.3390/e22111325","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096378360&doi=10.3390%2fe22111325&partnerID=40&md5=abfab6d94f51ed3aaefe3b15f2e81e0c","Information bottleneck (IB) and privacy funnel (PF) are two closely related optimization problems which have found applications in machine learning, design of privacy algorithms, capacity problems (e.g., Mrs. Gerber’s Lemma), and strong data processing inequalities, among others. In this work, we first investigate the functional properties of IB and PF through a unified theoretical framework. We then connect them to three information-theoretic coding problems, namely hypothesis testing against independence, noisy source coding, and dependence dilution. Leveraging these connections, we prove a new cardinality bound on the auxiliary variable in IB, making its computation more tractable for discrete random variables. In the second part, we introduce a general family of optimization problems, termed “bottleneck problems”, by replacing mutual information in IB and PF with other notions of mutual information, namely f-information and Arimoto’s mutual information. We then argue that, unlike IB and PF, these problems lead to easily interpretable guarantees in a variety of inference tasks with statistical constraints on accuracy and privacy. While the underlying optimization problems are non-convex, we develop a technique to evaluate bottleneck problems in closed form by equivalently expressing them in terms of lower convex or upper concave envelope of certain functions. By applying this technique to a binary case, we derive closed form expressions for several bottleneck problems. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Data processing inequality; Information bottleneck; Mutual information; Privacy funnel",
"Asperti A., Wegner B.","MOWGLI – an approach to machine-understandable representation of the mathematical information in digital documents","10.1007/978-3-540-45155-6_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745164956&doi=10.1007%2f978-3-540-45155-6_2&partnerID=40&md5=31a2fcff5198f6b5a930028704415455","The acronym MOWGLI stands for “Mathematics On the Web: Get it by Logic and Interfaces”. MOWGLI is a European Project founded by the European Community in the “Information Society Technologies” (IST) Programme. The partners are the University of Bologna, INRIA (Rocquencourt), the German Research Centre for Artificial Intelligence (DFKI, Saarbrücken), the Katholieke Universiteit Nijmegen, the Max Planck Institute for Gravitational Physics (Albert Einstein Institute, Golm), Trusted Logic (Paris) and TU Berlin. The aim of the project is the study and the development of a technological infrastructure for the creation and maintenance of a virtual, distributed, hypertextual library of mathematical knowledge based on a content description of the information. Currently, almost all mathematical documents available on the Web are marked up only for presentation, severely crippling the potentialities for automation, interoperability, sophisticated searching mechanisms, intelligent applications, transformation and processing. The goal of MOWGLI is to overcome these limitations, passing from a machine-readable to a machine-understandable representation of the information, and developing the technological infrastructure for its exploitation. The project deals with problems traditionally belonging to different scientific communities: digital libraries, Web publishing, automation of mathematics and computer aided reasoning. Any serious solution to the complex problem of mathematical knowledge management needs a coordinated effort of all these groups and a synergy of their different expertise. MOWGLI attempts to build a solid co-operation environment between these communities. The current paper will describe the objectives and first achievements of MOWGLI in some detail. © Springer-Verlag Berlin Heidelberg 2003.",,"Computer circuits; Coordination reactions; Knowledge based systems; Knowledge management; Mathematical transformations; Gravitational physics; Information society technologies; Intelligent applications; Mathematical information; Mathematical knowledge; Mathematical knowledge management; Max Planck Institute; Technological infrastructure; Digital libraries"
"Asprino L., De Giorgis S., Gangemi A., Bulla L., Marinucci L., Mongiovì M.","Uncovering Values: Detecting Latent Moral Content from Natural Language with Explainable and Non-Trained Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136089309&partnerID=40&md5=e931f5644c9f03e6594d3d1c28893713","Moral values as commonsense norms shape our everyday individual and community behavior. The possibility to extract moral attitude rapidly from natural language is an appealing perspective that would enable a deeper understanding of social interaction dynamics and the individual cognitive and behavioral dimension. In this work we focus on detecting moral content from natural language and we test our methods on a corpus of tweets previously labeled as containing moral values or violations, according to Moral Foundation Theory. We develop and compare two different approaches: (i) a frame-based symbolic value detector based on knowledge graphs and (ii) a zero-shot machine learning model fine-tuned on a task of Natural Language Inference (NLI) and a task of emotion detection. Our approaches achieve considerable performances without the need for prior training. © 2022 Association for Computational Linguistics.",,"Computational linguistics; Knowledge graph; Emotion detection; Frame-based; Interaction dynamics; Knowledge graphs; Language inference; Machine learning models; Natural languages; Performance; Social interactions; Symbolic value; Zero-shot learning"
"Assaad R., El-Adaway I.H.","Evaluation and Prediction of the Hazard Potential Level of Dam Infrastructures Using Computational Artificial Intelligence Algorithms","10.1061/(ASCE)ME.1943-5479.0000810","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087063720&doi=10.1061%2f%28ASCE%29ME.1943-5479.0000810&partnerID=40&md5=d104724cdc9ba7936b658227a5c43437","Failures of dams cause immense property and environmental damages and take thousands of lives. As such, the goal of this paper is to evaluate and predict the hazard potential level of dams in the US using a comparative approach based on computational artificial intelligence (AI) algorithms. The research methodology comprised data collection from the National Inventory of Dams (NID); data preprocessing; data processing; and model selection and evaluation. To this end, the authors: (1) identified the best subset of variables that affect the prediction of the hazard potential level of dams in the US; (2) investigated the performance of two AI computational algorithms: artificial neural networks (ANNs) and k-nearest neighbors (KNNs) for the evaluation and prediction of the hazard potential levels of US dams; and (3) developed a decision support tool that could be used by the agencies responsible for the management of dams in the US with the capability to predict the hazard potential with good accuracy. The obtained results reflected that the ANN algorithm yielded better accuracy compared to the KNN algorithm. In addition, the conclusions indicated that 19 variables pertaining to dams in the US could affect the hazard potential level of dams. The output is a decision support system that is able to evaluate the hazard potential of dams with a prediction accuracy of 85.70%. This study contributes to the management in engineering's body of knowledge by devising a data-driven framework that is valuable for dams' owners and authorities. Ultimately, the developed computational AI algorithm could be used to evaluate and predict the hazard potential level of US dams with good accuracy while minimizing the efforts, time, and costs associated with formal inspection of the dams. © 2020 American Society of Civil Engineers.","Artificial intelligence (AI); Artificial neural networks (ANNs); Computational algorithms; Decision support tool; Hazard potential; Infrastructure US dams; k -nearest neighbors (KNNs)","Dams; Data handling; Decision support systems; Forecasting; Learning algorithms; Nearest neighbor search; Neural networks; Artificial intelligence algorithms; Comparative approach; Computational algorithm; Decision support tools; Environmental damage; Evaluation and predictions; K-nearest neighbors; Research methodologies; Hazards"
"Assaf R., Giurgiu I., Bagehorn F., Schumann A.","MTEX-CNN: Multivariate time series explanations for predictions with convolutional neural networks","10.1109/ICDM.2019.00106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078891281&doi=10.1109%2fICDM.2019.00106&partnerID=40&md5=9ae855cf612bcaf0a6efebc432548a38","In this work we present MTEX-CNN, a novel explainable convolutional neural network architecture which can not only be used for making predictions based on multivariate time series data, but also for explaining these predictions. The network architecture consists of two stages and utilizes particular kernel sizes. This allows us to apply gradient based methods for generating saliency maps for both the time dimension and the features. The first stage of the architecture explains which features are most significant to the predictions, while the second stage explains which time segments are the most significant. We validate our approach on two use cases, namely to predict rare server outages in the wild, as well as the average energy production of photovoltaic power plants based on a benchmark data set. We show that our explanations shed light over what the model has learned. We validate this by retraining the network using the most significant features extracted from the explanations and retaining similar performance to training with the full set of features. © 2019 IEEE.","Convolutional Neural Network; Deep Learning; Explainable Machine Learning; Multivariate Time Series","Convolution; Data mining; Deep learning; Deep neural networks; Forecasting; Neural networks; Photovoltaic cells; Time series; Average energy; Benchmark data; Convolutional neural network; Gradient-based method; Multivariate time series; Photovoltaic power plant; Time dimension; Time segments; Network architecture"
"Assaghir Z., Napoli A., Kaytoue M., Dubois D., Prade H.","Numerical information fusion: Lattice of answers with supporting arguments","10.1109/ICTAI.2011.98","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855775647&doi=10.1109%2fICTAI.2011.98&partnerID=40&md5=e860d41f44bbbeeaceca8a89dddb656e","The problem addressed in this paper is the merging of numerical information provided by several sources. Merging conflicting pieces of information into an interpretable and useful format is a tricky task even when an information fusion method is chosen. The use of formal concept analysis and pattern structures enables us to associate subsets of sources to combination results obtainable from consistent subsets of pieces of information. This provides a lattice of arguments where the reliability of sources can be taken into account. Instead of providing a unique fusion result, the method yields a structured view of partial results labelled by subsets of sources and allows us to argue about the most appropriate evaluation. The approach is illustrated with an experiment on a real-world application to decision aid in agricultural practices. © 2011 IEEE.","Formal concept analysis; Lattice of arguments; Numerical information fusion; Pattern structure; Reliability","Agricultural practices; Consistent subset; Formal Concept Analysis; Information fusion method; Lattice of arguments; Numerical information; Pattern structure; Real-world application; Artificial intelligence; Decision support systems; Information analysis; Merging; Reliability; Textile printing; Information fusion"
"Assaghir Z., Kaytoue M., Napoli A., Prade H.","Managing information fusion with formal concept analysis","10.1007/978-3-642-16292-3_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956262304&doi=10.1007%2f978-3-642-16292-3_12&partnerID=40&md5=17f1360476a669e226aae068440e9bd2","The main problem addressed in this paper is the merging of numerical information provided by several sources (databases, experts...). Merging pieces of information into an interpretable and useful format is a tricky task even when an information fusion method is chosen. Fusion results may not be in suitable form for being used in decision analysis. This is generally due to the fact that information sources are heterogeneous and provide inconsistent information, which may lead to imprecise results. In this paper, we propose the use of Formal Concept Analysis and more specifically pattern structures for organizing the results of fusion methods. This allows us to associate any subset of sources with its information fusion result. Once a fusion operator is chosen, a concept lattice is built. With examples throughout this paper, we show that this concept lattice gives an interesting classification of fusion results. When the fusion global result is too imprecise, the method enables the users to identify what maximal subset of sources that would support a more precise and useful result. Instead of providing a unique fusion result, the method yields a structured view of partial results labelled by subsets of sources. Finally, an experiment on a real-world application has been carried out for decision aid in agricultural practices. © Springer-Verlag Berlin Heidelberg 2010.",,"Agricultural practices; Concept Lattices; Decision analysis; Formal Concept Analysis; Fusion methods; Fusion operator; Inconsistent information; Information fusion method; Information sources; Numerical information; Pattern structure; Real-world application; Agricultural practices; Concept Lattices; Fusion operator; Inconsistent information; Information fusion method; Information sources; Numerical information; Pattern structure; Artificial intelligence; Decision support systems; Decision theory; Information fusion; Merging; Artificial intelligence; Decision support systems; Decision theory; Information analysis; Information fusion; Merging; Information analysis; Formal concept analysis"
"Assegie T.A., Karpagam T., Mothukuri R., Tulasi R.L., Engidaye M.F.","Extraction of human understandable insight from machine learning model for diabetes prediction","10.11591/eei.v11i2.3391","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128301761&doi=10.11591%2feei.v11i2.3391&partnerID=40&md5=5b740535e685d34232d2db4278947e88","Explaining the reason for model’s output as diabetes positive or negative is crucial for diabetes diagnosis. Because, reasoning the predictive outcome of model helps to understand why the model predicted an instance into diabetes positive or negative class. In recent years, highest predictive accuracy and promising result is achieved with simple linear model to complex deep neural network. However, the use of complex model such as ensemble and deep learning have trade-off between accuracy and interpretability. In response to the problem of interpretability, different approaches have been proposed to explain the predictive outcome of complex model. However, the relationship between the proposed approaches and the preferred approach for diabetes prediction is not clear. To address this problem, the authors aimed to implement and compare existing model interpretation approaches, local interpretable model agnostic explanation (LIME), shapely additive explanation (SHAP) and permutation feature importance by employing extreme boosting (XGBoost). Experiment is conducted on diabetes dataset with the aim of investigating the most influencing feature on model output. Overall, experimental result evidently appears to reveal that blood glucose has the highest impact on model prediction outcome. © 2022, Institute of Advanced Engineering and Science. All rights reserved.","Diabetes prediction; Extreme boosting (XGBoost); Interpretable model; LIME; SHAP",
"Assibong P.A., Wogu I.A.P., Sholarin M.A., Misra S., Damasevičius R., Sharma N.","The Politics of Artificial Intelligence Behaviour and Human Rights Violation Issues in the 2016 US Presidential Elections: An Appraisal","10.1007/978-981-13-9364-8_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075237299&doi=10.1007%2f978-981-13-9364-8_22&partnerID=40&md5=03d41b48d9488035be63d64ea252965b","The outcry from organizations and concerned bodies responsible for regulating and protecting the security and privacy of citizens’ data have suddenly been on the increase since the unearthing of the recent scandal regarding data gathered from the users of Facebook by Cambridge Analytica (CA) without prior consent. Data belonging to over 87 million Facebook users were admittedly passed on to third-party organizations who used them to manipulate the views and opinions of American citizens, a move largely believed influenced and altered the results and outcome of the 2016 US presidential elections in favour of certain individuals, an action largely believed undermines the Fundamental Human Rights (FHR) of the citizens affected. The Marxian Alienation Theory and Derrider’s critical and qualitative analytical method for evaluating arguments and data gathered on the subject matter of the paper were adopted for the study, with the view to highlight the inimical influences of AI politicking on the FHR of Americans and her institution of democracy. The paper observed that the Facebook and CA scandal has given room for questioning the integrity and credibility of future election outcomes in American. There is an alarming dearth of viable regulations and ethical codes governing the reckless use of AI politicking platforms in the public domain. The paper suggests measures of improving and securing the private rights and data of citizens generally. It also suggests measures for preserving the sanctity and credibility of future election results in American. © 2020, Springer Nature Singapore Pte Ltd.","American elections; American politics; Artificial intelligence behaviour; Artificial intelligence politicking; Cambridge Analytica; Facebook; Fundamental human rights; Human rights violations; Marxian alienation theory; Presidential elections","Artificial intelligence; Information management; Laws and legislation; Social aspects; Social networking (online); American elections; American politics; Cambridge; Facebook; Human rights; Marxian alienation theory; Presidential election; Behavioral research"
"Assous F., Chaskalovic J.","Indeterminate constants in numerical approximations of PDEs: A pilot study using data mining techniques","10.1016/j.cam.2013.12.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901244657&doi=10.1016%2fj.cam.2013.12.015&partnerID=40&md5=771ff7f2a46c82cf4dccfbd106674cea","Rolle's theorem, and therefore, Lagrange and Taylor's theorems are responsible for the inability to determine precisely the error estimate of numerical methods applied to partial differential equations. Basically, this comes from the existence of a non unique unknown point which appears in the remainder of Taylor's expansion. In this paper we consider the case of finite elements method. We show in detail how Taylor's theorem gives rise to indeterminate constants in the a priori error estimates. As a consequence, we highlight that classical conclusions have to be reformulated if one considers local error estimate. To illustrate our purpose, we consider the implementation of P1 and P2 finite elements method to solve Vlasov-Maxwell equations in a paraxial configuration. If the Bramble-Hilbert theorem claims that global error estimates for finite elements P2 are ""better"" than the P1 ones, we show how data mining techniques are powerful to identify and to qualify when and where local numerical results of P1 and P2 are equivalent. © 2013 Elsevier B.V. All rights reserved.","Data mining; Error estimates; Finite element; Vlasov-Maxwell","Finite element method; Error estimates; Local error estimate; Numerical approximations; Numerical results; Priori error estimate; Taylor's expansion; Vlasov-Maxwell; Vlasov-Maxwell equations; Data mining"
"Asthana S., Kwatra S., Wolf C.T., Chowdhary P., Nakamura T.","Human-in-the-Loop Business Modelling for Emergent External Factors","10.1109/BigData50022.2020.9377743","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103848722&doi=10.1109%2fBigData50022.2020.9377743&partnerID=40&md5=14e9ea4bdc44de2557aaee5151dc9c1b","In the face of emergent external factors (e.g., supply chain disruptions or public health crises like COVID-19), businesses must adapt their business model quickly in order to ensure service continuity. However, providing recommendations regarding changes should be made to the business model is a challenging problem. First, it requires details of interactions between different components of the business (e.g., service offerings, inventory, staffing, demand) to understand what possible courses of action will have the most business impact. Second, automated models may provide recommendations on changes required in the business operations. However, with lack of human insight, it will be hard to verify the feasibility of these recommendations. Third, a generic model may not be able to provide good recommendations for diverse set of business models. Fourth, the model may not have enough features or training data to provide good recommendations.In this paper, we propose a novel approach to provide actionable items that can be recommended to business users given their business features and recommendations given to businesses in similar domain. Here we first use clustering to find the business domain and similar feature set of the domain. Then, we build a machine-learning model with explainable insights to provide recommendations on different business actions that can be taken to ensure business operations in the face of emergent external factors. Next we augment our approach with human-in-the-loop to improve its performance. Finally, we federate the machine-learning model in a similar domain to add more explainable and trusted insights and recommendations by other businesses. We describe our method, illustrate its utility with results from our implementation, and discuss areas for future work. © 2020 IEEE.","Business operations; Emergent factors; Explainability; Federated Learning; Human-in-the-loop","Machine learning; Supply chains; Turing machines; Business modeling; Business modelling; Business operation; External factors; Human-in-the-loop; Machine learning models; Service continuity; Supply-chain disruptions; Big data"
"Asudeh A., Shahbazi N., Jin Z., Jagadish H.V.","Identifying Insufficient Data Coverage for Ordinal Continuous-Valued Attributes","10.1145/3448016.3457315","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108946437&doi=10.1145%2f3448016.3457315&partnerID=40&md5=6289b04db6c91433d369b24da895208e","Appropriate training data is a requirement for building good machine-learned models. In this paper, we study the notion of coverage for ordinal and continuous-valued attributes, by formalizing the intuition that the learned model can accurately predict only at data points for which there are ""enough""similar data points in the training data set. We develop an efficient algorithm to identify uncovered regions in low-dimensional attribute feature space, by making a connection to Voronoi diagrams. We also develop a randomized approximation algorithm for use in high-dimensional attribute space. We evaluate our algorithms through extensive experiments on real datasets. © 2021 ACM.","bias detection; fairness in machine learning; responsible data science; trustworthy AI","Database systems; Information management; Continuous-valued attribute; Data coverage; High-dimensional; Low dimensional; Randomized approximation; Real data sets; Training data sets; Voronoi diagrams; Approximation algorithms"
"Aswad F.M., Kareem A.N., Khudhur A.M., Khalaf B.A., Mostafa S.A.","Tree-based machine learning algorithms in the Internet of Things environment for multivariate flood status prediction","10.1515/jisys-2021-0179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120880023&doi=10.1515%2fjisys-2021-0179&partnerID=40&md5=474fc700d88440fbf984de317e8bbaaf","Floods are one of the most common natural disasters in the world that affect all aspects of life, including human beings, agriculture, industry, and education. Research for developing models of flood predictions has been ongoing for the past few years. These models are proposed and built-in proportion for risk reduction, policy proposition, loss of human lives, and property damages associated with floods. However, flood status prediction is a complex process and demands extensive analyses on the factors leading to the occurrence of flooding. Consequently, this research proposes an Internet of Things-based flood status prediction (IoT-FSP) model that is used to facilitate the prediction of the rivers flood situation. The IoT-FSP model applies the Internet of Things architecture to facilitate the flood data acquisition process and three machine learning (ML) algorithms, which are Decision Tree (DT), Decision Jungle, and Random Forest, for the flood prediction process. The IoT-FSP model is implemented in MATLAB and Simulink as development platforms. The results show that the IoT-FSP model successfully performs the data acquisition and prediction tasks and achieves an average accuracy of 85.72% for the three-fold cross-validation results. The research finding shows that the DT scores the highest accuracy of 93.22%, precision of 92.85, and recall of 92.81 among the three ML algorithms. The ability of the ML algorithm to handle multivariate outputs of 13 different flood textual statuses provides the means of manifesting explainable artificial intelligence and enables the IoT-FSP model to act as an early warning and flood monitoring system. © 2022 Firas Mohammed Aswad et al., published by De Gruyter.","explainable artificial intelligence; flood prediction; Internet of Things; machine learning; multivariate classification","Data acquisition; Decision trees; Disasters; Floods; Forecasting; Learning algorithms; Machine learning; Agriculture industries; Explainable artificial intelligence; Flood prediction; Human being; Machine learning algorithms; Multivariate classification; Natural disasters; Prediction modelling; Risks reduction; Tree-based; Internet of things"
"Aswin Vignesh N., George Amalarethinam D.I.","Rule Extraction for Diagnosis of Diabetes Mellitus Used for Enhancing Regular Covering Technique","10.1109/WCCCT.2016.34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039918722&doi=10.1109%2fWCCCT.2016.34&partnerID=40&md5=573eec2abfd1405bd388b333eab446e9","Diabetes is a complex disease then increasing in prevalence around the world. Type 2 diabetes mellitus (T2DM accounts for about 80-90% of all diagnosed adult cases of diabetes. Most present diagnostic methods for T2DM are black-box models, which are unable to provide the reasons underlying diagnosis to physicians therefore, algorithms that can provide further insight are needed. Rule extraction can provide such explanations. The extracted rules may not be only highly accurate, but also simple and easy to understand. Therefore in the study, The rule extraction algorithm is proposed for Enhancing regular covering technique(ERCT) to achieve highly accurate, concise, and interpretable classification rules for the pima Indian diabetes(PID) dataset, which comprises 768 samples with two classes(diabetes or non-diabetes) and eight continuous attributes. The result of the proposed algorithm is an average accuracy of 85.55% after 10 runs of 10-fold cross validation. Regular covering technique achieved substantially better accuracy and provided a considerably fewer average number of rules and antecedents. These results suggest that proposed algorithm, and are therefore more suitable for medical decision making including the diagnosis of all type of diabetes mellitus. © 2017 IEEE.","Data mining; Pima Indian diabetes; Regular Covering; Rule Extraction; sampling selection; Type 2 Diabetes mellitus","Classification (of information); Decision making; Diagnosis; Extraction; Pima Indian Diabetes; Regular Covering; Rule extraction; Sampling selections; Type 2 diabetes mellitus; Data mining"
"Atak Z.K., Taskiran I.I., Demeulemeester J., Flerin C., Mauduit D., Minnoye L., Hulselmans G., Christiaens V., Ghanem G.-E., Wouters J., Aerts S.","Interpretation of allele-specific chromatin accessibility using cell state–aware deep learning","10.1101/gr.260851.120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107711721&doi=10.1101%2fgr.260851.120&partnerID=40&md5=8d083ccafded4576d9e7a2bbb527425e","Genomic sequence variation within enhancers and promoters can have a significant impact on the cellular state and phenotype. However, sifting through the millions of candidate variants in a personal genome or a cancer genome, to identify those that impact cis-regulatory function, remains a major challenge. Interpretation of noncoding genome variation benefits from explainable artificial intelligence to predict and interpret the impact of a mutation on gene regulation. Here we generate phased whole genomes with matched chromatin accessibility, histone modifications, and gene expression for 10 melanoma cell lines. We find that training a specialized deep learning model, called DeepMEL2, on melanoma chromatin accessibility data can capture the various regulatory programs of the melanocytic and mesenchymal-like melanoma cell states. This model outperforms motif-based variant scoring, as well as more generic deep learning models. We detect hundreds to thousands of allele-specific chromatin accessibility variants (ASCAVs) in each melanoma genome, of which 15%–20% can be explained by gains or losses of transcription factor binding sites. A considerable fraction of ASCAVs are caused by changes in AP-1 binding, as confirmed by matched ChIP-seq data to identify allele-specific binding of JUN and FOSL1. Finally, by augmenting the DeepMEL2 model with ChIP-seq data for GABPA, the TERT promoter mutation, as well as additional ETS motif gains, can be identified with high confidence. In conclusion, we present a new integrative genomics approach and a deep learning model to identify and interpret functional enhancer mutations with allelic imbalance of chromatin accessibility and gene expression. © 2021 Atak et al.",,"transcription factor AP 1; transposase; allele; Article; binding site; chromatin; chromatin immunoprecipitation sequencing; controlled study; deep learning; ETS domain; gene expression; genome analysis; histone modification; human; human cell; melanoma cell; melanoma cell line; oncogenomics; promoter region; protein binding; transcriptome sequencing; whole genome sequencing; allele; artificial intelligence; genetics; Alleles; Artificial Intelligence; Chromatin; Deep Learning; Promoter Regions, Genetic"
"Atapour-Abarghouei A., Bonner S., McGough A.S.","Rank over Class: The Untapped Potential of Ranking in Natural Language Processing","10.1109/BigData52589.2021.9671386","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125297801&doi=10.1109%2fBigData52589.2021.9671386&partnerID=40&md5=8462c5c2b2f90d55ef156a06b54b8c36","Text classification has long been a staple within Natural Language Processing (NLP) with applications spanning across diverse areas such as sentiment analysis, recommender systems and spam detection. With such a powerful solution, it is often tempting to use it as the go-to tool for all NLP problems since when you are holding a hammer, everything looks like a nail. However, we argue here that many tasks which are currently addressed using classification are in fact being shoehorned into a classification mould and that if we instead address them as a ranking problem, we not only improve the model, but we achieve better performance. We propose a novel end-to-end ranking approach consisting of a Transformer network responsible for producing representations for a pair of text sequences, which are in turn passed into a context aggregating network outputting ranking scores used to determine an ordering to the sequences based on some notion of relevance. We perform numerous experiments on publicly-available datasets and investigate the applications of ranking in problems often solved using classification. In an experiment on a heavily- skewed sentiment analysis dataset, converting ranking results to classification labels yields an approximately 22% improvement over state-of-the-art text classification, demonstrating the efficacy of text ranking over text classification in certain scenarios. © 2021 IEEE.","Deep Learning; Natural Language Processing; Sentiment Analysis; Text Classification; Text Ranking","Classification (of information); Deep learning; Deep learning; End to end; Performance; Processing problems; Ranking approach; Ranking problems; Sentiment analysis; Spam detection; Text classification; Text rankings; Sentiment analysis"
"Ataş M.","Open Cezeri Library: A novel java based matrix and computer vision framework","10.1002/cae.21745","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969951526&doi=10.1002%2fcae.21745&partnerID=40&md5=eba63f3ae9472d810d2dc9f63140d455","In this paper we introduce the Open Cezeri Library (OCL) framework as a domain specific language (DSL) for researchers, scientists, and engineering students to enable them to develop basic linear algebra operations via simple matrix calculations, image processing, computer vision, and machine learning applications in JAVA programming language. OCL provides a strong intuition of coding for the developer while implementing by means of a fluent interface. The significant aspect of the OCL is to combine the methods of well-known platforms; MATLAB and JAVA, accordingly. Moreover, OCL supports a fluent interface so that users can extend a single line of codes by putting a dot between the methods because all the methods implemented actually return the host class. It was observed that the learning curve of the OCL is lower than the MATLAB and the native JAVA languages, and makes coding more readable, understandable, traceable, and enjoyable. In addition to this, the experiments revealed that the running performance of the OCL is quite comparable and can be used in a variety of diverse applications. © 2016 Wiley Periodicals, Inc. Comput Appl Eng Educ 24:736–743, 2016; View this article online at wileyonlinelibrary.com/journal/cae; DOI 10.1002/cae.21745. © 2016 Wiley Periodicals, Inc.","façade design pattern; fluent interface; matrix library; method chain; visualization tool","Artificial intelligence; Codes (symbols); Computational linguistics; Computer programming; Computer programming languages; Computer vision; DSL; Image processing; Learning systems; Linear algebra; MATLAB; Matrix algebra; Object oriented programming; Problem oriented languages; Design Patterns; Diverse applications; Domain specific languages; Linear algebra operations; Machine learning applications; Matrix calculations; Vision frameworks; Visualization tools; Java programming language"
"Athanasiadis I.N., Mitkas P.A., Laleci G.B., Kabak Y.","Embedding data-driven decision strategies on software agents: The case of a multi-agent system for monitoring air-quality indexes",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542645056&partnerID=40&md5=e7adee3bf2504fd739aeea2712af446a","This paper describes the design and deployment of an agent community, which is responsible for monitoring and assessing air quality, based on measurements generated by a meteorological station. Software agents acting as mediators or decision makers deliver validated information to the appropriate destinations. We outline the procedure for creating agent ontologies, agent types, and finally, for training agents based on historical data volumes. The C4.5 algorithm for decision tree extraction is applied on meteorological and air-pollutant measurements. The decision models extracted are related to the validation of incoming measurements and to the estimation of missing or erroneous measurements. Emphasis is given on the agent training process, which must embed these data-driven decision models on software agents in a simple and effortless way. We developed a prototype system, which demonstrates the advantages of agent-based solutions for intelligent environmental applications.",,"Air quality; Algorithms; Data mining; Environmental engineering; Information retrieval; Intelligent agents; Knowledge engineering; Software prototyping; Concurrent programming; Decision tree; Multi agent systems"
"Athanasiou M., Sfrintzeri K., Zarkogianni K., Thanopoulou A.C., Nikita K.S.","An explainable XGBoost-based approach towards assessing the risk of cardiovascular disease in patients with Type 2 Diabetes Mellitus","10.1109/BIBE50027.2020.00146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099607111&doi=10.1109%2fBIBE50027.2020.00146&partnerID=40&md5=f8ce851a2dd6b27005759f63d333e9ef","Cardiovascular Disease (CVD) is an important cause of disability and death among individuals with Diabetes Mellitus (DM). International clinical guidelines for the management of Type 2 DM (T2DM) are founded on primary and secondary prevention and favor the evaluation of CVD-related risk factors towards appropriate treatment initiation. CVD risk prediction models can provide valuable tools for optimizing the frequency of medical visits and performing timely preventive and therapeutic interventions against CVD events. The integration of explainability modalities in these models can enhance human understanding on the reasoning process, maximize transparency and embellish trust towards the models' adoption in clinical practice. The aim of the present study is to develop and evaluate an explainable personalized risk prediction model for the fatal or non-fatal CVD incidence in T2DM individuals. An explainable approach based on the eXtreme Gradient Boosting (XGBoost) and the Tree SHAP (SHapley Additive exPlanations) method is deployed for the calculation of the 5-year CVD risk and the generation of individual explanations on the model's decisions. Data from the 5-year follow up of 560 patients with T2DM are used for development and evaluation purposes. The obtained results (AUC=71.13%) indicate the potential of the proposed approach to handle the unbalanced nature of the used dataset, while providing clinically meaningful insights about the model's decision process. © 2020 IEEE.","Cardiovascular Disease; Diabetes; explainability; interpretability; machine learning; unbalanced data","Bioinformatics; Cardiology; Diseases; Risk assessment; Cardiovascular disease; Clinical guideline; Clinical practices; Human understanding; Risk prediction models; Secondary prevention; Therapeutic intervention; Type 2 diabetes mellitus; Predictive analytics"
"Athreya A.P., Brückl T., Binder E.B., John Rush A., Biernacka J., Frye M.A., Neavin D., Skime M., Monrad D., Iyer R.K., Mayes T., Trivedi M., Carter R.E., Wang L., Weinshilboum R.M., Croarkin P.E., Bobo W.V.","Prediction of short-term antidepressant response using probabilistic graphical models with replication across multiple drugs and treatment settings","10.1038/s41386-020-00943-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100014047&doi=10.1038%2fs41386-020-00943-x&partnerID=40&md5=70108b8b203789cdf610226fffab474f","Heterogeneity in the clinical presentation of major depressive disorder and response to antidepressants limits clinicians’ ability to accurately predict a specific patient’s eventual response to therapy. Validated depressive symptom profiles may be an important tool for identifying poor outcomes early in the course of treatment. To derive these symptom profiles, we first examined data from 947 depressed subjects treated with selective serotonin reuptake inhibitors (SSRIs) to delineate the heterogeneity of antidepressant response using probabilistic graphical models (PGMs). We then used unsupervised machine learning to identify specific depressive symptoms and thresholds of improvement that were predictive of antidepressant response by 4 weeks for a patient to achieve remission, response, or nonresponse by 8 weeks. Four depressive symptoms (depressed mood, guilt feelings and delusion, work and activities and psychic anxiety) and specific thresholds of change in each at 4 weeks predicted eventual outcome at 8 weeks to SSRI therapy with an average accuracy of 77% (p = 5.5E-08). The same four symptoms and prognostic thresholds derived from patients treated with SSRIs correctly predicted outcomes in 72% (p = 1.25E-05) of 1996 patients treated with other antidepressants in both inpatient and outpatient settings in independent publicly-available datasets. These predictive accuracies were higher than the accuracy of 53% for predicting SSRI response achieved using approaches that (i) incorporated only baseline clinical and sociodemographic factors, or (ii) used 4-week nonresponse status to predict likely outcomes at 8 weeks. The present findings suggest that PGMs providing interpretable predictions have the potential to enhance clinical treatment of depression and reduce the time burden associated with trials of ineffective antidepressants. Prospective trials examining this approach are forthcoming. © 2021, The Author(s).",,"duloxetine; escitalopram; fluoxetine; paroxetine; serotonin noradrenalin reuptake inhibitor; serotonin uptake inhibitor; sertraline; tricyclic antidepressant agent; antidepressant agent; drug; serotonin uptake inhibitor; anxiety; Article; demography; hospital patient; human; information processing; major depression; outpatient; prediction; priority journal; probability; prognostic assessment; prospective study; remission; treatment response; unsupervised machine learning; Antidepressive Agents; Depressive Disorder, Major; Humans; Pharmaceutical Preparations; Prospective Studies; Serotonin Uptake Inhibitors"
"Atighetchi M., Benyo B., Gosain A., MacIntyre R., Pal P., Travers V., Zinky J.","Transparent insertion of custom logic in HTTP(S) streams using PbProxy","10.1109/MIC.2010.103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955552295&doi=10.1109%2fMIC.2010.103&partnerID=40&md5=87ea5b0bbc2d851e9a7e259352a93bdc","Cost and testing considerations limit the acceptance and deployment of technologies that make information exchanges more secure, reliable, semantically understandable, and self-improving. PbProxy is a flexible proxy that enables transparent insertion of custom logic into HTTP and HTTPS interactions. It has successfully been used to facilitate behavior-based prevention of phishing attacks, machine learning of Web service procedures, and Web browsing over disruption-tolerant networks by injecting custom logic into existing applications and communication streams. PbProxy encapsulates common functionality into a proxy base and supports customizable plugins to foster code reuse. © 2011 IEEE.","artificial intelligence; Distributed systems; middleware technologies; networking and communications; security and privacy","Behavior-based; Code reuse; Customizable; Disruption tolerant networks; Distributed systems; Information exchanges; Machine-learning; middleware technologies; networking and communications; Phishing attacks; Plug-ins; security and privacy; Web browsing; Artificial intelligence; Middleware; Network security; Web services; HTTP"
"Atkin J.A.D., Burke E.K., Greenwood J.S., Reeson D.","An examination of take-off scheduling constraints at London Heathrow airport","10.1007/s12469-009-0011-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952972152&doi=10.1007%2fs12469-009-0011-z&partnerID=40&md5=7443e519238998cdd0cc50a973275424","In this paper, we focus upon the departure system for London Heathrow airport, one of the busiest airports in the world. Decreasing the delay for aircraft awaiting take-off with their engines running would decrease fuel usage and have consequent cost and pollution benefits. We explain how the departure system at Heathrow currently works and we describe the various constraints that apply to take-off schedules. A model for the take-off order problem is presented from the point of view of the runway controller, the person who is responsible for the take-off scheduling. We investigate the effects of each constraint and combination of constraints, using a simulation of the Heathrow departure system. The role of the runway controller in the simulation is performed by a search which was designed to form the basis of an online decision support system. Both the simulation and the decision support system are fully described. We use the results to evaluate the effect upon delay that we would expect from various changes that could be made to the departure system. We end the paper by drawing conclusions about the predicted effectiveness of different changes that could be made to the departure system and focus upon a further opportunity for decision support research. © 2009 Springer-Verlag.",,"Decision supports; London Heathrow airport; Online decisions; Order problems; Take-off scheduling; Airports; Artificial intelligence; Decision making; Decision theory; Scheduling; Takeoff; Decision support systems"
"Atkinson D.J., Clark M.H.","Autonomous agents and human interpersonal trust: Can we engineer a human-machine social interface for trust?",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883343068&partnerID=40&md5=6cc69b32f8ad320818d9945e394148aa","There is a recognized need to employ autonomous agents in domains that are not amenable to conventional automation and/or which humans find difficult, dangerous, or undesirable to perform. These include time-critical and mission-critical applications in health, defense, transportation, and industry, where the consequences of failure can be catastrophic. A prerequisite for such applications is the establishment of well-calibrated trust in autonomous agents. Our focus is specifically on human-machine trust in deployment and operations of autonomous agents, whether they are embodied in cyber-physical systems, robots, or exist only in the cyber-realm. The overall aim of our research is to investigate methods for autonomous agents to foster, manage, and maintain an appropriate trust relationship with human partners when engaged in joint, mutually interdependent activities. Our approach is grounded in a systems-level view of humans and autonomous agents as components in (one or more) encompassing meta-cognitive systems. Given human predisposition for social interaction, we look to the multi-disciplinary body of research on human interpersonal trust as a basis from which we specify engineering requirements for the interface between human and autonomous agents. If we make good progress in reverse engineering this ""human social interface,"" it will be a significant step towards devising the algorithms and tests necessary for trustworthy and trustable autonomous agents. This paper introduces our program of research and reports on recent progress. © 2013, Association for the Advancement of artificial intelligence.",,"Cyber physical systems (CPSs); Metacognitives; Mission critical applications; Recent progress; Social interactions; Social interfaces; Time-critical; Trust relationship; Cognitive systems; Embedded systems; Research; Reverse engineering; Autonomous agents"
"Atkinson G., Metsis V.","Identifying label noise in time-series datasets","10.1145/3410530.3414366","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091872872&doi=10.1145%2f3410530.3414366&partnerID=40&md5=ac9099e0ac18a51ead3330d10e7a0ce9","Reliably labeled datasets are crucial to the performance of supervised learning methods. Time-series data pose additional challenges. Data points lying on borders between classes can be mislabeled due to perception limitations of human labelers. Sensor measurements may not be directly interpretable by humans. Thus label noise cannot be manually removed. As a result, time-series datasets often contain a significant amount of label noise that can degrade the performance of machine learning models. This work focuses on label noise identification and removal by extending previous methods developed for static instances to the domain of time-series data. We use a combination of deep learning and visualization algorithms to facilitate automatic noise removal. We show that our approach can identify mislabeled instances, which results in improved classification accuracy on four synthetic and two real publicly available human activity datasets. © 2020 ACM.","accelerometer; CNN; human activity recognition; label cleaning; label noise; neural networks; time-series data","Classification (of information); Deep learning; Time series; Ubiquitous computing; Wearable computers; Classification accuracy; Human activities; Machine learning models; Noise identification; Sensor measurements; Supervised learning methods; Time-series data; Visualization algorithms; Learning systems"
"Atkinson K., Collenette J., Bench-Capon T., Dzehtsiarou K.","Practical tools from formal models: The ECHR as a case study","10.1145/3462757.3466095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112365822&doi=10.1145%2f3462757.3466095&partnerID=40&md5=fbfb27dcc6041a4381731af7a0f30710","One approach to building legal support systems is to run an executable model of the relevant knowledge through an interface designed to collect information from the user and provide explanations. The usability of such systems depends on the terms used in the law being represented: often only users familiar with the practice and application of the law will be able to provide the required information. Earlier work applied this approach to the European Convention on Human Rights (ECHR). Although the performance of the tool built for that domain was good, the questions posed to the user demanded a good deal of knowledge and experience of the ECHR. Here we use the knowledge of an expert with extensive experience of the ECHR to extend the model, through intermediate levels, to identify questions that are appropriate to the target user. We have undertaken a pilot evaluation in which a small number of lawyers have used the prototype program and provided very positive feedback, showing that they are receptive to AI solutions that give effective, explainable decision support. © 2021 ACM.","ADFs; case-based reasoning; ECHR; explainability","Decision support systems; Laws and legislation; Decision supports; Executable model; Formal model; Human rights; Intermediate level; Knowledge and experience; Legal supports; Artificial intelligence"
"Atkinson K., Bench-Capon T., Bollegala D.","Explanation in AI and law: Past, present and future","10.1016/j.artint.2020.103387","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091241809&doi=10.1016%2fj.artint.2020.103387&partnerID=40&md5=c93bc6e551922b0a4248ea0f1dcf6c14","Explanation has been a central feature of AI systems for legal reasoning since their inception. Recently, the topic of explanation of decisions has taken on a new urgency, throughout AI in general, with the increasing deployment of AI tools and the need for lay users to be able to place trust in the decisions that the support tools are recommending. This paper provides a comprehensive review of the variety of techniques for explanation that have been developed in AI and Law. We summarise the early contributions and how these have since developed. We describe a number of notable current methods for automated explanation of legal reasoning and we also highlight gaps that must be addressed by future systems to ensure that accurate, trustworthy, unbiased decision support can be provided to legal professionals. We believe that insights from AI and Law, where explanation has long been a concern, may provide useful pointers for future development of explainable AI. © 2020 Elsevier B.V.","AI and law; Case-based reasoning; Computational models of argument; Explainable AI","Artificial intelligence; AI and law; AI systems; Decision supports; Legal reasoning; Support tool; Decision support systems"
"Atkinson P.M., Stein A., Jeganathan C.","Spatial sampling, data models, spatial scale and ontologies: Interpreting spatial statistics and machine learning applied to satellite optical remote sensing","10.1016/j.spasta.2022.100646","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126311840&doi=10.1016%2fj.spasta.2022.100646&partnerID=40&md5=96ed425415761e3f315a3ab8a3b47e33","This paper summarizes the development and application of spatial statistical models in satellite optical remote sensing. The paper focuses on the development of a conceptual model that includes the measurement and sampling processes inherent in remote sensing. We organized this paper into five main sections: introducing the basis of remote sensing, including measurement and sampling; spatial variation, including variation through the object-based data model; advances in spatial statistical modelling; machine learning and explainable AI; a hierarchical ontological model of the nature of remotely sensed scenes. The paper finishes with a summary. We conclude that optical remote sensing provides an important source of data and information for the development of spatial statistical techniques that, in turn, serve as powerful tools to obtain important information from the images. © 2022 The Authors","Ontology; Remote sensing; Sampling; Scale; Spatial statistical modelling",
"Atluri V., Hong Y., Chun S.A.","Security, privacy and trust for responsible innovations and governance","10.1145/3396956.3396978","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086900139&doi=10.1145%2f3396956.3396978&partnerID=40&md5=83f76d9a223276c38ff434df02658aaa","Cybersecurity, Privacy and Trust issues are of paramount importance in the data-driven smart government and smart society. In this tutorial, we present the traditional information security and technical solutions, the privacy and privacy enhancing technologies (PET), as well as novel data integrity and trust issues that are rapidly emerging in the cyberspace with social media and AI technology advances. The goal of the tutorial is to have the government policy makers be aware of the security, privacy and trust risks and their impacts in the intelligent society, and be able to have multiple options to consider, such as technical, social, and legal/policy solutions and framework to enhance the trust in the cyber and real government. © 2020 ACM.","AI; Data; Disinformation; Privacy; Responsible innovation and governance; Security; Trust","Privacy by design; AI Technologies; Cyber security; Cyberspaces; Data integrity; Privacy and trusts; Privacy enhancing technologies; Social media; Technical solutions; Artificial intelligence"
"Atreja S., Dumrewal A., Aggarwal P., Basu A., Mohapatra P., Dasgupta G.B.","Citicafe: An interactive interface for citizen engagement","10.1145/3172944.3172955","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045442883&doi=10.1145%2f3172944.3172955&partnerID=40&md5=6df3b5cded123d04397128d46bd284ad","Community engagement is a new and emerging trend in urban cities driven by the mission of developing responsible citizenship. Technology is playing a vital role in helping this mission. For example, over the last couple of years, there have been a plethora of social media avenues to report civic issues and complaints. We present one such contribution of technology, in terms of an intelligent platform, ""Citicafe"". The platform has a conversation based interface that enhances citizen engagement by enabling a direct communication with them. The platform ingests data from different sources, which is exploited by a virtual agent to enable informed interactions. It can help citizens to (a) report problems and (b) gather information related to civic issues for different locations and their neighborhoods. We report the results of a user study carried out to establish the effectiveness of our interface and draw a comparison with an existing platform. A detailed qualitative and quantitative analysis of the survey results shows a definite and statistically significant (p < 0.05) preference for our interface over the existing platform. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Citizen engagement; Clustering; Conversational agent; CRF; Knowledge mining; Natural language; Social good; Topic modeling","Data mining; Modeling languages; Surveys; Citizen engagements; Clustering; Conversational agents; Knowledge mining; Natural languages; Social good; Topic Modeling; User interfaces"
"Atri H., Varshney A., Mehra S., Parashar R.","Design and development of wizardry gauntlet","10.23919/INDIACom49435.2020.9083704","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085241348&doi=10.23919%2fINDIACom49435.2020.9083704&partnerID=40&md5=5720120039d26b1203659ef099d18eec","Gesture-based language is a unique methodology for communication among conventional and hearing-impaired. Nevertheless, people see an issue in human activity with standard people who don't understand their signing. In this manner, they ceaselessly feel a barrier to remove this boundary. This paper proposes a device that would convert hand signals of nonconventional persons into voice so that an individual can understand their gestures. The proposed device will help nonconventional people to represent their thoughts. Internet of Things (IoT) and Machine Learning (ML) algorithms have been used for the betterment of the system. The proposed methodology consists of two sides: transmitter side and receiver side. The transmitter side is responsible for sending electrical signals wirelessly through Arduino combined with Radio Frequency (RF) transmitter and flex sensors connected through wires. On the receiver side, Global System for Mobile Communication (GSM), RF receiver, Liquid Crystal Display (LCD) and Arduino are used to receive the electrical signals and display the output on the screen.The proposed system allows the person to send or monitor the health condition of the user during their critical time and have made the communication process wireless. Also, the output through the system would surely help in transitioning day-day life decision making of non-conventional people. © 2020 Bharati Vidyapeeth, New Delhi.","Actuators; Arduino UNO Board; Flex sensors; Gauntlet; Sound and Visual structure; Synthesizer circuit; Wires bound","Audition; Behavioral research; Decision making; Global system for mobile communications; Internet of things; Liquid crystals; Machine learning; Radio transmission; Transmitters; Communication process; Design and Development; Electrical signal; Global system for mobile communications (GSM); Hearing impaired; Internet of Things (IOT); Liquid crystal display(LCD); Radio frequency transmitters; Liquid crystal displays"
"Atsawarungruangkit A., Laoveeravat P., Promrat K.","Machine learning models for predicting non-alcoholic fatty liver disease in the general United States population: NHANES database","10.4254/wjh.v13.i10.1417","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123898370&doi=10.4254%2fwjh.v13.i10.1417&partnerID=40&md5=d3cf3f0a58c7a92b59f415cabe18910b","Background Non-alcoholic fatty liver disease (NAFLD) is the most common chronic liver disease, affecting over 30% of the United States population. Early patient identification using a simple method is highly desirable. Aim To create machine learning models for predicting NAFLD in the general United States population. Methods Using the NHANES 1988-1994. Thirty NAFLD-related factors were included. The dataset was divided into the training (70%) and testing (30%) datasets. Twentyfour machine learning algorithms were applied to the training dataset. The bestperforming models and another interpretable model (i.e., coarse trees) were tested using the testing dataset. Results There were 3235 participants (n = 3235) that met the inclusion criteria. In the training phase, the ensemble of random undersampling (RUS) boosted trees had the highest F1 (0.53). In the testing phase, we compared selective machinelearning models and NAFLD indices. Based on F1, the ensemble of RUS boosted trees remained the top performer (accuracy 71.1% and F1 0.56) followed by the fatty liver index (accuracy 68.8% and F1 0.52). A simple model (coarse trees) had an accuracy of 74.9% and an F1 of 0.33. CONCLUSION Not every machine learning model is complex. Using a simpler model such as coarse trees, we can create an interpretable model for predicting NAFLD with only two predictors: fasting C-peptide and waist circumference. Although the simpler model does not have the best performance, its simplicity is useful in clinical practice © The Author(s) 2021. Published by Baishideng Publishing Group Inc. All rights reserved","Artificial intelligence; Fatty liver; Machine learning; NHANES; Non-alcoholic fatty liver disease; United States population",
"Attaluri P.K., Chen Z., Lu G.","Applying neural networks to classify influenza virus antigenic types and hosts","10.1109/CIBCB.2010.5510726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955633246&doi=10.1109%2fCIBCB.2010.5510726&partnerID=40&md5=00ad84978fb41d87b6b22f20d04ea024","Influenza viruses continue to evolve rapidly and are responsible for seasonal epidemics and occasional, but catastrophic, pandemics. We recently demonstrated the use of decision tree and support vector machine methods in classifying pandemic swine flu viral strains with high accuracy. Here, we applied the technique of artificial neural networks for the prediction of important influenza virus antigenic types (H1, H3, and H5) and hosts (Human, Avian, and Swine), which fulfills a critical need for a computational system for influenza surveillance. A comprehensive experiment on different k-mers and different binary encoding types showed classification based upon frequencies of k-mer nucleotide strings performed better than transformed binary data of nucleotides. It has been found for the first time that the accuracy of virus classification varies from host to host and from gene segment to gene segment. In particular, compared to avian and swine viruses, human influenza viruses can be classified with high accuracy, which indicates influenza virus strains might have become well adapted to their human host and hence less variation occurs in human viruses. In addition, the accuracy of host classification varies from genome segment to segment, achieving the highest values when using the HA and NA segments for human host classification. This research, along with our previous studies, shows machine learning techniques play an indispensable role in virus classification. © IEEE.",,"Artificial Neural Network; Binary data; Binary encodings; Comprehensive experiments; Computational system; Human influenzas; Human virus; Influenza virus; Machine learning techniques; Support vector machine method; Bioinformatics; Decision trees; Neural networks; Nucleotides; Viruses; Computer viruses"
"Attanasio A., da Silva C., Krehbiel P.","Electrostatic Conditions That Produce Fast Breakdown in Thunderstorms","10.1029/2021JD034829","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116751746&doi=10.1029%2f2021JD034829&partnerID=40&md5=fbd92653cb51527fa3de6c473de13f14","Fast breakdown (FB), a breakdown process composed of systems of high-velocity streamers, has been observed to precede lightning leader formation and play a critical role in lightning initiation. Vigorous FB events are responsible for the most powerful natural radio emissions on Earth, known as narrow bipolar events (NBEs). In this paper, an improved version of the Griffiths and Phelps (1976, https://doi.org/10.1029/jc081i021p03671) model of streamer breakdown is used alongside supervised machine learning techniques to probe the required electric fields and potentials inside thunderstorms to produce FB and NBEs. Our results show that the electrostatic conditions needed to produce (Formula presented.) FB observed in New Mexico at 9 km altitude and (Formula presented.) FB in Florida at 14 km altitude are about the same, each requiring about 100 MV potential difference to propagate 500 m. Additionally, the model illustrates how electric field enhancement ahead of propagating FB can initiate rebounding FB of the opposite polarity. © 2021. American Geophysical Union. All Rights Reserved.","fast breakdown; lightning; lightning initiation; machine learning; NBE; streamer","electric field; lightning; machine learning; thunderstorm; Florida [United States]; New Mexico; United States; Florida"
"Attanasio A., Piscitelli M.S., Chiusano S., Capozzoli A., Cerquitelli T.","Towards an automated, fast and interpretable estimation model of heating energy demand: A data-driven approach exploiting building energy certificates","10.3390/en12071273","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065540047&doi=10.3390%2fen12071273&partnerID=40&md5=59db3472944a1062adc0cfbee218868d","Energy performance certification is an important tool for the assessment and improvement of energy efficiency in buildings. In this context, estimating building energy demand also in a quick and reliable way, for different combinations of building features, is a key issue for architects and engineers who wish, for example, to benchmark the performance of a stock of buildings or optimise a refurbishment strategy. This paper proposes a methodology for (i) the automatic estimation of the building Primary Energy Demand for space heating (PEDh) and (ii) the characterization of the relationship between the PEDh value and the main building features reported by Energy Performance Certificates (EPCs). The proposed methodology relies on a two-layer approach and was developed on a database of almost 90,000 EPCs of flats in the Piedmont region of Italy. First, the classification layer estimates the segment of energy demand for a flat. Then, the regression layer estimates the PEDh value for the same flat. A different regression model is built for each segment of energy demand. Four different machine learning algorithms (Decision Tree, Support Vector Machine, Random Forest, Artificial Neural Network) are used and compared in both layers. Compared to the current state-of-the-art, this paper brings a contribution in the use of data mining techniques for the asset rating of building performance, introducing a novel approach based on the use of independent data-driven models. Such configuration makes the methodology flexible and adaptable to different EPCs datasets. Experimental results demonstrate that the proposed methodology can estimate the energy demand with reasonable errors, using a small set of building features. Moreover, the use of Decision Tree algorithm enables a concise interpretation of the quantitative rules used for the estimation of the energy demand. The methodology can be useful during both designing and refurbishment of buildings, to quickly estimate the expected building energy demand and set credible targets for improving performance. © 2019 by the authors.","Artificial neural network; Buildings; Classification; Data mining; Decision tree; Energy performance certificate; Heating energy demand; Random forest; Regression; Support vector machine","Benchmarking; Buildings; Classification (of information); Data mining; Decision trees; Energy management; Learning algorithms; Machine learning; Neural networks; Regression analysis; Support vector machines; Assessment and improvement; Building energy demands; Decision-tree algorithm; Energy efficiency in buildings; Energy performance; Heating energy; Random forests; Regression; Energy efficiency"
"Attanasio G., Cagliero L., Baralis E.","Leveraging the explainability of associative classifiers to support quantitative stock trading","10.1145/3401832.3402679","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091960859&doi=10.1145%2f3401832.3402679&partnerID=40&md5=4764018257a84d813ebe43c42281e3b4","Forecasting the stock market is particularly challenging due to the presence of a variety of inter-related economic and political factors. In recent years, the application of Machine Learning algorithms in quantitative stock trading systems has become established, as it enables a data-driven approach to investing in the financial markets. However, most professional traders still look for an explanation of automatically generated signals to verify their adherence to technical and fundamental rules. This paper presents an explainable approach to stock trading. It investigates the use of classification rules, which represent reliable associations between a set of discrete indicator values and the target class, to address next-day stock price prediction. Adopting associative classifiers in short-term stock trading not only provides reliable signals but also allows domain experts to understand the rationale behind signal generation. The backtesting of a state-of-the-art associative classifier, relying on a lazy pruning strategy, has shown promising performance in terms of equity appreciation and robustness of the trading system to market drawdowns. © 2020 ACM.","associative classification; explainable AI; quantitative trading; stock price forecasting","Commerce; Data Science; Financial markets; Learning algorithms; Machine learning; Associative classifiers; Automatically generated; Classification rules; Data-driven approach; Political factors; Signal generation; Stock price prediction; Stock trading system; Electronic trading"
"Attari N., Heckmann M., Schlangen D.","From explainability to explanation: Using a dialogue setting to elicit annotations with justifications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091576096&partnerID=40&md5=1a9213295ce805f3658af2795bae160a","Despite recent attempts in the field of explainable AI to go beyond black box prediction models, typically already the training data for supervised machine learning is collected in a manner that treats the annotator as a “black box”, the internal workings of which remains unobserved. We present an annotation method where a task is given to a pair of annotators who collaborate on finding the best response. With this we want to shed light on the questions if the collaboration increases the quality of the responses and if this “thinking together” provides useful information in itself, as it at least partially reveals their reasoning steps. Furthermore, we expect that this setting puts the focus on explanation as a linguistic act, vs. explainability as a property of models. In a crowd-sourcing experiment, we investigated three different annotation tasks, each in a collaborative dialogical (two annotators) and monological (one annotator) setting. Our results indicate that our experiment elicits collaboration and that this collaboration increases the response accuracy. We see large differences in the annotators’ behavior depending on the task. Similarly, we also observe that the dialog patterns emerging from the collaboration vary significantly with the task. ©2019 Association for Computational Linguistics",,"Supervised learning; Annotation methods; Best response; Black boxes; Prediction model; Supervised machine learning; Training data; Predictive analytics"
"Attenberger U.I., Langs G.","How does Radiomics actually work? - Review","10.1055/a-1293-8953","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107054318&doi=10.1055%2fa-1293-8953&partnerID=40&md5=4fa86a02d466de6abc7ba9da4f7abb3e","Personalized precision medicine requires highly accurate diagnostics. While radiological research has focused on scanner and sequence technologies in recent decades, applications of artificial intelligence are increasingly attracting scientific interest as they could substantially expand the possibility of objective quantification and diagnostic or prognostic use of image information. In this context, the term radiomics describes the extraction of quantitative features from imaging data such as those obtained from computed tomography or magnetic resonance imaging examinations. These features are associated with predictive goals such as diagnosis or prognosis using machine learning models. It is believed that the integrative assessment of the feature patterns thus obtained, in combination with clinical, molecular and genetic data, can enable a more accurate characterization of the pathophysiology of diseases and more precise prediction of therapy response and outcome. This review describes the classical radiomics approach and discusses the existing very large variability of approaches. Finally, it outlines the research directions in which the interdisciplinary field of radiology and computer science is moving, characterized by increasingly close collaborations and the need for new educational concepts. The aim is to provide a basis for responsible and comprehensible handling of the data and analytical methods used. Key points: Radiomics is playing an increasingly important role in imaging research. Radiomics has great potential to meet the requirements of precision medicine. Radiomics analysis is still subject to great variability. There is a need for quality-assured application of radiomics in medicine. Citation Format Attenberger UI, Langs G,. How does Radiomics actually work? - Review. Fortschr Röntgenstr 2021; 193: 652-657. © 2021 Georg Thieme Verlag. All rights reserved.","artificial intelligence; features; machine learning; radiomics","classification algorithm; feature extraction; feature selection; human; image processing; image segmentation; information processing; priority journal; radiomics; Review; validation process; artificial intelligence; biology; nuclear magnetic resonance imaging; personalized medicine; procedures; radiology; x-ray computed tomography; Artificial Intelligence; Computational Biology; Humans; Magnetic Resonance Imaging; Precision Medicine; Radiology; Tomography, X-Ray Computed"
"Attia P.M., Severson K.A., Witmer J.D.","Statistical Learning for Accurate and Interpretable Battery Lifetime Prediction","10.1149/1945-7111/ac2704","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116357137&doi=10.1149%2f1945-7111%2fac2704&partnerID=40&md5=e0b3be55a9306e7c0851109c0653a4c3","Data-driven methods for battery lifetime prediction are attracting increasing attention for applications in which the degradation mechanisms are poorly understood and suitable training sets are available. However, while advanced machine learning and deep learning methods promise high performance with minimal data preprocessing, simpler linear models with engineered features often achieve comparable performance, especially for small training sets, while also providing physical and statistical interpretability. In this work, we use a previously published dataset to develop simple, accurate, and interpretable data-driven models for battery lifetime prediction. We first present the ""capacity matrix""concept as a compact representation of battery electrochemical cycling data, along with a series of feature representations. We then create a number of univariate and multivariate models, many of which achieve comparable performance to the highest-performing models previously published for this dataset; thus, our work can serve as a comprehensive benchmarking study for this dataset. These models also provide insights into the degradation of these cells. Our approaches can be used both to quickly train models for a new battery cycling dataset and to benchmark the performance of more advanced machine learning methods. © 2021 The Author(s). Published on behalf of The Electrochemical Society by IOP Publishing Limited.",,"Benchmarking; Deep learning; Electric batteries; Forecasting; Battery lifetime prediction; Data preprocessing; Data-driven methods; Degradation mechanism; Learning methods; Linear modeling; Performance; Simplest linear; Statistical learning; Training sets; Degradation"
"Attié E., Bars S.L., Quenel I.","Towards ethical neuromarketing 2.0 based on artificial intelligence","10.4018/978-1-7998-6985-6.ch029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128121106&doi=10.4018%2f978-1-7998-6985-6.ch029&partnerID=40&md5=cff068e79b7fe57fd36a6d5ca1439008","Eighty percent of consumer behaviors and purchases rely on subconscious processes. The use of neuromarketing tools to study consumer behavior is not clear, notably regarding its practices and intentions toward consumers. This chapter aims to understand how neuromarketing can explain consumer behavior thanks to Neuromarketing 2.0 tools, how companies can manage the collected data in a responsible way and build a neuroethical charter to regulate the way companies use it. Most companies choose to not communicate about it when they use neuromarketing tools, and therefore, this chapter aims to pave the way towards solutions and recommendations and democratize its use by making Neuromarketing 2.0 more responsible and ethical. © 2021, IGI Global.",,
"Attinà F., Carammia M., Iacus S.M.","Forecasting Change in Conflict Fatalities with Dynamic Elastic Net","10.1080/03050629.2022.2090934","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133026582&doi=10.1080%2f03050629.2022.2090934&partnerID=40&md5=3720ca5776b0b5aab3c2486a4efa59fa","This article illustrates an approach to forecasting change in conflict fatalities designed to address the complexity of the drivers and processes of armed conflicts. The design of this approach is based on two main choices. First, to account for the specificity of conflict drivers and processes over time and space, we model conflicts in each individual country separately. Second, we draw on an adaptive model—Dynamic Elastic Net, DynENet—which is able to efficiently select relevant predictors among a large set of covariates. We include over 700 variables in our models, adding event data on top of the data features provided by the convenors of the forecasting competition. We show that our approach is suitable and computationally efficient enough to address the complexity of conflict dynamics. Moreover, the adaptive nature of our model brings a significant added value. Because for each country our model only selects the variables that are relevant to predict conflict intensity, the retained predictors can be analyzed to describe the dynamic configuration of conflict drivers both across countries and within countries over time. Countries can then be clustered to observe the emergence of broader patterns related to correlates of conflict. In this sense, our approach produces interpretable forecasts, addressing one key limitation of contemporary approaches to forecasting. © 2022 Taylor & Francis Group, LLC.","Armed conflict; forecasting; machine learning",
"Attiq N., Arshad U., Brogi S., Shafiq N., Imtiaz F., Parveen S., Rashid M., Noor N.","Exploring the anti-SARS-CoV-2 main protease potential of FDA approved marine drugs using integrated machine learning templates as predictive tools","10.1016/j.ijbiomac.2022.09.086","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138148492&doi=10.1016%2fj.ijbiomac.2022.09.086&partnerID=40&md5=f1c97bc14df3dbebb774d9c2ddc6b926","Since the inception of COVID-19 pandemic in December 2019, socio-economic crisis begins to rise globally and SARS-CoV-2 was responsible for this outbreak. With this outbreak, currently, world is in need of effective and safe eradication of COVID-19. Hence, in this study anti-SAR-Co-2 potential of FDA approved marine drugs (Biological macromolecules) data set is explored computationally using machine learning algorithm of Flare by Cresset Group, Field template, 3D-QSAR and activity Atlas model was generated against FDA approved M-pro SARS-CoV-2 repurposed drugs including Nafamostat, Hydroxyprogesterone caporate, and Camostat mesylate. Data sets were categorized into active and inactive molecules on the basis of their structural and biological resemblance with repurposed COVID-19 drugs. Then these active compounds were docked against the five different M-pro proteins co-crystal structures. Highest LF VS score of Holichondrin B against all main protease co-crystal structures ranked it as lead drug. Finally, this new technique of drug repurposing remained efficient to explore the anti-SARS-CoV-2 potential of FDA approved marine drugs. © 2022 Elsevier B.V.","Activity atlas model; Activity cliff; COVID-19; Field template; Holichondrin B; Marine drugs","antivirus agent; apilidin; bretuximab; bryostatin; camostat mesilate; didemnin B; halichondrin B; hydroxyprogesterone caporate; lestaurtinib; M protein; marine drug; nafamstat; salinosporamide A; SARS-CoV-2 antibody; trabectedin; unclassified drug; unclassified drug; viral protease; antivirus agent; proteinase inhibitor; algorithm; antiviral activity; Article; computer model; controlled study; coronavirus disease 2019; crystal structure; drug repositioning; Food and Drug Administration; information processing; machine learning; macromolecule; molecular docking; molecular dynamics; nonhuman; pharmacophore; prediction; Severe acute respiratory syndrome coronavirus 2; three dimensional quantitative structure activity relationship; chemistry; drug therapy; human; machine learning; pandemic; Antiviral Agents; COVID-19; Drug Repositioning; Humans; Machine Learning; Molecular Docking Simulation; Pandemics; Protease Inhibitors; SARS-CoV-2"
"Attwal K.P.S., Dhiman A.S.","Mining Effect of Temperature and Rainfall to Develop an Empirical Model for Wheat Yield Prediction","10.2174/2666255813666200129105708","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122529468&doi=10.2174%2f2666255813666200129105708&partnerID=40&md5=225f1b577a6a7878a2023ab374c0be91","Background: Crop yield is affected by several agronomic factors such as soil type and date of sowing, and meteorological factors such as temperature and rainfall. While the agronomic factors are responsible for inter-region variations in yield, the year-wise yield variation in a particu-lar region may be attributed to meteorological factors. Various Data Mining Techniques can be applied to analyse the effect of these factors on crop yield. Objective: To develop a model for prediction of Block-wise average wheat yield in the Patiala district of Punjab, India. Methods: Sampling is used for the collection of the yield data, and the data concerning temperature and rainfall is obtained from the Indian Meteorological Department, Pune. The data is then pre-processed and analysed to study the effect of phase-wise average temperature and total rainfall on the wheat yield. The factors that are found to significantly affect yield are used for building a model for yield prediction. Results: It has been found that the average temperature and the total rainfall for the whole wheat growing season are not much of help in explaining the variations in yearly wheat yield. The temperature and rainfall have different effects at different stages of plant growth and the yield is affected ac-cordingly. It is inferred that the average temperature and the total rainfall during the vegetative phase and the grain development and ripening phase are the most important parameters for the prediction of wheat yield. Conclusion: The stepwise selection mechanism is used to choose the variables whose inclusion ex-plains the maximum variance in yield. The model is evaluated based on different parameters and is found to explain 95.6% of the yearly variations in yield. © 2021 Bentham Science Publishers.","Effect of rainfall on yield; Effect of temperature on yield; Moderation analysis; Prediction models; Wheat; Yield prediction","Agronomy; Crops; Data mining; Forecasting; Rain; Effect of rainfall on yield; Effect of temperature on yield; Effects of rainfalls; Effects of temperature; Moderation analyse; Prediction modelling; Total rainfall; Wheat; Wheat yield; Yield prediction; Temperature"
"Atutxa A., de Ilarraza A.D., Gojenola K., Oronoz M., Perez-de-Viñaspre O.","Interpretable deep learning to map diagnostic texts to ICD-10 codes","10.1016/j.ijmedinf.2019.05.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066502163&doi=10.1016%2fj.ijmedinf.2019.05.015&partnerID=40&md5=a02c5de40ed7caffad8c5e1fdb1620d2","Background: Automatic extraction of morbid disease or conditions contained in Death Certificates is a critical process, useful for billing, epidemiological studies and comparison across countries. The fact that these clinical documents are written in regular natural language makes the automatic coding process difficult because, often, spontaneous terms diverge strongly from standard reference terminology such as the International Classification of Diseases (ICD). Objective: Our aim is to propose a general and multilingual approach to render Diagnostic Terms into the standard framework provided by the ICD. We have evaluated our proposal on a set of clinical texts written in French, Hungarian and Italian. Methods: ICD-10 encoding is a multi-class classification problem with an extensive (thousands) number of classes. After considering several approaches, we tackle our objective as a sequence-to-sequence task. According to current trends, we opted to use neural networks. We tested different types of neural architectures on three datasets in which Diagnostic Terms (DTs) have their ICD-10 codes associated. Results and conclusions: Our results give a new state-of-the art on multilingual ICD-10 coding, outperforming several alternative approaches, and showing the feasibility of automatic ICD-10 prediction obtaining an F-measure of 0.838, 0.963 and 0.952 for French, Hungarian and Italian, respectively. Additionally, the results are interpretable, providing experts with supporting evidence when confronted with coding decisions, as the model is able to show the alignments between the original text and each output code. © 2019 Elsevier B.V.","Electronic health records; International Classification of Diseases; Neural machine translation; Sequence-to-sequence mapping","Codes (symbols); Diagnosis; Information retrieval systems; Neural networks; Electronic health record; Epidemiological studies; International classification of disease; Machine translations; Multiclass classification problems; Multilingual approach; Neural architectures; Sequence mappings; Deep learning; algorithm; Article; death certificate; deep learning; electronic health record; false negative result; false positive result; human; ICD-10; medical record; nomenclature; priority journal; artificial neural network; International Classification of Diseases; Deep Learning; Electronic Health Records; International Classification of Diseases; Neural Networks (Computer)"
"Atzeni M., Reforgiato Recupero D.","Deep learning and sentiment analysis for human-robot interaction","10.1007/978-3-319-98192-5_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051657097&doi=10.1007%2f978-3-319-98192-5_3&partnerID=40&md5=4af2cc4d8bc81108d54df932b3415dd9","In this paper we present an ongoing work showing to what extent semantic technologies, deep learning and natural language processing can be applied within the field of Human-Robot Interaction. The project has been developed for Zora, a completely programmable and autonomous humanoid robot, and it aims at allowing Zora to interact with humans using natural language. The robot is capable of talking to the user and understanding sentiments by leveraging our external services, such as a Sentiment Analysis engine and a Generative Conversational Agent, which is responsible for generating Zora’s answers to open-dialog natural language utterances. © 2018, Springer Nature Switzerland AG.","Deep learning; Human-robot interaction; LSTM; Natural language processing; Sentiment analysis; Word embeddings","Anthropomorphic robots; Data mining; Deep learning; Long short-term memory; Man machine systems; Natural language processing systems; Semantic Web; Sentiment analysis; Autonomous humanoid robots; Conversational agents; Embeddings; LSTM; Natural languages; Semantic technologies; Human robot interaction"
"Atzmueller M., Kanawati R.","Explainability in Cyber Security using Complex Network Analysis: A Brief Methodological Overview","10.1145/3528580.3532839","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135366820&doi=10.1145%2f3528580.3532839&partnerID=40&md5=2ee3b2fe9ae985daed959c4f0974f0c2","Artificial intelligence (AI) approaches are widely applied in cyber security, while they currently lack explainability towards their users. Here, complex network analysis (CNA) can be leveraged for providing explainability. The goal of this overview paper is to present a brief methodological view on explainability in cyber security using CNA. In particular, we (1) motivate the concept, use and application of explainability, (2) present CNA methods, and (3) outline challenges and open issues in the domain of cyber security. © 2022 ACM.","Complex Network Analysis; Cyber Security; Explainable AI","Complex networks; Cybersecurity; Analysis method; Complex network analysis; Cyber security; Explainable artificial intelligence; Network security"
"Atzmueller M., Kliegr T., Schmid U.","Proceedings of the 1st international workshop on explainable and interpretable machine learning (XI-ML)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099193766&partnerID=40&md5=77047e3bffc0d232fee961aadf049120",[No abstract available],,
"Atzmueller M.","Towards socio-technical design of explicative systems: Transparent, interpretable and explainable analytics and its perspectives in social interaction contexts",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085971362&partnerID=40&md5=8043f889ca17ccf3b30384326f14bfe9","This paper outlines an approach towards socio-technical design of explicative systems enabling transparent, interpretable and explainable analytics. We sketch the TIE approach for the socio-technical explicative system design and discuss its fundamental principles and perspectives. Furthermore, we exemplify the application of the proposed approach in social interaction contexts. Copyright © 2019 for this paper by its authors.","Affective Computing; Explainable AI Systems; Social Interactions","Artificial intelligence; Fundamental principles; Social interactions; Socio-technical designs; Sociotechnical; Ambient intelligence"
"Atzmueller M., Bloemheuvel S., Kloepper B.","A Framework for Human-Centered Exploration of Complex Event Log Graphs","10.1007/978-3-030-33778-0_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075827720&doi=10.1007%2f978-3-030-33778-0_26&partnerID=40&md5=8958b1758e7610c9375eaccb59506dee","Graphs can conveniently model complex multi-relational characteristics. For making sense of such data, effective interpretable methods for their exploration are crucial, in order to provide insights that cover the relevant analytical questions and are understandable to humans. This paper presents a framework for human-centered exploration of attributed graphs on complex, i.e., large and heterogeneous event logs. The proposed approach is based on specific graph modeling, graph summarization and local pattern mining methods. We demonstrate promising results in the context of a real-world industrial dataset. © Springer Nature Switzerland AG 2019.",,"Computer science; Computers; Attributed graphs; Complex events; Event logs; Graph model; Local patterns; Model complexes; Real-world; Artificial intelligence"
"Atzmueller M., Kibanov M., Hayat N., Trojahn M., Kroll D.","Adaptive class association rule mining for human activity recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962609681&partnerID=40&md5=3145889f1e951e285ecaa616bbf30dd2","The analysis of human activity data is an important research area in the context of ubiquitous and social environments. Using sensor data obtained by mobile devices, e. g., utilizing accelerometer sensors contained in mobile phones, behavioral patterns and models can then be obtained. However, the utilized models are often not simple to interpret by humans in order to facilitate assessment, evaluation and validation, e. g., in computational social science or in medical contexts. In this paper, we propose a novel approach for generating interpretable rule sets for classification: We present an adaptive framework for mining class association rules using subgroup discovery, and analyze different techniques for obtaining the final classifier. The approach is investigated in the context of human activity recognition. For our evaluation, we apply real-world activity data collected using mobile phone sensors. Copyright © 2015 by the paper's authors.",,"Artificial intelligence; Association rules; Cellular telephones; E-learning; Learning systems; Mobile devices; Mobile phones; Telephone sets; Accelerometer sensor; Behavioral patterns; Class association rules; Computational social science; Human activity recognition; Interpretable rules; Mobile phone sensors; Real-world activities; Pattern recognition"
"Atzmueller M., Roth-Berghofer T.","The mining and analysis continuum of explaining uncovered","10.1007/978-0-85729-130-1_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881437110&doi=10.1007%2f978-0-85729-130-1_20&partnerID=40&md5=11f58b27a13235747d9255f2b80afa41","The result of data mining is a set of patterns or models. When presenting these, all or part of the result needs to be explained to the user in order to be understandable and for increasing the user acceptance of the patterns. In doing that, a variety of dimensions for explaining needs to be considered, e.g., from concrete to more abstract explanations. This paper discusses a continuum of explaining for data mining and analysis: It describes how data mining results can be analysed on continuous dimensions and levels. © 2011 Springer-Verlag London Limited.",,"Artificial intelligence; User acceptance; Data mining"
"Atzmueller M., Roth-Berghofer T.","Towards explanation-aware social software: Applying the mining and analysis continuum of explaining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885603275&partnerID=40&md5=f827fb904087b88aa09eaa0098ca0178","Data mining methods build patterns or models. When presenting these, all or part of the result needs to be explained to the user in order to be understandable and for increasing the user acceptance of the patterns. In doing that, a variety of dimensions in the Mining and Analysis Continuum of Explaining (MACE) needs to be considered, e.g., from concrete to more abstract explanations. This paper discusses the application of the MACE in the context of social software. We consider applications of the proposed approaches in three social software systems, and show how the data mining results can seamlessly be analysed on the presented continuous dimensions and levels.",,"Data mining methods; Social software; User acceptance; Artificial intelligence; Data mining"
"Atzmueller M., Baumeister J., Puppe F.","Quality measures and semi-automatic mining of diagnostic rule bases","10.1007/11415763_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-26844518515&doi=10.1007%2f11415763_5&partnerID=40&md5=86c34a59fb2191e20e4b78dfa7fa638c","Semi-automatic data mining approaches often yield better results than plain automatic methods, due to the early integration of the user's goals. For example in the medical domain, experts are likely to favor simpler models instead of more complex models. Then, the accuracy of discovered patterns is often not the only criterion to consider. Instead, the simplicity of the discovered knowledge is of prime importance, since this directly relates to the understandability and the interpretability of the learned knowledge. In this paper, we present quality measures considering the understandability and the accuracy of (learned) rule bases. We describe a unifying quality measure, which can trade-off small losses concerning accuracy vs. an increased simplicity. Furthermore, we introduce a semi-automatic data mining method for learning understandable and accurate rule bases. The presented work is evaluated using cases from a real world application in the medical domain. © Springer-Verlag Berlin Heidelberg 2005.",,"Knowledge acquisition; Knowledge based systems; Large scale systems; Learning systems; Complex models; Learned knowledge; Semi-automatic data mining; Data mining"
"Au Q., Herbinger J., Stachl C., Bischl B., Casalicchio G.","Grouped feature importance and combined features effect plot","10.1007/s10618-022-00840-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132133154&doi=10.1007%2fs10618-022-00840-5&partnerID=40&md5=5c0750fa7b0af4a0b94ad75f157ded55","Interpretable machine learning has become a very active area of research due to the rising popularity of machine learning algorithms and their inherently challenging interpretability. Most work in this area has been focused on the interpretation of single features in a model. However, for researchers and practitioners, it is often equally important to quantify the importance or visualize the effect of feature groups. To address this research gap, we provide a comprehensive overview of how existing model-agnostic techniques can be defined for feature groups to assess the grouped feature importance, focusing on permutation-based, refitting, and Shapley-based methods. We also introduce an importance-based sequential procedure that identifies a stable and well-performing combination of features in the grouped feature space. Furthermore, we introduce the combined features effect plot, which is a technique to visualize the effect of a group of features based on a sparse, interpretable linear combination of features. We used simulation studies and real data examples to analyze, compare, and discuss these methods. © 2022, The Author(s).","Combined features effects; Dimension reduction; Grouped feature importance; Interpretable machine learning","Learning algorithms; Active area; Combined feature effect; Combined features; Dimension reduction; Feature groups; Grouped feature importance; Interpretability; Interpretable machine learning; Machine learning algorithms; Machine-learning; Machine learning"
"Au Yeung W.K., Maruyama O., Sasaki H.","A convolutional neural network-based regression model to infer the epigenetic crosstalk responsible for CG methylation patterns","10.1186/s12859-021-04272-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108850216&doi=10.1186%2fs12859-021-04272-8&partnerID=40&md5=1f9df8f549164d5c96275de009baf545","Background: Epigenetic modifications, including CG methylation (a major form of DNA methylation) and histone modifications, interact with each other to shape their genomic distribution patterns. However, the entire picture of the epigenetic crosstalk regulating the CG methylation pattern is unknown especially in cells that are available only in a limited number, such as mammalian oocytes. Most machine learning approaches developed so far aim at finding DNA sequences responsible for the CG methylation patterns and were not tailored for studying the epigenetic crosstalk. Results: We built a machine learning model named epiNet to predict CG methylation patterns based on other epigenetic features, such as histone modifications, but not DNA sequence. Using epiNet, we identified biologically relevant epigenetic crosstalk between histone H3K36me3, H3K4me3, and CG methylation in mouse oocytes. This model also predicted the altered CG methylation pattern of mutant oocytes having perturbed histone modification, was applicable to cross-species prediction of the CG methylation pattern of human oocytes, and identified the epigenetic crosstalk potentially important in other cell types. Conclusions: Our findings provide insight into the epigenetic crosstalk regulating the CG methylation pattern in mammalian oocytes and other cells. The use of epiNet should help to design or complement biological experiments in epigenetics studies. © 2021, The Author(s).","Convolutional neural network; DNA methylation; Epigenetic crosstalk; Histone modification; Oocyte; Stem cell","Alkylation; Convolutional neural networks; Crosstalk; DNA; DNA sequences; Machine learning; Mammals; Regression analysis; Turing machines; Biological experiments; Epigenetic modification; Genomic distribution; Histone modification; Machine learning approaches; Machine learning models; Mammalian oocytes; Regression model; Methylation; animal; DNA methylation; epigenetics; genetic epigenesis; histone code; mouse; Animals; DNA Methylation; Epigenesis, Genetic; Epigenomics; Histone Code; Mice; Neural Networks, Computer"
"Audemard G., Bellart S., Bounia L., Koriche F., Lagniez J.-M., Marquis P.","On Preferred Abductive Explanations for Decision Trees and Random Forests",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137914973&partnerID=40&md5=db4e50039c9958c0454df49906aeb77c","Abductive explanations take a central place in eXplainable Artificial Intelligence (XAI) by clarifying with few features the way data instances are classified. However, instances may have exponentially many minimum-size abductive explanations, and this source of complexity holds even for “intelligible” classifiers, such as decision trees. When the number of such abductive explanations is huge, computing one of them, only, is often not informative enough. Especially, better explanations than the one that is derived may exist. As a way to circumvent this issue, we propose to leverage a model of the explainee, making precise her/his preferences about explanations, and to compute only preferred explanations. In this paper, several models are pointed out and discussed. For each model, we present and evaluate an algorithm for computing preferred majoritary reasons, where majoritary reasons are specific abductive explanations suited to random forests. We show that in practice the preferred majoritary reasons for an instance can be far less numerous than its majoritary reasons. © 2022 International Joint Conferences on Artificial Intelligence. All rights reserved.",,"Artificial intelligence; Forestry; Random forests; Classifieds; Random forests; Decision trees"
"Audemard G., Koriche F., Marquis P.","On tractable xai queries based on compiled representations",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104593516&partnerID=40&md5=a930272d29d7bece26e706c7dbdaa9b1","One of the key purposes of eXplainable AI (XAI) is to develop techniques for understanding predictions made by Machine Learning (ML) models and for assessing how much reliable they are. Several encoding schemas have recently been pointed out, showing how ML classifiers of various types can be mapped to Boolean circuits exhibiting the same inputoutput behaviours. Thanks to such mappings, XAI queries about classifiers can be delegated to the corresponding circuits. In this paper, we present some explanation queries and verification queries about classifiers. We show how they can be addressed by combining queries and transformations about the associated Boolean circuits. Taking advantage of previous results from the knowledge compilation map, this allows us to identify a number of cases for which XAI queries are tractable provided that the circuit has been first turned into a compiled representation. © 2020 17th International Conference on Principles of Knowledge Representation and Reasoning, KR 2020. All rights reserved.",,"Logic circuits; Boolean circuit; Input-output; Knowledge compilation; Knowledge representation"
"Auer M., Griffiths M.D.","Predicting Limit-Setting Behavior of Gamblers Using Machine Learning Algorithms: A Real-World Study of Norwegian Gamblers Using Account Data","10.1007/s11469-019-00166-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075942067&doi=10.1007%2fs11469-019-00166-2&partnerID=40&md5=cfec4642ae5f6b5711d0b32d80941a81","Player protection and harm minimization have become increasingly important in the gambling industry along with the promotion of responsible gambling (RG). Among the most widespread RG tools that gaming operators provide are limit-setting tools that help players limit the amount of time and/or money they spend gambling. Research suggests that limit-setting significantly reduces the amount of money that players spend. If limit-setting is to be encouraged as a way of facilitating responsible gambling, it is important to know what variables are important in getting individuals to set and change limits in the first place. In the present study, 33 variables assessing the player behavior among Norsk Tipping clientele (N = 70,789) from January to March 2017 were computed. The 33 variables which reflect the players’ behavior were then used to predict the likelihood of gamblers changing their monetary limit between April and June 2017. The 70,789 players were randomly split into a training dataset of 56,532 and an evaluation set of 14,157 players (corresponding to an 80/20 split). The results demonstrated that it is possible to predict future limit-setting based on player behavior. The random forest algorithm appeared to predict limit-changing behavior much better than the other algorithms. However, on the independent test data, the random forest algorithm’s accuracy dropped significantly. The best performance on the test data along with a small decrease in accuracy in comparison to the training data was delivered by the gradient boost machine learning algorithm. The most important variables predicting future limit-setting using the gradient boost machine algorithm were players receiving feedback that they had reached 80% of their personal monthly global loss limit, personal monthly loss limit, the amount bet, theoretical loss, and whether the players had increased their limits in the past. With the help of predictive analytics, players with a high likelihood of changing their limits can be proactively approached. © 2019, The Author(s).","Gambling; Gambling algorithms; Limit-setting; Problem gambling; Responsible gambling tools","adult; algorithm; article; controlled study; female; gambling; human; major clinical study; male; random forest; theoretical study"
"Aufiero S., Bleijendaal H., Robyns T., Vandenberk B., Krijger C., Bezzina C., Zwinderman A.H., Wilde A.A.M., Pinto Y.M.","A deep learning approach identifies new ECG features in congenital long QT syndrome","10.1186/s12916-022-02350-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129421112&doi=10.1186%2fs12916-022-02350-z&partnerID=40&md5=22fca510d1c3df244d9eb076ad10a4e0","Background: Congenital long QT syndrome (LQTS) is a rare heart disease caused by various underlying mutations. Most general cardiologists do not routinely see patients with congenital LQTS and may not always recognize the accompanying ECG features. In addition, a proportion of disease carriers do not display obvious abnormalities on their ECG. Combined, this can cause underdiagnosing of this potentially life-threatening disease. Methods: This study presents 1D convolutional neural network models trained to identify genotype positive LQTS patients from electrocardiogram as input. The deep learning (DL) models were trained with a large 10-s 12-lead ECGs dataset provided by Amsterdam UMC and externally validated with a dataset provided by University Hospital Leuven. The Amsterdam dataset included ECGs from 10000 controls, 172 LQTS1, 214 LQTS2, and 72 LQTS3 patients. The Leuven dataset included ECGs from 2200 controls, 32 LQTS1, and 80 LQTS2 patients. The performance of the DL models was compared with conventional QTc measurement and with that of an international expert in congenital LQTS (A.A.M.W). Lastly, an explainable artificial intelligence (AI) technique was used to better understand the prediction models. Results: Overall, the best performing DL models, across 5-fold cross-validation, achieved on average a sensitivity of 84 ± 2%, 90 ± 2% and 87 ± 6%, specificity of 96 ± 2%, 95 ± 1%, and 92 ± 4%, and AUC of 0.90 ± 0.01, 0.92 ± 0.02, and 0.89 ± 0.03, for LQTS 1, 2, and 3 respectively. The DL models were also shown to perform better than conventional QTc measurements in detecting LQTS patients. Furthermore, the performances held up when the DL models were validated on a novel external cohort and outperformed the expert cardiologist in terms of specificity, while in terms of sensitivity, the DL models and the expert cardiologist in LQTS performed the same. Finally, the explainable AI technique identified the onset of the QRS complex as the most informative region to classify LQTS from non-LQTS patients, a feature previously not associated with this disease. Conclusions: This study suggests that DL models can potentially be used to aid cardiologists in diagnosing LQTS. Furthermore, explainable DL models can be used to possibly identify new features for LQTS on the ECG, thus increasing our understanding of this syndrome. © 2022, The Author(s).","Deep learning; ECG; Explainable AI; LQTS","adolescent; Article; artificial intelligence; cardiologist; clinical feature; cohort analysis; congenital long QT syndrome; controlled study; convolutional neural network; deep learning; diagnostic accuracy; diagnostic test accuracy study; electrocardiography; female; genotype; human; major clinical study; male; Netherlands; prediction; QTc interval; receiver operating characteristic; sensitivity and specificity; electrocardiography; genetics; long QT syndrome; procedures; Artificial Intelligence; Deep Learning; Electrocardiography; Humans; Long QT Syndrome; Neural Networks, Computer"
"Augasta M.G., Kathirvalavakumar T.","Rule extraction from neural networks - A comparative study","10.1109/ICPRIME.2012.6208380","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863938716&doi=10.1109%2fICPRIME.2012.6208380&partnerID=40&md5=535b0466fce1c0e2718fc9e9c37a5ade","Though neural networks have achieved highest classification accuracy for many classification problems, the obtained results may not be interpretable as they are often considered as black box. To overcome this drawback researchers have developed many rule extraction algorithms. This paper has discussed on various rule extraction algorithms based on three different rule extraction approaches namely decompositional, pedagogical and eclectic. Also it evaluates the performance of those approaches by comparing different algorithms with these three approaches on three real datasets namely Wisconsin breast cancer, Pima Indian diabetes and Iris plants. © 2012 IEEE.","classification; data mining; decompositional; eclectic; neural networks; pedagogical; rule extraction","Black boxes; Breast Cancer; Classification accuracy; Comparative studies; decompositional; eclectic; pedagogical; Pima Indian Diabetes; Real data sets; Rule extraction; Rule extraction algorithms; Rule extraction from neural networks; WISCONSIN; Biomedical engineering; Classification (of information); Data mining; Information science; Pattern recognition; Neural networks"
"Augenstein Friedrich, Ottmann Thomas, Schoening Juergen","How to incorporate expert knowledge into an authoring system",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027755469&partnerID=40&md5=aa75ccc9d7ba8ad4550e8cff96805967","We report about our experience in the development of courseware and hypermedia documents. We show that current authoring systems to develop hypermedia courseware do not support the casual and inexperienced author well enough. Therefore, it is not surprising that educational software is quite often of non-acceptable quality and suffers under `coloritis, fontitis, linkitis'. We demonstrate how expert knowledge can be incorporated into the authoring system Authorware Professional and what problems we had been confronted with. Then we report about the design and implementation of an new prototype authoring system called TRAIN which allows a clear distinction between the tasks and responsibility of the three different kinds of experts involved in courseware development: The author is responsible for the content, the design expert for the layout of the multimedia document, and the pedagogue for regarding didactic principles.",,"Artificial intelligence; Computer software; Expert systems; Information retrieval systems; Teaching; Educational software; Multimedia information systems; TRAIN authoring system; Computer aided instruction"
"Augusto A., Conforti R., Dumas M., Rosa M.L., Bruno G.","Automated discovery of structured process models: Discover structured vs. Discover and structure","10.1007/978-3-319-46397-1_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997427866&doi=10.1007%2f978-3-319-46397-1_25&partnerID=40&md5=9ef98f4adeda5e17b2f01a34ed304488","This paper addresses the problem of discovering business process models from event logs. Existing approaches to this problem strike various tradeoffs between accuracy and understandability of the discovered models. With respect to the second criterion, empirical studies have shown that block-structured process models are generally more understandable and less error-prone than unstructured ones. Accordingly, several automated process discovery methods generate blockstructured models by construction. These approaches however intertwine the concern of producing accurate models with that of ensuring their structuredness, sometimes sacrificing the former to ensure the latter. In this paper we propose an alternative approach that separates these two concerns. Instead of directly discovering a structured process model, we first apply a well-known heuristic that discovers more accurate but sometimes unstructured (and even unsound) process models, and then transform the resulting model into a structured one. An experimental evaluation shows that our “discover and structure” approach outperforms traditional “discover structured” approaches with respect to a range of accuracy and complexity measures. © Springer International Publishing AG 2016.","Automated process discovery; BPMN; Process structuring","Artificial intelligence; Computer science; Computers; Automated discovery; Automated process; BPMN; Business process model; Complexity measures; Experimental evaluation; nocv1; Process structuring; Structured process models; Automation"
"Augusto J.C., Bohlen M., Cook D., Flentge F., Marreiros G., Ramos C., Qin W., Suo Y.","The darmstadt challenge: The turing test revisited",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349464969&partnerID=40&md5=6b0eac96aae819fca150c3daa8a28bfa","Significant work has been done in the areas of Pervcomp/Ubicomp/Smart Environments with advances on making proactive systems, but those advances have not made these type of systems accurately proactive. On the other hand a great deal is needed to make systems more sensible/sensitive and trustable (both in terms of reliability and privacy). We put forward the thesis that a more integral and social-aware sort of intelligence is needed to effectively interact, decide and act on behalf of people's interest and that a way to test how effective systems are achieving these desirable behaviour is needed as a consequence. We support our thesis by providing examples on how to measure effectiveness in variety of different environments.","Ambient intelligence; Smart environments; Turing test; Validation","Ambient intelligence; Proactive systems; Smart environments; Turing test; Turing tests; Validation; Artificial intelligence"
"Augusto P., Cardoso J.S., Fonseca J.","Automotive Interior Sensing-Towards a Synergetic Approach between Anomaly Detection and Action Recognition Strategies","10.1109/IPAS50080.2020.9334942","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100824142&doi=10.1109%2fIPAS50080.2020.9334942&partnerID=40&md5=c61e52b542811877081038427b08fe9d","With the appearance of Shared Autonomous Vehicles there will no longer be a driver responsible for maintaining the car interior and well-being of passengers. To counter this, it is imperative to have a system that is able to detect any abnormal behaviors, more specifically, violence between passengers. Traditional action recognition algorithms build models around known interactions but activities can be so diverse, that having a dataset that incorporates most use cases is unattainable. While action recognition models are normally trained on all the defined activities and directly output a score that classifies the likelihood of violence, video anomaly detection algorithms present themselves as an alternative approach to build a good discriminative model since usually only non-violent examples are needed. This work focuses on anomaly detection and action recognition algorithms trained, validated and tested on a subset of human behavior video sequences from Bosch's internal datasets. The anomaly detection network architecture defines how to properly reconstruct normal frame sequences so that during testing, each sequence can be classified as normal or abnormal based on its reconstruction error. With these errors, regularity scores are inferred showing the predicted regularity of each frame. The resulting framework is a viable addition to traditional action recognition algorithms since it can work as a tool for detecting unknown actions, strange/violent behaviors and aid in understanding the meaning of such human interactions. © 2020 IEEE.","Action Recognition; Anomaly detection; Computer Vision; Deep Learning","Behavioral research; Image processing; Network architecture; Abnormal behavior; Action recognition; Action recognition algorithms; Anomaly-detection algorithms; Automotive interiors; Discriminative models; Human interactions; Reconstruction error; Anomaly detection"
"Aulenkamp J., Mikuteit M., Löffler T., Schmidt J.","Overview of digital health teaching courses in medical education in germany in 2020 [Erste übersicht der lehrveranstaltungen mit dem inhalt „digitale kompetenzen“ an den medizinischen universitäten in deutschland 2020]","10.3205/zma001476","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104834761&doi=10.3205%2fzma001476&partnerID=40&md5=ecfe4760c4c451d300373744da039065","Objective: The digitalization of the healthcare system poses new chal-lenges for physicians. Thus, the relevance of learning digital competencies (DiCo), such as dealing with data sets, apply telemedicine or using apps, is already growing in medical education. DiCo should be clearly separated from digitized teaching formats, which have been increasingly used since the COVID 19 pandemic. This article outlines the faculties in Germany where DiCo are already integrated into medical education. Methods: Courses with DiCo as teaching content were collected by a literature research on Pubmed and Google as well as by contacting all dean's offices and other persons responsible for teaching at German medical faculties. The courses were summarized in a table. Results: In a first survey, 16 universities were identified that offer courses on DiCo. In the elective area at the universities, 17 courses and in the compulsory area eight courses could be identified. The scope and content of the courses diverged between compulsory curricula, integrated courses of different lengths, and elective courses that are one-time or longitudinally integrated. The topics taught are heterogeneous and include fundamentals of medical informatics such as data manage-ment on the one hand and a collection of e.g. ethics, law, apps, artificial intelligence, telemedicine and robotics on the other hand. Conclusion: Currently, only some German medical faculties offer courses on DiCo. These courses vary in scope and design. They are frequently part of the elective curriculum and only reach some of the students. The possibility of embedding DiCo in the already existing cross-sectional area appears limited. In view of the ongoing digitalization of healthcare, it is necessary to make future courses on DiCo accessible to all medical students. In order to drive this expansion forward, the implementation of the new learning objectives catalogue, in which DiCo are integrated, a network formation, a teaching qualification as well as the involvement of students is recommended. © 2021 Aulenkamp et al.","Digital competencies; Digital health; Digital medicine; Digital teaching; Education; ELearning; Medical informatics","curriculum; education; Germany; human; medical education; procedures; questionnaire; Curriculum; Education, Distance; Education, Medical; Educational Measurement; Germany; Humans; Surveys and Questionnaires"
"Ault J., Hanna J.P., Sharon G.","Learning an interpretable traffic signal control policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096647945&partnerID=40&md5=5d9a3b0158d1dad6a7822607311b4d30","Signalized intersections are managed by controllers that assign right of way (green, yellow, and red lights) to non-conflicting directions. Optimizing the actuation policy of such controllers is expected to alleviate traffic congestion and its adverse impact. Given such a safety-critical domain, the affiliated actuation policy is required to be interpretable in a way that can be understood and regulated by a human. This paper presents and analyzes several on-line optimization techniques for tuning interpretable control functions. Although these techniques are defined in a general way, this paper assumes a specific class of interpretable control functions (polynomial functions) for analysis purposes. We show that such an interpretable policy function can be as effective as a deep neural network for approximating an optimized signal actuation policy. We present empirical evidence that supports the use of value-based reinforcement learning for on-line training of the control function. Specifically, we present and study three variants of the Deep Q-learning algorithm that allow the training of an interpretable policy function. Our Deep Regulatable Hardmax Q-learning variant is shown to be particularly effective in optimizing our interpretable actuation policy, resulting in up to 19.4% reduced vehicles delay compared to commonly deployed actuated signal controllers. © 2020 International Foundation for Autonomous.","Deep reinforcement learning; Intelligent transportation; Interpretable","Autonomous agents; Controllers; Deep learning; Deep neural networks; E-learning; Multi agent systems; Reinforcement learning; Safety engineering; Street traffic control; Traffic congestion; Traffic signals; Actuated signals; Control functions; Online optimization; Polynomial functions; Q-learning algorithms; Safety-critical domain; Signalized intersection; Traffic signal control; Learning algorithms"
"Aumentado-Armstrong T., Tsogkas S., Jepson A., DIckinson S.","Geometric disentanglement for generative latent shape models","10.1109/ICCV.2019.00827","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081893225&doi=10.1109%2fICCV.2019.00827&partnerID=40&md5=85cdf88676ea8d72c173e262fb58b6c0","Representing 3D shapes is a fundamental problem in artificial intelligence, which has numerous applications within computer vision and graphics. One avenue that has recently begun to be explored is the use of latent representations of generative models. However, it remains an open problem to learn a generative model of shapes that is interpretable and easily manipulated, particularly in the absence of supervised labels. In this paper, we propose an unsupervised approach to partitioning the latent space of a variational autoencoder for 3D point clouds in a natural way, using only geometric information, that builds upon prior work utilizing generative adversarial models of point sets. Our method makes use of tools from spectral geometry to separate intrinsic and extrinsic shape information, and then considers several hierarchical disentanglement penalties for dividing the latent space in this manner. We also propose a novel disentanglement penalty that penalizes the predicted change in the latent representation of the output,with respect to the latent variables of the initial shape. We show that the resulting latent representation exhibits intuitive and interpretable behaviour, enabling tasks such as pose transfer that cannot easily be performed by models with an entangled representation. © 2019 IEEE.",,"Computer vision; Geometry; 3D point cloud; Auto encoders; Generative model; Geometric information; Initial shape; Latent variable; Shape information; Unsupervised approaches; Three dimensional computer graphics"
"Aung S.T., Wongsawat Y.","Prediction of epileptic seizures based on multivariate multiscale modiﬁeddistribution entropy","10.7717/PEERJ-CS.744","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119454000&doi=10.7717%2fPEERJ-CS.744&partnerID=40&md5=6466df2d56f84981ea1113dcf61b6560","Epilepsy is a common neurological disease that affects a wide range of the world population and is not limited by age. Moreover, seizures can occur anytime and anywhere because of the sudden abnormal discharge of brain neurons, leading to malfunction. The seizures of approximately 30% of epilepsy patients cannot be treated with medicines or surgery; hence these patients would beneﬁt from a seizure prediction system to live normal lives. Thus, a system that can predict a seizure before its onset could improve not only these patients’ social lives but also their safety. Numerous seizure prediction methods have already been proposed, but the performance measures of these methods are still inadequate for a complete prediction system. Here, a seizure prediction system is proposed by exploring the advantages of multivariate entropy, which can reﬂect the complexity of multivariate time series over multiple scales (frequencies), called multivariate multiscale modiﬁed-distribution entropy (MM-mDistEn), with an artiﬁcial neural network (ANN). The phase-space reconstruction and estimation of the probability density between vectors provide hidden complex information. The multivariate time series property of MM-mDistEn provides more understandable information within the multichannel data and makes it possible to predict of epilepsy. Moreover, the proposed method was tested with two different analyses: simulation data analysis proves that the proposed method has strong consistency over the different parameter selections, and the results from experimental data analysis showed that the proposed entropy combined with an ANN obtains performance measures of 98.66% accuracy, 91.82% sensitivity, 99.11% speciﬁcity, and 0.84 area under the curve (AUC) value. In addition, the seizure alarm system was applied as a postprocessing step for prediction purposes, and a false alarm rate of 0.014 per hour and an average prediction time of 26.73 min before seizure onset were achieved by the proposed method. Thus, the proposed entropy as a feature extraction method combined with an ANN can predict the ictal state of epilepsy, and the results show great potential for all epilepsy patients. Subjects Bioinformatics, Computational Biology, Algorithms and Analysis of Algorithms, Artiﬁcial Intelligence, Brain-Computer Interface © 2021. Aung and Wongsawat","ANN; Distribution entropy; EEG; Entropy; Epilepsy","Alarm systems; Complex networks; Data handling; Electroencephalography; Forecasting; Information analysis; Neurology; Patient treatment; Phase space methods; Time series; Brain neurons; Distribution entropies; Multivariate time series; Neurological disease; Performance measure; Prediction methods; Prediction systems; Seizure prediction; Social life; World population; Entropy"
"Aung Y.T., Hlaing Win M., Lwin Z.M., Mikhaylov I.S., Aung Z.","Implementation of the Neural Network for Solving the Problem of Regression of Controlled Parameters of Oil Wells","10.1109/ElConRus54750.2022.9755480","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129542325&doi=10.1109%2fElConRus54750.2022.9755480&partnerID=40&md5=d61bfc44d77c8ea2aaf05397d09560c5","This work describes the research of neural network techniques Data Mining regression tasks-controlled oil wells parameters. In this task, a large amount of data is collected from oil-producing wells on the basis of which a further study of changes in parameters is carried out. The use of methods of neural network methods Data Mining allows you to effectively solve problems of varying complexity and also makes it possible to obtain interpretable results. © 2022 IEEE.","data mining; Gas; gas consumption; Liquid; Liquid flow rate; machine learning; oil wells; water cut","Machine learning; Controlled parameter; Gas consumption; Large amounts of data; Liquid flow rates; Neural network method; Neural network techniques; Neural-networks; Water cuts; Data mining"
"Aurelia J.E., Rustam Z., Lestari D.","An Analysis of Convolutional Neural Network - Random Forest for Liver Cancer CT Scan Images","10.1109/DASA53625.2021.9682343","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125777329&doi=10.1109%2fDASA53625.2021.9682343&partnerID=40&md5=2bf03dbab7714f08820bfda613010b65","Cancer is an ailment which results from uncontrolled cell growth in the corpus. Meanwhile, liver cancer constitutes one of five categories cancer leading to the highest number of deaths, it is a malignant tumor that starts in the liver. Based on WHO research in 2015, liver cancer is responsible for over 700,000 out of the 9 million deaths resulting from cancer. Although several studies have classified cancer, there are no symptoms to suggest the presence of liver cancer. Therefore, this study used different types of machine learning methods namely Convolutional Neural Networks and Random Forests to classify liver cancer. The Convolutional Neural Networks is a prevalent used with a broad length of application domains. Meanwhile, the Random Forests method has also been applied in the classification. The Convolutional Neural Networks method was utilized in the early stages of the convolution section, while Random Forests was applied in the classification section. A dataset in form of a CT scan image implemented in the algorithm were obtained from the liver patients CT scan. This study aims to evaluate both performance and accuracy and to determine whether the renewal method is more accurate than the Convolutional Neural Networks in classifying the CT scan dataset for liver cancer patients by evaluating the performance results using the Convolutional Neutral Network-Random Forest. The method is also expected to provide higher accuracy for future studies; hence, more database can provide better results for predicting and classifying different diseases. © 2021 IEEE.","classification; convolutional neural networks; liver cancer; machine learning; random forests","Cell proliferation; Classification (of information); Computerized tomography; Convolution; Convolutional neural networks; Diseases; Image classification; Random forests; Applications domains; Classifieds; Convolutional neural network; CT-scan; CT-scan images; Liver cancers; Machine learning methods; Malignant tumors; Performance; Random forests; Decision trees"
"Aurentz J.L., Navarro A.M., Insua D.R.","Learning the Rules of the Game: An Interpretable AI for Learning How to Play","10.1109/TG.2021.3066245","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103207406&doi=10.1109%2fTG.2021.3066245&partnerID=40&md5=5b416aaa015971b5e6e157c29f284eaa","In this article, we present an interpretable artificial intelligence, and its associated machine learning algorithm, that is capable of automatically learning the rules of a game whenever the rules - the relationship between a player's current state and their corresponding set of legal moves - can be represented as a set of low degree Zhegalkin polynomials, a special class of Boolean functions. This is true for many popular games including Spanish Dominó and the card game President. Our method takes advantage of such low polynomial degree to compute an exact representation of the rules in polynomial time instead of the required exponential time for generic Boolean functions. The rules can also be represented using significantly less storage than in the generic case which, for many games, leads to a representation that is easy to interpret. © 2018 IEEE.","Boolean functions; game rules; interpretable artificial intelligence; zhegalkin polynomials","Boolean functions; Machine learning; Polynomial approximation; Card games; Exact representations; Exponential time; Low degree; Polynomial degree; Polynomial-time; Special class; Zhegalkin polynomials; Learning algorithms"
"Auslander N., Gussow A.B., Koonin E.V.","Incorporating machine learning into established bioinformatics frameworks","10.3390/ijms22062903","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102169078&doi=10.3390%2fijms22062903&partnerID=40&md5=30d4e5deb739cd3efdb9eea2bdb512a2","The exponential growth of biomedical data in recent years has urged the application of numerous machine learning techniques to address emerging problems in biology and clinical re-search. By enabling the automatic feature extraction, selection, and generation of predictive models, these methods can be used to efficiently study complex biological systems. Machine learning techniques are frequently integrated with bioinformatic methods, as well as curated databases and biological networks, to enhance training and validation, identify the best interpretable features, and enable feature and model investigation. Here, we review recently developed methods that incorpo-rate machine learning within the same framework with techniques from molecular evolution, protein structure analysis, systems biology, and disease genomics. We outline the challenges posed for machine learning, and, in particular, deep learning in biomedicine, and suggest unique opportuni-ties for machine learning techniques integrated with established bioinformatics approaches to over-come some of these challenges. © by the author. Licensee MDPI, Basel, Switzerland.","Bioinformatics methods; Deep learning; Machine learning; Phylogenetics","biological marker; bioinformatics; biomedicine; deep learning; genomics; human; machine learning; medical research; molecular evolution; protein structure; Review; structure analysis; systems biology; algorithm; biology; factual database; machine learning; Algorithms; Computational Biology; Databases, Factual; Humans; Machine Learning; Systems Biology"
"Autto T., Kultanen J., Uusnakki J., Ovaska M., Kariluoto A., Himmanen J., Frantti T., Abrahamsson P., Virtaneva M., Kaitila P.","Visualizing Human Interactions in a Workspace Setting and Maintaining Privacy","10.1109/QRS-C55045.2021.00072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140916545&doi=10.1109%2fQRS-C55045.2021.00072&partnerID=40&md5=002126b50e932a26cb18f05106a5c6fe","To better understand how humans interact in office setting, it is necessary to provide sufficiently understandable tooling for interaction analysis. At the same time there is a requirement to maintain the balance between privacy and utility. Using interaction measurement framework developed by the MIT, we utilized network analysis for determining an approximation of location, voice activity detection (VAD) for determining discussions, and spatio-temporal analysis (STA) for creating a visualization. We aimed to provide tooling for evasive analysis of workplace interactions, which would be intuitive enough to be understandable by non-technically oriented actors. Here we detail implementation details and decision reasoning. © 2021 IEEE.","Data visualization; Human interactions; Location approximation; Network analysis; Privacy preserving data mining; Sociometric badges; Spatio-temporal analysis","Data mining; Electric network analysis; Privacy-preserving techniques; Speech recognition; Humaninteraction; Interaction analysis; Location approximation; Privacy-preserving data mining; Sociometric badge; Sociometrics; Spatiotemporal analysis; Voice-activity detections; Data visualization"
"Avdic A., Åkerblom L.","Flipped classroom and learning strategies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977083327&partnerID=40&md5=d3bd21df1a82e1f02033ca84a6d15905","This paper seeks to answer the research question, ""How does the flipped classroom affect students' learning strategies?"". In e-learning research, several studies have focused on how students and teachers perceive the flipped classroom approach. In general, these studies have reported pleasing results. Nonetheless, few, if any, have attempted to identify the potential effects of the flipped classroom approach on how students learn. This study was based on two cases: 1) a business modelling course and 2) a research methodology course. In both cases, participating students were from information systems courses at Dalarna University in Sweden. Recorded lectures replaced regular lectures. The recorded lectures were followed by seminars that focused on the learning content of each lecture in various ways. Three weeks after the final seminar, we arranged for two focus group interviews to take place per course, with 8 to 10 students participating in each group. We asked open questions on how the students thought they had been affected and more specific questions that were generated from a literature study on the effects of flipped classroom courses. These questions dealt with issues of mobility, the potential for repeating lectures, formative feedback, the role of seminars, responsibility, empowerment, lectures before seminars, and any problems encountered. Our results show that, on completion of the courses, students thought differently about learning in relation to more traditional approaches, especially regarding the need to be more active. Most students enjoyed the mobility aspect of the flipped classroom approach, as well the accessibility of recorded lectures. However, a few claimed it demanded a more disciplined attitude. Most students expressed a feeling of increased activity and responsibility when participating in seminars. Some even felt empowered, because they could influence seminar content. The length of recorded lectures and the opportunity to navigate within them were also considered important. The arrangement of the seminar rooms should promote face-to-face discussions. Finally, the types of questions and tasks were found to affect the outcomes of the seminars. In conclusion, we found that, if students are to be active, responsible, empowered, and critical, they have to be informed. They also need to have the opportunities and mandate to influence how, where and when to learn. Finally, they should be able to receive continuous feedback during the learning process. A flipped classroom approach can support such a learning strategy.","Active learning; Empowerment; Flipped classroom; Learning strategies; Responsibility","Artificial intelligence; E-learning; Education; Learning systems; Systems engineering; Teaching; Active Learning; Empowerment; Flipped classroom; Learning strategy; Responsibility; Students"
"Avdičaušević E., Lenič M., Kokol P.","Unsupervised learning using multivariate symbolic hybrid",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-17944389242&partnerID=40&md5=8d05fae8a92fc4f99089799b46edee4f","One of the most challenging tasks in the area of knowledge discovery is to express learned knowledge in a form, which can be understood by domain experts (e.g. medical experts). In the paper we present our approach to unsupervised learning using multivariate symbolic hybrid. Main advantage of multimethod symbolic hybrid is that learned knowledge is expressed in a form of symbolic rules. Learned knowledge is much more understandable to domain experts, which increases its value and makes it much easier to apply.",,"Data mining; Database systems; Knowledge based systems; Medical applications; Unsupervised learning; Unsupervised learning; Learning systems; Medical education; Domain experts; Medical experts; Multi methods"
"Avelino J.N.M., Felizmenio Jr E.P., Naval Jr P.C.","Unraveling COVID-19 Misinformation with Latent Dirichlet Allocation and CatBoost","10.1007/978-3-031-16210-7_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140465898&doi=10.1007%2f978-3-031-16210-7_2&partnerID=40&md5=b85a2017d7504dfc43a96524c1a31637","The COVID-19 pandemic brought upon a plethora of misinformation from fake news articles and posts on social media platforms. This necessitates the task of identifying whether a particular piece of information about COVID-19 is legitimate or not. However, with excessive misinformation spreading rapidly over the internet, manual verification of sources becomes infeasible. Several studies have already explored the use of machine learning towards automating COVID-19 misinformation detection. This paper will investigate COVID-19 misinformation detection in three parts. First, we identify the common themes found in COVID-19 misinformation data using Latent Dirichlet Allocation (LDA). Second, we use CatBoost as a classifier for detecting misinformation and compare its performance against other classifiers such as SVM, XGBoost, and LightGBM. Lastly, we highlight CatBoost’s most important features and decision-making mechanism using Shapley values. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","CatBoost; COVID-19; Explainable AI; Latent Dirichlet Allocation; Misinformation detection","Artificial intelligence; Decision making; Fake detection; Statistics; Catboost; Decision-making mechanisms; Explainable AI; Important features; Latent Dirichlet allocation; Machine-learning; Misinformation detection; News articles; Performance; Social media platforms; COVID-19"
"Averkin A., Yarushev S.","Explainable Artificial Intelligence: Rules Extraction from Neural Networks","10.1007/978-3-030-92127-9_17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123315188&doi=10.1007%2f978-3-030-92127-9_17&partnerID=40&md5=8239fcbf0c7f70610dfb46d0c6661f2b","Researchers are currently working on the development of “Explainable Artificial Intelligence” or “Explainable Artificial Intelligence (XAI)”. Such systems are designed to help the user understand the decisions made by the neural network, which will increase confidence in such systems, will allow making more effective decisions based on the results of the system operation. The first results of applying this approach allowed developers and users to study the factors that are used by the neural network to solve a specific problem and what parameters of the neural network need to be changed to improve the accuracy of its work. In addition, studying how neural networks extract, store, and transform knowledge may be useful for the future development of machine learning methods. To overcome this disadvantage of neural networks, it is proposed to consider methods for extracting rules from neural networks, which can become a link between symbolic and connectionistic models of knowledge representation in artificial intelligence. In this paper, we propose a neuro-fuzzy approach to rule extraction using time series forecasting and text recognition as examples. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Artificial Intelligence; Explainable Artificial Intelligence; Neural networks; Rules extraction; XAI",
"Averkin A.","Explanatory artificial intelligence, results and prospects",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116638542&partnerID=40&md5=95920f55e960cb2f0e052d1e45084b02","Describes the DARPA Explanatory Artificial Intelligence (XAI) program, which seeks to create artificial intelligence systems whose learning models and solutions can be understood and properly validated by end users. DARPA considers XAI as artificial intelligence systems AI that can explain their decision to a human user, characterize their strengths and weaknesses, and how they will behave in the future. To achieve this goal, methods have been developed for constructing explainable models of intelligent systems that are effective explanatory interfaces and psychological models of users for effective explanation. The XAI development teams are described that solve these three problems by creating and developing explainable machine learning (ML) technologies, developing principles, strategies, and methods of human-computer interaction for obtaining effective explanations and applying psychological explanatory theories to assess the quality of XAI systems. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","DARPA; Explainable artificial intelligence; Machine learning; Neural networks","Human computer interaction; Intelligent systems; Artificial intelligence systems; DARPA; Development teams; End-users; Explainable artificial intelligence; Human users; Interface modeling; Learning models; Neural-networks; Psychological model; Machine learning"
"Averkin A.N., Yarushev S.A.","Review of Research in the Field of Developing Methods to Extract Rules From Artificial Neural Networks","10.1134/S1064230721060046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121449439&doi=10.1134%2fS1064230721060046&partnerID=40&md5=7db311c4808b16c1195521caab81270a","Abstract: A large-scale review and analysis of the existing methods and approaches to extract rules from artificial neural networks, including deep learning neural networks, is carried out. A wide range of methods and approaches to extract rules and related approaches to develop explainable artificial intelligence (AI) systems are considered. The taxonomy and several directions in studies of explainable neural networks related to the extraction of rules from neural networks, which allow the user to get an idea of how the neural network uses the input data, and also, using rules, to reveal the hidden relationships of the input data and the results found, are explored. This review focuses on the relationship of the most common rule-based explanation systems in AI with the most powerful machine learning algorithms using neural networks. In addition to rule extraction, other methods of constructing explainable AI systems are considered based on the construction of special modules that interpret each step of changing the neural network’s weights. A comprehensive analysis of the existing research makes it possible to draw conclusions about the appropriateness of using certain approaches. The results of the analysis will allow us to get a detailed picture of the state of research in this area and create our own applications based on neural networks, the results of which can be studied in detail and their reliability evaluated. The development of such systems is necessary for the development of the digital economy in Russia and the creation of applications that allow making responsible and explainable management decisions in critical areas of the national economy. © 2021, Pleiades Publishing, Ltd.",,"Data mining; Deep learning; Extraction; Input output programs; Learning algorithms; Reliability analysis; Artificial intelligence systems; Comprehensive analysis; Explanation systems; Input datas; Large-scales; Learning neural networks; Machine learning algorithms; Neural-networks; Rule based; Rules extraction; Neural networks"
"Aversano L., Bernardi M.L., Cimitile M., Ducange P., Fazzolari M., Pecori R.","An Explainable and Evolving Car Driver Identification System based on Decision Trees","10.1109/EAIS51927.2022.9787517","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132253360&doi=10.1109%2fEAIS51927.2022.9787517&partnerID=40&md5=a8467c9afb236e5f3e40bfde25a72e5b","Shared mobility represents a more and more widespread model ensuring several advantages for citizens and reducing gas emissions. The birth of car-sharing models drives the necessity to use car monitoring systems able to reduce the possibility that unauthorized people drive a certain car. In this paper, we discuss the architecture of car driver identification systems based on incremental fuzzy decision trees. The main features of the proposed system are i) the explainability, namely the possibility of giving explanations regarding its decisions, provided in terms of linguistic rules, and ii) the possibility of continuously updating the classification model. We show the preliminary results of an experimental campaign in which we compare both fuzzy and non-fuzzy incremental decision trees, both in terms of classification performance and model complexity/explainability. © 2022 IEEE.","Car driver identification; Data Stream Classification; Explainable Artificial Intelligence; Fuzzy Decision Tree","Classification (of information); Data mining; Digital storage; Fuzzy inference; Gas emissions; Car driver; Car driver identification; Classification models; Data stream classifications; Driver identification; Explainable artificial intelligence; Fuzzy decision trees; Model drive; Monitoring system; Reducing gas; Decision trees"
"Aviles-Rivero A.I., Sellars P., Schönlieb C.-B., Papadakis N.","GraphXCOVID: Explainable deep graph diffusion pseudo-Labelling for identifying COVID-19 on chest X-rays","10.1016/j.patcog.2021.108274","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114264218&doi=10.1016%2fj.patcog.2021.108274&partnerID=40&md5=81c79dec2d940f97b5ce5be186677463","Can one learn to diagnose COVID-19 under extreme minimal supervision? Since the outbreak of the novel COVID-19 there has been a rush for developing automatic techniques for expert-level disease identification on Chest X-ray data. In particular, the use of deep supervised learning has become the go-to paradigm. However, the performance of such models is heavily dependent on the availability of a large and representative labelled dataset. The creation of which is a heavily expensive and time consuming task, and especially imposes a great challenge for a novel disease. Semi-supervised learning has shown the ability to match the incredible performance of supervised models whilst requiring a small fraction of the labelled examples. This makes the semi supervised paradigm an attractive option for identifying COVID-19. In this work, we introduce a graph based deep semi-supervised framework for classifying COVID-19 from chest X-rays. Our framework introduces an optimisation model for graph diffusion that reinforces the natural relation among the tiny labelled set and the vast unlabelled data. We then connect the diffusion prediction output as pseudo-labels that are used in an iterative scheme in a deep net. We demonstrate, through our experiments, that our model is able to outperform the current leading supervised model with a tiny fraction of the labelled examples. Finally, we provide attention maps to accommodate the radiologist's mental model, better fitting their perceptual and cognitive abilities. These visualisation aims to assist the radiologist in judging whether the diagnostic is correct or not, and in consequence to accelerate the decision. © 2021 Elsevier Ltd","Chest X-ray; COVID-19; Deep learning; Explainability; Semi-Supervised learning","Deep learning; Diagnosis; Graphic methods; Iterative methods; Large dataset; Supervised learning; Automatic technique; Chest X-ray; COVID-19; Deep learning; Explainability; Labelings; Learn+; Performance; Semi-supervised; Semi-supervised learning; Diffusion"
"Avisdris N., Link Sourani D., Ben-Sira L., Joskowicz L., Malinger G., Yagel S., Miller E., Ben Bashat D.","Improved differentiation between hypo/hypertelorism and normal fetuses based on MRI using automatic ocular biometric measurements, ocular ratios, and machine learning multi-parametric classification","10.1007/s00330-022-08976-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134214452&doi=10.1007%2fs00330-022-08976-0&partnerID=40&md5=885fbd4d29a5793d3a6c06256e9874ab","Objectives: To differentiate hypo-/hypertelorism (abnormal) from normal fetuses using automatic biometric measurements and machine learning (ML) classification based on MRI. Methods: MRI data of normal (n = 244) and abnormal (n = 52) fetuses of 22–40 weeks’ gestational age (GA), scanned between March 2008 and June 2020 on 1.5/3T systems with various T2-weighted sequences and image resolutions, were included. A fully automatic method including deep learning and geometric algorithms was developed to measure the binocular (BOD), inter-ocular (IOD), ocular (OD) diameters, and ocular volume (OV). Two new parameters, BOD-ratio and IOD-ratio, were defined as the ratio between BOD/IOD relative to the sum of both globes’ OD, respectively. Eight ML classifiers were evaluated to detect abnormalities using measured and computed parameters. Results: The automatic method yielded a mean difference of BOD = 0.70 mm, IOD = 0.81 mm, OD = 1.00 mm, and a 3D-Dice score of OV = 93.7%. In normal fetuses, all four measurements increased with GA. Constant values were detected for BOD-ratio = 1.56 ± 0.05 and IOD-ratio = 0.60 ± 0.05 across all GA and when calculated from previously published reference data of both MRI and ultrasound. A random forest classifier yielded the best results on an independent test set (n = 58): AUC-ROC = 0.941 and F1-Score = 0.711 in comparison to AUC-ROC = 0.650 and F1-Score = 0.385 achieved based on the accepted criteria that define hypo/hypertelorism based on IOD (&lt; 5th or &gt; 95th percentiles). Using the explainable ML method, the two computed ratios were found as the most contributing parameters. Conclusions: The developed fully automatic method demonstrates high performance on varied clinical imaging data. The new BOD and IOD ratios and ML multi-parametric classifier are suggested to improve the differentiation of hypo-/hypertelorism from normal fetuses. Key Points: • A fully automatic method for computing fetal ocular biometry from MRI is proposed, achieving high performance, comparable to that of an expert fetal neuro-radiologist. • Two new parameters, IOD-ratio and BOD-ratio, are proposed for routine clinical use in ultrasound and MRI. These two ratios are constant across gestational age in normal fetuses, consistent across studies, and differentiate between fetuses with and without hypo/hypertelorism. • Multi-parametric machine learning classification based on automatic measurements and the two new ratios improves the identification of fetal ocular anomalies beyond the accepted criteria (&lt;5thor &gt;95thIOD percentiles). © 2022, The Author(s), under exclusive licence to European Society of Radiology.","Biometry; Deep learning; Fetus; Hypertelorism; Magnetic resonance imaging",
"Aviso K.B., Capili M.J., Chin H.H., van Fan Y., Klemeš J.J., Tan R.R.","Detecting Patterns in Energy Use and Greenhouse Gas Emissions of Cities Using Machine Learning","10.3303/CET2188067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122559107&doi=10.3303%2fCET2188067&partnerID=40&md5=737a3bceedd36ffb2bf6bd196425c399","Cities are expected to play a major role in managing climate change in the coming decades. The actual environmental performance of urban centres is difficult to predict due to the complex interplay of technologies and infrastructure with social, economic, and political factors. Machine learning (ML) techniques can be used to detect patterns in high-level city data to determine factors that influence favourable climate performance. In this work, rough set-based ML (RSML) is used to identify such patterns in the Sustainable Cities Index (SCI), which ranks 100 of the world's major urban centres based on three broad criteria that cover social, environmental, and economic dimensions. These main criteria are further broken down into 18 detailed criteria that are used to calculate the aggregate SCI scores of the listed cities. Two of the environmental criteria measure energy intensity and greenhouse gas (GHG) emissions. RSML is used to generate interpretable rule-based (if/then) models that predict energy utilisation and GHG emissions performance of cities based on the other criteria in the database. Attribute reduction techniques are used to identify a set of 7 non-redundant criteria for energy use and 9 non-redundant criteria for GHG emissions; 6 criteria are common to these two sets. Then, RSML is used to generate rule-based models. A 10-rule model is determined for energy intensity, while an 11-rule model is found for GHG emissions. Both models were reduced further by eliminating rules with weak generalisation capability. A key insight from the rule-based models is that social, environmental, and economic attributes are associated with energy intensity and GHG emissions due to indirect effects. © 2021, AIDIC Servizi S.r.l.",,
"Aviyente S., Karaaslanli A.","Explainability in Graph Data Science: Interpretability, replicability, and reproducibility of community detection","10.1109/MSP.2022.3149471","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133809629&doi=10.1109%2fMSP.2022.3149471&partnerID=40&md5=0c0685bf71b6de8ac0d46ea893ea6694","In many modern data science problems, data are represented by a graph (network), e.g., social, biological, and communication networks. Over the past decade, numerous signal processing and machine learning (ML) algorithms have been introduced for analyzing graph structured data. With the growth of interest in graphs and graph-based learning tasks in a variety of applications, there is a need to explore explainability in graph data science. In this article, we aim to approach the issue of explainable graph data science, focusing on one of the most fundamental learning tasks, community detection, as it is usually the first step in extracting information from graphs. A community is a dense subnetwork within a larger network that corresponds to a specific function. Despite the success of different community detection methods on synthetic networks with strong modular structure, much remains unknown about the quality and significance of the outputs of these algorithms when applied to real-world networks with unknown modular structure. Inspired by recent advances in explainable artificial intelligence (AI) and ML, in this article, we present methods and metrics from network science to quantify three different aspects of explainability, i.e., interpretability, replicability, and reproducibility, in the context of community detection. © 1991-2012 IEEE.",,"Machine learning; Population dynamics; Signal processing; Structure (composition); Biological networks; Community detection; Graph data; Graph networks; Interpretability; Learning tasks; Modular structures; Problem datum; Replicability; Reproducibilities; Graphic methods"
"Avola D., Bacciu A., Cinque L., Fagioli A., Marini M.R., Taiello R.","Study on transfer learning capabilities for pneumonia classification in chest-x-rays images","10.1016/j.cmpb.2022.106833","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129701097&doi=10.1016%2fj.cmpb.2022.106833&partnerID=40&md5=6a324ec14b4b4cdb42070671d5d936cb","Background:over the last year, the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) and its variants have highlighted the importance of screening tools with high diagnostic accuracy for new illnesses such as COVID-19. In that regard, deep learning approaches have proven as effective solutions for pneumonia classification, especially when considering chest-x-rays images. However, this lung infection can also be caused by other viral, bacterial or fungi pathogens. Consequently, efforts are being poured toward distinguishing the infection source to help clinicians to diagnose the correct disease origin. Following this tendency, this study further explores the effectiveness of established neural network architectures on the pneumonia classification task through the transfer learning paradigm. Methodology:to present a comprehensive comparison, 12 well-known ImageNet pre-trained models were fine-tuned and used to discriminate among chest-x-rays of healthy people, and those showing pneumonia symptoms derived from either a viral (i.e., generic or SARS-CoV-2) or bacterial source. Furthermore, since a common public collection distinguishing between such categories is currently not available, two distinct datasets of chest-x-rays images, describing the aforementioned sources, were combined and employed to evaluate the various architectures. Results:the experiments were performed using a total of 6330 images split between train, validation, and test sets. For all models, standard classification metrics were computed (e.g., precision, f1-score), and most architectures obtained significant performances, reaching, among the others, up to 84.46% average f1-score when discriminating the four identified classes. Moreover, execution times, areas under the receiver operating characteristic (AUROC), confusion matrices, activation maps computed via the Grad-CAM algorithm, and additional experiments to assess the robustness of each model using only 50%, 20%, and 10% of the training set were also reported to present an informed discussion on the networks classifications. Conclusion: this paper examines the effectiveness of well-known architectures on a joint collection of chest-x-rays presenting pneumonia cases derived from either viral or bacterial sources, with particular attention to SARS-CoV-2 contagions for viral pathogens; demonstrating that existing architectures can effectively diagnose pneumonia sources and suggesting that the transfer learning paradigm could be a crucial asset in diagnosing future unknown illnesses. © 2022","Deep learning; Explainable AI; Pneumonia classification; Transfer learning","Deep learning; Diagnosis; Diseases; Image classification; Network architecture; Bacterial source; Chest X-ray image; Chest x-rays; Deep learning; Explainable AI; F1 scores; Learning paradigms; Pneumonia classification; Severe acute respiratory syndrome coronavirus; Transfer learning; Classification (of information); Article; bacterial infection; classification algorithm; comparative study; controlled study; coronavirus disease 2019; deep learning; diagnostic accuracy; diagnostic test accuracy study; disease classification; false positive result; human; nonhuman; pneumonia; sensitivity and specificity; Severe acute respiratory syndrome coronavirus 2; thorax radiography; transfer of learning; validation process; virus infection; diagnostic imaging; pneumonia; X ray; COVID-19; Deep Learning; Humans; Pneumonia; SARS-CoV-2; X-Rays"
"Avolio M., Fuduli A., Vocaturo E., Zumpano E.","Multiple Instance Learning for Viral Pneumonia Chest X-ray Classification",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137444843&partnerID=40&md5=92fde996df3ead7744d953af8deb9987","At the end of 2019 anew coronavirus, SARS-CoV-2, was identified as responsible for the lung infection, now called COVID-19 (coronavirus disease 2019). Since then there has been an exponential growth of infections and at the beginning of March 2020 the WHO declared the epidemic a global emergency. An early diagnosis of those carrying the virus becomes crucial to contain the spread, morbidity and mortality of the pandemic. The definitive diagnosis is made through specific tests, among which imaging tests play an important role in the care path of the patient with suspected or confirmed COVID-19. Patients with serious COVID-19 typically experience viral pneumonia. This paper uses the Multiple Instance Learning paradigm to classify pneumonia X-ray images, considering three different classes: radiographies of healthy people, radiographies of people with bacterial pneumonia and of people with viral pneumonia. The proposed algorithms, which are very fast in practice, appear promising especially if we take into account that no preprocessing technique has been used. © 2022 CEUR-WS. All rights reserved.","Machine Learning; Multiple Instance Learning; Pneumonia imaging Classification","Diagnosis; Machine learning; Radiography; Viruses; Coronaviruses; Early diagnosis; Exponential growth; Imaging tests; Learning paradigms; Lung infection; Machine-learning; Multiple-instance learning; Pneumonia imaging classification; X-ray image; COVID-19"
"Avraham G., Zuo Y., Drummond T.","Parallel optimal transport gan","10.1109/CVPR.2019.00454","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078809799&doi=10.1109%2fCVPR.2019.00454&partnerID=40&md5=ef09a7df457dc098c71fa18234b4d138","Although Generative Adversarial Networks (GANs) are known for their sharp realism in image generation, they often fail to estimate areas of the data density. This leads to low modal diversity and at times distorted generated samples. These problems essentially arise from poor estimation of the distance metric responsible for training these networks. To address these issues, we introduce an additional regularisation term which performs optimal transport in parallel within a low dimensional representation space. We demonstrate that operating in a low dimension representation of the data distribution benefits from convergence rate gains in estimating the Wasserstein distance, resulting in more stable GAN training. We empirically show that our regulariser achieves a stabilising effect which leads to higher quality of generated samples and increased mode coverage of the given data distribution. Our method achieves significant improvements on the CIFAR-10, Oxford Flowers and CUB Birds datasets over several GAN baselines both qualitatively and quantitatively. © 2019 IEEE.","Deep Learning; Optimization Methods; Representation Learning","Deep learning; Adversarial networks; Convergence rates; Image generations; Low-dimensional representation; Optimal transport; Optimization method; Representation Learning; Wasserstein distance; Computer vision"
"Avramidis G.P., Avramidou M.P., Papakostas G.A.","Rheumatoid Arthritis Diagnosis: Deep Learning vs. Humane","10.3390/app12010010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125407565&doi=10.3390%2fapp12010010&partnerID=40&md5=fcb710ba804416eb58a23dd611a02728","Rheumatoid arthritis (RA) is a systemic autoimmune disease that preferably affects small joints. As the well-timed diagnosis of the disease is essential for the treatment of the patient, several works have been conducted in the field of deep learning to develop fast and accurate automatic methods for RA diagnosis. These works mainly focus on medical images as they use X-ray and ultrasound images as input for their models. In this study, we review the conducted works and compare the methods that use deep learning with the procedure that is commonly followed by a medical doctor for the RA diagnosis. The results show that 93% of the works use only image modalities as input for the models as distinct from the medical procedure where more patient medical data are taken into account. Moreover, only 15% of the works use direct explainability methods, meaning that the efforts for solving the trustworthiness issue of deep learning models were limited. In this context, this work reveals the gap between the deep learning approaches and the medical doctors’ practices traditionally applied and brings to light the weaknesses of the current deep learning technology to be integrated into a trustworthy context inside the existed medical infrastructures. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence; Computer-aided diagnosis; Deep learning; Explainable AI; Medical imaging; Rheumatoid arthritis (RA); Trustworthiness",
"Avvenuti M., Cresci S., Marchetti A., Meletti C., Tesconi M.","EARS (earthquake alert and report system): A real time decision support system for earthquake crisis management","10.1145/2623330.2623358","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907033606&doi=10.1145%2f2623330.2623358&partnerID=40&md5=76eb578d21dfc3a9f8eaa2a7f8ffb975","Social sensing is based on the idea that communities or groups of people can provide a set of information similar to those obtainable from a sensor network. Emergency management is a candidate field of application for social sensing. In this work we describe the design, implementation and deployment of a decision support system for the detection and the damage assessment of earthquakes in Italy. Our system exploits the messages shared in real-time on Twitter, one of the most popular social networks in the world. Data mining and natural language processing techniques are employed to select meaningful and comprehensive sets of tweets. We then apply a burst detection algorithm in order to promptly identify outbreaking seismic events. Detected events are automatically broadcasted by our system via a dedicated Twitter account and by email notifications. In addition, we mine the content of the messages associated to an event to discover knowledge on its consequences. Finally we compare our results with official data provided by the National Institute of Geophysics and Volcanology (INGV), the authority responsible for monitoring seismic events in Italy. The INGV network detects shaking levels produced by the earthquake, but can only model the damage scenario by using empirical relationships. This scenario can be greatly improved with direct information site by site. Results show that the system has a great ability to detect events of a magnitude in the region of 3.5, with relatively low occurrences of false positives. Earthquake detection mostly occurs within seconds of the event and far earlier than the notifications shared by INGV or by other official channels. Thus, we are able to alert interested parties promptly. Information discovered by our system can be extremely useful to all the government agencies interested in mitigating the impact of earthquakes, as well as the news agencies looking for fresh information to publish. © 2014 ACM.","decision support system; disaster management; event detection; social mining; social sensing",
"Awad A.A., Sayed S.G., Salem S.A.","Collaborative Framework for Early Detection of RAT-Bots Attacks","10.1109/ACCESS.2019.2919680","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067380757&doi=10.1109%2fACCESS.2019.2919680&partnerID=40&md5=19030244a264f3f5fb0586149e79a9a6","Attackers tend to use Remote Access Trojans (RATs) to compromise and control a targeted computer, which makes the RAT detection as an active research field. This paper introduces a machine learning-based framework for detecting compromised hosts and networks that are infected by the RAT-Bots. The proposed framework consists of two agents that are integrated to achieve reliable early detection of the RAT-bots. The first agent, the host agent, is responsible for monitoring the system behavior of the running host and raising an alarm for any anomalies. The second agent, the network agent, monitors the network traffic to extract any malicious patterns. The integrated approach improves both the detection ratio and accuracy. However, each approach cannot separately achieve the same performance as the proposed RAT-Bots detection framework. The performance of the introduced framework is evaluated by using real-world benchmark datasets. The experimental results show that the proposed approach can achieve an accuracy of 98.83% with 1.45% false positive rate. © 2013 IEEE.","botnets; Bots; host-based detection; machine learning algorithms; network-based detection; rootkit behavior","Benchmarking; Learning algorithms; Learning systems; Machine learning; Malware; Rat control; Rats; Botnets; Bots; Collaborative framework; False positive rates; Host-based; Integrated approach; Network-based; Remote access trojans; Botnet"
"Awais M.M.","Application of internal model control methods to industrial combustion","10.1016/j.asoc.2004.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-10844285344&doi=10.1016%2fj.asoc.2004.07.001&partnerID=40&md5=1703fd96b1e128f409f12b7a9cdfce41","Most practical systems are inherently non-linear to some extent in their behaviour and for their cost effective, smooth and safe operation, optimised control systems based on the non-linear models are required. To this end many useful techniques such as the stochastic modelling, sliding mode control and adaptive identification and control have been proposed in the literature. However, the high cost of implementation, the inability to capture imprecision with the required level of tolerance, and the inflexibility against distortions in the operating variables, make them less attractive. To this end new artificial intelligence based techniques such as fuzzy logic, neural networks and probabilistic reasoning, are becoming more and more popular. Among these techniques neural networks have an edge over the others, mainly because of their ability to process large amount of available data, subsequent to the development of some interpretable models for solving engineering problems. Moreover, the ability to capture the non-linearities of a real system accurately and the versatility in being able to accommodate with ease, the various conventional and advanced strategies within their structures, make them much more attractive. The problem becomes more computationally worse and uncontrollable when inverse of the system does not exist. This problem is resolved when neural network based techniques such as internal model control (IMC) are applied to the real systems. This paper outlines the application of neural networks based IMC methods for estimation/control of important input and output variables of a 0.5 MW laboratory scale industrial furnace. The application involves inputs such as the airflow rate, swirl number and momentum ratio. The outputs include emission levels of oxides of nitrogen especially nitric oxide. The response to step and staircase inputs has been analysed. The results have been compared with standard linear quadratic controller. The control output of the IMC methods has resulted in almost similar steady state error performance to the linear quadratic regulator. Although the development process of the IMC method might take longer time because of the training and data arrangement but has the capability of readjustment after being developed. © 2004 Elsevier B.V. All rights reserved.","Burnout; Furnace; Internal model control; Momentum ratio; Neural networks; Nitric oxides","Combustion; Formal logic; Mathematical models; Neural networks; Nitrogen oxides; Probability distributions; Problem solving; Burnout; Internal model control (IMC); Momentum ratio; Nitric oxides; Industrial furnaces"
"Awan U., Kraslawski A., Huiskonen J.","Governing interfirm relationships for social sustainability: The relationship between governance mechanisms, sustainable collaboration, and cultural intelligence","10.3390/su10124473","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057437713&doi=10.3390%2fsu10124473&partnerID=40&md5=6df9e85c42e0a19a66552cf58492fd6d","The concept of social sustainability is gaining attention within the field of supply chain relationships and international business. There are conflicting arguments regarding the effectiveness of contract governance and collaboration in an interfirm relationship. Previous studies have investigated the effect of a national culture on contract governance and opportunism. This study examines the effects of contract governance on collaboration, incorporating the moderating influence of cultural intelligence. Survey data were collected from 239 export manufacturing firms in different industries. The current authors suggest that contract governance might be more effective under conditions of a greater level of firm cultural intelligence capabilities. Cultural intelligence plays an important role in the shaping and implementation of collaboration and is the key to manage cross-culture relationship management in a supply chain. Cultural intelligence constitutes one potential way for the export industry to manage intercultural differences and profitably achieve an increase in collaboration. Collaboration with a socially responsible partner brings about improved social performance. The social dimensions of sustainability, such as fair labor practices and decent worker conditions, health and safety, no child labor, and employee empowerment must be addressed to accomplish the most sustainable growth. Managers also need to take advantage of cultural intelligence to adapt, collaborate, and share cultural knowledge. © 2018 by the authors.","Buyer-supplier relationship; Contract governance; Cultural difference; Cultural intelligence; Social sustainability performance","artificial intelligence; business development; cultural change; empowerment; governance approach; implementation process; manufactured export; manufacturing; supply chain management; sustainability; sustainable development"
"Awasthi K., Bhattacharya S., Bhattacharya A.","Tissue-specific isoform expression of GNE gene in human tissues","10.1007/s10974-022-09618-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129473242&doi=10.1007%2fs10974-022-09618-0&partnerID=40&md5=92fafc2b0ce457324ff905e5c9914313","Mutations in the sialic acid biosynthesis enzyme GNE lead to a late-onset, debilitating neuromuscular disorder, GNE myopathy, characterized by progressive skeletal muscle weakness. The mechanisms responsible for skeletal muscle specificity, late-onset, and disease progression are unknown. Our main aim is to understand the reason for skeletal muscle-specific phenotype. To answer this question, we have analyzed the expression profile of the GNE gene and its multiple mRNA variants in different human tissues. A combinatorial approach encompassing bioinformatics tools and molecular biology techniques was used. NCBI, Ensembl, and GTEx were used for data mining. The expression analysis of GNE and its variants was performed with cDNA tissue panel using PCR and targeted RNA-seq. Among nine different GNE isoforms reported in this study, transcript variants 1, X1, and X2 were not tissue specific. Transcript variants 1, 6, X1, and X2, were found in skeletal muscles suggesting their possible role in GNE myopathy. In the current study, we present new data about GNE expression patterns in human tissues. Our results suggest that there may be a link between tissue-specific pathology and isoform pattern in skeletal muscles, which could provide clues for the development of new treatment strategies for GNE myopathy. Graphical abstract: [Figure not available: see fulltext.] © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG.","Gene expression; GNE myopathy; Isoforms; Sialic acid; Skeletal muscles",
"Awate S.P., Win L., Yushkevich P., Schultz R.T., Gee J.C.","3D cerebral cortical morphometry in autism: Increased folding in children and adolescents in frontal, parietal, and temporal lobes","10.1007/978-3-540-85988-8_67","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58849101875&doi=10.1007%2f978-3-540-85988-8_67&partnerID=40&md5=d858fe20c2f8ae4c98a1c3ce0f2c38b3","This paper presents a systematic evaluation of cortical folding, or complexity, in autism. It introduces two novel measures to analyze folding in a specific region of interest, which, unlike traditional measures, produce an intuitive easily-interpretable description of folding and inform the nature of folding change by incorporating local surface-patch orientation. This study reports new findings of increased cortical folding in autistics in the frontal, parietal, and temporal lobes, as compared to controls. These differences are stronger in children than adolescents. The paper validates part of the findings using the new measures based on comparisons with traditional measures. Unlike studies in the literature, this paper reports new findings, via a fully 3D folding analysis on all brain lobes, based on the consensus of virtually all 6 folding measures used (2 new, 4 traditional) via rigorous statistical permutation testing. In these ways, this paper not only strengthens some previous clinical findings, but also extends the state of the art in autism research. © 2008 Springer-Verlag Berlin Heidelberg.",,"Children and adolescents; Cortical folding; Local surfaces; Morphometry; Permutation testing; Region of interest; State of the art; Systematic evaluation; Temporal lobes; Computer science; Medical computing; Medical imaging; Three dimensional; adolescent; adult; algorithm; article; artificial intelligence; autism; automated pattern recognition; biological model; brain cortex; child; computer assisted diagnosis; computer simulation; female; human; image enhancement; male; methodology; nuclear magnetic resonance imaging; pathology; reproducibility; sensitivity and specificity; statistical model; three dimensional imaging; Adolescent; Adult; Algorithms; Artificial Intelligence; Autistic Disorder; Cerebral Cortex; Child; Computer Simulation; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Models, Biological; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity"
"Awotunde J.B., Adeniyi E.A., Ajamu G.J., Balogun G.B., Taofeek-Ibrahim F.A.","Explainable Artificial Intelligence in Genomic Sequence for Healthcare Systems Prediction","10.1007/978-3-030-97929-4_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132652050&doi=10.1007%2f978-3-030-97929-4_19&partnerID=40&md5=e6d8c6afbd82b11e44f87e22d34b1c8e","Various Classification techniques have been developed in past years and applied on genomic sequence for the dynamic modelling. These methods have resulted to impressive answers in term of correctness and analytical capability. Most of their techniques and applications based on Black-box models that use more understandable methodologies that are supported and verified by the scientific world, thus limited the power of interpretations. Despite the development and application of many statistical and machine learning approaches to expose genomic sequence for disease prediction, integrative understanding of the massive statistical and ML remains a challenge. Hence, the introduction and application of Explainable Artificial Intelligence (XAI) paradigm has provides a solution for this problem, were rule-based methods are particularly well suited to explanatory purposes. Additional steps toward more explanatory and genomic sequence sound models include integrating the technique of data gathering with sequence analysis and route studies. Therefore, this chapter present the applicability of XAI in genomic sequence for healthcare system. Also, the chapter discusses the challenges facing using eXplainable AI in genomic sequence for disease prediction and diagnosis, and in the healthcare system generally. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Black-box model; Dynamic modelling; Explainable artificial intelligence; Genomic sequence; Machine learning; Pandemic prediction",
"Ayachit S.S., Kumar T., Deshpande S., Sharma N., Chaurasia K., Dixit M.","Predicting H1N1 and Seasonal Flu : Vaccine Cases using Ensemble Learning approach","10.1109/ICACCCN51052.2020.9362909","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102567157&doi=10.1109%2fICACCCN51052.2020.9362909&partnerID=40&md5=f4af330f92ee2a90a5ed433ce0e894cc","We propose a data-driven machine learning model to predict the likelihood of a person being vaccinated against H1N1 and seasonal flu. In 2009, a pandemic that was caused by the H1N1 influenza virus, named ""swine flu"", spread like wildfire across the world. Researchers and Healthcare specialists estimated that in the initial year, it was responsible for between \underline{150, 000\ {\text{to}}\ 600, 000\ {\text{deaths}}} in the whole world. A vaccine against the H1N1 (swine flu) virus was made available in October 2009. We have implemented 9 machine learning models. The models include MlBox (model1), TPOT (model 2), Random forest (model 3), MLP (model 4), Linear regression, (model 5), Decision trees (model 6), polynomial feature (model 7), XgBoost (model 8) and CatBoost (model 9). Out of these 9 models, CatBoost gave the best performance with an accuracy of 0.8617 followed by the XgBoost and MlBox. Hence, this study showed that cast boost gave the best performance of all the models and can be used for further prediction models in this category. © 2020 IEEE.","AutoML algorithms; ensemble learning; gradient boosting algorithms; H1N1 vaccine; Machine learning; seasonal flu vaccine","Decision trees; Forecasting; Machine learning; Vaccines; Viruses; Data driven; Ensemble learning approach; H1N1 influenza virus; Machine learning models; Prediction model; Predictive analytics"
"Ayankoya K., Calitz A.P., Greyling J.H.","Using neural networks for predicting futures contract prices of white maize in South Africa","10.1145/2987491.2987508","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994252956&doi=10.1145%2f2987491.2987508&partnerID=40&md5=86e8b3ef044cde47a22e04090a6d76f7","The growing ability to collect and integrate data from disparate sources on a larger scale creates new opportunities for improved decision making. The concepts of using data for predictions by using statistical or computational intelligence models have been researched extensively. However, the tools and techniques that make it possible to analyse the data in real-time as it is created brings about additional opportunities for discovering useful patterns and timely actionable insights. The prices of agricultural grain commodities are known to be volatile due to several factors that influence the prices of grain commodities. Moreover, different combinations of these factors are responsible for the price volatility at different times. This paper carries out a real-time prediction of the futures contract prices of white maize in South Africa as a case study for the use of neural networks for predictive analytics in the financial markets. The predictive analytics model implemented in this study takes into consideration the volatility of the market and the need for the model to be contextual. Relevant data from disparate sources were identified, acquired and integrated into a single source. Thereafter, an exploratory analysis was carried out to understand the relationships that exist between the acquired datasets. A predictive model for the December futures contract prices of white maize on the Johannesburg Stock Exchange (JSE) was proposed using the Back Propagation Neural Network and SAP HANA was used as the enabling technology. The proposed model was used to predict the futures contract prices of white maize on the JSE over a period of time. The validation and evaluation of the proposed model indicate that this approach can be used to predict the futures prices of white maize in South Africa and can be incorporated into a Decision Support System for relevant stakeholders. © 2016 ACM.","Future Grain Prices; Neural Networks; Predictive Analytics","Artificial intelligence; Backpropagation; Commerce; Costs; Decision making; Decision support systems; Electronic trading; Engineers; Financial markets; Forecasting; Grain (agricultural product); Neural networks; Back propagation neural networks; Enabling technologies; Exploratory analysis; Future Grain Prices; Predictive analytics; Predictive modeling; Real-time prediction; Tools and techniques; Contracts"
"Ayats H., Cellier P., Ferré S.","A Two-Step Approach for Explainable Relation Extraction","10.1007/978-3-031-01333-1_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128712673&doi=10.1007%2f978-3-031-01333-1_2&partnerID=40&md5=c685f0631aa15696f3a72303c0630c3d","Knowledge Graphs (KG) offer easy-to-process information. An important issue to build a KG from texts is the Relation Extraction (RE) task that identifies and labels relationships between entity mentions. In this paper, to address the RE problem, we propose to combine a deep learning approach for relation detection, and a symbolic method for relation classification. It allows to have at the same time the performance of deep learning methods and the interpretability of symbolic methods. This method has been evaluated and compared with state-of-the-art methods on TACRED, a relation extraction benchmark, and has shown interesting quantitative and qualitative results. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,"Deep learning; Knowledge graph; Knowledge graphs; Learning approach; Learning methods; Performance; Process information; Relation classifications; Relation extraction; Relationships between entities; Symbolic methods; Two-step approach; Extraction"
"Ayats H., Cellier P., Ferré S.","Extracting Relations in Texts with Concepts of Neighbours","10.1007/978-3-030-77867-5_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111431238&doi=10.1007%2f978-3-030-77867-5_10&partnerID=40&md5=ce822622c48091c74f8d2d72475eb3f6","During the last decade, the need for reliable and massive Knowledge Graphs (KG) increased. KGs can be created in several ways: manually with forms or automatically with Information Extraction (IE), a natural language processing task for extracting knowledge from text. Relation Extraction is the part of IE that focuses on identifying relations between named entities in texts, which amounts to find new edges in a KG. Most recent approaches rely on deep learning, achieving state-of-the-art performances. However, those performances are still too low to fully automatize the construction of reliable KGs, and human interaction remains necessary. This is made difficult by the statistical nature of deep learning methods that makes their predictions hardly interpretable. In this paper, we present a new symbolic and interpretable approach for Relation Extraction in texts. It is based on a modeling of the lexical and syntactic structure of text as a knowledge graph, and it exploits Concepts of Neighbours, a method based on Graph-FCA for computing similarities in knowledge graphs. An evaluation has been performed on a subset of TACRED (a relation extraction benchmark), showing promising results. © 2021, Springer Nature Switzerland AG.",,"Deep learning; Information analysis; Knowledge representation; Learning systems; Natural language processing systems; Syntactics; Human interactions; Knowledge graphs; Learning methods; Named entities; NAtural language processing; Relation extraction; State-of-the-art performance; Syntactic structure; Formal concept analysis"
"Aydin A.","Identifying the relationship of teacher candidates' humor styles with anxiety and self-compassion levels [Ö.gretmen Adaylarinin Mizah Tarzlari Ile Kaygi Ve Ö.z-Duyarlik Düzeyleri Arasindaki Disimi! Belirlenmesi]","10.14689/ejer.2015.59.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937946208&doi=10.14689%2fejer.2015.59.1&partnerID=40&md5=82a5967912873e778656810cf8a0e1d7","Problem Statement: Teacher candidates who will soon be responsible for educating the future generations should possess certain characteristics. Specific teacher candidates should have specific characteristics taken into consideration: pre-school and primary teacher candidates should be seen as role models by younger students; psychological counseling and guidance teacher candidates should guide students in terms of choice of profession and provide counseling in case of problems; and special education teacher candidates should be fully equipped with the skills to handle students with special needs and characteristics. Purpose of Study: This study aims to identify the relationship between teacher candidates' humor styles, anxiety, and self-compassion levels, and to investigate these levels from the perspective of gender and grade variables. Method: In this research study, a multiple regression analysis was applied in order to explore the relationship between candidate teachers' humor styles and their anxiety as well as self-compassion levels. The sample of the study is comprised of a total of 1008 students studying in the following departments of the Atatürk Education Faculty at Marmara University: early childhood education, primary school teaching, psychological counseling and guidance, and special education. As data collection instruments, the Self-Compassion Scale, the Humor Styles Questionnaire, and the State and Trait Anxiety Scale were used. Findings and Results: According to the findings of the study, the t values calculated to find the significance of the regression coefficients indicate that isolation and self-judgment sub-dimensions have significant.","Cooperative learning; Deep learning; Learning; Learning style; Surface",
"Ayedoun E., Tokumaru M.","Towards Emotionally Expressive Virtual Agent to Foster Independent Speaking Tasks: A Preliminary Study","10.1007/978-3-031-06391-6_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133173864&doi=10.1007%2f978-3-031-06391-6_2&partnerID=40&md5=9d6c6f2be968f7a6f5b9c48c2a576763","When communicating with second language learners, empathetic conversational partners that can convey a suitable amount of emotional feedbacks may play a facilitative role in instilling positive emotions and thus catalyze learner’s production of the target language. On the other hand, facial expressions play a preponderant role in communicating emotion, intent, and even desired actions in a readily interpretable fashion. Moreover, the facial feedback hypothesis, an important part of several contemporary theories of emotion, suggests that facial expressions play a causal role in regulating emotional experience and behavior. In this study, we aim to investigate whether and how emotionally expressive computer-based agents that emulate non-verbal facial expressions may convey empathetic support to second language learners during communication tasks. To such extent, we leveraged the Facial Coding Action System and implemented a prototype virtual agent that can display a set of nonverbal feedbacks including Ekman’ six basic universal emotions in addition to gazing and nodding behaviors. Then, we designed a Wizard of Oz experiment in which second language learners are assigned independent speaking tasks with a virtual agent whose feedbacks are indirectly driven by the wizard’s facial expressions. In this paper, we present the outlines of our proposed method and our experimental settings towards validating the meaningfulness of our approach. We also present our next steps towards improving the system and validate its meaningfulness through large scale experiments. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Attentive listening; Facial feedbacks; Second language acquisition; Virtual agents","Artificial intelligence; Attentive listening; Catalyse; Emotional feedback; Facial Expressions; Facial feedback; Positive emotions; Second language acquisition; Second language learners; Target language; Virtual agent; Intelligent virtual agents"
"Ayidzoe M.A., Yu Y., Mensah P.K., Cai J., Bawah F.U.","Visual Interpretability of Capsule Network for Medical Image analysis","10.3906/ELK-2107-146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128304982&doi=10.3906%2fELK-2107-146&partnerID=40&md5=dfc127b7e96f3972b0fe137dcc151fda","Deep learning (DL) models are currently not widely deployed for critical tasks such as in health. This is attributable to the”black box,” making it difficult to gain the trust of practitioners. This paper proposes the use of visualizations to enhance performance verification, improve monitoring, ensure understandability, and improve interpretability needed to gain practitioners' confidence. These are demonstrated through the development of a CapsNet model for the recognition of gastrointestinal tract infection. The gastrointestinal tract comprises several organs joined in a long tube from the mouth to the anus. It is susceptive to diseases that are difficult for medics to diagnose, since it is not easy to have physical access to the sick regions. Consequently, manual access and analysis of images of the unhealthy parts requires the skills of an expert, as it is tedious, prone to errors, and costly. Experimental results show that visualizations in the form of post-hoc interpretability can demonstrate the reliability and interpretability of the CapsNet model applied to gastrointestinal tract diseases. The outputs can also be explained to gain practitioners' confidence in paving the way for its adoption in critical areas of society. © TÜBİTAK","Capsule network; convolutional neural network; explainable artificial intelligence (XAI)","Deep learning; Image analysis; Medical imaging; Neural networks; Black boxes; Box making; Capsule network; Convolutional neural network; Critical tasks; Explainable artificial intelligence (XAI); Gastrointestinal tract; Interpretability; Medical image analysis; Performance verification; Visualization"
"Ayinde B.O., Zurada J.M.","Deep Learning of Constrained Autoencoders for Enhanced Understanding of Data","10.1109/TNNLS.2017.2747861","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030765474&doi=10.1109%2fTNNLS.2017.2747861&partnerID=40&md5=4f2e294f174ff91823d0aaa95c53c862","Unsupervised feature extractors are known to perform an efficient and discriminative representation of data. Insight into the mappings they perform and human ability to understand them, however, remain very limited. This is especially prominent when multilayer deep learning architectures are used. This paper demonstrates how to remove these bottlenecks within the architecture of non-negativity constrained autoencoder. It is shown that using both L1 and L2 regularizations that induce non-negativity of weights, most of the weights in the network become constrained to be non-negative, thereby resulting into a more understandable structure with minute deterioration in classification accuracy. Also, this proposed approach extracts features that are more sparse and produces additional output layer sparsification. The method is analyzed for accuracy and feature interpretation on the MNIST data, the NORB normalized uniform object data, and the Reuters text categorization data set. © 2012 IEEE.","Deep learning (DL); part-based representation; receptive field; sparse autoencoder (SAE); white-box model","Computer architecture; Data mining; Data structures; Decoding; Encoding (symbols); Feature extraction; Network architecture; Personnel training; Text processing; Auto encoders; Classification accuracy; Feature extractor; Learning architectures; Part-based representation; Receptive fields; Text categorization; White-box models; Deep learning; article; deep learning; deterioration; human"
"Ayinde B.O., Hosseini-Asl E., Zurada J.M.","Visualizing and understanding nonnegativity constrained sparse autoencoder in deep learning","10.1007/978-3-319-39378-0_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976611238&doi=10.1007%2f978-3-319-39378-0_1&partnerID=40&md5=faf7a85656ee193cc285a9c9459d041d","In this paper, we demonstrate how complex deep learning structures can be understood by humans, if likened to isolated but understandable concepts that use the architecture of Nonnegativity Constrained Autoencoder (NCAE). We show that by constraining most of the weights in the network to be nonnegative using both L1 and L2 nonnegativity penalization, a more understandable structure can result with minute deterioration in classification accuracy. Also, this proposed approach yields a more sparse feature extraction and additional output layer sparsification. The concept is illustrated using MNIST and the NORB datasets. © Springer International Publishing Switzerland 2016.","Deep architecture; Part-based representation; Semi-supervised learning; White-box model","Artificial intelligence; Complex networks; Feature extraction; Network architecture; Soft computing; Supervised learning; Classification accuracy; Deep architectures; Deep learning; Non-negativity; Part-based representation; Semi- supervised learning; Sparsification; White-box models; Learning systems"
"Ayman N., Gharib T.F., Hamdy M., Afify Y.","Influence Ranking Model for Social Networks Users","10.1007/978-3-030-14118-9_91","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064054913&doi=10.1007%2f978-3-030-14118-9_91&partnerID=40&md5=64d040086b5f5ef3c09749ab349bc59d","Microblogging is the most important feature for Social Networks (SN) nowadays. It allows users to interact together by sharing and posting contents. The concept of spreading content between users raises an important question: Who are the users responsible for this content? In other words, the detection of content spreaders becomes one of the most important analytic issues. The common belief is that the best content spreaders are the best connected users (the most central users within network). Specifically, k-shell decomposition methodology defines the most efficient content spreaders as those located within the core of the network. In this paper, influence ranking model (IRM) is presented to rank SN users based on their contribution in spreading a specific content. The proposed model is inspired by the pruning process of the powerful k-shell decomposition methodology. IRM has been evaluated in realistic experiments using the famous datasets of Advogato trust network and Bitcoin Alpha trust weighted signed network. The proposed model was assessed in terms of distinction of nodes ranking and dissemination capability. Results have shown that IRM has promising results in SN users ranking. © 2020, Springer Nature Switzerland AG.","Influential users; K-shell; Microblogging; Social Networks","Learning algorithms; Machine learning; Spreaders; Important features; Influential users; K-shells; Microblogging; Ranking model; Signed networks; Trust networks; Social networking (online)"
"Ayobi A., Stawarz K., Katz D., Marshall P., Yamagata T., Santos-Rodriguez R., Flach P., O'Kane A.A.","Co-Designing Personal Health? Multidisciplinary Benefits and Challenges in Informing Diabetes Self-Care Technologies","10.1145/3479601","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117933041&doi=10.1145%2f3479601&partnerID=40&md5=cc0b32b1a43f0ef77bc8197900214881","Co-design is a widely applied design process with well-documented values, including mutual learning and collective creativity. However, the real-world challenges of conducting multidisciplinary co-design research to inform the design of self-care technologies are not well established. We provide a qualitative account of a multidisciplinary project that aimed to co-design machine learning applications for Type 1 Diabetes (T1D) self-management. Through interviews, we identify not only perceived social, technological and strategic benefits of co-design but also organisational, translational and pragmatic design challenges: participants with T1D experienced difficulties in co-designing systems that met their individual self-care needs as part of group activities; HCI and AI researchers described challenges resulting from applying co-design outcomes to data-driven ML work; and industry collaborators highlighted academic data sharing regulations as cross-organisational challenges that can impede co-design efforts. Based on this understanding, we discuss opportunities for supporting multidisciplinary collaborations and aligning individual health needs with collaborative co-design activities. © 2021 ACM.","co-design; diabetes; explainable artificial intelligence; HCI-AI; human-centred machine learning; participatory design; personal health; self-care; self-management; T1D","Machine learning; Co-designing; Co-designs; Explainable artificial intelligence; HCI-AI; Human-centered machine learning; Participatory design; Personal health; Self management; Self-care; Type 1 diabetes; Design"
"Ayobi A., Stawarz K., Katz D., Marshall P., Yamagata T., Santos-Rodra-Guez R., Flach P., O'Kane A.A.","Machine Learning Explanations as Boundary Objects: How AI Researchers Explain and Non-Experts Perceive Machine Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110523627&partnerID=40&md5=60724a5c00ad3888f1e0b7e1fdaf056f","Understanding artificial intelligence (AI) and machine learning (ML) approaches is becoming increasingly important for people with a wide range of professional backgrounds. However, it is unclear how ML concepts can be effectively explained as part of human-centred and multidisciplinary design processes. We provide a qualitative account of how AI researchers explained and non-experts perceived ML concepts as part of a co-design project that aimed to inform the design of ML applications for diabetes self-care. We identify benefits and challenges of explaining ML concepts with analogical narratives, information visualisations, and publicly available videos. Co-design participants reported not only gaining an improved understanding of ML concepts but also highlighted challenges of understanding ML explanations, including misalignments between scientific models and their lived self-care experiences and individual information needs. We frame our findings through the lens of Stars and Griesemer's concept of boundary objects to discuss how the presentation of user-centred ML explanations could strike a balance between being plastic and robust enough to support design objectives and people's individual information needs. Copyright © 2021 for this paper by its authors.","Ai literacy; Boundary objects; Diabetes; Explainable ai; Explanation","User interfaces; Boundary objects; Co-designs; Multi-disciplinary designs; Professional backgrounds; Self-care; Support design; Through the lens; User-centred; Machine learning"
"Ayodeji A., Amidu M.A., Olatubosun S.A., Addad Y., Ahmed H.","Deep learning for safety assessment of nuclear power reactors: Reliability, explainability, and research opportunities","10.1016/j.pnucene.2022.104339","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135374981&doi=10.1016%2fj.pnucene.2022.104339&partnerID=40&md5=4ccbddcf1a1365cd8686b5e5bb9045b2","Deep learning algorithms provide plausible benefits for efficient prediction and analysis of nuclear reactor safety phenomena. However, research works that discuss the critical challenges with deep learning models from the reactor safety perspective are limited. This article presents the state-of-the-art in deep learning application in nuclear reactor safety analysis, and the inherent limitations in deep learning models. In addition, critical issues such as deep learning model explainability, sensitivity and uncertainty constraints, model reliability, and trustworthiness are discussed from the nuclear safety perspective, and robust solutions to the identified issues are also presented. As a major contribution, a deep feedforward neural network is developed as a surrogate model to predict turbulent eddy viscosity in Reynolds-averaged Navier–Stokes (RANS) simulation. Further, the deep feedforward neural network performance is compared with the conventional Spalart Allmaras closure model in the RANS turbulence closure simulation. In addition, the Shapely Additive Explanation (SHAP) and the local interpretable model-agnostic explanations (LIME) APIs are introduced to explain the deep feedforward neural network predictions. Finally, exciting research opportunities to optimize deep learning-based reactor safety analysis are presented. © 2022 The Authors","Deep learning; Machine learning; Modeling and simulation; Nuclear reactor safety; Reliability; Sensitivity analysis; Uncertainty quantification","Forecasting; Learning algorithms; Learning systems; Lime; Nuclear reactors; Reliability analysis; Uncertainty analysis; Deep learning; Learning models; Machine-learning; Model and simulation; Nuclear reactor safety; Reactor safety; Reactor safety analysis; Research opportunities; Safety assessments; Uncertainty quantifications; Sensitivity analysis"
"Ayodele O.O., Yusoff N.","Explainable deep learning: Methods and challenges",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073429307&partnerID=40&md5=daeaa7fa50ada4a119c72a4d1bbf5d67","Deep Learning (DL) has been seen recently to evolve having worldwide attention due to its state-of-theart performance even exceeding the human level in a range of complex task. Such development is discovered in domains such as image classification, automatic speech recognition. Nevertheless, due to its nested non-linear internal complex structure, this highly successfully Machine Learning (ML) approach is treated as black box manner i.e. no explanation is provided on its decisions making process to which predictions are made in achieving such performance. With the rapid spread of DL across domains, these explanations are vital for critical decision-making process e.g. medical application. Thus, there is an urge to explaining how DL approaches make predictions. This paper presents a survey of various methods aimed at overcoming this critical setback of DL, its challenges and possible future research opportunities. We also present the insufficiency of current explainable approaches via classification of the approach problems and a plea for more explainable DL methods. The classification tends to present to the researcher proposals useful for opening DL models. © 2019 Institute of Advanced Scientific Research, Inc. All rights reserved.","Blackbox; Deep learning; Explainable; Interpretable",
"Ayoobi H., Cao M., Verbrugge R., Verheij B.","Argumentation-Based Online Incremental Learning","10.1109/TASE.2021.3120837","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118559803&doi=10.1109%2fTASE.2021.3120837&partnerID=40&md5=29c475bd94a9ec0e0dff37a525b2058b","The environment around general-purpose service robots has a dynamic nature. Accordingly, even the robot's programmer cannot predict all the possible external failures which the robot may confront. This research proposes an online incremental learning method that can be further used to autonomously handle external failures originating from a change in the environment. Existing research typically offers special-purpose solutions. Furthermore, the current incremental online learning algorithms cannot generalize well with just a few observations. In contrast, our method extracts a set of hypotheses, which can then be used for finding the best recovery behavior at each failure state. The proposed argumentation-based online incremental learning approach uses an abstract and bipolar argumentation framework to extract the most relevant hypotheses and model the defeasibility relation between them. This leads to a novel online incremental learning approach that overcomes the addressed problems and can be used in different domains including robotic applications. We have compared our proposed approach with state-of-the-art online incremental learning approaches, an approximation-based reinforcement learning method, and several online contextual bandit algorithms. The experimental results show that our approach learns more quickly with a lower number of observations and also has higher final precision than the other methods. Note to Practitioners-This work proposes an online incremental learning method that learns faster by using a lower number of failure states than other state-of-the-art approaches. The resulting technique also has higher final learning precision than other methods. Argumentation-based online incremental learning generates an explainable set of rules which can be further used for human-robot interaction. Moreover, testing the proposed method using a publicly available dataset suggests wider applicability of the proposed incremental learning method outside the robotics field wherever an online incremental learner is required. The limitation of the proposed method is that it aims for handling discrete feature values. © 2004-2012 IEEE.","argumentation theory; Argumentation-based learning; general purpose service robots; online incremental learning","Approximation algorithms; E-learning; Learning algorithms; Learning systems; Mobile robots; Online systems; Reinforcement learning; Argumentation theory; Argumentation-based learning; Cognition; Dynamic nature; General purpose service robot.; Learning approach; Machine-learning; Online incremental learning; Predictive models; Service robots; Semantics"
"Ayoub J., Du N., Yang X.J., Zhou F.","Predicting Driver Takeover Time in Conditionally Automated Driving","10.1109/TITS.2022.3154329","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126537495&doi=10.1109%2fTITS.2022.3154329&partnerID=40&md5=cb733bd89e514cb07d869740bb8610ac","It is extremely important to ensure a safe takeover transition in conditionally automated driving. One of the critical factors that quantifies the safe takeover transition is takeover time. Previous studies identified the effects of many factors on takeover time, such as takeover lead time, non-driving tasks, modalities of the takeover requests, and scenario urgency. However, there is a lack of research to predict takeover time by considering these factors all at the same time. Toward this end, we used eXtreme Gradient Boosting (XGBoost) to predict the takeover time using a dataset from a meta-analysis study [Zhang et al. (2019)]. In addition, we used SHAP (SHapley Additive exPlanation) to analyze and explain the effects of the predictors on takeover time. We identified seven most critical predictors that resulted in the best prediction performance. Their main effects and interaction effects on takeover time were examined. The results showed that the proposed approach provided both good performance and explainability. Our findings have implications on the design of in-vehicle monitoring and alert systems to facilitate the interaction between the drivers and the automated vehicle. © 2000-2011 IEEE.","explainable machine learning models; takeover control; Takeover time prediction","Automation; Intelligent systems; Intelligent vehicle highway systems; Machine learning; Vehicles; Computational modelling; Explainable machine learning model; Load modeling; Machine learning models; Prediction algorithms; Predictive models; Takeover control; Takeover Time; Takeover time prediction; Time predictions; Forecasting"
"Ayoub J., Yang X.J., Zhou F.","Combat COVID-19 infodemic using explainable natural language processing models","10.1016/j.ipm.2021.102569","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102851698&doi=10.1016%2fj.ipm.2021.102569&partnerID=40&md5=947a43e0ab4d3fcfbca3b0d3245f8461","Misinformation of COVID-19 is prevalent on social media as the pandemic unfolds, and the associated risks are extremely high. Thus, it is critical to detect and combat such misinformation. Recently, deep learning models using natural language processing techniques, such as BERT (Bidirectional Encoder Representations from Transformers), have achieved great successes in detecting misinformation. In this paper, we proposed an explainable natural language processing model based on DistilBERT and SHAP (Shapley Additive exPlanations) to combat misinformation about COVID-19 due to their efficiency and effectiveness. First, we collected a dataset of 984 claims about COVID-19 with fact-checking. By augmenting the data using back-translation, we doubled the sample size of the dataset and the DistilBERT model was able to obtain good performance (accuracy: 0.972; areas under the curve: 0.993) in detecting misinformation about COVID-19. Our model was also tested on a larger dataset for AAAI2021 — COVID-19 Fake News Detection Shared Task and obtained good performance (accuracy: 0.938; areas under the curve: 0.985). The performance on both datasets was better than traditional machine learning models. Second, in order to boost public trust in model prediction, we employed SHAP to improve model explainability, which was further evaluated using a between-subjects experiment with three conditions, i.e., text (T), text+SHAP explanation (TSE), and text+SHAP explanation+source and evidence (TSESE). The participants were significantly more likely to trust and share information related to COVID-19 in the TSE and TSESE conditions than in the T condition. Our results provided good implications for detecting misinformation about COVID-19 and improving public trust. © 2021 Elsevier Ltd","BERT; COVID-19; DistilBERT; Misinformation detection; SHAP; Trust","Deep learning; Learning algorithms; Bidirectional encoder representation from transformer; Condition; COVID-19; Distilbert; Misinformation detection; Natural languages; Performance; Shapley; Shapley additive explanation; Trust; Natural language processing systems"
"Ayoub M.","A review on machine learning algorithms to predict daylighting inside buildings","10.1016/j.solener.2020.03.104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082725063&doi=10.1016%2fj.solener.2020.03.104&partnerID=40&md5=78a47e78eec2a3a1742fb0ea85a07e58","Steep increases in air temperatures and CO2 emissions have been associated with the global demand for energy. This is coupled with population growth and improved living standards that encourages the reliance on mechanical acclimatization. Lighting energy alone is responsible for a large portion of total energy consumption in office buildings; and the demand for artificial light is expected to grow in the next years. One of sustainable approaches to enhance energy-efficiency is to incorporate daylighting strategies, which entail the controlled use of daylight inside buildings. Daylight simulation is an active area of research that offers accurate estimations, yet requires a complex set of inputs. Even with today's computers, simulations are computationally expensive and time-consuming, hindering to acquire accelerated preliminary approximations in acceptable timeframes, especially for the iterative design alternatives. Alternatively, predictive models that build on machine learning algorithms have granted much interest from the building design community due to their ability to handle such complex non-linear problems, acting as proxies to heavy simulations. This research presents a review on the growing directions that exploit machine learning to rapidly predict daylighting performance inside buildings, putting a particular focus on scopes of prediction, used algorithms, data sources and sizes, besides evaluation metrics. This work should improve architects’ decision-making and increase the applicability to predict daylighting. Another implication is to point towards knowledge gaps and missing opportunities in the related research domain, revealing future trends that allow for such innovative approaches to be exploited more commonly in Architectural practice. © 2020 International Solar Energy Society","Daylighting; Estimation; Illuminance; Machine learning; Prediction; Predictive algorithm","Architectural design; Daylight simulation; Daylighting; Decision making; Energy efficiency; Energy utilization; Estimation; Forecasting; Iterative methods; Learning systems; Machine learning; Office buildings; Population statistics; Structural design; Accurate estimation; Evaluation metrics; Illuminance; Innovative approaches; Nonlinear problems; Population growth; Predictive algorithms; Total energy consumption; Learning algorithms; algorithm; architectural design; daylight; estimation method; knowledge; machine learning; prediction; simulation"
"Ayoub N., Wang K., Kagiyama T., Seki H., Naka Y.","A planning support system for biomass-based power generation","10.1016/S1570-7946(06)80325-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956753752&doi=10.1016%2fS1570-7946%2806%2980325-1&partnerID=40&md5=37a2e1da9ee1069730439b34c27fb996","When planning biomass-based power generation, planners should take into account the different stakeholders along the biomass supply chains, e.g. biomass resources suppliers, transport, conversion and electricity suppliers etc. Also the planners have to consider the social concerns about environmental and economical impacts of establishing the biomass systems. Accordingly, in order to overcome these struggles in sustainable manner, we should take into account the new environmental institutions, e.g. RPS (Renewable Portfolio Standard) to promote them as well as the bioenergy production systems. To address the problems mentioned above, a Planning Support System (PSS) for biomass-based power generation is developed. This paper introduces the general structure for the PSS at both national and local levels of the biomass planning process. The PSS is a user-oriented system which employs data visualization, data analysis and simulation methods in interaction with the knowledge and intentions of the users to provide them with understandable results. A case study on planning forestry residues utilization at the national level is presented. © 2006 Elsevier B.V. All rights reserved.","Bioenergy; Data Mining; Fuzzy C-means Clustering [Shen et al., 2005]; Geographical Information System (GIS); Planning Support Systems (PSS)",
"Ayoub O., Bianco A., Andreoletti D., Troia S., Giordano S., Rottondi C.","On the Application of Explainable Artificial Intelligence to Lightpath QoT Estimation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136944196&partnerID=40&md5=f02bc705875f848020984867ac3b230d","We demonstrate the potentialities of explainable AI when applied to distill knowledge from a trained supervised machine learning model for lightpath quality of transmission estimation in optical networks, with synthetic datasets. © Optica Publishing Group 2022, © 2022 The Author(s)",,"Supervised learning; Lightpaths; Machine learning models; Quality of Transmission; Supervised machine learning; Synthetic datasets; Light transmission"
"Ayoub O., Musumeci F., Ezzeddine F., Passera C., Tornatore M.","On Using Explainable Artificial Intelligence for Failure Identification in Microwave Networks","10.1109/ICIN53892.2022.9758095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129597962&doi=10.1109%2fICIN53892.2022.9758095&partnerID=40&md5=aafb537ee4d880531db964d3b5a2c543","Artificial Intelligence (AI) has demonstrated superhuman capabilities in solving a significant number of tasks, leading to widespread industrial adoption. For in-field networkmanagement application, AI-based solutions, however, have often risen skepticism among practitioners as their internal reasoning is not exposed and their decisions cannot be easily explained, preventing humans from trusting and even understanding them. To address this shortcoming, a new area in AI, called Explainable AI (XAI), is attracting the attention of both academic and industrial researchers. XAI is concerned with explaining and interpreting the internal reasoning and the outcome of AI-based models to achieve more trustable and practical deployment. In this work, we investigate the application of XAI for automated failure-cause identification in microwave networks. We first show how existing supervised ML algorithms can be used to solve the problem of failure-cause identification, achieving an accuracy around 94 %. Then, we explore the application of well-known XAI frameworks (such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME)) to address important practical questions rising during the actual deployment of automated failure-cause identification in microwave networks. These questions, if answered, allow for a deeper understanding of the behavior of the ML algorithm adopted. Precisely, we exploit XAI to understand the main reasons leading to ML algorithm's decisions and to explain why the model makes identification errors over specific instances. © 2022 IEEE.",,"Lime; Failure identification; Identification errors; In-field; Industrial adoption; Microwave networks; Shapley; Artificial intelligence"
"Ayoub O., Bianco A., Andreoletti D., Troia S., Giordano S., Rottondi C.","On the Application of Explainable Artificial Intelligence to Lightpath QoT Estimation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128976233&partnerID=40&md5=78dda9185abe53667549c0f5cd39fa12","We demonstrate the potentialities of explainable AI when applied to distill knowledge from a trained supervised machine learning model for lightpath quality of transmission estimation in optical networks, with synthetic datasets. © 2021 The Author(s)",,"Light transmission; Lightpaths; Machine learning models; Quality of Transmission; Supervised machine learning; Synthetic datasets; Supervised learning"
"Ayoub O., Bianco A., Andreoletti D., Troia S., Giordano S., Rottondi C.","On the Application of Explainable Artificial Intelligence to Lightpath QoT Estimation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136556209&partnerID=40&md5=d880f2a64b92fea326b94cb410b20185","We demonstrate the potentialities of explainable AI when applied to distill knowledge from a trained supervised machine learning model for lightpath quality of transmission estimation in optical networks, with synthetic datasets. © 2022 The Author (s)",,"Supervised learning; Lightpaths; Machine learning models; Quality of Transmission; Supervised machine learning; Synthetic datasets; Light transmission"
"Ayranci P., Lai P., Phan N., Hu H., Kolinowski A., Newman D., Dou D.","OnML: an ontology-based approach for interpretable machine learning","10.1007/s10878-022-00856-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128881928&doi=10.1007%2fs10878-022-00856-z&partnerID=40&md5=b6fbc2ff740a458b99d3a0b2660902fd","In this paper, we introduce a novel interpreting framework that learns an interpretable model based on an ontology-based sampling technique to explain agnostic prediction models. Different from existing approaches, our interpretable algorithm considers contextual correlation among words, described in domain knowledge ontologies, to generate semantic explanations. To narrow down the search space for explanations, which is exponentially large given long and complicated text data, we design a learnable anchor algorithm to better extract local and domain knowledge-oriented explanations. A set of regulations is further introduced, combining learned interpretable representations with anchors and information extraction to generate comprehensible semantic explanations. To carry out an extensive experiment, we first develop a drug abuse ontology (DAO) on a drug abuse dataset on the Twittersphere, and a consumer complaint ontology (ConsO) on a consumer complaint dataset, especially for interpretable ML. Our experimental results show that our approach generates more precise and more insightful explanations compared with a variety of baseline approaches. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Anchor; Information extraction; Interpretable machine learning; Ontology","Data mining; Domain Knowledge; Information retrieval; Machine learning; Semantics; Consumer complaints; Drug abuse; Information extraction; Interpretable machine learning; Learn+; Model-based OPC; Ontology's; Ontology-based; Prediction modelling; Sampling technique; Ontology"
"Ayub M.J., Atwood J., Nuccio A., Tarleton R., Levin M.J.","Proteomic analysis of the Trypanosoma cruzi ribosomal proteins","10.1016/j.bbrc.2009.02.095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-63149095248&doi=10.1016%2fj.bbrc.2009.02.095&partnerID=40&md5=f09ebe4c00308686bcd196349f80ae0a","Trypanosoma cruzi is a parasite responsible for Chagas disease. The identification of new targets for chemotherapy is a major challenge for the control of this disease. Several lines of evidences suggest that the translational system in trypanosomatids show important differences compared to other eukaryotes. However, there little is known information about this. We have performed a detailed data mining search for ribosomal protein genes in T. cruzi genome data base combined with mass spectrometry analysis of purified T. cruzi ribosomes. Our results show that T. cruzi ribosomal proteins have ∼50% sequence identity to yeast ones. Nevertheless, some parasite proteins are longer due to the presence of several N- or C-terminal extensions, which are exclusive of trypanosomatids. In particular, L19 and S21 show C-terminal extensions of 168 and 164 amino acids, respectively. In addition, we detected two 60S subunit proteins that had not been previously detected in the T. cruzi total proteome; namely, L22 and L42. © 2009 Elsevier Inc. All rights reserved.","Mass spectrometry; Protein synthesis; Ribosome; Trypanosoma cruzi","protozoal protein; ribosome protein; amino terminal sequence; article; carboxy terminal sequence; mass spectrometry; nonhuman; nucleotide sequence; priority journal; protein analysis; proteomics; protozoal genetics; ribosome; Trypanosoma cruzi; Amino Acid Sequence; Animals; Molecular Sequence Data; Proteomics; Ribosomal Proteins; Trypanosoma cruzi; Eukaryota; Trypanosoma cruzi; Trypanosomatidae"
"Ayush K., Uzkent B., Burke M., Lobell D., Ermon S.","Generating interpretable poverty maps using object detection in satellite images",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093974861&partnerID=40&md5=4265a37d54f0ae1749ed44106d719db1","Accurate local-level poverty measurement is an essential task for governments and humanitarian organizations to track the progress towards improving livelihoods and distribute scarce resources. Recent computer vision advances in using satellite imagery to predict poverty have shown increasing accuracy, but they do not generate features that are interpretable to policymakers, inhibiting adoption by practitioners. Here we demonstrate an interpretable computational framework to accurately predict poverty at a local level by applying object detectors to high resolution (30cm) satellite images. Using the weighted counts of objects as features, we achieve 0.539 Pearson's r2 in predicting village level poverty in Uganda, a 31% improvement over existing (and less interpretable) benchmarks. Feature importance and ablation analysis reveal intuitive relationships between object counts and poverty predictions. Our results suggest that interpretability does not have to come at the cost of performance, at least in this important domain. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.",,"Artificial intelligence; Forecasting; Object recognition; Satellite imagery; Computational framework; High resolution; Interpretability; Object detectors; Policy makers; Satellite images; Scarce resources; Object detection"
"Ayyar M.P., Benois-Pineau J., Zemmari A.","Review of white box methods for explanations of convolutional neural networks in image classification tasks","10.1117/1.JEI.30.5.050901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118757602&doi=10.1117%2f1.JEI.30.5.050901&partnerID=40&md5=602e0e1b72b240fae231861d1e41d2be","In recent years, deep learning has become prevalent to solve applications from multiple domains. Convolutional neural networks (CNNs) particularly have demonstrated state-of-the-art performance for the task of image classification. However, the decisions made by these networks are not transparent and cannot be directly interpreted by a human. Several approaches have been proposed to explain the reasoning behind a prediction made by a network. We propose a topology of grouping these methods based on their assumptions and implementations. We focus primarily on white box methods that leverage the information of the internal architecture of a network to explain its decision. Given the task of image classification and a trained CNN, our work aims to provide a comprehensive and detailed overview of a set of methods that can be used to create explanation maps for a particular image, which assign an importance score to each pixel of the image based on its contribution to the decision of the network. We also propose a further classification of the white box methods based on their implementations to enable better comparisons and help researchers find methods best suited for different scenarios. © 2021 SPIE and IS&T.","convolutional neural networks; deep learning; explainable artificial intelligence; interpretability; object classification","Convolution; Deep learning; Image classification; Classification tasks; Convolutional neural network; Deep learning; Explainable artificial intelligence; Images classification; Interpretability; Multiple domains; Object classification; State-of-the-art performance; White box; Convolutional neural networks"
"Azab A., Alazab M., Aiash M.","Machine learning based botnet identification traffic","10.1109/TrustCom.2016.0275","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015232360&doi=10.1109%2fTrustCom.2016.0275&partnerID=40&md5=91b9d78ee58746b935ca3e48cca94423","The continued growth of the Internet has resulted in the increasing sophistication of toolkit and methods to conduct computer attacks and intrusions that are easy to use and publicly available to download, such as Zeus botnet toolkit. Botnets are responsible for many cyber-attacks, such as spam, distributed denial-of-service (DDoS), identity theft, and phishing. Most of existence botnet toolkits release updates for new features, development and support. This presents challenges in the detection and prevention of bots. Current botnet detection approaches mostly ineffective as botnets change their Command and Control (C&C) server structures, centralized (e.g., IRC, HTTP), distributed (e.g., P2P), and encryption deterrent. In this paper, based on real world data sets we present our preliminary research on predicting the new bots before they launch their attack. We propose a rich set of features of network traffic using Classification of Network Information Flow Analysis (CONIFA) framework to capture regularities in C&C communication channels and malicious traffic. We present a case study of applying the approach to a popular botnet toolkit, Zeus. The experimental evaluation suggest that it is possible to detect effectively botnets during the botnet C&C communication generated from new updated Zeus botnet toolkit by building the classifier using machine learning from an earlier version and before they launch their attacks using traffic behaviors. Also, show that there is similarity in C&C structures various Botnet toolkit versions and that the network characteristics of botnet C&C traffic is different from legitimate network traffic. Such methods could reduce many different resources needed to identify C&C communication channels and malicious traffic. © 2016 IEEE.","Botnet; Classification; Cyber security; Malware; Network security","Artificial intelligence; Big data; Botnet; Classification (of information); Communication channels (information theory); Computer crime; Crime; Cryptography; Data privacy; Denial-of-service attack; Distributed computer systems; HTTP; Learning systems; Malware; Transportation; Virtual reality; Botnet detections; Command and control; Cyber security; Distributed denial of service; Experimental evaluation; Malicious traffic; Network characteristics; Network information flow; Network security"
"Azad C., Bhushan B., Sharma R., Shankar A., Singh K.K., Khamparia A.","Prediction model using SMOTE, genetic algorithm and decision tree (PMSGD) for classification of diabetes mellitus","10.1007/s00530-021-00817-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107473920&doi=10.1007%2fs00530-021-00817-2&partnerID=40&md5=3823aa62f8d1f9aedef9caab71dc2ed3","Diabetes mellitus is a well-known chronic disease that diminishes the insulin producing capability of the human body. This results in high blood sugar level which might lead to various complications such as eye damage, nerve damage, cardiovascular damage, kidney damage and stroke. Although diabetes has attracted huge research attention, the overall performance of such medical disease classification using machine learning techniques is relatively low, majorly due to existence of class imbalance and missing values in the data. In this paper, we propose a novel Prediction Model using Synthetic Minority Oversampling Technique, Genetic Algorithm and Decision Tree (PMSGD) for Classification of Diabetes Mellitus on Pima Indians Diabetes Database (PIDD) dataset. The framework of the proposed PMSGD prediction model is composed of four different layers. The first layer is the pre-processing layer which is responsible for handling missing values, detection of outlier and oversampling the minority class. In the second layer, the most significant features are selected using correlation and genetic algorithm. In the third layer, the proposed model is trained, and its effectiveness is evaluated in the fourth layer in terms of classification accuracy (CA), classification error (CE), precision, recall (sensitivity), measure (FM), and Area_Under_ROC (AUROC). The proposed PMSGD algorithm clearly outperforms its counterparts and achieves a remarkable accuracy of 82.1256%. The best outcome achieved by the proposed system in terms of CA, CE, precision, sensitivity, FM and AUROC is 82.1256%, 17.8744%, 0.8070%, 0.8598, 0.8326 and 0.8511, respectively. The obtained simulation results show the effectiveness and superiority of our proposed PMSGD model and their by reduced error rate to help in decision-making process. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Data classification; Decision tree; Genetic algorithm; Healthcare; Machine learning; SMOTE","Classification (of information); Decision making; Decision trees; Forecasting; Learning systems; Predictive analytics; Trees (mathematics); Blood sugar levels; Classification accuracy; Classification errors; Decision making process; Disease classification; Handling missing values; Machine learning techniques; Synthetic minority over-sampling techniques; Genetic algorithms"
"Azad R.I., Mukhopadhyay S., Asadnia M.","Using explainable deep learning in da Vinci Xi robot for tumor detection","10.21307/ijssis-2021-017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122240846&doi=10.21307%2fijssis-2021-017&partnerID=40&md5=2facbb97724e0ff15dad0af7c937f550","Deep learning has proved successful in computer-aided detection in interpreting ultrasound images, COVID infections, identifying tumors from computed tomography (CT) scans for humans and animals. This paper proposes applications of deep learning in detecting cancerous cells inside patients via laparoscopic camera on da Vinci Xi surgical robots. The paper presents method for detecting tumor via object detection and classification/localizing using GRAD-CAM. Localization means heat map is drawn on the image highlighting the classified class. Analyzing images collected from publicly available partial robotic nephrectomy videos, for object detection, the final mAP was 0.974 and for classification the accuracy was 0.84. © 2021. Authors. This work is licensed under the Creative Commons Attribution-Non-Commercial-NoDerivs 4.0 License https://creativecommons.org/licenses/by-nc-nd/4.0/.","Convolutional neural network; da Vinci Xi; GRAD-CAM; Live surgery; Tumor detection; YOLOv4",
"Azam M.F., Younis M.S.","Multi-Horizon Electricity Load and Price Forecasting Using an Interpretable Multi-Head Self-Attention and EEMD-Based Framework","10.1109/ACCESS.2021.3086039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107334699&doi=10.1109%2fACCESS.2021.3086039&partnerID=40&md5=d8db437ba53c4cd25872cb8a388f7276","Accurate system marginal price and load forecasts play a pivotal role in economic power dispatch, system reliability and planning. Price forecasting helps electricity buyers and sellers in an energy market to make effective decisions when preparing their bids and making bilateral contracts. Despite considerable research work in this domain, load and price forecasting still remains to be a complicated task. Various uncertain elements contribute to electricity price and demand volatility, such as changes in weather conditions, outages of large power plants, fuel cost, complex bidding strategies and transmission congestion in the power system. Thus, to deal with such difficulties, we propose a novel hybrid deep learning method based upon bidirectional long short-term memory (BiLSTM) and a multi-head self-attention mechanisms that can accurately forecast locational marginal price (LMP) and system load on a day-ahead basis. Additionally, ensemble empirical mode decomposition (EEMD), a data-driven algorithm, is used for the extraction of hidden features from the load and price time series. Besides that, an intuitive understanding of how the proposed model works under the hood is demonstrated using different interpretability use cases. The performance of the presented method is compared with existing well-known techniques applied for short-term electricity load and price forecast in a comprehensive manner. The proposed method produces considerably accurate results in comparison to existing benchmarks. © 2013 IEEE.","Deep learning; energy markets; ensemble empirical mode decomposition; Kalman smoothing; model interpretability; multi-head self-attention; short-term load and price forecasting","Deep learning; Electric load dispatching; Electric power transmission; Forecasting; Learning systems; Power markets; Attention mechanisms; Bilateral contracts; Data-driven algorithm; Ensemble empirical mode decompositions (EEMD); Intuitive understanding; Locational marginal prices; System marginal price; Transmission congestion; Electric load forecasting"
"Azam S., Munir F., Rafique M.A., Sheri A.M., Hussain M.I., Jeon M.","N2C: Neural Network Controller Design Using Behavioral Cloning","10.1109/TITS.2020.3045096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099079546&doi=10.1109%2fTITS.2020.3045096&partnerID=40&md5=56cfa514f1a052dcd260106dca47b500","Modern vehicles communicate data to and from sensors, actuators, and electronic control units (ECUs) using Controller Area Network (CAN) bus, which operates on differential signaling. An autonomous ECU responsible for the execution of decision commands to an autonomous vehicle is developed by assimilating the information from the CAN bus. The conventional way of parsing the decision commands is motion planning, which uses a path tracking algorithm to evaluate the decision commands. This study focuses on designing a robust controller using behavioral cloning and motion planning of autonomous vehicle using a deep learning framework. In the first part of this study, we explore the pipeline of parsing decision commands from the path tracking algorithm to the controller and proposed a neural network-based controller ( $N^{2}C$ ) using behavioral cloning. The proposed network predicts throttle, brake, and torque when trained with the manual driving data acquired from the CAN bus. The efficacy of the proposed method is demonstrated by comparing the accuracy with the Proportional-Derivative-Integral (PID) controller in conjunction with the path tracking algorithm (pure pursuit and model predictive control based path follower). The second part of this study complements $N^{2}C$ , in which an end-to-end neural network for predicting the speed and steering angle is proposed with image data as an input. The performance of the proposed frameworks are evaluated in real-time and on the Udacity dataset, showing better metric scores in the former and reliable prediction in the later case when compared with the state-of-the-art methods. © 2000-2011 IEEE.","Autonomous vehicle control; behavioral cloning; controller area network (CAN); long short-term memory","Autonomous vehicles; Clone cells; Cloning; Control system synthesis; Deep learning; Model predictive control; Motion planning; Motion tracking; Neural networks; Proportional control systems; Tracking (position); Controller area networkbus; Differential signaling; Electronic control units; Learning frameworks; Network-based controllers; Neural network controllers; Proportional derivatives; State-of-the-art methods; Controllers"
"Azar S.M., Atigh M.G., Nickabadi A., Alahi A.","Convolutional relational machine for group activity recognition","10.1109/CVPR.2019.00808","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078813547&doi=10.1109%2fCVPR.2019.00808&partnerID=40&md5=e161511c151d17e98b1f2d5a05cf605b","We present an end-to-end deep Convolutional Neural Network called Convolutional Relational Machine (CRM) for recognizing group activities that utilizes the information in spatial relations between individual persons in image or video. It learns to produce an intermediate spatial representation (activity map) based on individual and group activities. A multi-stage refinement component is responsible for decreasing the incorrect predictions in the activity map. Finally, an aggregation component uses the refined information to recognize group activities. Experimental results demonstrate the constructive contribution of the information extracted and represented in the form of the activity map. CRM shows advantages over state-of-the-art models on Volleyball and Collective Activity datasets. © 2019 IEEE.","Action Recognition; Deep Learning","Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Action recognition; End to end; Group activities; Group activity recognition; Multi stage; Spatial relations; Spatial representations; State of the art; Computer vision"
"Azari A., Janeja V.P., Mohseni A.","Predicting Hospital Length of Stay (PHLOS) : AAA multi-tiered data mining approach","10.1109/ICDMW.2012.69","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873104994&doi=10.1109%2fICDMW.2012.69&partnerID=40&md5=dce0ded8bf8fc962fc9e1d17e81deb26","A model to predict the Length of Stay (LOS) for hospitalized patients can be an effective tool for healthcare providers. Such a model will enable early interventions to prevent complications and prolonged LOS and also enable more efficient utilization of manpower and facilities in hospitals. In this paper, we propose an approach for Predicting Hospital Length of Stay (PHLOS) using a multi-tiered data mining approach. In this paper we propose a methodology that employs clustering to create the training sets to train different classification algorithms. We compared the performance of different classifiers along several different performance measures and consistently found that using clustering as a precursor to form the training set gives better prediction results as compared to non-clustering based training sets. We have also found the accuracies to be consistently higher than some reported in the current literature for predicting individual patient LOS. The classification techniques used in this study are interpretable, enabling us to examine the details of the classification rules learned from the data. As a result, this study provides insight into the underlying factors that influence hospital length of stay. We also examine our results with domain expert insights. © 2012 IEEE.","Classification; Length of Stay; Predictive Models","Classification algorithm; Classification rules; Classification technique; Domain experts; Early intervention; Effective tool; Health care providers; Length of stay; Multi-tiered; Performance measure; Predictive models; Training sets; Underlying factors; Classification (of information); Forecasting; Health care; Hospitals; Data mining"
"Azari A.R., Lockhart J.W., Liemohn M.W., Jia X.","Incorporating Physical Knowledge Into Machine Learning for Planetary Space Physics","10.3389/fspas.2020.00036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103957076&doi=10.3389%2ffspas.2020.00036&partnerID=40&md5=f43b1eebedb66567ee6b575ca3683cbe","Recent improvements in data collection volume from planetary and space physics missions have allowed the application of novel data science techniques. The Cassini mission for example collected over 600 gigabytes of scientific data from 2004 to 2017. This represents a surge of data on the Saturn system. In comparison, the previous mission to Saturn, Voyager over 20 years earlier, had onboard a ~70 kB 8-track storage ability. Machine learning can help scientists work with data on this larger scale. Unlike many applications of machine learning, a primary use in planetary space physics applications is to infer behavior about the system itself. This raises three concerns: first, the performance of the machine learning model, second, the need for interpretable applications to answer scientific questions, and third, how characteristics of spacecraft data change these applications. In comparison to these concerns, uses of “black box” or un-interpretable machine learning methods tend toward evaluations of performance only either ignoring the underlying physical process or, less often, providing misleading explanations for it. The present work uses Cassini data as a case study as these data are similar to space physics and planetary missions at Earth and other solar system objects. We build off a previous effort applying a semi-supervised physics-based classification of plasma instabilities in Saturn's magnetic environment, or magnetosphere. We then use this previous effort in comparison to other machine learning classifiers with varying data size access, and physical information access. We show that incorporating knowledge of these orbiting spacecraft data characteristics improves the performance and interpretability of machine leaning methods, which is essential for deriving scientific meaning. Building on these findings, we present a framework on incorporating physics knowledge into machine learning problems targeting semi-supervised classification for space physics data in planetary environments. These findings present a path forward for incorporating physical knowledge into space physics and planetary mission data analyses for scientific discovery. © Copyright © 2020 Azari, Lockhart, Liemohn and Jia.","automated event detection; domain knowledge; feature engineering; interpretable machine learning; physics-informed machine learning; planetary science; Saturn; space physics",
"Azaza M., Wallin F.","Smart meter data clustering using consumption indicators: Responsibility factor and consumption variability","10.1016/j.egypro.2017.12.624","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041517474&doi=10.1016%2fj.egypro.2017.12.624&partnerID=40&md5=b3b31784b98e5e1f882e783b70a2f9a7","The wide spread of smart metering roll out enables a better understanding of the consumer behavior and tailoring demand response DR programs to achieve cost-efficient energy savings. In the residential sector smart metering allows detailed readings of the power consumption in the form of large volumes time series that encodes relevant information for distribution network operators DNOs to manage in optimal ways low-voltage networks. Further, DNOs may leverage the smart meter data to identify customer group for energy efficiency programs and demand side response DSR (e.g., dynamic pricing schemes). In this paper, we outline the application of smart meter data mining to identify consumers who are more responsible for the peak system using responsibility factor and consumption variability. Identification of consumers having higher responsibility to the peak system may yield to better enhance energy reduction recommendations and enable more tailored dynamic pricing plans depending on the consumer's influence on the utility peak. Responsibility factor and consumption variance have been investigated as input features of the clustering algorithms. Two clustering techniques, hierarchical clustering and self-organising map SOM, have been used to study the resulting customer groups and to have an effective graphical visualization of the customer's cluster distribution on the input feature space.","Consumers clustering; Demand response; Responsibility factor; Smart metering; Variabiliy","Cluster analysis; Clustering algorithms; Consumer behavior; Costs; Data mining; Electric measuring instruments; Energy conservation; Energy efficiency; Information management; Sales; Consumers clustering; Demand response; Responsibility factor; Smart metering; Variabiliy; Smart meters"
"Azazi A., Lutfi S.L., Venkat I.","Analysis and evaluation of SURF descriptors for automatic 3D facial expression recognition using different classifiers","10.1109/WICT.2014.7077296","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946690687&doi=10.1109%2fWICT.2014.7077296&partnerID=40&md5=05f3cedd3f5f94bd2c578d302021f1f7","Emotion recognition plays a vital role in the field of Human-Computer Interaction (HCI). Among the visual human emotional cues, facial expressions are the most commonly used and understandable cues. Different machine learning techniques have been utilized to solve the expression recognition problem; however, their performance is still disputed. In this paper, we investigate the capability of several classification techniques to discriminate between the six universal facial expressions using Speed Up Robust Features (SURF). The evaluation were conducted using the BU-3DFE database with four classifiers, namely, Support Vector machine (SVM), Neural Network (NN), k-Nearest Neighbors (k-NN), and Naïve Bayes (NB). Experimental results show that the SVM was successful in discriminating between the six universal facial expressions with an overall recognition accuracy of 79.36%, which is significantly better than the nearest accuracy achieved by Naïve Bayes at significance level p < 0.05. © 2014 IEEE.","3D Facial Expression Recognition; Human-computer interaction; k-nearest neighbors; Naïve Bayes; Neural Network; Support Vector machine","Artificial intelligence; Classification (of information); Human computer interaction; Learning systems; Motion compensation; Nearest neighbor search; Neural networks; Sodium; Support vector machines; 3-D facial expression recognition; Analysis and evaluation; Classification technique; Expression recognition; Human computer interaction (HCI); K-nearest neighbors; Machine learning techniques; Universal facial expressions; Face recognition"
"Azer S.A.","Challenges facing the detection of colonic polyps: What can deep learning do?","10.3390/medicina55080473","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071280904&doi=10.3390%2fmedicina55080473&partnerID=40&md5=34ecca0662d0a5f05b72810ccb7288f6","Colorectal cancer (CRC) is one of the most common causes of cancer mortality in the world. The incidence is related to increases with age and western dietary habits. Early detection through screening by colonoscopy has been proven to effectively reduce disease-related mortality. Currently, it is generally accepted that most colorectal cancers originate from adenomas. This is known as the “adenoma-carcinoma sequence”, and several studies have shown that early detection and removal of adenomas can effectively prevent the development of colorectal cancer. The other two pathways for CRC development are the Lynch syndrome pathway and the sessile serrated pathway. The adenoma detection rate is an established indicator of a colonoscopy’s quality. A 1% increase in the adenoma detection rate has been associated with a 3% decrease in interval CRC incidence. However, several factors may affect the adenoma detection rate during a colonoscopy, and techniques to address these factors have been thoroughly discussed in the literature. Interestingly, despite the use of these techniques in colonoscopy training programs and the introduction of quality measures in colonoscopy, the adenoma detection rate varies widely. Considering these limitations, initiatives that use deep learning, particularly convolutional neural networks (CNNs), to detect cancerous lesions and colonic polyps have been introduced. The CNN architecture seems to offer several advantages in this field, including polyp classification, detection, and segmentation, polyp tracking, and an increase in the rate of accurate diagnosis. Given the challenges in the detection of colon cancer affecting the ascending (proximal) colon, which is more common in women aged over 65 years old and is responsible for the higher mortality of these patients, one of the questions that remains to be answered is whether CNNs can help to maximize the CRC detection rate in proximal versus distal colon in relation to a gender distribution. This review discusses the current challenges facing CRC screening and training programs, quality measures in colonoscopy, and the role of CNNs in increasing the detection rate of colonic polyps and early cancerous lesions. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Adenoma; Artificial intelligence; Colonic polyps; Colonoscopy; Colorectal cancer; Computer-aided diagnosis; Convolutional neural network (CNN); Deep learning; Surveillance","adenoma; adenomatous polyp; classification; colonoscopy; colorectal tumor; early cancer diagnosis; human; pathology; Adenoma; Adenomatous Polyps; Colonoscopy; Colorectal Neoplasms; Deep Learning; Early Detection of Cancer; Humans; Neural Networks, Computer"
"Azevedo L.G., De Queiroz A.T.L., Barral A., Santos L.A., Ramos P.I.P.","Proteins involved in the biosynthesis of lipophosphoglycan in Leishmania: A comparative genomic and evolutionary analysis","10.1186/s13071-020-3914-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078710638&doi=10.1186%2fs13071-020-3914-9&partnerID=40&md5=f6b91f7ae9da89a55a047455f945ffa3","Background: Leishmania spp. are digenetic parasites capable of infecting humans and causing a range of diseases collectively known as leishmaniasis. The main mechanisms involved in the development and permanence of this pathology are linked to evasion of the immune response. Crosstalk between the immune system and particularities of each pathogenic species is associated with diverse disease manifestations. Lipophosphoglycan (LPG), one of the most important molecules present on the surface of Leishmania parasites, is divided into four regions with high molecular variability. Although LPG plays an important role in host-pathogen and vector-parasite interactions, the distribution and phylogenetic relatedness of the genes responsible for its synthesis remain poorly explored. The recent availability of full genomes and transcriptomes of Leishmania parasites offers an opportunity to leverage insight on how LPG-related genes are distributed and expressed by these pathogens. Results: Using a phylogenomics-based framework, we identified a catalog of genes involved in LPG biosynthesis across 22 species of Leishmania from the subgenera Viannia and Leishmania, as well as 5 non-Leishmania trypanosomatids. The evolutionary relationships of these genes across species were also evaluated. Nine genes related to the production of the glycosylphosphatidylinositol (GPI)-anchor were highly conserved among compared species, whereas 22 genes related to the synthesis of the repeat unit presented variable conservation. Extensive gain/loss events were verified, particularly in genes SCG1-4 and SCA1-2. These genes act, respectively, on the synthesis of the side chain attached to phosphoglycans and in the transfer of arabinose residues. Phylogenetic analyses disclosed evolutionary patterns reflective of differences in host specialization, geographic origin and disease manifestation. Conclusions: The multiple gene gain/loss events identified by genomic data mining help to explain some of the observed intra- A nd interspecies variation in LPG structure. Collectively, our results provide a comprehensive catalog that details how LPG-related genes evolved in the Leishmania parasite specialization process. © 2020 The Author(s).","Genome mining; Leishmania; Lipophosphoglycan; Phylogenomics; Trypanosomatids","arabinose; glycosylphosphatidylinositol; lipophosphoglycan; glycosphingolipid; lipophosphoglycan; protozoal RNA; Article; comparative genomics; data mining; gene loss; genetic conservation; genetic identification; geographic origin; Leishmania; Leishmania braziliensis; Leishmania trypanosomatids; microbial gene; nonhuman; parasite; phylogenomics; phylogeny; protein synthesis; SCA1 2 gene; SCG1 4 gene; species identification; subspecies; biosynthesis; chemistry; classification; comparative study; evolution; genetics; genome; human; Leishmania; nucleotide sequence; physiology; statistical model; Trypanosomatidae; Base Sequence; Biological Evolution; Data Mining; Genome, Protozoan; Glycosphingolipids; Humans; Leishmania; Likelihood Functions; Phylogeny; RNA, Protozoan; Trypanosomatina"
"Azhan M., Ahmad M.","LaDiff ULMFiT: A Layer Differentiated Training Approach for ULMFiT","10.1007/978-3-030-73696-5_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104725758&doi=10.1007%2f978-3-030-73696-5_6&partnerID=40&md5=720015e048ba4c0ef5f5019d2e8f993d","In our paper we present Deep Learning models with a layer differentiated training method which were used for the SHARED TASK @ CONSTRAINT 2021 sub-tasks COVID19 Fake News Detection in English and Hostile Post Detection in Hindi. We propose a Layer Differentiated training procedure for training a pre-trained ULMFiT [8] model. We used special tokens to annotate specific parts of the tweets to improve language understanding and gain insights on the model making the tweets more interpretable. The other two submissions included a modified RoBERTa model and a simple Random Forest Classifier. The proposed approach scored a precision and f1-score of 0.96728972 and 0.967324832 respectively for sub-task COVID19 Fake News Detection in English. Also, Coarse Grained Hostility f1 Score and Weighted Fine Grained f1 score of 0.908648 and 0.533907 respectively for sub-task Hostile Post Detection in Hindi. The proposed approach ranked 61st out of 164 in the sub-task “COVID19 Fake News Detection in English” and 18th out of 45 in the sub-task “Hostile Post Detection in Hindi”. The complete code implementation can be found at: GitHub Repository (https://github.com/sheikhazhanmohammed/AAAI-Constraint-Shared- Tasks-2021 ). © 2021, Springer Nature Switzerland AG.","Language model; Layer differentiated training; Text classification; Text interpretation","Decision trees; Coarse-grained; Language understanding; Learning models; Model-making; Random forest classifier; Task constraints; Training methods; Training procedures; Deep learning"
"Azhar I., Sharif M., Raza M., Khan M.A., Yong H.-S.","A decision support system for face sketch synthesis using deep learning and artificial intelligence","10.3390/s21248178","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120782843&doi=10.3390%2fs21248178&partnerID=40&md5=000ecd5ce627f787ec98c9db91f747f1","The recent development in the area of IoT technologies is likely to be implemented extensively in the next decade. There is a great increase in the crime rate, and the handling officers are responsible for dealing with a broad range of cyber and Internet issues during investigation. IoT technologies are helpful in the identification of suspects, and few technologies are available that use IoT and deep learning together for face sketch synthesis. Convolutional neural networks (CNNs) and other constructs of deep learning have become major tools in recent approaches. A new-found architecture of the neural network is anticipated in this work. It is called Spiral-Net, which is a modified version of U-Net fto perform face sketch synthesis (the phase is known as the compiler network C here). Spiral-Net performs in combination with a pre-trained Vgg-19 network called the feature extractor F. It first identifies the top n matches from viewed sketches to a given photo. F is again used to formulate a feature map based on the cosine distance of a candidate sketch formed by C from the top n matches. A customized CNN configuration (called the discriminator D) then computes loss functions based on differences between the candidate sketch and the feature. Values of these loss functions alternately update C and F. The ensemble of these nets is trained and tested on selected datasets, including CUFS, CUFSF, and a part of the IIT photo–sketch dataset. Results of this modified U-Net are acquired by the legacy NLDA (1998) scheme of face recognition and its newer version, OpenBR (2013), which demonstrate an improvement of 5% compared with the current state of the art in its relevant domain. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network; Face recognition; NLDA; OpenBR; Sketch synthesis; Smart cities; SpiralNet; U-Net; Vgg-19 net","C (programming language); Convolution; Convolutional neural networks; Decision support systems; Deep learning; Face recognition; Smart city; Convolutional neural network; Face sketch synthesis; Loss functions; Neural-networks; NLDA; Openbr; Sketch synthesis; Spiralnet; U-net; Vgg-19 net; Internet of things; algorithm; artificial intelligence; face; Algorithms; Artificial Intelligence; Deep Learning; Face; Neural Networks, Computer"
"Azher Z.L., Vaickus L.J., Salas L.A., Christensen B.C., Levy J.J.","Development of biologically interpretable multimodal deep learning model for cancer prognosis prediction","10.1145/3477314.3507032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130358170&doi=10.1145%2f3477314.3507032&partnerID=40&md5=43dfa6fbaede6a269f5e4d6fe420a081","Robust cancer prognostication can enable more effective patient care and management, which may potentially improve health outcomes. Deep learning has proven to be a powerful tool to extract meaningful information from cancer patient data. In recent years it has displayed promise in quantifying prognostication by predicting patient risk. However, most current deep learning-based cancer prognosis prediction methods use only a single data source and miss out on learning from potentially rich relationships across modalities. Existing multimodal approaches are challenging to interpret in a biological or medical context, limiting real-world clinical integration as a trustworthy prognostic decision aid. Here, we developed a multimodal modeling approach that can integrate information from the central modalities of gene expression, DNA methylation, and histopathological imaging with clinical information for cancer prognosis prediction. Our multimodal modeling approach combines pathway and gene-based sparsely coded layers with patch-based graph convolutional networks to facilitate biological interpretation of the model results. We present a preliminary analysis that compares the potential applicability of combining all modalities to uni- or bi-modal approaches. Leveraging data from four cancer subtypes from the Cancer Genome Atlas, results demonstrate the encouraging performance of our multimodal approach (C-index=0.660 without clinical features; C-index=0.665 with clinical features) across four cancer subtypes versus unimodal approaches and existing state-of-the-art approaches. This work brings insight to the development of interpretable multimodal methods of applying AI to biomedical data and can potentially serve as a foundation for clinical implementations of such software. We plan to follow up this preliminary analysis with an in-depth exploration of factors to improve multimodal modeling approaches on an in-house dataset. © 2022 ACM.","deep learning; DNA methylation; gene expression; multimodal; whole slide images","Alkylation; Decision support systems; Deep learning; Diagnosis; Diseases; Forecasting; Hospital data processing; Methylation; Cancer prognosis; Deep learning; DNA Methylation; Genes expression; Modeling approach; Multi-modal; Multi-modal approach; Multimodal models; Prognosis prediction; Whole slide images; Gene expression"
"Azhir A., Talebi S., Merino L.-H., Lukasiewicz T., Argulian E., Narula J., Mihaylova B.","Using Shapes of COVID-19 Positive Patient-Specific Trajectories for Mortality Prediction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134635245&partnerID=40&md5=9b4d3f3fa9a62928b8e92454e29f5123","Machine learning can be used to identify relevant trajectory shape features for improved predictive risk modeling, which can help inform decisions for individualized patient management in intensive care during COVID-19 outbreaks. We present explainable random forests to dynamically predict next day mortality risk in COVID -19 positive and negative patients admitted to the Mount Sinai Health System between March 1st and June 8th, 2020 using patient time-series data of vitals, blood and other laboratory measurements from the previous 7 days. Three different models were assessed by using time series with: 1) most recent patient measurements, 2) summary statistics of trajectories (min/max/median/first/last/count), and 3) coefficients of fitted cubic splines to trajectories. AUROC and AUPRC with cross-validation were used to compare models. We found that the second and third models performed statistically significantly better than the first model. Model interpretations are provided at patient-specific level to inform resource allocation and patient care. ©2022 AMIA - All rights reserved.",,"hospitalization; human; intensive care; machine learning; time factor; COVID-19; Critical Care; Hospitalization; Humans; Machine Learning; Time Factors"
"Azim S.M., Sharma A., Noshadi I., Shatabda S., Dehzangi I.","A convolutional neural network based tool for predicting protein AMPylation sites from binary profile representation","10.1038/s41598-022-15403-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133584101&doi=10.1038%2fs41598-022-15403-3&partnerID=40&md5=ef8eaed0a82eca65d921b541ad396030","AMPylation is an emerging post-translational modification that occurs on the hydroxyl group of threonine, serine, or tyrosine via a phosphodiester bond. AMPylators catalyze this process as covalent attachment of adenosine monophosphate to the amino acid side chain of a peptide. Recent studies have shown that this post-translational modification is directly responsible for the regulation of neurodevelopment and neurodegeneration and is also involved in many physiological processes. Despite the importance of this post-translational modification, there is no peptide sequence dataset available for conducting computation analysis. Therefore, so far, no computational approach has been proposed for predicting AMPylation. In this study, we introduce a new dataset of this distinct post-translational modification and develop a new machine learning tool using a deep convolutional neural network called DeepAmp to predict AMPylation sites in proteins. DeepAmp achieves 77.7%, 79.1%, 76.8%, 0.55, and 0.85 in terms of Accuracy, Sensitivity, Specificity, Matthews Correlation Coefficient, and Area Under Curve for AMPylation site prediction task, respectively. As the first machine learning model, DeepAmp demonstrate promising results which highlight its potential to solve this problem. Our presented dataset and DeepAmp as a standalone predictor are publicly available at https://github.com/MehediAzim/DeepAmp. © 2022, The Author(s).",,"adenylation; amino acid sequence; area under the curve; article; convolutional neural network; correlation coefficient; machine learning; prediction; protein processing; sensitivity and specificity; amino acid; Amino Acid Sequence; Amino Acids; Machine Learning; Neural Networks, Computer; Protein Processing, Post-Translational"
"Azimi A., Delavar M.R.","Urban crime mapping using spatial data mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-60749087695&partnerID=40&md5=952492c29419d0c188b30179a57ada11","A city is a mutant, dynamic, living body where deep differences exist and need to be managed on behalf of its population's quality of life. Over the last 20 years, the increase in crime has become a problem in the majority of the world's largest cities. Crimes are social nuisance and cost our societies dealing with in several ways. In urban management, law enforcement implies police force, which is responsible for the maintenance of law and order. Most law enforcement agencies today are faced with enormous quantities of data that must be processed and turned into useful information. This paper attempts to implement clustering algorithm as a data mining approach to assist detecting the crime patterns and speed up the process of responding to the crime. We have looked at k-means clustering with some enhancements to aid the process of identification of crime patterns. In this paper the benefits of using Geospatial Information Systems (GIS) to study the high potential crime risk areas has been successfully tested in Tehran using spatial data mining. The achieved results illustrated partitioning of the study area into difference levels of crime-prone locations to be monitored by police force. © 2008 Taylor & Francis Group.",,"Clustering algorithms; Information management; Geo-spatial information systems; High potentials; K-means clustering; Law-enforcement agencies; Living bodies; Police forces; Quality of lives; Risk areas; Spatial data minings; Speed-up; Study areas; Urban managements; Law enforcement"
"Azimi S., Pahl C.","The Effect of IoT Data Completeness and Correctness on Explainable Machine Learning Models","10.1007/978-3-030-86475-0_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115273217&doi=10.1007%2f978-3-030-86475-0_15&partnerID=40&md5=1325f708c32e0a2496b999df40896fa6","Many systems in the Edge Cloud, the Internet-of-Things or Cyber-Physical Systems are built for processing data, which is delivered from sensors and devices, transported, processed and consumed locally by actuators. This, given the regularly high volume of data, permits Artificial Intelligence (AI) strategies like Machine Learning (ML) to be used to generate the application and management functions needed. The quality of both source data and machine learning model is here unavoidably of high significance, yet has not been explored sufficiently as an explicit connection of the ML model quality that are created through ML procedures to the quality of data that the model functions consume in their construction. Here, we investigated the link between input data quality for ML function construction and the quality of these functions in data-driven software systems towards explainable model construction through an experimental approach with IoT Data using decision trees. We have 3 objectives in this research: 1. Search for indicators that influence data quality such as correctness and completeness and model construction factors on accuracy, precision and recall. 2. Estimate the impact of variations in model construction and data quality. 3. Identify change patterns that can be attributed to specific input changes. © 2021, Springer Nature Switzerland AG.","Data completeness; Data correctness; Data quality; Decision trees; Explainable AI; IoT systems; Machine learning","Data reduction; Decision trees; Embedded systems; Expert systems; Machine learning; Trees (mathematics); Data completeness; Experimental approaches; Explicit connections; Function construction; Machine learning models; Management functions; Model construction; Precision and recall; Internet of things"
"Aziz K.A., Jabar M.A., Abdullah S., Nor R.N.H.","Challenges from the disastrous COVID-19 pandemic: exposure to opportunities for branchless banking in Malaysia","10.11591/eei.v11i4.3865","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136777071&doi=10.11591%2feei.v11i4.3865&partnerID=40&md5=0880316ff86c688598fd0da753be636d","Branchless banking is a new digitalization of artificial intelligence (AI) for future banking services. The rapid digitalization and technological advancement of financial institutions (FI) have changed due to the COVID-19 pandemic. Such branchless banks are expected to offer financial customers meaningful access whilst promoting responsible usage of suitable and affordable financial solutions. The objectives of this paper are to identify CSF to conduct branchless banking for bank performance during the attack of the COVID-19 pandemic in Malaysia. These CSFs are important to reinforce the new business model recommended by the central bank of Malaysia (BNM). Through this study five CSFs have been identified which are customers do not physically attend to the bank for their banking needs, it is more efficient, saves time, secured, and low cost. As the industrial revolution (IR) 4.0 approaches, public acceptance, innovation, and strong financial services, as well as demand of digital payment will take over the country. National savings bank (BSN) was officially the first bank in Malaysia to conduct branchless banking through agent banking. In line with the BNM division strategy 2011–2020, BSN had launched banking beyond branches via BSN agents who offer banking services on behalf of BSN. © 2022, Institute of Advanced Engineering and Science. All rights reserved.","Agent banking; Branchless banking malaysia; COVID-19 pandemic; Digital banking",
"Aziz N.A., Sulaiman M.A.H., Zabidi A., Yassin I.M., Ali M.S.A.M., Rizman Z.I.","Lightweight Generative Adversarial Network Fundus Image Synthesis","10.30630/joiv.6.1-2.924","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132406172&doi=10.30630%2fjoiv.6.1-2.924&partnerID=40&md5=fadfeeccc7c9849215c89f46ba018a6c","Blindness is a global health problem that affects billions of lives. Recent advancements in Artificial Intelligence (AI), (Deep Learning (DL)) has the intervention potential to address the blindness issue, particularly as an accurate and non-invasive technique for early detection and treatment of Diabetic Retinopathy (DR). DL-based techniques rely on extensive examples to be robust and accurate in capturing the features responsible for representing the data. However, the number of samples required is tremendous for the DL classifier to learn properly. This presents an issue in collecting and categorizing many samples. Therefore, in this paper, we present a lightweight Generative Neural Network (GAN) to synthesize fundus samples to train AI-based systems. The GAN was trained using samples collected from publicly available datasets. The GAN follows the structure of the recent Lightweight GAN (LGAN) architecture. The implementation and results of the LGAN training and image generation are described. Results indicate that the trained network was able to generate realistic high-resolution samples of normal and diseased fundus images accurately as the generated results managed to realistically represent key structures and their placements inside the generated samples, such as the optic disc, blood vessels, exudates, and others. Successful and unsuccessful generation samples were sorted manually, yielding 56.66% realistic results relative to the total generated samples. Rejected generated samples appear to be due to inconsistencies in shape, key structures, placements, and color. © 2022, Politeknik Negeri Padang. All rights reserved.","artificial intelligence; data synthesis; fundus; Generative Adversarial Network (GAN)",
"Aziz S., Faiz M.T., Adeniyi A.M., Loo K.-H., Hasan K.N., Xu L., Irshad M.","Anomaly Detection in the Internet of Vehicular Networks Using Explainable Neural Networks (xNN)","10.3390/math10081267","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128817732&doi=10.3390%2fmath10081267&partnerID=40&md5=118363d0523ca4e77d2f7fd7dd5e464a","It is increasingly difficult to identify complex cyberattacks in a wide range of industries, such as the Internet of Vehicles (IoV). The IoV is a network of vehicles that consists of sensors, actuators, network layers, and communication systems between vehicles. Communication plays an important role as an essential part of the IoV. Vehicles in a network share and deliver information based on several protocols. Due to wireless communication between vehicles, the whole network can be sensitive towards cyber-attacks.In these attacks, sensitive information can be shared with a malicious network or a bogus user, resulting in malicious attacks on the IoV. For the last few years, detecting attacks in the IoV has been a challenging task. It is becoming increasingly difficult for traditional Intrusion Detection Systems (IDS) to detect these newer, more sophisticated attacks, which employ unusual patterns. Attackers disguise themselves as typical users to evade detection. These problems can be solved using deep learning. Many machine-learning and deep-learning (DL) models have been implemented to detect malicious attacks; however, feature selection remains a core issue. Through the use of training empirical data, DL independently defines intrusion features. We built a DL-based intrusion model that focuses on Denial of Service (DoS) assaults in particular. We used K-Means clustering for feature scoring and ranking. After extracting the best features for anomaly detection, we applied a novel model, i.e., an Explainable Neural Network (xNN), to classify attacks in the CICIDS2019 dataset and UNSW-NB15 dataset separately. The model performed well regarding the precision, recall, F1 score, and accuracy. Comparatively, it can be seen that our proposed model xNN performed well after the feature-scoring technique. In dataset 1 (UNSW-NB15), xNN performed well, with the highest accuracy of 99.7%, while CNN scored 87%, LSTM scored 90%, and the Deep Neural Network (DNN) scored 92%. xNN achieved the highest accuracy of 99.3% while classifying attacks in the second dataset (CICIDS2019); the Convolutional Neural Network (CNN) achieved 87%, Long Short-Term Memory (LSTM) achieved 89%, and the DNN achieved 82%. The suggested solution outperformed the existing systems in terms of the detection and classification accuracy. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","anomaly detection; IoV; K-MEANS; xNN",
"Azizah K., Jatmiko W.","Transfer Learning, Style Control, and Speaker Reconstruction Loss for Zero-Shot Multilingual Multi-Speaker Text-to-Speech on Low-Resource Languages","10.1109/ACCESS.2022.3141200","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122868470&doi=10.1109%2fACCESS.2022.3141200&partnerID=40&md5=7e6d90d76ecb8149ce270267babbeeee","Deep neural network (DNN)-based systems generally require large amounts of training data, so they have data scarcity problems in low-resource languages. Recent studies have succeeded in building zero-shot multi-speaker DNN-based TTS on high-resource languages, but they still have unsatisfactory performance on unseen speakers. This study addresses two main problems: overcoming the problem of data scarcity in the DNN-based TTS on low-resource languages and improving the performance of zero-shot speaker adaptation for unseen speakers. We propose a novel multi-stage transfer learning strategy using a partial network-based deep transfer learning to overcome the low-resource problem by utilizing pre-trained monolingual single-speaker TTS and d-vector speaker encoder on a high-resource language as the source domain. Meanwhile, to improve the performance of zero-shot speaker adaptation, we propose a new TTS model that incorporates an explicit style control from the target speaker for TTS conditioning and an utterance-level speaker reconstruction loss during TTS training. We use publicly available speech datasets for experiments. We show that our proposed training strategy is able to effectively train the TTS models using a limited amount of training data of low-resource target languages. The models trained using the proposed transfer learning successfully produce intelligible natural speech sounds, while in contrast using standard training fails to make the models synthesize understandable speech. We also demonstrate that our proposed style encoder network and speaker reconstruction loss significantly improves speaker similarity in zero-shot speaker adaptation task compared to the baseline model. Overall, our proposed TTS model and training strategy has succeeded in increasing the speaker cosine similarity of the synthesized speech on the unseen speakers test set by 0.468 and 0.279 in native and foreign languages respectively. © 2013 IEEE.","Deep neural network; Low-resource; Multi-speaker; Multilingual; Partial network-based deep transfer learning; Speaker reconstruction loss; Style control; Text-to-speech; Zero-shot speaker adaptation","Data transfer; Job analysis; Signal encoding; Adaptation models; Deep learning; Low-resource; Multi-speaker; Network-based; Partial network-based deep transfer learning; Speaker adaptation; Speaker reconstruction loss; Style control; Task analysis; Text to speech; Training data; Transfer learning; Zero-shot speaker adaptation; Deep neural networks"
"Azizi M.J., Vayanos P., Wilder B., Rice E., Tambe M.","Designing fair, efficient, and interpretable policies for prioritizing homeless youth for housing resources","10.1007/978-3-319-93031-2_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048622581&doi=10.1007%2f978-3-319-93031-2_3&partnerID=40&md5=daad56cf8ed72083251834f42e002bd4","We consider the problem of designing fair, efficient, and interpretable policies for prioritizing heterogeneous homeless youth on a waiting list for scarce housing resources of different types. We focus on point-based policies that use features of the housing resources (e.g., permanent supportive housing, rapid rehousing) and the youth (e.g., age, history of substance use) to maximize the probability that the youth will have a safe and stable exit from the housing program. The policies can be used to prioritize waitlisted youth each time a housing resource is procured. Our framework provides the policy-maker the flexibility to select both their desired structure for the policy and their desired fairness requirements. Our approach can thus explicitly trade-off interpretability and efficiency while ensuring that fairness constraints are met. We propose a flexible data-driven mixed-integer optimization formulation for designing the policy, along with an approximate formulation which can be solved efficiently for broad classes of interpretable policies using Bender’s decomposition. We evaluate our framework using real-world data from the United States homeless youth housing system. We show that our framework results in policies that are more fair than the current policy in place and than classical interpretable machine learning approaches while achieving a similar (or higher) level of overall efficiency. © Springer International Publishing AG, part of Springer Nature 2018.",,"Artificial intelligence; Computer programming; Constraint theory; Economic and social effects; Efficiency; Integer programming; Learning systems; Operations research; Fairness constraints; Housing programs; Housing systems; Interpretability; Machine learning approaches; Mixed integer optimization; Overall efficiency; Waiting lists; Housing"
"Azodi C.B., Tang J., Shiu S.-H.","Opening the Black Box: Interpretable Machine Learning for Geneticists","10.1016/j.tig.2020.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083345524&doi=10.1016%2fj.tig.2020.03.005&partnerID=40&md5=583c96b51e4caf9000e57b8b7561e165","Because of its ability to find complex patterns in high dimensional and heterogeneous data, machine learning (ML) has emerged as a critical tool for making sense of the growing amount of genetic and genomic data available. While the complexity of ML models is what makes them powerful, it also makes them difficult to interpret. Fortunately, efforts to develop approaches that make the inner workings of ML models understandable to humans have improved our ability to make novel biological insights. Here, we discuss the importance of interpretable ML, different strategies for interpreting ML models, and examples of how these strategies have been applied. Finally, we identify challenges and promising future directions for interpretable ML in genetics and genomics. © 2020 Elsevier Ltd","deep learning; interpretable machine learning; predictive biology","deep learning; geneticist; genomics; human; review; biology; human genome; machine learning; medical genetics; population genetics; procedures; Computational Biology; Genetics, Medical; Genetics, Population; Genome, Human; Humans; Machine Learning"
"Azrai E.P., Rini D.S., Suryanda A.","Micro-teaching in the Digital Industrial Era 4.0: Necessary or not?","10.13189/ujer.2020.081804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084213025&doi=10.13189%2fujer.2020.081804&partnerID=40&md5=0863fbece3afaf6fef1d42d13f103cfb","Background: The era of the industrial revolution 4.0 is often called the era of the digital revolution. In this era, the boundary line between biological, digital, and physical seems to fuse and become one so that it eventually becomes an era that has the characteristics of automation in all types of activities, the use of artificial intelligence in life, the use of machines and robots, the Internet of Things, networking and very open access to technology in life. Educators in the digital era are currently required to have 21st-century teaching skills to achieve the needs of superior generations in the digital age. Educational institutions of teaching professions are fully responsible for the fulfillment of teachers with the competencies needed in the current digital era. Micro-teaching is one of the teaching methods currently given to prospective teacher students to practice teaching skills in the classroom. However, the implementation of micro-teaching is considered to have many shortcomings including class conditioning that still seems unreal and lack of students' ability to design learning, especially in the current digital era. To answer this problem, researchers designed a series of studies to see what factors influenced the decline in performance. Aims: The focus of this research is conducted on micro-teaching learning methods that seem less effective in equipping teacher competencies, especially in the pedagogical and professional sections. This study aims to evaluate the implementation of micro-teaching, whether micro-teaching learning is still relevant to the digital era and to find out what factors are needed to fulfill the competencies of prospective teachers in the current digital era. Methods: A combination of quantitative and qualitative techniques was employed for the purpose of gathering the data. Mainly, a questionnaire and a focus group of discussions were used as the main tools for data collections. Result and conclusion: overall analysis of the findings indicated that the biggest obstacle in the implementation of micro-teaching is the lack of real experience regarding classroom conditions at school so that it still raises concerns in prospective teacher students. Prospective teachers described a variety of benefits they gained from micro-teaching experiences. The study ended with recommendations and directions for future studies to further examine the highlighted result. Copyright©2020 by authors, all rights reserved.","Evaluation; Micro-teaching; Pre-service Teacher; Revolution Industry Era 4.0",
"Azzali I., Vanneschi L., Mosca A., Bertolotti L., Giacobini M.","Towards the use of genetic programming in the ecological modelling of mosquito population dynamics","10.1007/s10710-019-09374-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077568943&doi=10.1007%2fs10710-019-09374-0&partnerID=40&md5=7d996d9d6e0f982a27de8b56d7fee28f","Predictive algorithms are powerful tools to support infection surveillance plans based on the monitoring of vector abundance. In this article, we explore the use of genetic programming (GP) to build a predictive model of mosquito abundance based on environmental and climatic variables. We claim, in fact, that the heterogeneity and complexity of this kind of dataset demands algorithms capable of discovering complex relationships among variables. For this reason, we benchmarked GP performance with state of the art machine learning predictive algorithms. In order to provide a real exploitable model of mosquito abundance, we trained GP and the other algorithms on mosquito collections from 2002 to 2005 and we tested the predictive ability in 2006 collections. Results reveal that, among the studied methods, GP has the best performance in terms of accuracy and generalization ability. Moreover, the intrinsic feature selection and readability of the solution provided by GP offer the possibility of a biological interpretation of the model which highlights known or new behaviours responsible for mosquito abundance. GP, therefore, reveals to be a promising tool in the field of ecological modelling, opening the way to the use of a vector based GP approach (VE-GP) which may be more appropriate and beneficial for the problems in analysis. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Ecological modelling; Genetic programming; Machine learning; Regression","Ecology; Genetic algorithms; Learning systems; Machine learning; Biological interpretation; Complex relationships; Ecological modelling; Generalization ability; Mosquito populations; Predictive abilities; Predictive algorithms; Regression; Genetic programming"
"Ba W., Wang S., Liu C., Wang Y., Shi H., Song Z.","Histopathological Diagnosis System for Gastritis Using Deep Learning Algorithm","10.1016/S1001-9294(21)00058-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117422681&doi=10.1016%2fS1001-9294%2821%2900058-4&partnerID=40&md5=b6d3450c02b1a87124b41ae1ab6e76e4","Objective: To develope a deep learning algorithm for pathological classification of chronic gastritis and assess its performance using whole-slide images (WSIs). Methods: We retrospectively collected 1,250 gastric biopsy specimens (1,128 gastritis, 122 normal mucosa) from PLA General Hospital. The deep learning algorithm based on DeepLab v3 (ResNet-50) architecture was trained and validated using 1,008 WSIs and 100 WSIs, respectively. The diagnostic performance of the algorithm was tested on an independent test set of 142 WSIs, with the pathologists' consensus diagnosis as the gold standard. Results: The receiver operating characteristic (ROC) curves were generated for chronic superficial gastritis (CSuG), chronic active gastritis (CAcG), and chronic atrophic gastritis (CAtG) in the test set, respectively. The areas under the ROC curves (AUCs) of the algorithm for CSuG, CAcG, and CAtG were 0.882, 0.90S and 0.910, respectively. The sensitivity and specificity of the deep learning algorithm for the classification of CSuG, CAcG, and CAtG were 0.790 and 1.000 (accuracy 0.880), 0.985 and 0.829 (accuracy 0.901), 0.952 and 0.992 (accuracy 0.986), respectively. The overall predicted accuracy for three different types of gastritis was 0.867. By flagging the suspicious regions identified by the algorithm in WSI, a more transparent and interpretable diagnosis can be generated. Conclusion: The deep learning algorithm achieved high accuracy for chronic gastritis classification using WSIs. By pre-highlighting the different gastritis regions, it might be used as an auxiliary diagnostic tool to improve the work efficiency of pathologists. © 2021 Chinese Academy Medical Sciences","algorithm; artificial intelligence; deep learning; gastritis; whole-slide pathological images","Article; atrophic gastritis; chronic gastritis; consensus; controlled study; deep learning; gold standard; histopathology; human; human tissue; image quality; learning algorithm; major clinical study; quality control; sensitivity and specificity; stomach biopsy; stomach mucosa; algorithm; gastritis; receiver operating characteristic; retrospective study; Algorithms; Deep Learning; Gastritis; Humans; Retrospective Studies; ROC Curve"
"Ba W., Wang R., Yin G., Song Z., Zou J., Zhong C., Yang J., Yu G., Yang H., Zhang L., Li C.","Diagnostic assessment of deep learning for melanocytic lesions using whole-slide pathological images","10.1016/j.tranon.2021.101161","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111034370&doi=10.1016%2fj.tranon.2021.101161&partnerID=40&md5=43fbc28ca1a36dd798d309dec97ee401","Background: Deep learning has the potential to improve diagnostic accuracy and efficiency in medical image recognition. In the current study, we developed a deep learning algorithm and assessed its performance in discriminating melanoma from nevus using whole-slide pathological images (WSIs). Methods: The deep learning algorithm was trained and validated using a set of 781 WSIs (86 melanomas, 695 nevi) from PLA General Hospital. The diagnostic performance of the algorithm was tested on an independent test set of 104 WSIs (29 melanomas, 75 nevi) from Tianjin Chang Zheng Hospital. The same test set was also diagnostically classified by 7 expert dermatopathologists. Results: The deep learning algorithm receiver operating characteristic (ROC) curve achieved a sensitivity 100% at the specificity of 94.7% in the classification of melanoma and nevus on the test set. The area under ROC curve was 0.99. Dermatopathologists achieved a mean sensitivity and specificity of 95.1% (95% confidence interval [CI]: 92.0%-98.2%) and 96.0% (95% CI: 94.2%-97.8%), respectively. At the operating point of sensitivity of 95.1%, the algorithm revealed a comparable specificity with 7 dermatopathologists (97.3% vs. 96.0%, P = 0.11). At the operating point of specificity of 96.0%, the algorithm also achieved a comparable sensitivity with 7 dermatopathologists (96.5% vs. 95.1%, P = 0.30). A more transparent and interpretable diagnosis could be generated by highlighting the regions of interest recognized by the algorithm in WSIs. Conclusion: The performance of the deep learning algorithm was on par with that of 7 expert dermatopathologists in interpreting WSIs with melanocytic lesions. By pre-screening the suspicious melanoma regions, it might serve as a supplemental diagnostic tool to improve working efficiency of pathologists. © 2021","Artificial intelligence; Deep learning algorithm; Melanoma; Nevus; Whole-slide pathological images",
"Baader F.","Engineering of logics for the content-based representation of information","10.1007/978-3-540-30227-8_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-22944443616&doi=10.1007%2f978-3-540-30227-8_2&partnerID=40&md5=9e6623bb0d39063ed30b0d4378278701","The content-based representation of information, which tries to represent the meaning of the information in a machine-understandable way, requires representation formalisms with a well-defined formal semantics. This semantics can elegantly be provided by the use of a logicbased formalism. However, in this setting there is a fundamental tradeoff between the expressivity of the representation formalism and the efficiency of reasoning with this formalism. This motivates the ""engineering of logics"", i.e., the design of logical formalisms that are tailored to specific representation tasks. The talk will illustrate this approach with the example of so-called Description Logics and their application for databases and as ontology languages for the semantic web. © Springer-Verlag Berlin Heidelberg 2004.",,"Artificial intelligence; Data description; Formal logic; Formal methods; Semantics; Classification (of information); Computational methods; Database systems; Information analysis; Semantics; World Wide Web; Content-based representation; Description logic; Formal Semantics; Logical formalism; Ontology language; Representation formalisms; Semantic Web; Formal logic; Content-based representation; Description logics; Expressivity; Representation formalisms"
"Baaj I., Poli J.-P., Ouerdane W., Maudet N.","Representation of Explanations of Possibilistic Inference Decisions","10.1007/978-3-030-86772-0_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116459225&doi=10.1007%2f978-3-030-86772-0_37&partnerID=40&md5=6955f69f7a15b0d7109df82c35a21191","In this paper, we study how to explain to end-users the inference results of possibilistic rule-based systems. We formulate a necessary and sufficient condition for justifying by a relevant subset of rule premises the possibility degree of each output attribute value. We apply functions to reduce the selected premises, in order to form two kinds of explanations: the justification and the unexpectedness of the possibility degree of an output attribute value. The justification is composed of possibilistic expressions that are sufficient to justify the possibility degree of the output attribute value. The unexpectedness is a set of possible or certain possibilistic expressions, which are not involved in the determination of the considered inference result although there may appear to be a potential incompatibility between them and the considered inference result. We then define a representation of explanations of possibilistic inference decisions that relies on conceptual graphs and may be the input of natural language generation systems. Our extracted justification and unexpectedness are represented by nested conceptual graphs. All our constructions are illustrated with an example of a possibilistic rule-based system that controls the blood sugar level of a patient with type 1 diabetes. © 2021, Springer Nature Switzerland AG.","Conceptual graphs; Explainable artificial intelligence; Possibility theory; Rule-based system","Knowledge based systems; Attribute values; Blood sugar levels; Conceptual graph; End-users; Explainable artificial intelligence; Natural language generation systems; Possibilistic; Possibility degree; Possibility theory; Rules based systems; Natural language processing systems"
"Baaj I., Poli J.-P.","Natural Language Generation of Explanations of Fuzzy Inference Decisions","10.1109/FUZZ-IEEE.2019.8858994","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073781179&doi=10.1109%2fFUZZ-IEEE.2019.8858994&partnerID=40&md5=6bad699a2bff95370b39c7d20bfa3e46","As Artificial Intelligence and fuzzy systems are at the center of the emergence of advanced technologies such as autonomous vehicles or medical decision support systems, a problem of trust from a human point of view is strongly appearing. In this article, we tackle the problem of explanation of a fuzzy inference system decision in its entirety: from the conception of an algorithm that produces a textual explanation to its evaluation.We define a function which is able to associate to any activated fuzzy rule, the structure responsible of its activation degree. To assess our method, we defined a protocol to evaluate AIgenerated explanation, and made an experiment: explanations obtained from the classification of pastas. Despite limitations, the results show a good transparency of the reasoning, consistency and good global effectiveness in generated explanations. © 2019 IEEE.","Explainable Artificial Intelligence; Fuzzy Inference System","Artificial intelligence; Decision support systems; Fuzzy systems; Inference engines; Natural language processing systems; Advanced technology; Fuzzy inference systems; Medical decision support system; Natural language generation; Fuzzy inference"
"Baaj I., Poli J.-P., Ouerdane W.","Some insights towards a unified semantic representation of explanation for explainable artificial intelligence (XAI)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091962756&partnerID=40&md5=5789f3d9408d1f8e4beb798524b81ac0","Among challenges for eXplainable Artificial Intelligence (XAI) is explanation generation. In this paper we put the stress on this issue by focusing on a semantic representation of the content of an explanation that could be common to any kind of XAI. We investigate knowledge representations, and discuss the benefits of conceptual graph structures for being a basis to represent explanations in AI. © 2019 Association for Computational Linguistics.",,"Graph structures; Knowledge representation; Semantics; Conceptual graph; Semantic representation; Natural language processing systems"
"Ba-Alawi W., Nair S.K., Li B., Mammoliti A., Smirnov P., Mer A.S., Penn L.Z., Haibe-Kains B.","Bimodal Gene Expression in Patients with Cancer Provides Interpretable Biomarkers for Drug Sensitivity","10.1158/0008-5472.CAN-21-2395","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134083725&doi=10.1158%2f0008-5472.CAN-21-2395&partnerID=40&md5=37cf032ccbcaa28c6d2aab876b2d8bd4","Identifying biomarkers predictive of cancer cell response to drug treatment constitutes one of the main challenges in precision oncology. Recent large-scale cancer pharmacogenomic studies have opened new avenues of research to develop predictive biomarkers by profiling thousands of human cancer cell lines at the molecular level and screening them with hundreds of approved drugs and experimental chemical compounds. Many studies have leveraged these data to build predictive models of response using various statistical and machine learning methods. However, a common pitfall to these methods is the lack of interpretability as to how they make predictions, hindering the clinical translation of these models. To alleviate this issue, we used the recent logic modeling approach to develop a new machine learning pipeline that explores the space of bimodally expressed genes in multiple large in vitro pharmacogenomic studies and builds multivariate, nonlinear, yet interpretable logic-based models predictive of drug response. The performance of this approach was showcased in a compendium of the three largest in vitro pharmacogenomic datasets to build robust and interpretable models for 101 drugs that span 17 drug classes with high validation rates in independent datasets. These results along with in vivo and clinical validation support a better translation of gene expression biomarkers between model systems using bimodal gene expression. Significance: A new machine learning pipeline exploits the bimodality of gene expression to provide a reliable set of candidate predictive biomarkers with a high potential for clinical translatability. © 2022 American Association for Cancer Research",,"biological marker; erlotinib; biological marker; Article; clinical feature; copy number variation; drug sensitivity; gene expression; genetic association; human; in vitro study; in vivo study; malignant neoplasm; pharmacogenomics; risk factor; gene expression; genetics; neoplasm; personalized medicine; pharmacogenetics; Biomarkers; Gene Expression; Humans; Neoplasms; Pharmacogenetics; Precision Medicine"
"Baba A., Imtiyaz Anwar M., Moon A.H., Khosla A.","Comparative Re-evaluation of Different Single Image Defogging Techniques: A Review","10.1007/978-981-15-8354-4_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098227349&doi=10.1007%2f978-981-15-8354-4_10&partnerID=40&md5=4bd713b73d837a68519a8c372dca6664","Image defogging has turned out to be a demanding task nowadays and to propose several assumptions and approaches for the visibility enhancement of the imagery under consideration, is a need for an hour. Poor weather conditions degrade the image contrast resulting in image blurring and pixel distortion and are thus responsible for the road accidents occurring in the world. For several other applications like remote sensing, video surveillance, navigation, etc., clear and high-quality images are needed. The main objective of the paper is to contemplate the existing state-of-the-art techniques for image defogging for the visibility improvement. Finally, the paper is concluded with the current status of image defogging techniques, their comparative results, and delivers insightful discussions and prospects for future work to boost the efficiency and accurateness of existing systems. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Computer vision; Deep learning; Fog removal; Image fusion; Performance evaluation",
"Babaei G., Giudici P., Raffinetti E.","Explainable artificial intelligence for crypto asset allocation","10.1016/j.frl.2022.102941","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130940245&doi=10.1016%2fj.frl.2022.102941&partnerID=40&md5=8a275adc92b345f8d244e0d46dce82bd","Many investors have been attracted by Crypto assets in the last few years. However, despite the possibility of gaining high returns, investors bear high risks in crypto markets. To help investors and make the markets more reliable, Robot advisory services are rapidly expanding in the field of crypto asset allocation. Robot advisors not only reduce costs but also improve the quality of the service by involving investors and make the market more transparent. However, the reason behind the given solutions is not clear and users face a black-box model that is complex. The aim of this paper is to improve trustworthiness of robot advisors, to facilitate their adoption. For this purpose, we apply Shapley values to the predictions generated by a machine learning model based on the results of a dynamic Markowitz portfolio optimization model and provide explanations for what is behind the selected portfolio weights. © 2022 Elsevier Inc.","Machine learning; Robo-advisory; Shapley values",
"Babar M., Tariq M.U., Jan M.A.","Secure and resilient demand side management engine using machine learning for IoT-enabled smart grid","10.1016/j.scs.2020.102370","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087591782&doi=10.1016%2fj.scs.2020.102370&partnerID=40&md5=b047be089467859962542601dc7f1f14","The national security, economy, and healthcare heavily rely on the reliable distribution of electricity. The incorporation of communication technologies and sensors in the power structures, recognized as the smart grid which revolutionizes the model of the production, distribution, monitoring, and control of the electricity. To realize the applicability of smart grid, several issues need to be addressed. Securing the smart grid is a very challenging task and a pressing issue. In this article, a secure demand-side management (DSM) engine is proposed using machine learning (ML) for the Internet of Things (IoT)-enabled grid. The proposed DSM engine is responsible to preserve the efficient utilization of energy based on priorities. A specific resilient model is proposed to control intrusions in the smart grid. The resilient agent predicts the dishonest entities using the ML classifier. Advanced energy management and interface controlling agents are proposed to process energy information to optimize energy utilization. The efficient simulation is executed to test the efficiency of the proposed scheme. The analysis results reveal that the projected DSM engine is less vulnerable to the intrusion and effective enough to reduce the power utilization of the smart grid. © 2020 Elsevier Ltd","Demand-side management; Home area network; Internet of things; Machine learning; Security; Smart grid","Demand side management; Electric power system control; Electric power transmission networks; Electric utilities; Energy utilization; Engines; Internet of things; Machine learning; National security; Communication technologies; Controlling agent; Efficient simulation; Energy-based; Internet of thing (IOT); Power structures; Process energy; Smart grid; Smart power grids"
"Babar Z., Yu E., Carbajales S., Chan A.","Managing and Simplifying Cognitive Business Operations Using Process Architecture Models","10.1007/978-3-030-21290-2_40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067361524&doi=10.1007%2f978-3-030-21290-2_40&partnerID=40&md5=1e51e8eba54107a8e59f991e20b31603","Enterprises increasingly rely on cognitive capabilities to enhance their core business processes by adopting systems that utilize machine learning and deep learning approaches to support cognitive decisions to aid humans responsible for business process execution. Unlike conventional information systems, for which the design and implementation is a much-studied area, the design of cognitive systems and their integration into existing enterprise business processes is less well understood. This results in long drawn-out implementation and adoption cycles, and requires individuals with highly specialized skills. As cognitively-assisted business processes involve human and machine collaboration, non-functional requirements, such as reusability and configurability that are prominent for software system design, must also be addressed at the enterprise level. Supporting processes may emerge and evolve over time to monitor, evaluate, adjust, or modify these cognitively-enhanced business processes. In this paper, we utilize a goal-oriented approach to analyze the requirements for designing cognitive systems for simplified adoption in enterprises, which are then used to guide and inform the design of a process architecture for cognitive business operations. &#x00A9; 2019, Springer Nature Switzerland AG.","Business process management; Cognitive business operations; Cognitive computing; Goal modeling; Requirements engineering","Computer software reusability; Deep learning; Enterprise resource management; Information systems; Information use; Requirements engineering; Reusability; Systems engineering; Business operation; Business process execution; Business process management; Cognitive Computing; Design and implementations; Enterprise business process; Goal modeling; Non-functional requirements; Cognitive systems"
"Babayev R., Wiese L.","Interpreting Decision-Making Process for Multivariate Time Series Classification","10.1007/978-3-030-85082-1_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115151151&doi=10.1007%2f978-3-030-85082-1_14&partnerID=40&md5=7d482ca611cb96c35f632164352bfc87","Over the last years, several time series classification (TSC) algorithms have been proposed both in traditional machine learning and deep learning domains which have shown remarkable enhancement over the previously published state-of-the-art methods. However, their decision-making processes generally stay as black boxes to the user. Model-agnostic (post-hoc) explainers, such as the state-of-the-art SHAP, are proposed to make the predictions of machine learning models explainable with the presence of well-designed domain mappings. In our paper, we first apply univariate classifiers on the dimensions of multivariate time series data individually. This is a straightforward technique for multivariate time series classification (MTSC). Then, we use state-of-the-art timeXplain framework to interpret the decision making process of the univariate classifiers on the multivariate time series data. With a careful choice of interpretability parameters, we demonstrate that it is possible to obtain explainability for such time series data. © 2021, Springer Nature Switzerland AG.","Interpretability of machine learning models; Multivariate time series classification; Time series analysis","Concrete pavements; Decision making; Deep learning; Information systems; Information use; Learning systems; Time series; Decision making process; Machine learning models; Multivariate time series; Multivariate time series classifications; State-of-the-art methods; Straightforward techniques; Time series classifications; Time-series data; Classification (of information)"
"Babbar R., Babbar S.","Predicting river water quality index using data mining techniques","10.1007/s12665-017-6845-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026254049&doi=10.1007%2fs12665-017-6845-9&partnerID=40&md5=13674b1f1a2de2027c84ac5e331a956b","This paper demonstrates the application of data mining techniques to predict river water quality index. The usefulness of these techniques lies in the automated extraction of novel knowledge from the data to improve decision-making. The popular classification techniques, namely k-nearest neighbor, decision trees, Naive Bayes, artificial neural networks, rule-based and support vector machines were used to develop the predictive environment to classify water quality into understandable terms based on the Overall Index of Pollution. Experimentation was conducted on two types of data sets: synthetic and real. A repeated k-fold cross-validation procedure was followed to design the learning and testing frameworks of the predictive environment. Based on the validation results, it was found that the error rate in defining the true water quality class was 20 and 28%, 11 and 24%, 1 and 38% and 10 and 20% for the k-nearest neighbor, Naive Bayes, artificial neural network and rule-based classifiers for synthetic and real data sets, respectively. The decision tree and support vector machines classifiers were found to be the best predictive models with 0% error rates during automated extraction of the water quality class. This study reveals that data mining techniques have the potential to quickly predict water quality class, provided data given are a true representation of the domain knowledge. © 2017, Springer-Verlag GmbH Germany.","Data mining classifiers; k-fold cross-validation; Overall Index of Pollution; Water quality index; Water quality parameters","Classification (of information); Classifiers; Decision making; Decision trees; Extraction; Forecasting; Nearest neighbor search; Neural networks; Pollution; Quality assurance; River pollution; Support vector machines; Water pollution; Water quality; Water resources; Automated extraction; Classification technique; K fold cross validations; K-nearest neighbors; Rule-based classifier; Synthetic and real data; Water quality indexes; Water quality parameters; Data mining; artificial neural network; data mining; pollution incidence; river water; support vector machine; water quality"
"Babic B.B., Gerke S., Evgeniou T., Glenn Cohen I.","Beware explanations from AI in health care the benefits of explainable artificial intelligence are not what they appear","10.1126/science.abg1834","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110525466&doi=10.1126%2fscience.abg1834&partnerID=40&md5=9c4f4a466a114ee3eded6ee4d2cdcff2",[No abstract available],,"artificial intelligence; health care; numerical model; simulation; Article; artificial intelligence; awareness; beware explanation; cost; ecosystem resilience; health care; human; machine learning; tenuous connection"
"Babič F., Pusztová Ľ., Majnarić L.T.","Mild cognitive impairment detection using association rules mining","10.18267/J.AIP.135","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102276635&doi=10.18267%2fJ.AIP.135&partnerID=40&md5=aab29d4638025b8f5ba6b3c539e07676","A single Mild cognitive impairment (MCI) is a transitional state between normal cognition and dementia. The typical diagnostic procedure relies on neuropsychological testing, which is insufficiently accurate and does not provide information on patients' clinical profiles. The objective of this paper is to improve the recognition of elderly primary care patients with MCI by using an approach typically applied in the market basket analysis - association rules mining. In our case, the association rules represent various combinations of the clinical features or patterns associated with MCI. The analytical process was performed in line with the CRISP-DM, the methodology for data mining projects widely used in various research or industry domains. In the data preparation phase, we applied several approaches to improve the data quality like the k-Nearest Neighbour, correlation analysis, Chi Merge and K-Means algorithms. The analytical solution´s success was confirmed not only by the novelty and correctness of new knowledge, but also by the form of visualization that is easily understandable for domain experts. This iterative approach provides a set of rules (patterns) that meet minimum support and reliability. The extracted rules may help medical professionals recognize clinical patterns; however, the final decision depends on the expert. A medical expert has a crucial role in this process by enabling the link between the information contained in the rules and the evidence-based knowledge. It markedly contributes to the interpretability of the results. Copyright © 2020 by the author(s). Licensee Prague University of Economics and Business, Czech Republic. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution License (CC BY), which permits use, distribution and reproduction in any medium, provided the original publication is properly cited, see http://creativecommons.org/licenses/by/4.0/. No use, distribution or reproduction is permitted which does not comply with these terms.","Association rules; Interpretability; Mild cognitive impairment; Patterns",
"Babič F., Vadovský M., Muchová M., Paralič J., Majnarić L.","Simple understandable analysis of medical data to support the diagnostic process","10.1109/SAMI.2017.7880293","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017649899&doi=10.1109%2fSAMI.2017.7880293&partnerID=40&md5=48fa42c34da2a86bffc681b020dc3ff3","Medical diagnostic is a complex process consisting of many input variables, which the general practitioner (GP) or specialist should take into account before confirm the expected diagnosis. In the case of electronic records, they have an opportunity to support this process within simple understandable results of the correctly applied suitable methods from machine learning or statistics. We used a small sample of patient's data from Croatia for experimental evaluation of this potential. We applied the methods as Welch's t-test, Pearson chi-square independence test, Youden's index, decision trees and simple K-Means. The cooperating medical expert evaluated the obtained results and confirmed the expected potential for daily medical practice. © 2017 IEEE.",,"Artificial intelligence; Decision trees; Learning systems; Complex Processes; Diagnostic process; Electronic records; Experimental evaluation; General practitioners; Independence tests; Medical diagnostics; Medical practice; Diagnosis"
"Babiceanu R.F., Chen F.F., Sturges R.H.","Internal agent architecture for Agent-based Material Handling Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-30044439568&partnerID=40&md5=286c481b47435b023993e2c38068f5af","The increasing impact of global economy is changing the traditional way a manufacturing company used to be managed. Real-time response to any changes in shop-floor operations, agility in satisfying any customer requests, and reconfigurability in both software and hardware equipment are likely to become essential characteristics for next generation manufacturing systems. A Multi-Agent System (MAS) is a distributed system composed of two or more agents, each having their independent problem-solving mechanism, individual goals and specific capabilities. MAS have been regularly used in a large number of different applications such as distributed computing, Internet, and telecommunications. More recently, MAS theory is increasingly employed in manufacturing applications. Traditionally, Material Handling Systems (MHS) are controlled through a central computer and the individual Material Handling (MH) devices moves are pre-established by this computer at the same moment when job scheduling is performed. Because of their decentralized architecture, Agent-based Material Handling Systems (AMHS) give the possibility to account for any changes that might occur in the manufacturing system, and can adapt the individual MH devices schedules during the actual system operation. The resulting emergent schedule is obtained by combining the individual MH devices schedules. The proposed AMHS is formed by the physical MH devices each of them having their own control unit, that together form the MH Agents, a central computer with a global system perspective, called Global View Agent, and a System Monitor entity having an associated Database (SMD). Software modules, in the form of Order Agents, are assigned for each order that comes into the system. The three types of agents in the AMHS have similar structures in terms of components forming the agent architecture, but the internal evaluation algorithms, communication capabilities, and learning mechanisms are specific for each type of agent. Moreover, individual utility functions, expressed as objective functions to be optimized, could vary also among the same type of agents. The four main components included in each agent in the proposed AMHS, Evaluation, Communication, Internal Database, and Learning are modeled as software modules, and each of them has a specific capability: computational, exchanging information, storing and retrieving data, and machine learning, respectively. The internal evaluation module, which is a key part of every agent in the distributed architecture, is responsible for assessing the value of the incoming service requests in terms of agent objectives and selecting the ones to be executed. The communication module manages all the incoming and outgoing messages, and uses a common communication language and agreed communication protocols with other agents in the architecture. By exchanging information among the agents having common goals, coordination can be achieved. The internal database is used to store all the information related with the awarded tasks, and to make it available for future reference. The addition of the learning module for each agent in the architecture delivers significant improvements to the solutions. At the individual agent level, single-agent learning mechanisms result in improved problem solutions by using past experience or simulation of the future. At the system level, multi-agent learning is based on single-agent learning, and the emergence properties of MAS. Learning of individual agents is influenced by the decisions made and actions performed by the other agents in the system, resulting, at the system level, in an emergent type of learning. By using machine learning mechanisms, the decisions made both at the agent and system level are faster, more reliable, and of better quality.","Internal Agent Architecture; Machine Learning; Material Handling Systems; Multi-Agent Systems","Decision theory; Internet; Learning systems; Multi agent systems; Scheduling; Telecommunication; Internal agent architecture; Job scheduling; Material handling system; Materials handling"
"Bacardit J., Brownlee A.E.I., Cagnoni S., Iacca G., McCall J., Walker D.","The intersection of evolutionary computation and explainable AI","10.1145/3520304.3533974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136333389&doi=10.1145%2f3520304.3533974&partnerID=40&md5=f2142e2efa131154de1f6ee3986e2b54","In the past decade, Explainable Artificial Intelligence (XAI) has attracted a great interest in the research community, motivated by the need for explanations in critical AI applications. Some recent advances in XAI are based on Evolutionary Computation (EC) techniques, such as Genetic Programming. We call this trend EC for XAI. We argue that the full potential of EC methods has not been fully exploited yet in XAI, and call the community for future efforts in this field. Likewise, we find that there is a growing concern in EC regarding the explanation of population-based methods, i.e., their search process and outcomes. While some attempts have been done in this direction (although, in most cases, those are not explicitly put in the context of XAI), we believe that there are still several research opportunities and open research questions that, in principle, may promote a safer and broader adoption of EC in real-world applications. We call this trend XAI within EC. In this position paper, we briefly overview the main results in the two above trends, and suggest that the EC community may play a major role in the achievement of XAI. © 2022 ACM.","evolutionary computation; explainable artificial intelligence; machine learning; optimization","Calculations; Genetic algorithms; Genetic programming; AI applications; Computation methods; Evolutionary computation techniques; Explainable artificial intelligence; Machine-learning; Optimisations; Research communities; Research opportunities; Research questions; Search process; Machine learning"
"Bacardit J., Llorà X.","Large scale data mining using genetics-based machine learning","10.1145/2330784.2330936","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865019979&doi=10.1145%2f2330784.2330936&partnerID=40&md5=6a6ac3c5d36f75dd205f83449e98fc90","We are living in the peta-byte era. We have larger and larger data to analyze, process and transform into useful answers for the domain experts. Robust data mining tools, able to cope with petascale volumes and/or high dimensionality producing human-understandable solutions are key on several domain areas. Genetics-based machine learning (GBML) techniques are perfect candidates for this task. Recent advances in representations, learning paradigms, and theoretical modelling have showed the competitiveness of non EC techniques in herding large scale data analysis. If evolutionary learning techniques aspire to be a relevant player in this context, they need to have the capacity of processing these vast amounts of data and they need to process this data within reasonable time. Moreover, massive computation cycles are getting cheaper and cheaper every day, allowing researchers to have access to unprecedented computational resources on the edge of petascale computing. Several topics are interlaced in these two requirements: (1) having the proper learning paradigms and knowledge representations, (2) understanding them and knowing when are they suitable for the problem at hand, (3) using efficiency enhancement techniques, and (4) transforming and visualizing the produced solutions to give back as much insight as possible to the domain experts are few of them. This tutorial will try to shed light to the above mentioned questions, following a roadmap that starts exploring what large scale means, and why large is a challenge and opportunity for GBML methods. As we will show later, opportunity has multiple facets: Efficiency enhancement techniques, representations able to cope with large dimensionality spaces, scalability of learning paradigms, and alternative programming models, each of them helping to make GBML very attractive for large-scale data mining. Given these building blocks, we will continue to unfold how we can model the scalability of the components of GBML systems targeting a better engineering effort that will make embracing large datasets routine. Finally, we will illustrate how all these ideas fit by reviewing real applications of GBML systems and what further directions will require serious consideration. Copyright is held by the author/owner(s).",,"Chromosomes; Efficiency; Knowledge acquisition; Knowledge representation; Large dataset; Machine learning; Scalability; Computational resources; Efficiency enhancement technique; Evolutionary Learning; Genetics based machine learning; High dimensionality; Large-scale data analysis; Peta-scale computing; Theoretical modelling; Data mining"
"Bacardit J., Llorà X.","Large scale data mining using genetics-based machine learning","10.1145/2001858.2002137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051942980&doi=10.1145%2f2001858.2002137&partnerID=40&md5=9f6e050d02c2ab7c9b3c65fb8845ee7f","We are living in the peta-byte era. We have larger and larger data to analyze, process and transform into useful answers for the domain experts. Robust data mining tools, able to cope with petascale volumes and/or high dimensionality producing human understandable solutions are key on several domain areas. Genetics-based machine learning (GBML) techniques are perfect candidates for this task. Recent advances in representations, learning paradigms, and theoretical modeling have show the competitiveness of non EC techniques in herding large scale data analysis. If evolutionary learning techniques aspire to be a relevant player in this context, they need to have the capacity of processing these vast amounts of data and they need to process this data within reasonable time. Moreover, massive computation cycles are getting cheaper and cheaper every day, allowing researchers to have access to unprecedented computational resources on the edge of petascale computing. Several topics are interlaced in these two requirements: (1) having the proper learning paradigms and knowledge representations, (2) understanding them and knowing when are they suitable for the problem at hand, (3) using efficiency enhancement techniques, and (4) transforming and visualizing the produced solutions to give back as much insight as possible to the domain experts are few of them. This tutorial will try to shed light to the above mentioned questions, following a roadmap that starts exploring what large scale means, and why large is a challenge and opportunity for GBML methods. As we will show later, opportunity has multiple facets: Efficiency enhancement techniques, representations able to cope with large dimensionality spaces, scalability of learning paradigms, and alternative programming models, each of them helping to make GBML very attractive for large-scale data mining. Given these building blocks, we will continue to unfold how can we model the scalability of the components of GBML systems targeting a better engineering effort that will make embracing large datasets routine. Finally, we will illustrate how all these ideas fit by reviewing real applications of GBML systems and what further directions will require serious consideration. © 2011 Authors.","efficiency enhancement; genetics-based machine learning; large-scale data mining; parallelization","Building blockes; Computation cycle; Computational resources; Domain experts; Efficiency enhancement; Efficiency enhancement technique; Evolutionary Learning; Genetics based machine learning; High dimensionality; Large datasets; Large scale data; large-scale data mining; Learning paradigms; Parallelizations; Peta-scale computing; Petascale; Programming models; Proper learning; Real applications; Roadmap; Robust datum; Theoretical modeling; Chromosomes; Competition; Computer programming; Data mining; Data reduction; Efficiency; Knowledge representation; Learning systems; Scalability; Metadata"
"Bacardit J., Burke E.K., Krasnogor N.","Improving the scalability of rule-based evolutionary learning","10.1007/s12293-008-0005-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-65149084519&doi=10.1007%2fs12293-008-0005-4&partnerID=40&md5=484801186ad9b28e7d5c6557b98517d2","Evolutionary learning techniques are comparable in accuracy with other learning methods such as Bayesian Learning, SVM, etc. These techniques often produce more interpretable knowledge than, e.g. SVM; however, efficiency is a significant drawback. This paper presents a new representation motivated by our observations that Bioinformatics and Systems Biology often give rise to very large-scale datasets that are noisy, ambiguous and usually described by a large number of attributes. The crucial observation is that, in the most successful rules obtained for such datasets, only a few key attributes (from the large number of available ones) are expressed in a rule, hence automatically discovering these few key attributes and only keeping track of them contributes to a substantial speed up by avoiding useless match operations with irrelevant attributes. Thus, in effective terms this procedure is performing a fine-grained feature selection at a rule-wise level, as the key attributes may be different for each learned rule. The representation we propose has been tested within the BioHEL machine learning system, and the experiments performed show that not only the representation has competent learning performance, but that it also manages to reduce considerably the system run-time. That is, the proposed representation is up to 2-3 times faster than state-of-the-art evolutionary learning representations designed specifically for efficiency purposes. © 2008 Springer-Verlag.","Bioinformatics; Evolutionary algorithms; Evolutionary learning; Learning classifier systems; Protein structure prediction; Rule induction",
"Bacardit J., Llorà X.","Large scale data mining using genetics-based machine learning","10.1145/1570256.1570424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120719838&doi=10.1145%2f1570256.1570424&partnerID=40&md5=3282fc09381c61d89c7c8796ac456c84","We are living in the peta-byte era. We have larger and larger data to analyze, process and transform into useful answers for the domain experts. Robust data mining tools, able to cope with petascale volumes and/or high dimensionality producing human-understandable solutions are key on several domain areas. Genetics-based machine learning (GBML) techniques are perfect candidates for this task, among others, due to the recent advances in representations, learning paradigms, and theoretical modeling. If evolutionary learning techniques aspire to be a relevant player in this context, they need to have the capacity of processing these vast amounts of data and they need to process this data within reasonable time. Moreover, massive computation cycles are getting cheaper and cheaper every day, allowing researchers to have access to unprecedented parallelization degrees. Several topics are interlaced in these two requirements: (1) having the proper learning paradigms and knowledge representations, (2) understanding them and knowing when are they suitable for the problem at hand, (3) using efficiency enhancement techniques, and (4) transforming and visualizing the produced solutions to give back as much insight as possible to the domain experts are few of them. This tutorial will try to answer this question, following a roadmap that starts with the questions of what large means, and why large is a challenge for data mining methods. Afterwards, we will discuss different facets in which we can overcome this challenge: Efficiency enhancement techniques, representations able to cope with large dimensionality spaces, scalability of learning paradigms, hardware solutions, parallel models and data-intensive computing. The roadmap continues with examples of real applications of GBML systems and finishes with an analysis of further directions. © 2009 Copyright is held by the author/owner(s).","data-intensive computing; efficiency enhancement; evolutionary algorithms; genetics-based machine learning; large-scale datasets; parallel computing","Chromosomes; Data mining; Efficiency; Evolutionary algorithms; Learning systems; Metadata; Parallel processing systems; Data-intensive computing; Domain experts; Efficiency enhancement; Efficiency enhancement technique; Genetic-based machine learning; Genetics based machine learning; Large-scale datasets; Learning paradigms; Parallel com- puting; Roadmap; Knowledge representation"
"Bacardit J., Stout M., Hirst J.D., Sastry K., Llor X., Krasnogor N.","Automated alphabet reduction method with evolutionary algorithms for protein structure prediction","10.1145/1276958.1277033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548056763&doi=10.1145%2f1276958.1277033&partnerID=40&md5=6583a63e6585a2e391bdc05a2b245c24","This paper focuses on automated procedures to reduce the dimensionality ofprotein structure prediction datasets by simplifying the way in which the primary sequence of a protein is represented. The potential benefits ofthis procedure are faster and easier learning process as well as the generationof more compact and human-readable classifiers.The dimensionality reduction procedure we propose consists on the reductionof the 20-letter amino acid (AA) alphabet, which is normally used to specify a protein sequence, into a lower cardinality alphabet. This reduction comes about by a clustering of AA types accordingly to their physical and chemical similarity. Our automated reduction procedure is guided by a fitness function based on the Mutual Information between the AA-based input attributes of the dataset and the protein structure featurethat being predicted. To search for the optimal reduction, the Extended Compact Genetic Algorithm (ECGA) was used, and afterwards the results of this process were fed into (and validated by) BioHEL, a genetics-based machine learningtechnique. BioHEL used the reduced alphabet to induce rules forprotein structure prediction features. BioHEL results are compared to two standard machine learning systems. Our results show that it is possible to reduce the size of the alphabet used for prediction fromtwenty to just three letters resulting in more compact, i.e. interpretable,rules. Also, a protein-wise accuracy performance measure suggests that the loss of accuracy acrued by this substantial alphabet reduction is not statistically significant when compared to the full alphabet. Copyright 2007 ACM.","Alphabet reduction; Bioinformatics; Coordination number prediction; Estimation of distribution algorithms; Evolutionary algorithms; Learning classifier systems; Protein structure prediction; Rule induction","Bioinformatics; Distribution functions; Learning systems; Logic programming; Proteins; Alphabet reduction; Coordination number prediction; Distribution algorithms; Learning classifier systems; Protein structure prediction; Rule induction; Evolutionary algorithms"
"Bacciu D., Numeroso D.","Explaining Deep Graph Networks via Input Perturbation","10.1109/TNNLS.2022.3165618","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128637302&doi=10.1109%2fTNNLS.2022.3165618&partnerID=40&md5=8e4fc3a835868a59a066def3e10610ec","Deep graph networks (DGNs) are a family of machine learning models for structured data which are finding heavy application in life sciences (drug repurposing, molecular property predictions) and on social network data (recommendation systems). The privacy and safety-critical nature of such domains motivates the need for developing effective explainability methods for this family of models. So far, progress in this field has been challenged by the combinatorial nature and complexity of graph structures. In this respect, we present a novel local explanation framework specifically tailored to graph data and DGNs. Our approach leverages reinforcement learning to generate meaningful local perturbations of the input graph, whose prediction we seek an interpretation for. These perturbed data points are obtained by optimizing a multiobjective score taking into account similarities both at a structural level as well as at the level of the deep model outputs. By this means, we are able to populate a set of informative neighboring samples for the query graph, which is then used to fit an interpretable model for the predictive behavior of the deep network locally to the query graph prediction. We show the effectiveness of the proposed explainer by a qualitative analysis on two chemistry datasets, TOX21 and Estimated SOLubility (ESOL) and by quantitative results on a benchmark dataset for explanations, CYCLIQ. IEEE","Analytical models; Convolution; Data models; Deep graph networks (DGNs); deep reinforcement learning; explainable artificial intelligence (XAI); graph; interpretability.; Perturbation methods; Predictive models; Task analysis; Training","Chemical analysis; Forecasting; Graph neural networks; Job analysis; Perturbation techniques; Reinforcement learning; Safety engineering; Deep graph network; Explainable artificial intelligence (XAI); Graph networks; Input perturbation; Interpretability; Interpretability.; Perturbation method; Predictive models; Query graph; Task analysis; Deep learning"
"Bacco L., Russo F., Ambrosio L., D’Antoni F., Vollero L., Vadalà G., Dell’Orletta F., Merone M., Papalia R., Denaro V.","Natural language processing in low back pain and spine diseases: A systematic review","10.3389/fsurg.2022.957085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135163810&doi=10.3389%2ffsurg.2022.957085&partnerID=40&md5=2347572117957fb43b1341aa1ec7dae4","Natural Language Processing (NLP) is a discipline at the intersection between Computer Science (CS), Artificial Intelligence (AI), and Linguistics that leverages unstructured human-interpretable (natural) language text. In recent years, it gained momentum also in health-related applications and research. Although preliminary, studies concerning Low Back Pain (LBP) and other related spine disorders with relevant applications of NLP methodologies have been reported in the literature over the last few years. It motivated us to systematically review the literature comprised of two major public databases, PubMed and Scopus. To do so, we first formulated our research question following the PICO guidelines. Then, we followed a PRISMA-like protocol by performing a search query including terminologies of both technical (e.g., natural language and computational linguistics) and clinical (e.g., lumbar and spine surgery) domains. We collected 221 non-duplicated studies, 16 of which were eligible for our analysis. In this work, we present these studies divided into sub-categories, from both tasks and exploited models’ points of view. Furthermore, we report a detailed description of techniques used to extract and process textual features and the several evaluation metrics used to assess the performance of the NLP models. However, what is clear from our analysis is that additional studies on larger datasets are needed to better define the role of NLP in the care of patients with spinal disorders. 2022 Bacco, Russo, Ambrosio, D'Antoni, Vollero, Vadalà, Dell'Orletta, Merone, Papalia and Denaro.","artificial intelligence; deep learning; low back pain; natural language processing; spine disorders; systematic review",
"Bacco L., Cimino A., Dell’orletta F., Merone M.","Explainable sentiment analysis: A hierarchical transformer-based extractive summarization approach","10.3390/electronics10182195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114289346&doi=10.3390%2felectronics10182195&partnerID=40&md5=0fd0fc163bf8a1f996a80adca1cd2bd1","In recent years, the explainable artificial intelligence (XAI) paradigm is gaining wide research interest. The natural language processing (NLP) community is also approaching the shift of paradigm: building a suite of models that provide an explanation of the decision on some main task, without affecting the performances. It is not an easy job for sure, especially when very poorly interpretable models are involved, like the almost ubiquitous (at least in the NLP literature of the last years) transformers. Here, we propose two different transformer-based methodologies exploiting the inner hierarchy of the documents to perform a sentiment analysis task while extracting the most important (with regards to the model decision) sentences to build a summary as the explanation of the output. For the first architecture, we placed two transformers in cascade and leveraged the attention weights of the second one to build the summary. For the other architecture, we employed a single transformer to classify the single sentences in the document and then combine the probability scores of each to perform the classification and then build the summary. We compared the two methodologies by using the IMDB dataset, both in terms of classification and explainability performances. To assess the explainability part, we propose two kinds of metrics, based on benchmarking the models’ summaries with human annotations. We recruited four independent operators to annotate few documents retrieved from the original dataset. Furthermore, we conducted an ablation study to highlight how implementing some strategies leads to important improvements on the explainability performance of the cascade transformers model. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Explainability; Extractive summarization; Hierarchical transformers; Sentiment analysis",
"Bacco L., Cimino A., Dell'Orletta F., Merone M.","Extractive summarization for explainable sentiment analysis using transformers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112103475&partnerID=40&md5=9452108c37117cfae44ed447b7f84e13","In recent years, the paradigm of eXplainable Artificial Intelligence (XAI) systems has gained wide research interest and beyond. The Natural Language Processing (NLP) community is also approaching this new way of understanding AI applications: building a suite of models that provide an explanation for the decision, without affecting performance. This is certainly not an easy task, considering the wide use of very poorly interpretable models such as Transformers, which in recent years are found to be almost ubiquitous in the NLP literature because of the great strides they have allowed. Here we propose two different methodologies to exploit the performance of these models in a task of sentiment analysis and, in the meantime, to generate a summary that serves as an explanation of the decision taken by the system. To compare the classification performance of the two methodologies, we used the IMDB dataset while, to assess the explainability performance, we annotated some samples of this dataset to retrieve human extractive summaries, benchmarking them with the summaries generated by the systems. © 2021 CEUR-WS. All rights reserved.",,"Classification (of information); Deep learning; Ontology; Sentiment analysis; AI applications; Classification performance; Community IS; Extractive summarizations; NAtural language processing; Research interests; Benchmarking"
"Bach K.","From data to context-aware decision making: Challenges and opportunities",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052709491&partnerID=40&md5=1f4d91b6a8969b21f953fabdecb8ed56","This extended abstract presents the research challenges for developing context-aware, explainable artificial intelligence challenges and briey presents the opportunities ahead when utilizing the vast amount of data generated. © 2018 CEUR-WS. All rights reserved.","Big data; Context-awareness; Decision support systems; Explainability","Ambient intelligence; Artificial intelligence; Decision making; Decision support systems; Context aware ,; Context- awareness; Context-aware decision makings; Explainability; Extended abstracts; Research challenges; Big data"
"Bachl M., Hartl A., Fabini J., Zseby T.","Walling up backdoors in intrusion detection systems","10.1145/3359992.3366638","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078950970&doi=10.1145%2f3359992.3366638&partnerID=40&md5=2e731e26411bd0ff5dbd56832ea06b6e","Interest in poisoning attacks and backdoors recently resurfaced for Deep Learning (DL) applications. Several successful defense mechanisms have been recently proposed for Convolutional Neural Networks (CNNs), for example in the context of autonomous driving. We show that visualization approaches can aid in identifying a backdoor independent of the used classifier. Surprisingly, we find that common defense mechanisms fail utterly to remove backdoors in DL for Intrusion Detection Systems (IDSs). Finally, we devise pruning-based approaches to remove backdoors for Decision Trees (DTs) and Random Forests (RFs) and demonstrate their effectiveness for two different network security datasets. © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6999-2/19/12...$15.00","Deep Learning; Explainable AI; Network security; Poisoning attack; Pruning; Random Forests","Big data; Computer crime; Convolutional codes; Decision trees; Deep learning; Intrusion detection; Machine learning; Network security; Neural networks; Autonomous driving; Convolutional neural network; Decision trees (DTs); Defense mechanism; Intrusion Detection Systems; Poisoning attacks; Pruning; Random forests; Data communication systems"
"Back A.D., Weigend A.S.","A first application of independent component analysis to extracting structure from stock returns.","10.1142/S0129065797000458","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031197006&doi=10.1142%2fS0129065797000458&partnerID=40&md5=e01db40edc156c2c14930cc0b82c7ba7","This paper explores the application of a signal processing technique known as independent component analysis (ICA) or blind source separation to multivariate financial time series such as a portfolio of stocks. The key idea of ICA is to linearly map the observed multivariate time series into a new space of statistically independent components (ICs). We apply ICA to three years of daily returns of the 28 largest Japanese stocks and compare the results with those obtained using principal component analysis. The results indicate that the estimated ICs fall into two categories, (i) infrequent large shocks (responsible for the major changes in the stock prices), and (ii) frequent smaller fluctuations (contributing little to the overall level of the stocks). We show that the overall stock price can be reconstructed surprisingly well by using a small number of thresholded weighted ICs. In contrast, when using shocks derived from principal components instead of independent components, the reconstructed price is less similar to the original one. ICA is shown to be a potentially powerful method of analyzing and understanding driving mechanisms in financial time series. The application to portfolio optimization is described in Chin and Weigend (1998).",,"algorithm; article; artificial intelligence; artificial neural network; statistical model; Algorithms; Artificial Intelligence; Models, Economic; Neural Networks (Computer)"
"Backus P.R., Jordan J.C., Harper D.G.","Real time data acquisition in SETI","10.1016/0094-5765(92)90090-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-44049116257&doi=10.1016%2f0094-5765%2892%2990090-6&partnerID=40&md5=65d6e6491e80fb0a78accb988df167a0","NASA's Search for Extraterrestrial Intelligence presents unique challenges for data acquisition and analysis. During the proposed Microwave Observing Project each MultiChannel Spectrum Analyser (MCSA) system will process more than 60 million bytes of information every second. To prevent a backlog of data and to avoid delays in the confirmation of potential signals, the data will be processed in real time with special purpose hardware and software. The software is required to control and coordinate the hardware systems and the data that they produce. The performance of the system must be continuously monitored to assure that the search achieves maximum sensitivity, minimum contamination by Radio Frequency Interference, and optimal use of observing resources. We have developed a prototype data acquisition system that has been used in field tests of a prototype MCSA and software signal detection algorithms. The system, called VERA 2.0, is based on virtually concurrent communication software processes within a single computer. A new version under development will use several independent computers, communicating over a network. Each computer will be responsible for a portion of the data processing. Some techinques of artificial intelligence will be used to evaluate potential signals during the observation. © 1992.",,
"Baclawski K., Bennett M., Berg-Cross G., Fritzsche D., Sharma R., Singer J., Sowa J.F., Sriram R.D., Underwood M., Whitten D.","Ontology summit 2019 communiqué: Explanations","10.3233/AO-200226","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080882756&doi=10.3233%2fAO-200226&partnerID=40&md5=18b016723164f4aa94e13ab00fd48590","With the increasing amount of software devoted to industrial automation and process control, it is becoming more important than ever for systems to be able to explain their behavior. In some domains, such as financial services, explainability is mandated by law. In spite of this, explanation today is largely handled in an unsystematic manner, if it is handled at all. The decisions of modern artificially intelligent systems, such as those built on deep neural networks, are especially difficult to explain. The goal of the recent Ontology Summit 2019 was concerned with the role of ontologies for explaining the functioning of a system. More specifically, the Ontology Summit focused on critical explanation gaps and the role of ontologies for dealing with these gaps. The sessions examined current technologies and real needs driven by risks and requirements to meet legal or other standards. The sessions covered explainable artificial intelligence, commonsense reasoning and knowledge, the role of narrative, and explanations in the fields of finance and medicine. The goal of this Communiqué is to foster research and development of approaches to explanations and to drive towards explanation support which can be incorporated into both knowledge engineering processes and ontology design best practices. © 2020-IOS Press and the authors. All rights reserved.","commonsense reasoning; context; explainable AI; financial explanations; inference; medical explanations; narrative; Ontologies","Deep neural networks; Finance; Intelligent systems; Outsourcing; Commonsense reasoning; context; financial explanations; inference; medical explanations; narrative; Ontology"
"Badam J., Bonagiri A., Raju K., Chakraborty D.","Aletheia: A Fake News Detection System for Hindi","10.1145/3493700.3493736","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122688286&doi=10.1145%2f3493700.3493736&partnerID=40&md5=3d139a9ce38c222600f46e87a03b3291","""Fake News""and Misinformation can have far-reaching negative social impacts. Scalable fake news classification techniques for resource-poor languages such as Hindi are in their infancy due to the lack of data sets and lack of robust NLP libraries in these languages. We present Aletheia, a Fake News classification system for Hindi. We curate a dataset of approximately 13,000 news articles by media organizations that flag authentic and fake news. We present preliminary results using several Machine Learning models on this dataset. We also developed a system accessible over the web (http://responsible-tech.bits-hyderabad.ac.in/aletheia/demo/) using which users can test if a given piece of news is fake or authentic. We also use the website to collect crowd-sourced labelled news data and present additional information on the dataset and the models to the users. © 2022 Owner/Author.","AI for Social Impact; Fake News Detection; Indian Languages; NLP","Classification (of information); Economic and social effects; Fake detection; AI for social impact; Classification system; Classification technique; Data set; Detection system; Fake news detection; Indian languages; Machine learning models; News articles; Social impact; Natural language processing systems"
"Badami M., Tafazzoli F., Nasraoui O.","A case study for intelligent event recommendation","10.1007/s41060-018-0120-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059034616&doi=10.1007%2fs41060-018-0120-3&partnerID=40&md5=ef949985f67e4ebf643fadc64485580a","Social networks, along with their “event” organization, planning, and sharing tools, play an important role in connecting and engaging individuals and groups. These online spaces thrive with multifaceted activities and interests which give rise to rich content and user interaction that often crossover to the world of events. For these reasons, the data trails associated with “events” in the virtual world can be complex and challenging to understand and predict. This paper presents our efforts to build an interpretable framework to analyze event data and recommend relevant events to social media users with different preferences. The datasets for this challenge were provided by a competition on Kaggle. We conduct an extensive data analysis and exploration to help gain a better understanding of the data. We then proceed to the critical phase of feature engineering, storytelling and modeling for computing event recommendations. We explore fuzzy approximate reasoning for modeling because of its rich linguistic expression ability which allows handling uncertainty, while maintaining human interpretability of the built models and predictions. This interpretability is critical in the data mining enterprise because data mining often requires team collaboration and yields results that need to be consumed by people of diverse technical and non-technical background. Such teams tend to question the meaning of models and emphasize the importance of telling stories from the data. We evaluate our event recommendation system on a real-world dataset with more than one million events and 38,000 users. The proposed methodology achieved 70% accuracy, outperforming existing event recommendation algorithms. © 2018, Springer International Publishing AG, part of Springer Nature.","Adaptive neuro-fuzzy classifier; Big data; Collaborative filtering; Data mining; Data science; Event recommendation; Explainability; Feature engineering; Interpretability; Recommender system","Uncertainty analysis; Analysis and explorations; Approximate reasoning; Feature engineerings; Linguistic expressions; Mining enterprise; Recommendation algorithms; Team collaboration; Technical background; Data mining"
"Baddoo P.J., Herrmann B., McKeon B.J., Brunton S.L.","Kernel learning for robust dynamic mode decomposition: linear and nonlinear disambiguation optimization","10.1098/rspa.2021.0830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129113740&doi=10.1098%2frspa.2021.0830&partnerID=40&md5=ce73b3711ef5655ba5b37725fccf3d1d","Research in modern data-driven dynamical systems is typically focused on the three key challenges of high dimensionality, unknown dynamics and nonlinearity. The dynamic mode decomposition (DMD) has emerged as a cornerstone for modelling high-dimensional systems from data. However, the quality of the linear DMD model is known to be fragile with respect to strong nonlinearity, which contaminates the model estimate. By contrast, sparse identification of nonlinear dynamics learns fully nonlinear models, disambiguating the linear and nonlinear effects, but is restricted to low-dimensional systems. In this work, we present a kernel method that learns interpretable data-driven models for high-dimensional, nonlinear systems. Our method performs kernel regression on a sparse dictionary of samples that appreciably contribute to the dynamics. We show that this kernel method efficiently handles high-dimensional data and is flexible enough to incorporate partial knowledge of system physics. It is possible to recover the linear model contribution with this approach, thus separating the effects of the implicitly defined nonlinear terms. We demonstrate our approach on data from a range of nonlinear ordinary and partial differential equations. This framework can be used for many practical engineering tasks such as model order reduction, diagnostics, prediction, control and discovery of governing laws. © 2022 The Authors","kernel methods; machine learning; modal decomposition; system identification","Clustering algorithms; Dynamical systems; Machine learning; Nonlinear analysis; Nonlinear equations; Nonlinear optics; Nonlinear systems; Ordinary differential equations; Data driven; Dynamic mode decompositions; High dimensionality; High-dimensional systems; Kernel learning; Kernel-methods; Learn+; Modal decomposition; Optimisations; System-identification; Dynamics"
"Bade F.M., Vollenberg C., Koch J., Koch J., Coners A.","The Dark Side of Process Mining. How Identifiable Are Users Despite Technologically Anonymized Data? A Case Study from the Health Sector","10.1007/978-3-031-16103-2_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138762655&doi=10.1007%2f978-3-031-16103-2_16&partnerID=40&md5=d350cf8f62cc653545093410d3548c4b","Over the past decade, process mining has emerged as a new area of research focused on analyzing end-to-end processes through the use of event data and novel techniques for process discovery and conformance testing. While the benefits of process mining are widely recognized scientifically, research has increasingly addressed privacy concerns regarding the use of personal data and sensitive information that requires protection and compliance with data protection regulations. However, the privacy debate is currently answered exclusively by technical safeguards that lead to the anonymization of process data. This research analyzes the real-world utility of these process data anonymization techniques and evaluates their suitability for privacy protection. To this end, we use process mining in a case study to investigate how responsible users and specific user groups can be identified despite the technical anonymization of process mining data. © 2022, Springer Nature Switzerland AG.","Healthcare sector; Hospital information system; Privacy measures; Process mining","Health care; Information use; Sensitive data; Anonymization; Case-studies; End-to-end process; Healthcare sectors; Hospital information systems; Novel techniques; Privacy measures; Process data; Process Discovery; Process mining; Data mining"
"Badea L., Stǎnescu E.","Identifying transcriptomic correlates of histology using deep learning","10.1371/journal.pone.0242858","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096818135&doi=10.1371%2fjournal.pone.0242858&partnerID=40&md5=dc75faa2a7a05cd8401fb171af799052","Linking phenotypes to specific gene expression profiles is an extremely important problem in biology, which has been approached mainly by correlation methods or, more fundamentally, by studying the effects of gene perturbations. However, genome-wide perturbations involve extensive experimental efforts, which may be prohibitive for certain organisms. On the other hand, the characterization of the various phenotypes frequently requires an expert's subjective interpretation, such as a histopathologist's description of tissue slide images in terms of complex visual features (e.g. 'acinar structures'). In this paper, we use Deep Learning to eliminate the inherent subjective nature of these visual histological features and link them to genomic data, thus establishing a more precisely quantifiable correlation between transcriptomes and phenotypes. Using a dataset of whole slide images with matching gene expression data from 39 normal tissue types, we first developed a Deep Learning tissue classifier with an accuracy of 94%. Then we searched for genes whose expression correlates with features inferred by the classifier and demonstrate that Deep Learning can automatically derive visual (phenotypical) features that are well correlated with the transcriptome and therefore biologically interpretable. As we are particularly concerned with interpretability and explainability of the inferred histological models, we also develop visualizations of the inferred features and compare them with gene expression patterns determined by immunohistochemistry. This can be viewed as a first step toward bridging the gap between the level of genes and the cellular organization of tissues. © 2020 Badea, Stanescu. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Article; cell structure; convolutional neural network; correlation analysis; deep learning; gene expression; gene expression profiling; gene identification; genetic transcription; histopathology; human; human cell; human tissue; immunohistochemistry; long short term memory network; machine learning; mathematical computing; mathematical model; phenotype; recurrent neural network; gene expression regulation; genetics; genome; histology; neoplasm; pathology; tissue distribution; transcriptome; tumor protein; Deep Learning; Gene Expression Regulation, Neoplastic; Genome; Histology; Humans; Neoplasm Proteins; Neoplasms; Tissue Distribution; Transcriptome"
"Badea L.","Multirelational consensus clustering with nonnegative decompositions","10.3233/978-1-61499-098-7-97","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878810948&doi=10.3233%2f978-1-61499-098-7-97&partnerID=40&md5=4de32b835c7ec6414fb732435ac50ff6","Unsupervised multirelational learning (clustering) in non-sparse domains such as molecular biology is especially difficult as most clustering algorithms tend to produce distinct clusters in slightly different runs (either with different initializations or with slightly different training data). In this paper we develop a multirelational consensus clustering algorithm based on nonnegative decompositions, which are known to produce sparser and more interpretable clusterings than other data-oriented algorithms. We apply this algorithm to the joint analysis of the largest available gene expression datasets for leukemia and respectively normal hematopoiesis in order to develop a more comprehensive genomic characterization of the heterogeneity of leukemia in terms of 38 normal hematopoietic cell states. Surprisingly, we find unusually complex expression programs involving large numbers of transcription factors, whose further in-depth analysis may help develop personalized therapies. © 2012 The Author(s).",,"Artificial intelligence; Cluster analysis; Diseases; Molecular biology; Transcription; Consensus clustering; Gene expression datasets; Genomic characterization; Hematopoietic cell; In-depth analysis; Joint analysis; Multirelational learning; Personalized therapies; Clustering algorithms"
"Badr E., Heath L.S.","CoSREM: A graph mining algorithm for the discovery of combinatorial splicing regulatory elements","10.1186/s12859-015-0698-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940729412&doi=10.1186%2fs12859-015-0698-6&partnerID=40&md5=4223e3ab9d9537ebad8d5ae3c6b016b1","Background: Alternative splicing (AS) is a post-transcriptional regulatory mechanism for gene expression regulation. Splicing decisions are affected by the combinatorial behavior of different splicing factors that bind to multiple binding sites in exons and introns. These binding sites are called splicing regulatory elements (SREs). Here we develop CoSREM (Combinatorial SRE Miner), a graph mining algorithm to discover combinatorial SREs in human exons. Our model does not assume a fixed length of SREs and incorporates experimental evidence as well to increase accuracy. CoSREM is able to identify sets of SREs and is not limited to SRE pairs as are current approaches. Results: We identified 37 SRE sets that include both enhancer and silencer elements. We show that our results intersect with previous results, including some that are experimental. We also show that the SRE set GGGAGG and GAGGAC identified by CoSREM may play a role in exon skipping events in several tumor samples. We applied CoSREM to RNA-Seq data for multiple tissues to identify combinatorial SREs which may be responsible for exon inclusion or exclusion across tissues. Conclusion: The new algorithm can identify different combinations of splicing enhancers and silencers without assuming a predefined size or limiting the algorithm to find only pairs of SREs. Our approach opens new directions to study SREs and the roles that AS may play in diseases and tissue specificity. © 2015 Badr and Heath.","Algorithms; Alternative splicing; Graph mining; Splicing regulatory elements","Algorithms; Binding sites; Data mining; Gene expression; Gene expression regulation; Histology; Tissue; Tissue engineering; Alternative splicing; Experimental evidence; Graph mining; Multiple binding sites; Post-transcriptional; Regulatory elements; Regulatory mechanism; Tissue specificity; Combinatorial mathematics; tumor protein; algorithm; computer graphics; exon; gene expression regulation; genetics; human; intron; neoplasm; regulatory sequence; RNA splicing; Algorithms; Computer Graphics; Exons; Gene Expression Regulation, Neoplastic; Humans; Introns; Neoplasm Proteins; Neoplasms; Regulatory Sequences, Nucleic Acid; RNA Splicing"
"Badr Y.","Data Transparency and Fairness Analysis of the NYPD Stop-and-Frisk Program","10.1145/3460533","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129230173&doi=10.1145%2f3460533&partnerID=40&md5=497e43fd6662eb284ef82f2ae82390f9","Given the increased concern of racial disparities in the stop-and-frisk programs, the New York Police Department (NYPD) requires publicly displaying detailed data for all the stops conducted by police authorities, including the suspected offense and race of the suspects. By adopting a public data transparency policy, it becomes possible to investigate racial biases in stop-and-frisk data and demonstrate the benefit of data transparency to approve or disapprove social beliefs and police practices. Thus, data transparency becomes a crucial need in the era of Artificial Intelligence (AI), where police and justice increasingly use different AI techniques not only to understand police practices but also to predict recidivism, crimes, and terrorism. In this study, we develop a predictive analytics method, including bias metrics and bias mitigation techniques to analyze the NYPD Stop-and-Frisk datasets and discover whether underline bias patterns are responsible for stops and arrests. In addition, we perform a fairness analysis on two protected attributes, namely, the race and the gender, and investigate their impacts on arrest decisions. We also apply bias mitigation techniques. The experimental results show that the NYPD Stop-and-Frisk dataset is not biased toward colored and Hispanic individuals and thus law enforcement authorities can apply the bias predictive analytics method to inculcate more fair decisions before making any arrests. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.","artificial intelligence; Bias; data transparency; machine learning","Crime; Machine learning; Predictive analytics; Analytic method; Artificial intelligence techniques; Bias; Data transparency; Enforcement authorities; Hispanics; Mitigation techniques; New york police departments; Public data; Racial bias; Transparency"
"Badre A., Pan C.","LINA: A Linearizing Neural Network Architecture for Accurate First-Order and Second-Order Interpretations","10.1109/ACCESS.2022.3163257","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127463857&doi=10.1109%2fACCESS.2022.3163257&partnerID=40&md5=03faac7f88496da951600c4a8d33b6c3","While neural networks can provide high predictive performance, it was a challenge to identify the salient features and important feature interactions used for their predictions. This represented a key hurdle for deploying neural networks in many biomedical applications that require interpretability, including predictive genomics. In this paper, linearizing neural network architecture (LINA) was developed here to provide both the first-order and the second-order interpretations on both the instance-wise and the model-wise levels. LINA combines the representational capacity of a deep inner attention neural network with a linearized intermediate representation for model interpretation. In comparison with DeepLIFT, LIME, Grad∗Input and L2X, the first-order interpretation of LINA had better Spearman correlation with the ground-truth importance rankings of features in synthetic datasets. In comparison with NID and GEH, the second-order interpretation results from LINA achieved better precision for identification of the ground-truth feature interactions in synthetic datasets. These algorithms were further benchmarked using predictive genomics as a real-world application. LINA identified larger numbers of important single nucleotide polymorphisms (SNPs) and salient SNP interactions than the other algorithms at given false discovery rates. The results showed accurate and versatile model interpretation using LINA. © 2013 IEEE.","bioinformatics; deep neural networks; Interpretable machine learning; predictive genomics","Bioinformatics; Deep neural networks; Learning algorithms; Lime; Medical applications; Network architecture; Computational modelling; First order; Interpretable machine learning; Machine learning algorithms; Neural network architecture; Neural-networks; Prediction algorithms; Predictive genomic; Predictive models; Second orders; Genome"
"Bae J., Mathiason G., Li Y., Kojola N., Ståhl N.","Understanding Robust Target Prediction in Basic Oxygen Furnace","10.1145/3447432.3447435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104954252&doi=10.1145%2f3447432.3447435&partnerID=40&md5=9b68e3da1895f4a493f0c705011b1545","The problem of using machine learning (ML) to predict the process endpoint for a Basic Oxygen Furnace (BOF) process used for steelmaking has been largely studied. However, current research often lacks both the usage of a rich dataset and does not address revealing influential factors that explain the process. The process is complex and difficult to control and has a multi-objective target endpoint with a proper range of heat temperature combined with sufficiently low levels of carbon and phosphorus. Reaching this endpoint requires skilled process operators, who are manually controlling the heat throughout the process by using both implicit and explicit control variables in their decisions. Trained ML models can reach good BOF target prediction results, but it is still a challenge to extract the influential factors that are significant to the ML prediction accuracy. Thus, it becomes a challenge to explain and validate an ML prediction model that claims to capture the process well. This paper makes use of a complex and full production dataset to evaluate and compare different approaches for understanding how the data can determine the process target prediction. One approach is based on the collected process data and the other on the ML approach trained on that data to find the influential factors. These complementary approaches aim to explain the BOF process to reveal actionable information on how to improve process control. c 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Basic Oxygen Furnace; Explainable AI; Machine learning; Production Data","Basic oxygen converters; Forecasting; Industrial management; Oxygen; Predictive analytics; Steelmaking furnaces; Implicit and explicit controls; Influential factors; Multi objective; Prediction accuracy; Prediction model; Process data; Process operators; Target prediction; Process control"
"Bae S., Lee H., Shin J., Kim H.S., Kim Y., Kim D.H., Lee J.M.","Data-Driven Inference of Synthesis Guidelines for High-Performance Zeolite-Based Selective Catalytic Reduction Catalysts at Low Temperatures","10.1021/acs.chemmater.2c01092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137293260&doi=10.1021%2facs.chemmater.2c01092&partnerID=40&md5=ebd934fe83b879708f8b1df067eae48a","Numerous zeolite-based selective catalytic reduction (SCR) catalysts have been investigated to improve nitrogen oxide (NOx) removal efficiency at low temperatures of 25-200 °C in diesel vehicles. However, the majority of these studies examined only one of each feature's effects. The catalysis mechanism consists of complex reactions, and the various features interact, making it difficult to predict their combinatorial effects on the catalytic activity. Recently, machine learning-based models have been widely employed in catalysis science to infer hidden information about catalysts without knowledge of the underlying physical principles. Interpretable machine learning models are particularly useful for catalyst research because they can explain the causal relationship between characteristics and catalytic performance. In this study, we construct a machine learning model utilizing a decision tree, one of the representative interpretable machine learning models. Using this model, we evaluate the causal relationship between features and the NOx removal efficiency of zeolite-based SCR catalysts at low temperatures, which is difficult to deduce due to the high number of features. Additionally, we extract several synthesis guidelines for catalysts that show superior NOx removal performance at low temperatures. New catalysts were synthesized using the proposed rules, and their performance was validated experimentally. © 2022 American Chemical Society. All rights reserved.",,"Catalyst activity; Decision trees; Efficiency; Machine learning; Nitrogen oxides; Nitrogen removal; Selective catalytic reduction; Causal relationships; Data driven; Diesel vehicles; Lows-temperatures; Machine learning models; Nitrogen oxides removal; Performance; Removal efficiencies; Selective catalytic reduction catalysts; ]+ catalyst; Zeolites"
"Baechle C., Agarwal A., Behara R., Zhu X.","A cost sensitive approach to predicting 30-day hospital readmission in COPD patients","10.1109/BHI.2017.7897269","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018381084&doi=10.1109%2fBHI.2017.7897269&partnerID=40&md5=155ce4ddb11fda01fb6f7e2e738afd93","Chronic Obstructive Pulmonary Disease is a painful chronic disease responsible for many unplanned hospital readmissions. Recent Federal legislation has begun to financially penalize hospitals which have excess patient readmissions. Predictive analytics offers a method to statistically predict which patients are at greatest risk for hospital readmission. Many readmission models currently exist, but few incorporate cost. Our research proposes several methods to directly incorporate cost into patient readmission prediction. Additionally, a method for evaluating the cost of existing models is proposed. Results show traditional evaluation methods such as AUC to have little relation to actual financial penalties (correlation = -0.21) and that dynamic cost evaluation to result in the largest cost savings. © 2017 IEEE.","Data Mining; Decision Support Systems; Health Economics","Artificial intelligence; Data mining; Decision support systems; Forecasting; Health; Hospitals; Predictive analytics; Pulmonary diseases; Chronic disease; Chronic obstructive pulmonary disease; Cost-sensitive; Dynamic cost; Evaluation methods; Federal legislations; Financial penalty; Health economics; Costs"
"Baecke P., Bocca L.","The value of vehicle telematics data in insurance risk selection processes","10.1016/j.dss.2017.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018351180&doi=10.1016%2fj.dss.2017.04.009&partnerID=40&md5=7891e826e2640fc74bfaa158744c3de8","The advent of the Internet of Things enables companies to collect an increasing amount of sensor generated data which creates plenty of new business opportunities. This study investigates how this sensor data can improve the risk selection process in an insurance company. More specifically, several risk assessment models based on three different data mining techniques are augmented with driving behaviour data collected from In-Vehicle Data Recorders. This study proves that including standard telematics variables significantly improves the risk assessment of customers. As a result, insurers will be better able to tailor their products to the customers’ risk profile. Moreover, this research illustrates the importance of including industry knowledge, combined with data expertise, in the variable creation process. Especially when a regulator forces the use of easily interpretable data mining techniques, expert-based telematics variables are able to improve the risk assessment model in addition to the standard telematics variables. Further, the results suggest that if a manager wants to implement Usage-Based-Insurances, Pay-As-You-Drive related variables are most valuable to tailor the premium to the risk. Finally, the study illustrates that this new type of telematics-based insurance product can quickly be implemented since three months of data is already sufficient to obtain the best risk estimations. © 2017 Elsevier B.V.","Artificial neural networks; Internet of things; Logistic regression; Random forests; Risk assessment model; Usage-based-insurance","Data mining; Decision trees; Digital storage; Insurance; Internet of things; Neural networks; Risk perception; Wireless telecommunication systems; Business opportunities; Driving behaviour; Insurance companies; Logistic regressions; Random forests; Related variables; Risk assessment models; Vehicle telematics; Risk assessment"
"Baek J., Lee C., Yu H., Baek S., Lee S., Lee S., Park C.","Automatic Sleep Scoring Using Intrinsic Mode Based on Interpretable Deep Neural Networks","10.1109/ACCESS.2022.3163250","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127497783&doi=10.1109%2fACCESS.2022.3163250&partnerID=40&md5=7fa423a9377cb076b3d33f8be100cc5c","Sleep experts manually label sleep stages via polysomnography (PSG) to diagnose sleep disorders. However, this process is time-consuming, requires a lot of labor from sleep experts, and makes the participants uncomfortable with the attachment of multiple sensors. Thus, automatic sleep scoring methods are essential for practical sleep monitoring in our daily lives. In this study, we propose an automatic sleep scoring model based on intrinsic oscillations in a single channel electroencephalogram (EEG) signal. We applied noise assisted bivariate empirical mode decomposition (NA-BEMD) to extract the intrinsic mode components and an attention mechanism in deep neural networks to provide weights to the components depending on their significance to sleep scoring. In particular, through the attention mechanism, we found an interpretable model by examining the oscillations that correspond to specific sleep stages. Therefore, we analyzed which frequency components are more weighted to a sleep stage than the others, when the model classifies sleep stages, and, as a result, confirmed that the model assigns convincing weights to the frequency components for each sleep stage. Additionally, the model consists of a one-dimensional convolutional neural network (1D-CNN) to extract features of an epoch and bidirectional long short-term memory (Bi-LSTM) to learn the sequential information of the consecutive epochs. We evaluated proposed model using Fpz-Cz, Pz-Oz, and F3-M2 channel EEG from three different public datasets (Sleep-EDF-2013, Sleep-EDF-2018, WSC) and demonstrated that our model yielded the best overall accuracy (Fpz-Cz: 86.22%-82.67%, Pz-Oz: 83.63%-80.15%, F3-M2: 84.20%) and macro F1-score (Fpz-Cz: 80.79%-76.90%, Pz-Oz: 76.89%-72.98%, F3-M2: 74.88%) compared with the state-of-the-art sleep scoring algorithms using single channel EEG. As a benchmark test, FIR bandpass filters were compared, and it was confirmed that NA-BEMD was superior to the traditional filters in all experiments, demonstrating that the proposed model is interpretable and a state-of-the-art sleep scoring algorithm. © 2013 IEEE.","attention mechanism; automatic sleep scoring; bivariate empirical mode decomposition; deep neural networks; Electroencephalogram (EEG)","Bandpass filters; Benchmarking; Biomedical signal processing; Brain; Classification (of information); Convolution; Electroencephalography; Electrophysiology; Feature extraction; Forecasting; Long short-term memory; Sleep research; Attention mechanisms; Automatic sleep scoring; Bivariate empirical mode decompositions; Brain modeling; Classification algorithm; Convolutional neural network; Deep learning; Electroencephalogram; Features extraction; Sleep stage; Deep neural networks"
"Baek J.-W., Chung K.","Captioning model based on meta-learning using prior-convergence knowledge for explainable images","10.1007/s00779-021-01558-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103655303&doi=10.1007%2fs00779-021-01558-9&partnerID=40&md5=ca74d5822862efe7f60fa9610faacaf7","Big data has a variety of data types, including image and text. In particular, image data-based research on face recognition and objection detection has been conducted in diverse areas. Deep learning needs a massive amount of data for learning a model accurately. The amount of data collected is different in each area, and thus it is likely to lack data for analysis through deep learning. Accordingly, it is necessary a method of learning a model effectively and predicting a result accurately with the use of a small amount of data. Also, captions and tags are generated to obtain image information. In the case of tagging, an image is expressed with words, while, in the case of captioning, a sentence can be created in connection with words. For this reason, it is possible to obtain image information in detail through captioning, compared to tagging. However, when a caption is created with words, there is the limitation of end-to-end to lower performance if labeled data are not sufficient. As a solution to the problem, meta-learning, in which a small amount of data can be used, is applied. This study proposes the captioning model based on meta-learning using prior-convergence knowledge for explainable images. The proposed method collects multimodal image data for predicting image information. From the collected data, the attributes representing object information and context information are used. After that, with the use of a small amount of data, meta-learning is applied in a bottom-up approach to creating a sentence for captioning. It can solve the problem caused by data shortage. Lastly, for the extraction of image features, LSTM for convolution network and captioning is established, and the basis for explanation is generated through the reverse operation. The generated basis is an image object. An appropriate explanation sentence is displayed in line with a particular object. Performance evaluation is conducted in two ways for accuracy. Firstly, BLEU score is evaluated according to whether there is meta-learning. Secondly, the proposed captioning model based on prior knowledge, RNN-based captioning model, and bidirectional RNN-based captioning model is evaluated in terms of BLEU score. Therefore, through the proposed method, LSTM’s bottom-up method reduces the cost of improving image resolution and solves the data shortage problem through meta-learning. In addition, it is possible to find the basis of image information using subtitles and to more accurately describe information about photos based on XAI. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Captioning; Explainable image; Image object; Meta-learning; Prior-knowledge","Deep learning; Face recognition; Image enhancement; Image resolution; Information use; Learning systems; Bottom up approach; Bottom up methods; Context information; Image information; Method of learning; Multi-modal image; Object information; Reverse operations; Long short-term memory"
"Baek S., Baek J., Yu H., Lee C., Park C.","Explainable Sleep Staging Algorithm using a Single-channel Electroencephalogram","10.5573/IEIESPC.2021.11.1.8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127484563&doi=10.5573%2fIEIESPC.2021.11.1.8&partnerID=40&md5=b45bee584276ec20ba920e4340739088","The evaluation of sleep stages is the most crucial part in diagnosing and treating patients with sleeping disorders. However, in most healthcare environments, doctors evaluate sleep stages manually by using patients’ polysomnography (PSG) data, which leads to high economic and time costs. PSG data are extremely complicated due to the amount of data and its recording process. In this study, instead of using PSG data single-channel EEG data are used to create an automated model for evaluating the five stages of sleep. The proposed model is an explainable artificial intelligence model for applications in a real-world medical environment. For this purpose, single-channel EEG data are decomposed into each signal component by band-pass filters. For post-hoc analysis, the learning rate for each key component in determining the sleep stages was estimated using the attention mechanism. A cross-evaluation was conducted on data from 80 subjects. The result was an averaged F1-score of 72.66 (±22.24) and an explainable model where EEG components were more effective in estimating each sleep stage. Copyrights © 2022 The Institute of Electronics and Information Engineers.","Attention; Electroencephalogram; Explainable artificial intelligence; Signal decompose; Sleep stage","Artificial intelligence; Bandpass filters; Signal processing; Sleep research; Attention; Economic costs; Explainable artificial intelligence; Healthcare environments; Polysomnography; Recording process; Single channels; Sleep stage; Sleep staging; Time cost; Electroencephalography"
"Baeza-Yates R., Estévez-Almenzar M.","The Relevance of Non-Human Errors in Machine Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134889765&partnerID=40&md5=ae3ef11b5fac27339d8cc4af4e8cbd18","The current practice of focusing the evaluation of a machine learning model on the accuracy of validation has been lately questioned, and has been declared as a systematic habit that is ignoring some important aspects when developing a possible solution to a problem. This lack of diversity in evaluation procedures reinforces the difference between human and machine perception on the relevance of data features, and reinforces the lack of alignment between the fidelity of current benchmarks and human-centered tasks. Hence, we argue that there is an urgent need to start paying more attention to the search for metrics that, given a task, take into account the most humanly relevant aspects. We propose to base this search on the errors made by the machine and the consequent risks involved in moving human logic away from that of the machine. If we work on identifying these errors and organize them hierarchically according to this logic, we can use this information to provide a reliable evaluation of machine learning models, and improve the alignment between training processes and the different considerations humans make when solving a problem and analyzing outcomes. In this context we define the concept of non-human errors, exemplifying it with an image classification task and discussing its implications. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Error Analysis; Evaluation; Machine Learning; Non-Human Errors; Responsible AI","Computer circuits; Errors; Learning algorithms; Current practices; Data feature; Evaluation; Human errors; Human perception; Machine learning models; Machine perception; Machine-learning; Non-human error; Responsible AI; Machine learning"
"Bagga P., Paoletti N., Stathis K.","Deep Learnable Strategy Templates for Multi-Issue Bilateral Negotiation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134306881&partnerID=40&md5=ebfc9fd06d2b5b81cb76661005deff30","We propose the notion of deep reinforcement learning-based strategy templates for multi-issue bilateral negotiation. Each strategy template consists of a set of interpretable parameterized tactics that are used to decide an optimal action at any time. This contrasts with existing work that only estimates the threshold utility for those tactics that require it. As a result, we build automated agents for multi-issue negotiations that can adapt to different negotiation domains without the need to be pre-programmed. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","Bilateral Automated Negotiation; Deep Reinforcement Learning; Interpretable Negotiation Strategies; Multi-Issue Negotiation","Autonomous agents; Deep learning; Multi agent systems; Automated negotiations; Bilateral automated negotiation; Bilateral negotiations; Deep reinforcement learning; Interpretable negotiation strategy; Multi-issue; Multi-issue negotiation; Negotiation strategy; Parameterized; Reinforcement learnings; Reinforcement learning"
"Baghaei K.T., Rahimi S.","Sepsis Prediction: An Attention-Based Interpretable Approach","10.1109/FUZZ-IEEE.2019.8858808","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073785709&doi=10.1109%2fFUZZ-IEEE.2019.8858808&partnerID=40&md5=851f8dd7df88271b81fd50fcb96d4636","Sepsis is the leading cause of death in ICUs and a very costly medical phenomena. The earlier it is predicted, the less inpatient mortality and the less the length of ICU stay, thus a major cut in medical expenses. Although the current deep learning models are able to make predictions about the possibility of sepsis in the ICU, they still lack the ability to reveal the major factors that lead to the outcomes of the predictions. In this paper, we have explored the use of an attention-based model in prediction of sepsis which provides more details on the amount of contribution of each of the medical measurements to the final prediction. This would help health care providers to improve their procedures to reduce sepsis related mortality rate. © 2019 IEEE.",,"Deep learning; Fuzzy systems; Intensive care units; Health care providers; Learning models; Major factors; Medical expense; Medical measurement; Medical phenomena; Mortality rate; Forecasting"
"Baghel N., Verma U., Nagwanshi K.K.","WBCs-Net: type identification of white blood cells using convolutional neural network","10.1007/s11042-021-11449-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114039887&doi=10.1007%2fs11042-021-11449-z&partnerID=40&md5=549dab1ce04770be71e0a46af400b834","On monitoring an individual's health condition, White Blood Cells play a significant role. The opinion on blood-related disease requires the detection and description of the blood of a patient. Blood cell defects are responsible for numerous health conditions. The conventional technique of manually visualizing White Blood Cells under the microscope is a time-consuming, tedious process and its interpretation requires professionals. There are significant medical applications for an automated method for detecting and classifying blood cells and their subtypes. This work presents an automatic classification method with the help of machine learning for blood cell classification from blood sample medical images. The proposed method can identify and classify the function of each segmented White Blood Cells cell image as granular and non-granular White Blood Cells cell type. It further classifies granular into Eosinophil, Neutrophil and non-granular into Lymphocyte, Monocyte in various forms. Because of its high precision, the proposed framework includes a neural network model to detect white blood cell types. To improve the accuracy of multiple cells overlapping and increase the robustness, data augmentation techniques have been used in the proposed system. Which has improved the accuracy in binary and multi-classification of blood cell subtypes. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Blood cell; Convolutional neural network; Deep learning; Image classification; Multi-label classification","Cells; Convolutional neural networks; Cytology; Medical applications; Medical imaging; Automated methods; Automatic classification; Blood cell classifications; Conventional techniques; Data augmentation; Multi-classification; Neural network model; White blood cells; Blood"
"Bagherian-Marandi N., Ravanshadnia M., Akbarzadeh-T M.-R.","Two-layered fuzzy logic-based model for predicting court decisions in construction contract disputes","10.1007/s10506-021-09281-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099812241&doi=10.1007%2fs10506-021-09281-9&partnerID=40&md5=cc077bd471dd89dd1e306462afa24ac3","The dynamic nature and increasing complexity of the construction industry have led to increased conflicts in construction projects. An accurate prediction of the outcome of a dispute resolution in courts could effectively reduce the number of disputes that would otherwise conclude by spending more money through litigation. This study aims to introduce a two-layered fuzzy logic model for predicting court decisions in construction contract disputes. 100 cases of construction contract disputes are selected from the courts of Iran. A questionnaire survey is then conducted to extract a set of fuzzy rules for identifying important decision parameters and expert knowledge. Accordingly, a two-layered fuzzy logic-based decision-making architecture is proposed for the prediction model. Furthermore, the fuzzy system is trained based on 10-fold cross-validation. Analysis of results indicates that 51 out of the 100 cases are filed after the dissolution and termination of the contract show a significant impact of these clauses as the root cause in construction contract disputes. Our results present a proposed hierarchical fuzzy system that can correctly predict nearly 60% of the test data. Also, we demonstrate a methodology of using argument before ML to establish interpretable AI models. Based on our findings, a fuzzy model with a hierarchical structure may be used as a simple and efficient method for predicting court decisions in construction contract disputes. © 2021, The Author(s), under exclusive licence to Springer Nature B.V. part of Springer Nature.","Artificial intelligence; Construction project; Fuzzy expert systems; Judicial decisions; Litigation","Computer circuits; Construction industry; Decision making; Forecasting; Fuzzy inference; Fuzzy systems; Laws and legislation; Predictive analytics; Surveys; 10-fold cross-validation; Accurate prediction; Construction contract disputes; Construction projects; Decision parameters; Hierarchical fuzzy systems; Hierarchical structures; Questionnaire surveys; Fuzzy logic"
"Bagherifard K., Nilashi M., Ibrahim O., Janahmadi N., Ebrahimi L.","Comparative study of artificial neural network and ARIMA models in predicting exchange rate",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866049271&partnerID=40&md5=b5366b811750c57f08c1ee5e26efcf69","Capital market as an organized market has an effective role in mobilizing financial resources due to have growth and economic development of countries and many countries now in the finance firms is responsible for the required credits. In the stock market, shareholders are always seeking the highest efficiency, so the stock price prediction is important for them. Since the stock market is a nonlinear system under conditions of political, economic and psychological, it is difficult to predict the correct stock price. Thus, in the present study artificial intelligence and ARIMA method has been used to predict stock prices. Multilayer Perceptron neural network and radial basis functions are two methods used in this research. Evaluation methods, selection methods and exponential smoothing methods are compared to random walk. The results showed that AI-based methods used in predicting stock performance are more accurate. Between two methods used in artificial intelligence, a method based on radial basis functions is capable to estimate stock prices in the future with higher accuracy. © Maxwell Scientific Organization, 2012.","ARIMA; Artificial neural network; Intelligent systems; Stock prices","ARIMA; ARIMA models; Capital markets; Comparative studies; Economic development; Evaluation Method; Exchange rates; Exponential smoothing method; Financial resources; Multi-layer perceptron neural networks; Radial basis functions; Random Walk; Selection methods; Stock market; Stock performance; Stock price; Stock price prediction; Commerce; Economics; Finance; Intelligent systems; Neural networks; Radial basis function networks; Forecasting"
"Baghersalimi S., Teijeiro T., Atienza D., Aminifar A.","Personalized Real-Time Federated Learning for Epileptic Seizure Detection","10.1109/JBHI.2021.3096127","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123562682&doi=10.1109%2fJBHI.2021.3096127&partnerID=40&md5=8a37318b2466e81d809ffa7913285084","Epilepsy is one of the most prevalent paroxystic neurological disorders. It is characterized by the occurrence of spontaneous seizures. About 1 out of 3 patients have drug-resistant epilepsy, thus their seizures cannot be controlled by medication. Automatic detection of epileptic seizures can substantially improve the patient's quality of life. To achieve a high-quality model, we have to collect data from various patients in a central server. However, sending the patient's raw data to this central server puts patient privacy at risk and consumes a significant amount of energy. To address these challenges, in this work, we have designed and evaluated a standard federated learning framework in the context of epileptic seizure detection using a deep learning-based approach, which operates across a cluster of machines. We evaluated the accuracy and performance of our proposed approach on the NVIDIA Jetson Nano Developer Kit based on the EPILEPSIAE database, which is one of the largest public epilepsy datasets for seizure detection. Our proposed framework achieved a sensitivity of 81.25%, a specificity of 82.00%, and a geometric mean of 81.62%. It can be implemented on embedded platforms that complete the entire training process in 1.86 hours using 344.34 mAh energy on a single battery charge. We also studied a personalized variant of the federated learning, where each machine is responsible for training a deep neural network (DNN) to learn the discriminative electrocardiography (ECG) features of the epileptic seizures of the specific person monitored based on its local data. In this context, the DNN benefitted from a well-trained model without sharing the patient's raw data with a server or a central cloud repository. We observe in our results that personalized federated learning provides an increase in all the performance metric, with a sensitivity of 90.24%, a specificity of 91.58%, and a geometric mean of 90.90%. © 2013 IEEE.","Deep learning; electrocardiogram (ECG); epilepsy; federated learning (FL); low-power; privacy-preserving; seizure detection","Deep neural networks; Neurodegenerative diseases; Neurophysiology; Privacy-preserving techniques; Central servers; Deep learning; Electrocardiogram; Energy; Epileptic seizure detection; Federated learning; Geometric mean; Low Power; Privacy preserving; Seizure-detection; Electrocardiography; Article; body movement; clinical article; deep learning; deep neural network; electrocardiography; energy consumption; epilepsy; heart rate; human; image segmentation; learning algorithm; machine learning; predictive value; receiver operating characteristic; risk assessment; seizure; sensitivity and specificity; signal processing; training; validation process; algorithm; electroencephalography; epilepsy; quality of life; seizure; Algorithms; Electroencephalography; Epilepsy; Humans; Neural Networks, Computer; Quality of Life; Seizures"
"Bagwari N., Kumar O.","Indexing optimizations on Hadoop","10.1109/CIACT.2017.7977360","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027500412&doi=10.1109%2fCIACT.2017.7977360&partnerID=40&md5=7362a0849a7db767cb6084a501aba1b1","Hadoop is an efficient open source framework to store and process the big data. Its component HDFS stores data in distributed manner preserving its consistency and availability while MapReduce is responsible for parallel processing. Hadoop fits best for fault tolerant storage and batch processing but searching is not optimized in Hadoop as it stores data in the form of blocks. It lacks in optimized index design leading to costly searching mechanism. To deal with this various indexing approaches have been proposed as an improvement in Hadoop architecture. In most of the approaches, MapReduce typically generates index at run time to process the data distributed across the cluster. This paper compares the existing indexing approaches and proposes a new index creation and storage technique for Hadoop eco system which will lead to better search results in Hadoop environment. © 2017 IEEE.","Hadoop eco system; Indexing; searching","Artificial intelligence; Batch data processing; Digital storage; Indexing (of information); Data distributed; Index creation; Indexing approaches; Open source frameworks; Parallel processing; searching; Searching mechanism; Storage technique; Big data"
"Bah A., Touré I., Le Page C., Ickowicz A., Diop A.T.","An agent-based model to understand the multiple uses of land and resources around drillings in Sahel","10.1016/j.mcm.2005.02.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745658420&doi=10.1016%2fj.mcm.2005.02.014&partnerID=40&md5=e7eca72c496450111b36d91aa59f98f7","In Sahel, land and resources are used, according to the seasons, by various actors with interests sometimes difficult to conciliate. This spatial competition for the access to the natural resources induces a set of processes leading to overgrazing and conflicts for the access to water. In such a complex agro-sylvo-pastoral context, acquiring and formalizing knowledge about the practices and rules which govern the system are essential to help and accompany the rural populations towards the sustainable management of their environment. Among the new modelling tools coming from the field of Artificial Intelligence, Agent-Based Models (ABMs) are now widely used to tackle the issue of integrated natural resource management. The increase in popularity of ABMs is partially due to their intelligibility, not only by scientists from various research fields, but also by stakeholders. It then becomes easier to build an understandable representation of the system as an ""artificial world"" and to perform simulations in order to collectively test and discuss various scenarios about the resource management. This paper describes an ABM which has been designed to formalize the interactions between the biophysics dynamics of the natural resources and the socio-economic factors driving the land-use dynamics around the drilling of Thieul village in the sylvo-pastoral area of Ferlo (Senegal). The first results show that rainfall still plays an extremely significant role in the pastoral system despite the drilling of boreholes. Paradoxically, steady rainfall over long periods of time can lead to negative effects on the relations between herders and farmers. © 2005 Elsevier Ltd. All rights reserved.","Agent-based modelling; Grazing; Integrated natural resource management; Sahel; Senegal","Artificial intelligence; Boreholes; Computer simulation; Drilling; Economic and social effects; Environmental impact; Resource allocation; Software engineering; Spatial variables control; Agent-Based Models (ABMs); Grazing; Integrated natural resource management; Sahel; Computer science"
"Bah M.D., Dericquebourg E., Hafiane A., Canals R.","Deep learning based classification system for identifying weeds using high-resolution UAV imagery","10.1007/978-3-030-01177-2_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057109309&doi=10.1007%2f978-3-030-01177-2_13&partnerID=40&md5=8264ac427cd6c698d552135a1cbc7312","In recent years, weeds is responsible for most of the agricultural yield losses. To deal with this problem Omega, farmers resort to spraying pesticides throughout the field. Such method not only requires huge quantities of herbicides but impact environment and humans health. In this paper, we propose a new vision-based classification system for identifying weeds in vegetable fields such as spinach, beet and bean by applying convolutional neural networks (CNNs) and crop lines information. In this study, we combine deep learning with line detection to enforce the classification procedure. The proposed method is applied to high-resolution Unmanned Aerial Vehicles (UAV) images of vegetables taken about 20 m above the soil. We have performed an extensive evaluation of the method with real data. The results showed that the proposed method of weeds detection was effective in different crop fields. The overall precision for the beet, spinach and bean is respectively of 93%, 81% and 69%. © Springer Nature Switzerland AG 2019.","Convolutional neural networks; Deep learning; Precision agriculture; Unmanned aerial vehicles; Weeds detection","Antennas; Classification (of information); Convolution; Crops; Image classification; Intelligent computing; Neural networks; Precision agriculture; Unmanned aerial vehicles (UAV); Vegetables; Agricultural yields; Classification procedure; Classification system; Convolutional neural network; High resolution; Line detection; Vegetable Field; Weeds detection; Deep learning"
"Bahani K., Moujabbir M., Ramdani M.","An accurate fuzzy rule-based classification systems for heart disease diagnosis","10.1016/j.sciaf.2021.e01019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122793654&doi=10.1016%2fj.sciaf.2021.e01019&partnerID=40&md5=f7a3b5c2acfbb8712ec3b27c39f83488","Physicians and healthcare providers need to better understand the thought processes and methods used in clinical decision-making. This allows physicians to diagnose and detect diseases early, especially heart disease that causes death. The diversity and availability of healthcare data encourage clinicians to use healthcare applications in the diagnosis process. Most of these applications use machine learning techniques to make accurate and fast decisions. On the other hand, Explainability in healthcare applications increase the level of clinician confidence and reduces the risk of making wrong decisions, thus expands the scope and efficiency of healthcare applications. In this paper, we propose a novel data-driven method based on fuzzy clustering and linguistic modifiers to design a fuzzy rule-based classification system for heart disease diagnosis. The proposed system provides an interpretable knowledge base to explain the decision-making process. Regarding the experiment, we have used Cleveland, Hungarian and Va long beach heart disease datasets to compare the proposed method with five known machine learning methods for predicting heart disease: Artificial neural network, Support Vector Machine, K-Nearest Neighbor, Naïve Bayes, and Random Forest. The findings show that the proposed model is superior in terms of balancing interpretability and precision. © 2021 The Author(s)","Classification rules learning; Explainable artificial intelligence; Fuzzy rule-based classification systems; Heart disease prediction; Machine learning",
"Bahi M., Batouche M.","Drug-Target Interaction Prediction in Drug Repositioning Based on Deep Semi-Supervised Learning","10.1007/978-3-319-89743-1_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046538793&doi=10.1007%2f978-3-319-89743-1_27&partnerID=40&md5=735ff4164d93d3f2545b92236377bcc3","Drug repositioning or repurposing refers to identifying new indications for existing drugs and clinical candidates. Predicting new drug-target interactions (DTIs) is of great challenge in drug repositioning. This tricky task depends on two aspects. The volume of data available on drugs and proteins is growing in an exponential manner. The known interacting drug-target pairs are very scarce. Besides, it is hard to select the negative samples because there are not experimentally verified negative drug-target interactions. Many computational methods have been proposed to address these problems. However, they suffer from the high rate of false positive predictions leading to biologically interpretable errors. To cope with these limitations, we propose in this paper an efficient computational method based on deep semi-supervised learning (DeepSS-DTIs) which is a combination of a stacked autoencoders and a supervised deep neural network. The objective of this approach is to predict potential drug targets and new drug indications by using a large scale chemogenomics data while improving the performance of DTIs prediction. Experimental results have shown that our approach outperforms state-of-the-art techniques. Indeed, the proposed method has been compared to five machine learning algorithms applied all on the same reference datasets of DrugBank. The overall accuracy performance is more than 98%. In addition, the DeepSS-DTIs has been able to predict new DTIs between approved drugs and targets. The highly ranked candidate DTIs obtained from DeepSS-DTIs are also verified in the DrugBank database and in literature. © IFIP International Federation for Information Processing 2018.","Deep learning; Deep neural network; Drug repositioning; Drug-target interactions; Semi-supervised learning; Stacked autoencoders","Artificial intelligence; Computational methods; Deep learning; Deep neural networks; Forecasting; Learning algorithms; Supervised learning; Autoencoders; Drug repositioning; Drug-target interactions; Negative samples; Overall accuracies; Potential drug targets; Semi- supervised learning; State-of-the-art techniques; Drug interactions"
"Bahrami S., Torgheh F.","Genetic algorithms for finding optimal locations of mobile agents in scalable active networks","10.1109/AIMSEC.2011.6011020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053229684&doi=10.1109%2fAIMSEC.2011.6011020&partnerID=40&md5=c8a555b2f9806ecfa0b484608a33e445","The idea of active networks has been emerged in recent years to increase the processing power inside the network. The intermediate nodes such as routers will be able to host mobile agents and many management tasks can be handled using autonomous mobile agents inside the network. One of the important limitations, which should be considered in active networks, is the restricted processing power of active nodes. In this paper, we define an optimal location problem for monitoring mobile agents in a scalable active network as a p-median problem, which is indeed a kind of facility location problem. The agents are responsible to monitor and manage the performance of all of the network nodes such that the total monitoring traffic overhead is minimized. Then we proposed two methods of finding an appropriate sub set of intermediate nodes for hosting mobile agents. In our first method, we have not considered the limited processing power of active nodes, which host mobile agents. In our second method, we have solved the problem so that the processing loads of host nodes do not exceed a predefined threshold. Since p-median problems are NP-complete and the search space of these problems is very large, our methods are based on genetic algorithms. We have tested our two methods for finding mobile agents optimal locations on four network topologies with different number of nodes and compared the obtained location. By this comparison, we have shown the importance of considering processing load limitation for active nodes as a parameter in choosing them as hosts of mobile agents in a scalable active network. The proposed locations in our second method eliminates the probability of CPU overload in the active nodes hosting the mobile agents and reduces the processing time required for finding the optimal locations of mobile agents. © 2011 IEEE.","Active networks; Genetic algorithm; Mobile agents; P-median problem; Performance monitoring","Active node; Autonomous mobile agent; Facility location problem; Intermediate node; Management tasks; Network node; Network topology; NP Complete; Optimal locations; P-median problems; Performance monitoring; Processing load; Processing power; Processing Time; Scalable active network; Search spaces; Traffic overhead; Active networks; Artificial intelligence; Autonomous agents; Electric network topology; Electronic commerce; Genetic algorithms; Intelligent agents; Location; Management science; Optimization; Mobile agents"
"Bahsi H., Nomm S., La Torre F.B.","Dimensionality Reduction for Machine Learning Based IoT Botnet Detection","10.1109/ICARCV.2018.8581205","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060777585&doi=10.1109%2fICARCV.2018.8581205&partnerID=40&md5=98a8562044cd2f8c682fc36823293ba4","The rapid development of the internet of things caused severe security problems such as the cyber attacks launched by extremely huge botnets comprised of IoT devices. The detection of these devices is essential for protecting the networks. Recently, some of the studies have demonstrated the high accuracy of machine learning methods, including deep learning, in detecting IoT botnets. However, the minimizing of the required features for classification is highly needed for overcoming scalability and computation resource problems in IoT environments. Having results which can be readily interpretable by cyber security analysts and producing signatures for the contemporary intrusion detection or network monitoring systems are other significant factors in this area in which quick and widespread security adaption is highly required. In this study, we applied feature selection to minimize the number of features in detecting the IoT bots. It is shown that fewer features can achieve very high accuracy rates and afford interpretable results with a multi-class classifier based on a shallow method, decision tree. © 2018 IEEE.",,"Botnet; Classification (of information); Computer vision; Decision trees; Deep learning; Intrusion detection; Machine learning; Network security; Robotics; Botnet detections; Computation resources; Cyber security; Cyber-attacks; Dimensionality reduction; Machine learning methods; Multi-class classifier; Security problems; Internet of things"
"Bai B., Liang J., Zhang G., Li H., Bai K., Wang F.","Why Attentions May Not Be Interpretable?","10.1145/3447548.3467307","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114914683&doi=10.1145%2f3447548.3467307&partnerID=40&md5=84dc874f3372e2eec55d098b082fa515","Attention-based methods have played important roles in model interpretations, where the calculated attention weights are expected to highlight the critical parts of inputs (e.g., keywords in sentences). However, recent research found that attention-as-importance interpretations often do not work as we expected. For example, learned attention weights sometimes highlight less meaningful tokens like ""[SEP]"", "","", and ""."", and are frequently uncorrelated with other feature importance indicators like gradient-based measures. A recent debate over whether attention is an explanation or not has drawn considerable interest. In this paper, we demonstrate that one root cause of this phenomenon is the combinatorial shortcuts, which means that, in addition to the highlighted parts, the attention weights themselves may carry extra information that could be utilized by downstream models after attention layers. As a result, the attention weights are no longer pure importance indicators. We theoretically analyze combinatorial shortcuts, design one intuitive experiment to show their existence, and propose two methods to mitigate this issue. We conduct empirical studies on attention-based interpretation models. The results show that the proposed methods can effectively improve the interpretability of attention mechanisms. © 2021 ACM.","attention mechanism; casual effect estimation; model interpretation","Information management; Attention mechanisms; Empirical studies; Gradient based; Interpretability; Interpretation model; Model interpretations; Recent researches; Root cause; Data mining"
"Bai B.-L., Wu Z.-Y., Weng S.-J., Yang Q.","Application of interpretable machine learning algorithms to predict distant metastasis in osteosarcoma","10.1002/cam4.5225","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137505356&doi=10.1002%2fcam4.5225&partnerID=40&md5=8f37cdf0b382d01690e50b87159fe7cd","Background: Osteosarcoma is well-established as the most common bone cancer in children and adolescents. Patients with localized disease have different prognoses and management than those with metastasis at the time of diagnosis. The purpose of this study was to explore potential risk factors for metastatic disease. Methods: The Surveillance, Epidemiology, and End Results (SEER) Program database was used to identify patients diagnosed with osteosarcoma between 2004 and 2015. We developed prediction models for distant metastasis using six machine learning (ML) techniques, including logistic regression (LR), support vector machine (SVM), Gaussian Naive Bayes (GaussianNB), Extreme Gradient Boosting (XGBoost), random forest (RF), and k-nearest neighbor algorithm (kNN). The adaptive synthetic (ADASYN) technique was used to deal with imbalanced data. The Shapley Additive Explanation (SHAP) analysis generated visualized explanations for each patient. Finally, the average precision (AP), sensitivity, specificity, accuracy, F1 score, precision-recall curves, calibration plots, and decision curve analysis (DCA) were conducted to evaluate the models' effectiveness. Results: The six machine learning algorithms achieved AP of 0.661–0.781 for predicting distant metastasis. The RF model yielded the best performance with an accuracy of 71.8 percent and an AP of 0.781 and was highly dependent on tumor size, primary surgery, and age. SHAP analysis provided model-independent interpretation, highlighting significant clinical factors associated with the risk of metastasis in osteosarcoma patients. Conclusions: An accurate machine learning-based prediction model was established for metastasis in osteosarcoma patients to help clinicians during clinical decision-making. © 2022 The Authors. Cancer Medicine published by John Wiley & Sons Ltd.","adaptive synthetic technique; distant metastasis; machine learning; osteosarcoma; SEER; Shapley additive explanation",
"Bai H., Ouyang D., He L.","GPU-based frequent pattern mining algorithm",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049120191&partnerID=40&md5=afe3f53ae4a0b8a11d3636267cc9a5b8","Frequent pattern mining is an important issue in data mining area. Traditionally, parallel frequent pattern mining is carried out in PC clusters, and seldom related to multi-processors or massive cores with shared memories. In this paper, we propose a parallel frequent pattern mining algorithm suitable for GPU (graphics processing unit) based on width search and direct support strategy. It is implemented under compute unified device architecture (CUDA) of GPU. In this algorithm, CPU takes charge of search process and GPU is responsible for counting using data partition. In addition, transactions are dynamically pruned according to the length (k) of candidate frequent itemsets. Performance analysis shows that GPU-based FPMA reaches an average speed as fast as that of 10 times of CPU-based counterpart.","Association rule; CUDA; Frequent pattern; GPU; Parallel computing","Average speed; Compute unified device architectures; Data partition; Frequent Itemsets; Frequent pattern; Frequent pattern mining; Frequent patterns; Graphics Processing Unit; Multi-processors; Parallel Computing; PC clusters; Performance analysis; Search process; Shared memories; Association rules; Associative processing; Computer science; Data mining; Mining; Parallel algorithms; Parallel architectures; Program processors; Computer graphics equipment"
"Bai L., Li H., Gao W., Xie J.","A cooperative genetic algorithm based on extreme learning machine for data classification","10.1007/s00500-022-07202-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131434849&doi=10.1007%2fs00500-022-07202-9&partnerID=40&md5=8d631ed556df227f521bf513db62504a","It is a challenging task to optimize network structure and connection parameters simultaneously in a single hidden layer feedforward neural network (SLFN). Extreme learning machine (ELM) is a popular non-iterative learning method in recent years, which often provides good generalization performance of a SLFN at extremely fast learning speed, yet only for fixed network structure. In this work, a cooperative binary-real genetic algorithm (CGA) based on ELM, called CGA-ELM, is proposed to adjust the structure and parameters of a SLFN simultaneously for achieving a compact network with good generalization performance. In CGA-ELM, a hybrid coding scheme is designed to evolve the network structure and input parameters, i.e., input weights between input nodes and hidden nodes as well as the biases of hidden nodes. Then output parameters, i.e., output weights between hidden nodes and output nodes, are determined by the ELM. A combination of training error and network complexity is taken as the fitness function to evaluate the performance of a SLFN. A binary GA is responsible for optimizing network structure, while a real GA and the ELM optimize collaboratively network parameters. Experimental results on classification applications demonstrate that CGA-ELM outperforms CGA and ELM significantly in terms of the generalization ability. Also, CGA-ELM has more competitive capacity when compared with other state-of-the-art algorithms. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Cooperative genetic algorithm; Extreme learning machine; Feedforward neural network; Parameter optimization; Structure learning","Iterative methods; Knowledge acquisition; Learning algorithms; Machine learning; Multilayer neural networks; Network layers; Parameter estimation; Structural optimization; Cooperative genetic algorithm; Data classification; Generalization performance; Hidden nodes; Network connection; Network structures; Non-iterative; Parameter optimization; Single-hidden layer feedforward neural networks; Structure-learning; Genetic algorithms"
"Bai L.","Analysis on Various Approaches to Visualize and Interpret Convolution Neural Network","10.1109/ICFTIC54370.2021.9647332","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124341847&doi=10.1109%2fICFTIC54370.2021.9647332&partnerID=40&md5=716cc64691e308d12bf1ca83f80e5664","The paper aims to explore and evaluate the efficacy of methods that visualizes the internal structure of convolution neural networks as well as neural networks that are inherently interpretable. The paper implements three data set, including MNIST digits, MNIST Fashion, and CUB 200-2011. Tow base structures implemented to construct the convolution neural network are VGG 19 and a two-convolution layer CNN to provide intuitions upon analyzation. The paper toke the two approaches in analyzing the various methods: post hoc interpretation, which includes visualization of convolution kernels, and interpretable models, which includes a novel convolution neural network-based model. As for the outcomes, while the post hoc interpretations only offer superficial and unfaithful explanations to the CNN behavior, the use of the ProtoPNet in real-time interpretation provides an in-depth analysis of the model's reasoning process in decision making in contrast to the inexplicability of mainstream deep learning models. © 2021 IEEE.","Convolution Neural Network; Interpretable Neural Network; Neural Network Visualization","Decision making; Deep learning; Multilayer neural networks; Visualization; Base structure; Convolution kernel; Convolution neural network; Data set; Internal structure; Interpretable neural network; Network visualization; Neural network visualization; Neural-networks; Convolution"
"Bai Q., Liu S., Tian Y., Xu T., Banegas-Luna A.J., Pérez-Sánchez H., Huang J., Liu H., Yao X.","Application advances of deep learning methods for de novo drug design and molecular dynamics simulation","10.1002/wcms.1581","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117033904&doi=10.1002%2fwcms.1581&partnerID=40&md5=37b501a1722b6968ae0c8368c1af61cb","De novo drug design is a stationary way to build novel ligands in the confined pocket of receptor by assembling the atoms or fragments, while molecular dynamics (MD) simulation is a dynamical way to study the interaction mechanism between the ligands and receptors based on the molecular force field. De novo drug design and MD simulation are effective tools for novel drug discovery. With the development of technology, deep learning methods, and interpretable machine learning (IML) have emerged in the research area of drug design. Deep learning methods and IML can be used further to improve the efficiency and accuracy of de novo drug design and MD simulations. The application summary of deep learning methods for de novo drug design, MD simulations, and IML can further promote the technical development of drug discovery. In this article, two major workflow methods and the related components of classical algorithm and deep learning are described for de novo drug design from a new perspective. The application progress of deep learning is also summarized for MD simulations. Furthermore, IML is introduced for the deep learning model interpretability of de novo drug design and MD simulations. Our paper deals with an interesting topic about deep learning applications of de novo drug design and MD simulations for the scientific community. This article is categorized under: Data Science > Chemoinformatics Data Science > Artificial Intelligence/Machine Learning. © 2021 The Authors. WIREs Computational Molecular Science published by Wiley Periodicals LLC.","de novo drug design; deep learning; explainable artificial intelligence; interpretable machine learning; MD simulation","Deep learning; Ligands; De novo drug design; Deep learning; Drug Design; Drug discovery; Explainable artificial intelligence; Interaction mechanisms; Interpretable machine learning; Learning methods; Molecular force field; Novel ligands; Molecular dynamics"
"Bai T., Zhu X., Zhou X., Grathwohl D., Yang P., Zha Y., Jin Y., Chong H., Yu Q., Isberner N., Wang D., Zhang L., Kortüm K.M., Song J., Rasche L., Einsele H., Ning K., Hou X.","Reliable and Interpretable Mortality Prediction With Strong Foresight in COVID-19 Patients: An International Study From China and Germany","10.3389/frai.2021.672050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117881646&doi=10.3389%2ffrai.2021.672050&partnerID=40&md5=ddedce82596a34b6155fd7bdc2efaa1a","Cohort-independent robust mortality prediction model in patients with COVID-19 infection is not yet established. To build up a reliable, interpretable mortality prediction model with strong foresight, we have performed an international, bi-institutional study from China (Wuhan cohort, collected from January to March) and Germany (Würzburg cohort, collected from March to September). A Random Forest-based machine learning approach was applied to 1,352 patients from the Wuhan cohort, generating a mortality prediction model based on their clinical features. The results showed that five clinical features at admission, including lymphocyte (%), neutrophil count, C-reactive protein, lactate dehydrogenase, and α-hydroxybutyrate dehydrogenase, could be used for mortality prediction of COVID-19 patients with more than 91% accuracy and 99% AUC. Additionally, the time-series analysis revealed that the predictive model based on these clinical features is very robust over time when patients are in the hospital, indicating the strong association of these five clinical features with the progression of treatment as well. Moreover, for different preexisting diseases, this model also demonstrated high predictive power. Finally, the mortality prediction model has been applied to the independent Würzburg cohort, resulting in high prediction accuracy (with above 90% accuracy and 85% AUC) as well, indicating the robustness of the model in different cohorts. In summary, this study has established the mortality prediction model that allowed early classification of COVID-19 patients, not only at admission but also along the treatment timeline, not only cohort-independent but also highly interpretable. This model represents a valuable tool for triaging and optimizing the resources in COVID-19 patients. © Copyright © 2021 Bai, Zhu, Zhou, Grathwohl, Yang, Zha, Jin, Chong, Yu, Isberner, Wang, Zhang, Kortüm, Song, Rasche, Einsele, Ning and Hou.","COVID-19; foresight; interpretability; mortality prediction model; reliability; Wuhan cohort; Würzburg cohort",
"Bai T., Egleston B.L., Zhang S., Vucetic S.","Interpretable representation learning for healthcare via capturing disease progression through time","10.1145/3219819.3219904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051526108&doi=10.1145%2f3219819.3219904&partnerID=40&md5=b5466675c6432cb1b6891aeb958980cd","Various deep learning models have recently been applied to predictive modeling of Electronic Health Records (EHR). In medical claims data, which is a particular type of EHR data, each patient is represented as a sequence of temporally ordered irregularly sampled visits to health providers, where each visit is recorded as an unordered set of medical codes specifying patient's diagnosis and treatment provided during the visit. Based on the observation that different patient conditions have different temporal progression patterns, in this paper we propose a novel interpretable deep learning model, called Timeline. The main novelty of Timeline is that it has a mechanism that learns time decay factors for every medical code. This allows the Timeline to learn that chronic conditions have a longer lasting impact on future visits than acute conditions. Timeline also has an attention mechanism that improves vector embeddings of visits. By analyzing the attention weights and disease progression functions of Timeline, it is possible to interpret the predictions and understand how risks of future visits change over time. We evaluated Timeline on two large-scale real world data sets. The specific task was to predict what is the primary diagnosis category for the next hospital visit given previous visits. Our results show that Timeline has higher accuracy than the state of the art deep learning models based on RNN. In addition, we demonstrate that time decay factors and attentions learned by Timeline are in accord with the medical knowledge and that Timeline can provide a useful insight into its predictions. © 2018 Association for Computing Machinery.","Attention model; Deep learning; Electronic Health Records; Healthcare","Data mining; Diagnosis; Forecasting; Health care; Patient treatment; Records management; Attention mechanisms; Attention model; Chronic conditions; Disease progression; Electronic health record; Interpretable representation; Medical knowledge; Predictive modeling; Deep learning"
"Bai X., Wang X., Liu X., Liu Q., Song J., Sebe N., Kim B.","Explainable deep learning for efficient and robust pattern recognition: A survey of recent developments","10.1016/j.patcog.2021.108102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109044442&doi=10.1016%2fj.patcog.2021.108102&partnerID=40&md5=5d472a1696d3231196fde525e8acbbc1","Deep learning has recently achieved great success in many visual recognition tasks. However, the deep neural networks (DNNs) are often perceived as black-boxes, making their decision less understandable to humans and prohibiting their usage in safety-critical applications. This guest editorial introduces the thirty papers accepted for the Special Issue on Explainable Deep Learning for Efficient and Robust Pattern Recognition. They are grouped into three main categories: explainable deep learning methods, efficient deep learning via model compression and acceleration, as well as robustness and stability in deep learning. For each of the three topics, a survey of the representative works and latest developments is presented, followed by the brief introduction of the accepted papers belonging to this topic. The special issue should be of high relevance to the reader interested in explainable deep learning methods for efficient and robust pattern recognition applications and it helps promoting the future research directions in this field. © 2021 Elsevier Ltd","Adversarial robustness; Explainable deep learning; Network compression and acceleration; Stability in deep learning","Deep neural networks; Pattern recognition; Safety engineering; Adversarial robustness; Black boxes; Box making; Explainable deep learning; Learning methods; Network compression and acceleration; Neural-networks; Robust patterns; Stability in deep learning; Visual recognition; Surveys"
"Bai X., Yang M., Liu Z.","On the robustness of skeleton detection against adversarial attacks","10.1016/j.neunet.2020.09.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092035349&doi=10.1016%2fj.neunet.2020.09.018&partnerID=40&md5=c49504ea379162a4dba6f86cfe13267c","Human perception of an object's skeletal structure is particularly robust to diverse perturbations of shape. This skeleton representation possesses substantial advantages for parts-based and invariant shape encoding, which is essential for object recognition. Multiple deep learning-based skeleton detection models have been proposed, while their robustness to adversarial attacks remains unclear. (1) This paper is the first work to study the robustness of deep learning-based skeleton detection against adversarial attacks, which are only slightly unlike the original data but still imperceptible to humans. We systematically analyze the robustness of skeleton detection models through exhaustive adversarial attacking experiments. (2) We propose a novel Frequency attack, which can directly exploit the regular and interpretable perturbations to sharply disrupt skeleton detection models. Frequency attack consists of an excitatory-inhibition waveform with high frequency attribution, which confuses edge-sensitive convolutional filters due to the sudden contrast between crests and troughs. Our comprehensive results verify that skeleton detection models are also vulnerable to adversarial attacks. The meaningful findings will inspire researchers to explore more potential robust models by involving explicit skeleton features. © 2020 Elsevier Ltd","Adversarial attacks; Convolutional neural network; Robustness; Skeleton detection","Deep learning; Object recognition; Detection models; High frequency HF; Human perception; Robust models; Shape encoding; Skeletal structures; Wave forms; Musculoskeletal system; adversarial attack; Article; black box attack; convolutional neural network; deep learning; deep neural network; excitatory junction potential; experimental study; frequency; gray box attack; human; machine learning; priority journal; skeleton detection; white box attack; automated pattern recognition; biometry; pattern recognition; procedures; skeleton; Biometric Identification; Deep Learning; Humans; Pattern Recognition, Automated; Pattern Recognition, Visual; Skeleton"
"Bai Y., Guo X., Tian B., Liang Y., Peng D., Wang Z.","Self-Charging Persistent Mechanoluminescence with Mechanics Storage and Visualization Activities","10.1002/advs.202203249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135911354&doi=10.1002%2fadvs.202203249&partnerID=40&md5=7f557a6bf78def05f8c9af25a1fe6090","Persistent mechanoluminescence (ML) with long lifetime is highly required to break the limits of the transient emitting behavior under mechanics stimuli. However, the existing materials with persistent ML are completely trap-controlled, and a pre-irradiation is required, which severely hinders the practical applications. In this work, a novel type of ML, self-charging persistent ML, is created by compositing the Sr3Al2O5Cl2:Dy3+ (SAOCD) powders into flexible polydimethylsiloxane (PDMS) matrix. With no need for any pre-irradiation, the as-fabricated SAOCD/PDMS elastomer could exhibit intense and persistent ML under mechanics stimuli directly, which greatly facilitates its applications in mechanics lighting, displaying, imaging, and visualization. By investigating the matrix effects as well as the thermoluminescence, cathodoluminescence, and triboelectricity properties, the interfacial triboelectrification-induced electron bombardment processes are demonstrated to be responsible for the self-charged energy in SAOCD under mechanics stimuli. Based on the unique self-charging processes, the SAOCD/PDMS further exhibits mechanics storage and visualized reading activities, which brings novel ideas and approaches to deal with the mechanics-related problems in the fields of mechanical engineering, bioengineering, and artificial intelligence. © 2022 The Authors. Advanced Science published by Wiley-VCH GmbH.","mechanics storage; mechanics visualization; persistent mechanoluminescence; self-charging materials","Irradiation; Mechanics; Polydimethylsiloxane; Storage (materials); Triboluminescence; Visualization; Charging (materials); Compositing; Long lifetime; matrix; Mechanic storage; Mechanic visualization; Mechanics stimulus; Persistent mechanoluminescence; Preirradiation; Self-charging material; Silicones"
"Bai Y., Hao J., Fu H., Hu Y., Ge X., Liu J., Zhao Y., Zhang J.","Unsupervised Lesion-Aware Transfer Learning for Diabetic Retinopathy Grading in Ultra-Wide-Field Fundus Photography","10.1007/978-3-031-16434-7_54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139043607&doi=10.1007%2f978-3-031-16434-7_54&partnerID=40&md5=6957889598d8af4324eacfef4b9f62cc","Ultra-wide-field (UWF) fundus photography is a new imaging technique with providing a broader field of view images, and it has become a popular and effective tool for the screening and diagnosis for many eye diseases, such as diabetic retinopathy (DR). However, it is practically challenging to train a robust deep learning model for DR grading in UWF images, due to the limited scale of data and manual annotations. By contrast, we may find large-scale high-quality regular color fundus photography datasets in the research community, with either image-level or pixel-level annotation. In consequence, we propose an Unsupervised Lesion-aware TRAnsfer learning framework (ULTRA) for DR grading in UWF images, by leveraging a large amount of publicly well-annotated regular color fundus images. Inspired by the clinical identification of DR severity, i.e., the decision making process of ophthalmologists based on the type and number of associated lesions, we design an adversarial lesion map generator to provide the auxiliary lesion information for DR grading. A Lesion External Attention Module (LEAM) is introduced to integrate the lesion feature into the model, allowing a relative explainable DR grading. Extensive experimental results show the proposed method is superior to the state-of-the-art methods. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Diabetic retinopathy; Unsupervised; UWF imaging","Decision making; Deep learning; Eye protection; Grading; Large dataset; Medical imaging; Transfer learning; Diabetic retinopathy; Diabetic retinopathy grading; Field images; Fundus photography; Transfer learning; Ultra-wide; Ultra-wide-field imaging; Unsupervised; Wide field imaging; Wide-field; Diagnosis"
"Bai Y., Chen W., Ai B., Zhong Z., Wassell I.J.","Prior Information Aided Deep Learning Method for Grant-Free NOMA in mMTC","10.1109/JSAC.2021.3126071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121862914&doi=10.1109%2fJSAC.2021.3126071&partnerID=40&md5=6eb639919a7f5f082f7f309de0f8d6bc","In massive machine-type communications (mMTC), the conflict between millions of potential access devices and limited channel freedom leads to a sharp decrease in spectrum efficiency. The nature of sporadic activity in mMTC provides a solution to enhance spectrum efficiency by employing compressive sensing (CS) to perform multiuser detection (MUD). However, CS-MUD suffers from high computation complexity and fails to meet the strict latency requirement in some critical applications. To address this problem, in this paper, we propose a novel deep learning (DL) based framework for grant-free non-orthogonal multiple access (GF-NOMA), where we utilize the information distilled from the initial data recovery phase to further enhance channel estimation, which in turn improves data recovery performance. Besides, we design an interpretable and structured Model-driven Prior Information Aided Network (M-PIAN) and provide theoretical analysis that demonstrates the proposed M-PIAN can converge faster and support more users. Experiments show that the proposed method outperforms existing CS algorithms and DL methods in both computation complexity and reconstruction accuracy. © 1983-2012 IEEE.","Compressive sensing; Deep Learning; Massive access; Massive machine-type communication","Channel estimation; Complex networks; Deep learning; Efficiency; Multiuser detection; Compressive sensing; Computation complexity; Data recovery; Deep learning; Learning methods; Machinetype communication (MTC); Massive access; Massive machine-type communication; Prior information; Spectra efficiency; Compressed sensing"
"Bai Y., Guan Y., Ng W.-F.","Fatigue assessment using ECG and actigraphy sensors","10.1145/3410531.3414308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091585867&doi=10.1145%2f3410531.3414308&partnerID=40&md5=aa677ba8e84f1fe129b253ecbe62c337","Fatigue is one of the key factors in the loss of work efficiency and health-related quality of life, and most fatigue assessment methods were based on self-reporting, which may suffer from many factors such as recall bias. To address this issue, we developed an automated system using wearable sensing and machine learning techniques for objective fatigue assessment. ECG/Actigraphy data were collected from subjects in free-living environments. Preprocessing and feature engineering methods were applied, before interpretable solution and deep learning solution were introduced. Specifically, for interpretable solution, we proposed a feature selection approach which can select less correlated and high informative features for better understanding system's decision-making process. For deep learning solution, we used state-of-the-art self-attention model, based on which we further proposed a consistency self-attention (CSA) mechanism for fatigue assessment. Extensive experiments were conducted, and very promising results were achieved. © 2020 ACM.","fatigue assessment; machine/deep learning; wearable sensing","Automation; Decision making; Deep learning; Electrocardiography; Patient monitoring; Wearable computers; Automated systems; Decision making process; Fatigue assessments; Feature engineerings; Health-related quality of lives; Machine learning techniques; State of the art; Wearable sensing; Learning systems"
"Bai Y., Wang H., Watson T.J., Zaniolo C.","Load shedding in classifying multi-source streaming data: A bayes risk approach","10.1137/1.9781611972771.39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449133306&doi=10.1137%2f1.9781611972771.39&partnerID=40&md5=d8f0c302572c02edb6d5e26c01d439bb","Monitoring multiple streaming sources for collective decision making presents several challenges. First, streaming data are often of large volume, fast speed, and highly bursty nature. Second, it is impossible to offload classification decisions to individual data sources, each of which lacks full knowledge for the decision making. Hence, the central classifier responsible for decision making may be frequently overloaded. In this paper, we study intelligent load shedding for classifying multi-source data. We aim at maximizing classification quality under resource (CPU and bandwidth) constraints. We use a Markov model to predict the distribution of feature values over time. Then, leveraging Bayesian decision theory, we use Bayes risk analysis to model the variances among different data sources in their contributions to the classification quality. We adopt an Expected Observational Risk criterion to quantify the loss of classification quality due to load shedding, and propose a Best Feature First (BFF) algorithm that greedily minimizes such risk. The effectiveness of the approach proposed is confirmed by experiments.",,"Data mining; Decision making; Decision theory; Electric load shedding; Electric power plant loads; Markov processes; Risk analysis; Risk assessment; Bayesian decision theory; Classification decision; Classification quality; Collective decision making; Intelligent load shedding; Multi-source streaming; Multisource data; Streaming data; Classification (of information)"
"Bai Y.-Q., Shen K.-J.","Alternating Direction Method of Multipliers for ℓ1- ℓ2 -Regularized Logistic Regression Model","10.1007/s40305-015-0090-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971268465&doi=10.1007%2fs40305-015-0090-2&partnerID=40&md5=526ef2378ddfd0bb0c337f2104658c4e","Logistic regression has been proved as a promising method for machine learning, which focuses on the problem of classification. In this paper, we present an ℓ1- ℓ2-regularized logistic regression model, where the ℓ1-norm is responsible for yielding a sparse logistic regression classifier and the ℓ2-norm for keeping better classification accuracy. To solve the ℓ1- ℓ2-regularized logistic regression model, we develop an alternating direction method of multipliers with embedding limited-Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) method. Furthermore, we implement our model for binary classification problems by using real data examples selected from the University of California, Irvine Machines Learning Repository (UCI Repository). We compare our numerical results with those obtained by the well-known LIBSVM and SVM-Light software. The numerical results show that our ℓ1- ℓ2-regularized logistic regression model achieves better classification and less CPU Time. © 2015, Operations Research Society of China, Periodicals Agency of Shanghai University, Science Press and Springer-Verlag Berlin Heidelberg.","Alternating direction method of multipliers; Classification problems; Logistic regression model; Sparsity","Learning systems; Nonlinear programming; Alternating direction method of multipliers; Binary classification problems; Broyden-Fletcher-Goldfarb-Shanno; Classification accuracy; Logistic regression classifier; Logistic Regression modeling; Sparsity; University of California; Regression analysis"
"Bai Y.-Q., Shen Y.-J., Shen K.-J.","Consensus Proximal Support Vector Machine for Classification Problems with Sparse Solutions","10.1007/s40305-014-0037-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897407931&doi=10.1007%2fs40305-014-0037-z&partnerID=40&md5=5229cd0b7d9c89e925d742e28ac6c5d4","Classification problem is the central problem in machine learning. Support vector machines (SVMs) are supervised learning models with associated learning algorithms and are used for classification in machine learning. In this paper, we establish two consensus proximal support vector machines (PSVMs) models, based on methods for binary classification. The first one is to separate the objective functions into individual convex functions by using the number of the sample points of the training set. The constraints contain two types of the equations with global variables and local variables corresponding to the consensus points and sample points, respectively. To get more sparse solutions, the second one is l1-l2 consensus PSVMs in which the objective function contains an ℓ1-norm term and an ℓ2-norm term which is responsible for the good classification performance while ℓ1-norm term plays an important role in finding the sparse solutions. Two consensus PSVMs are solved by the alternating direction method of multipliers. Furthermore, they are implemented by the real-world data taken from the University of California, Irvine Machine Learning Repository (UCI Repository) and are compared with the existed models such as ℓ1-PSVM, ℓp-PSVM, GEPSVM, PSVM, and SVM-light. Numerical results show that our models outperform others with the classification accuracy and the sparse solutions. © 2014 Operations Research Society of China, Periodicals Agency of Shanghai University, and Springer-Verlag Berlin Heidelberg.","Alternating direction method of multipliers; Classification problems; Consensus; Proximal support vector machine; Support vector machine","Artificial intelligence; Functions; Learning algorithms; Vectors; Alternating direction method of multipliers; Classification accuracy; Classification performance; Consensus; Machine learning repository; Proximal support vector machines; Support vector machine (SVMs); University of California; Support vector machines"
"Bai Z., Ravi S.S., Davidson I.","Towards Description of Block Model on Graph","10.1007/978-3-030-67664-3_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103276675&doi=10.1007%2f978-3-030-67664-3_3&partnerID=40&md5=1b4fa599802c59d946e1d0f3a8284006","Existing block modeling methods can detect communities as blocks. However it remains a challenge to easily explain to a human why nodes belong to the same block. Such a description is very useful for answering why people in the same community tend to interact cohesively. In this paper we explore a novel problem: Given a block model already found, describe the blocks using an auxiliary set of information. We formulate a combinatorial optimization problem which finds a unique disjunction of the auxiliary information shared by the nodes either in the same block or between a pair of different blocks. The former terms intra-block description, the latter inter-block description. Given an undirected graph and its k- block model, our method generates k+k(k-1)2 different descriptions. If the tags are descriptors of events occurring at the vertices, our descriptions can be interpreted as common events occurring within blocks and between blocks. We show that this problem is intractable even for simple cases, e.g., when the underlying graph is a tree with just two blocks. However, simple and efficient ILP formulations and algorithms exist for its relaxation and yield insights different from a state-of-the-art related work in unsupervised description. We empirically show the power of our work on multiple real-world large datasets. © 2021, Springer Nature Switzerland AG.","Block model; Explainable artificial intelligence; Unsupervised graph analysis","Combinatorial optimization; Data mining; Inductive logic programming (ILP); Large dataset; Machine learning; Auxiliary information; Block modeling; Combinatorial optimization problems; ILP formulation; Large datasets; State of the art; Underlying graphs; Undirected graph; Trees (mathematics)"
"Baid U., Shah N.A., Talbar S.","Brain tumor segmentation with cascaded deep convolutional neural network","10.1007/978-3-030-46643-5_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085504016&doi=10.1007%2f978-3-030-46643-5_9&partnerID=40&md5=6c885f458db92aa0376f3d97c165fcbc","Cancer is the second leading cause of death globally and is responsible for an estimated 9.6 million deaths in 2018. Approximately 70% of deaths from cancer occur in low and middle-income countries. One defining feature of cancer is the rapid creation of abnormal cells that grow uncontrollably causing tumor. Gliomas are brain tumors that arises from the glial cells in brain and comprise of 80% of all malignant brain tumors. Accurate delineation of tumor cells from healthy tissues is important for precise treatment planning. Because of different forms, shapes, sizes and similarity of the tumor tissues with rest of the brain segmentation of the Glial tumors is challenging. In this study we have proposed fully automatic two step approach for Glioblastoma (GBM) brain tumor segmentation with Cascaded U-Net. Training patches are extracted from 335 cases from Brain Tumor Segmentation (BraTS) Challenge for training and results are validated on 125 patients. The proposed approach is evaluated quantitatively in terms of Dice Similarity Coefficient (DSC) and Hausdorff95 distance. © Springer Nature Switzerland AG 2020.","Brain tumor segmentation; Convolutional Neural Networks; Deep learning; GPU; U-Net","Brain; Convolutional neural networks; Deep neural networks; Diseases; Histology; Medical imaging; Brain segmentation; Brain tumor segmentation; Glioblastomas; Healthy tissues; Low and middle income countries; Similarity coefficients; Treatment planning; Two-step approach; Tumors"
"Baig A.R., Kayani H.M.","Swarm intelligence based author identification for digital typewritten text","10.1109/Anti-Cybercrime.2015.7351933","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963611881&doi=10.1109%2fAnti-Cybercrime.2015.7351933&partnerID=40&md5=2124d16c9f87eaeec95471334379fe75","In this study we report our research on learning an accurate and easily interpretable classifier model for authorship classification of typewritten digital texts. For this purpose we use Ant Colony Optimization; a meta-heuristic based on swarm intelligence. Unlike black box type classifiers, the decision making rules produced by the proposed method are understandable by people familiar to the domain and can be easily enhanced with the addition of domain knowledge. Our experimental results show that the method is feasible and more accurate than decision trees. © 2015 IEEE.","author identification; author verification; data mining; digital forensic; machine learning","Ant colony optimization; Artificial intelligence; Behavioral research; Computer crime; Data mining; Decision making; Decision trees; Learning systems; Author identification; Classifier models; Decision-making rules; Digital forensic; Digital text; Domain knowledge; Metaheuristic; Swarm Intelligence; Electronic document identification systems"
"Bailey-Kellogg C., Ramakrishnan N.","Spatial aggregation for qualitative assessment of scientific computations",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-9444263050&partnerID=40&md5=4278592d404e6203b04393b73dbccade","Qualitative assessment of scientific computations is an emerging application area that applies a data-driven approach to characterize, at a high level, phenomena including conditioning of matrices, sensitivity to various types of error propagation, and algorithmic convergence behavior. This paper develops a spatial aggregation approach that formalizes such analysis in terms of model selection utilizing spatial structures extracted from matrix perturbation datasets. We focus in particular on the characterization of matrix eigenstructure, both analyzing sensitivity of computations with spectral portraits and determining eigenvalue multiplicity with Jordan portraits. Our approach employs spatial reasoning to overcome noise and sparsity by detecting mutually reinforcing interpretations, and to guide subsequent data sampling. It enables quantitative evaluation of properties of a scientific computation in terms of confidence in a model, explainable in terms of the sampled data and domain knowledge about the underlying mathematical structure. Not only is our methodology more rigorous than the common approach of visual inspection, but it also is often substantially more efficient, due to well-defined stopping criteria. Results show that the mechanism efficiently samples perturbation space and successfully uncovers high-level properties of matrices.",,"Data-driven approaches; Qualitative assesment; Spatial aggregation; Spatial structures; Algorithms; Artificial intelligence; Eigenvalues and eigenfunctions; Errors; Matrix algebra; Perturbation techniques; Problem solving; Natural sciences computing"
"Bailey-Kellogg C., Ramakrishnan N.","Ambiguity-directed sampling for qualitative analysis of sparse data from spatially-distributed physical systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347258966&partnerID=40&md5=b56a1774c1d1e3bed2a8328067d13530","A number of important scientific and engineering applications, such as fluid dynamics simulation and aircraft design, require analysis of spatially-distributed data from expensive experiments and complex simulations. In such data-scarce applications, it is advantageous to use models of given sparse data to identify promising regions for additional data collection. This paper presents a principled mechanism for applying domain-specific knowledge to design focused sampling strategies. In particular, our approach uses ambiguities identified in a multi-level qualitative analysis of sparse data to guide iterative data collection. Two case studies demonstrate that this approach leads to highly effective sampling decisions that are also explainable in terms of problem structures and domain knowledge.",,"Complex simulation; Domain-specific knowledge; Fluid dynamics simulations; Physical systems; Problem structure; Qualitative analysis; Sampling strategies; Scientific and engineering applications; Artificial intelligence; Data acquisition"
"Bailey-Kellogg C., Zhao F.","Influence-based model decomposition for reasoning about spatially distributed physical systems","10.1016/S0004-3702(01)00090-X","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035423385&doi=10.1016%2fS0004-3702%2801%2900090-X&partnerID=40&md5=0e93d5ba92f800f0a0a80d7e2d71896f","Many important science and engineering applications, such as regulating the temperature distribution over a semiconductor wafer and controlling the noise from a photocopy machine, require interpreting distributed data and designing decentralized controllers for spatially distributed systems. Developing effective computational techniques for representing and reasoning about these systems, which are usually modeled with partial differential equations (PDEs), is one of the major challenge problems for qualitative and spatial reasoning research. This paper introduces a novel approach to decentralized control design, influence-based model decomposition, and applies it in the context of thermal regulation. Influence-based model decomposition uses a decentralized model, called an influence graph, as a key data abstraction representing influences of controls on distributed physical fields. It serves as the basis for novel algorithms for control placement and parameter design for distributed systems with large numbers of coupled variables. These algorithms exploit physical knowledge of locality, linear superposability, and continuity, encapsulated in influence graphs representing dependencies of field nodes on control nodes. The control placement design algorithms utilize influence graphs to decompose a problem domain so as to decouple the resulting regions. The decentralized control parameter optimization algorithms utilize influence graphs to efficiently evaluate thermal fields and to explicitly trade off computation, communication, and control quality. By leveraging the physical knowledge encapsulated in influence graphs, these control design algorithms are more efficient than standard techniques, and produce designs explainable in terms of problem structures. © 2001 Elsevier Science B.V. All rights reserved.","Distributed control design; Model decomposition; Qualitative reasoning; Spatial reasoning; Spatially distributed systems","Algorithms; Artificial intelligence; Distributed parameter control systems; Graph theory; Mathematical models; Optimization; Influence-based model decomposition; Qualitative reasoning; Spatial reasoning; Decentralized control"
"Bailke P.A., Patil S.T.","Rule grouping & multiple minimum support thresholds for semantic multi-label associative classifier using feature reoccurrences","10.1504/IJDMMM.2017.085647","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027080487&doi=10.1504%2fIJDMMM.2017.085647&partnerID=40&md5=fcd3d0e8052f35081c7916b774b6d5ee","Multi-label classification is one of the important tasks in data mining. Researchers have addressed and extensively studied supervised classification which has vast applications in many domains. Associative classifiers are better performing classifiers, but they still have some issues which need to be addressed. This paper handles class imbalance problem, semantically organises vast number of generated rules, and applies relevant rules during classification. An algorithm called semantic multi-label associative classifier using feature reoccurrences (SeMACR) is proposed. Considering reoccurrence of features while generating rules proves to be beneficial, in particular for text documents. Class imbalance problem is handled with the help of balanced training and use of multiple minimum support thresholds based on the class distribution. A novel semantic-based approach is proposed for grouping of association rules using relatedness score between features rather than the traditional distance-based measure. Such organisation of rules makes them manageable and interpretable. During classification, only the relevant rules i.e., the rules present in the semantically most related group are applied. SeMACR algorithm has shown improved or comparable performance as compared to state-of-The-Art techniques. © 2017 Inderscience Enterprises Ltd.","Association rules; Balanced training; Multi-label classification; Multiple minimum support thresholds; Reoccurrence of features; Semantic rule grouping",
"Baillargeon J.-T., Lamontagne L., Marceau E.","Mining actuarial risk predictors in accident descriptions using recurrent neural networks","10.3390/risks9010007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098874389&doi=10.3390%2frisks9010007&partnerID=40&md5=5174ed3d3879f2128e19755099b37875","One crucial task of actuaries is to structure data so that observed events are explained by their inherent risk factors. They are proficient at generalizing important elements to obtain useful forecasts. Although this expertise is beneficial when paired with conventional statistical models, it becomes limited when faced with massive unstructured datasets. Moreover, it does not take profit from the representation capabilities of recent machine learning algorithms. In this paper, we present an approach to automatically extract textual features from a large corpus that departs from the traditional actuarial approach. We design a neural architecture that can be trained to predict a phenomenon using words represented as dense embeddings. We then extract features identified as important by the model to assess the relationship between the words and the phenomenon. The technique is illustrated through a case study that estimates the number of cars involved in an accident using the accident’s description as input to a Poisson regression model. We show that our technique yields models that are more performing and interpretable than some usual actuarial data mining baseline. © 2020 by the authors. Li-censee MDPI, Basel, Switzerland.","Data mining; Data representation; Hierarchical attention neural networks; Insurance big data; Natural language processing; Representational learning; Unstructured data",
"Bajorath J.","Explainable machine learning for medicinal chemistry: exploring multi-target compounds","10.4155/fmc-2022-0122","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136342248&doi=10.4155%2ffmc-2022-0122&partnerID=40&md5=581bcca80867f1dda6d46fda7bfc2d13",[No abstract available],"compound design; explainable machine learning; single- versus multi-target activity; small molecules; structural characteristics","drug; algorithm; artificial intelligence; correlation analysis; drug activity; drug binding; drug design; drug structure; drug targeting; experimental study; machine learning; measurement accuracy; medicinal chemistry; molecular size; prediction; Review; molecular model; Chemistry, Pharmaceutical; Machine Learning; Models, Molecular"
"Bajwa M.N., Malik M.I., Siddiqui S.A., Dengel A., Shafait F., Neumeier W., Ahmed S.","Two-stage framework for optic disc localization and glaucoma classification in retinal fundus images using deep learning","10.1186/s12911-019-0842-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069464979&doi=10.1186%2fs12911-019-0842-8&partnerID=40&md5=0db0c74824846d0a3a47a25441def0ac","Background: With the advancement of powerful image processing and machine learning techniques, Computer Aided Diagnosis has become ever more prevalent in all fields of medicine including ophthalmology. These methods continue to provide reliable and standardized large scale screening of various image modalities to assist clinicians in identifying diseases. Since optic disc is the most important part of retinal fundus image for glaucoma detection, this paper proposes a two-stage framework that first detects and localizes optic disc and then classifies it into healthy or glaucomatous. Methods: The first stage is based on Regions with Convolutional Neural Network (RCNN) and is responsible for localizing and extracting optic disc from a retinal fundus image while the second stage uses Deep Convolutional Neural Network to classify the extracted disc into healthy or glaucomatous. Unfortunately, none of the publicly available retinal fundus image datasets provides any bounding box ground truth required for disc localization. Therefore, in addition to the proposed solution, we also developed a rule-based semi-automatic ground truth generation method that provides necessary annotations for training RCNN based model for automated disc localization. Results: The proposed method is evaluated on seven publicly available datasets for disc localization and on ORIGA dataset, which is the largest publicly available dataset with healthy and glaucoma labels, for glaucoma classification. The results of automatic localization mark new state-of-the-art on six datasets with accuracy reaching 100% on four of them. For glaucoma classification we achieved Area Under the Receiver Operating Characteristic Curve equal to 0.874 which is 2.7% relative improvement over the state-of-the-art results previously obtained for classification on ORIGA dataset. Conclusion: Once trained on carefully annotated data, Deep Learning based methods for optic disc detection and localization are not only robust, accurate and fully automated but also eliminates the need for dataset-dependent heuristic algorithms. Our empirical evaluation of glaucoma classification on ORIGA reveals that reporting only Area Under the Curve, for datasets with class imbalance and without pre-defined train and test splits, does not portray true picture of the classifier's performance and calls for additional performance metrics to substantiate the results. © 2019 The Author(s).","Computer aided diagnosis; Deep learning; Glaucoma detection; Machine learning; Medical image analysis; Optic disc localization","computer assisted diagnosis; diagnostic imaging; eye fundus; glaucoma; human; optic disk; procedures; Deep Learning; Diagnosis, Computer-Assisted; Fundus Oculi; Glaucoma; Humans; Image Interpretation, Computer-Assisted; Optic Disk"
"Bakalo R., Goldberger J., Ben-Ari R.","Weakly and semi supervised detection in medical imaging via deep dual branch net","10.1016/j.neucom.2020.09.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092406332&doi=10.1016%2fj.neucom.2020.09.037&partnerID=40&md5=e190d55f6c39879ce5c81a7a56079eab","This study presents a novel deep learning architecture for multi-class classification and localization of abnormalities in medical imaging illustrated through experiments on mammograms. The proposed network combines two learning branches. One branch is for region classification with a newly added normal-region class. Second branch is region detection branch for ranking regions relative to one another. Our method enables detection of abnormalities at full mammogram resolution for both weakly and semi-supervised settings. A novel objective function allows for the incorporation of local annotations into the model. We present the impact of our schemes on several performance measures for classification and localization, to evaluate the cost effectiveness of the lesion annotation effort. Our evaluation was primarily conducted over a large multi-center mammography dataset of ~3,000 mammograms with various findings. The results for weakly supervised learning showed significant improvement compared to previous approaches. We show that the time consuming local annotations involved in supervised learning can be addressed by a weakly supervised method that can leverage a subset of locally annotated data. Weakly and semi-supervised methods coupled with detection can produce a cost effective and explainable model to be adopted by radiologists in the field. © 2020 Elsevier B.V.","Abnormality detection; Breast radiology; Deep learning; Mammography; Semi-supervised detection; Weakly supervised detection","Cost effectiveness; Deep learning; Large dataset; Mammography; Medical imaging; Semi-supervised learning; X ray screens; Learning architectures; Multi-class classification; Objective functions; Performance measure; Region classifications; Semi-supervised method; Supervised methods; Weakly supervised learning; Learning systems; article; breast; controlled study; cost effectiveness analysis; deep learning; diagnostic imaging; human; mammography; multiclass classification; radiologist; radiology; remission"
"Bakar A.A., Othman Z.A., Hamdan A.R., Yusof R., Ismail R.","Agent based data classification approach for data mining","10.1109/ITSIM.2008.4631677","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349183141&doi=10.1109%2fITSIM.2008.4631677&partnerID=40&md5=2097a0711966e7d21e30cab5402ad762","Classification is one of the tasks in data mining. The form of classifier depends on the classification technique used. For example, neural network produce a set of weight as a classifier, regression form an equation as a predictor while decision tree, C4.5, CART, Rough Set and Bayesian theory generate set of rules known as rule based classifier. Rules are more interpretable by human when compared to other form of classifiers. The process of classification involves applying the rules onto a set of unseen data. There are many issues appeared in rule application process such as more than one rule match, multiple scanning of large rule base and uncertainty. In this study an agent based approach is proposed to improve the rule application process. The proposed agents are embedded within the standard rule application techniques. The result shows the significant improvements in classification time and the number of matched rules with comparable classification accuracy. © 2008 IEEE.",,"Agents; Applications; Classifiers; Data mining; Decision support systems; Decision theory; Decision trees; Information management; Information technology; Learning systems; Neural networks; Set theory; Agent based; Agent based approaches; Application processes; Bayesian theories; Classification accuracies; Classification techniques; Classification times; Data classifications; Multiple scanning; One rules; Rough sets; Rule base; Set of rules; Standard rules; Rough set theory"
"Bakare A., Masyagin S., Succi G., Vasquez X.","Toward Understanding Personalities Working on Computer: A Preliminary Study Focusing on Collusion/Plagiarism",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137970106&partnerID=40&md5=d7d4995af2f130af02c902afd1ddb093","Ample research has been carried out in the area of collusion, plagiarism and e-learning. Collusion is a form of active cheating where two or more parties secretly or illegally cooperate. Collusion is at the root of common knowledge plagiarism. While plagiarism requires two or more entities to compare, collusion can be determined in isolation. It is also possible that collusion does not lead to positive plagiarism checks. It is therefore the aims of this preliminary study to; (i) identify the factors responsible for collusion in e-assessment (ii) determine the prominent factor that is representative of collusion and (iii) through user behaviour including, but not limited to, application switching time, determine collusion. Innometrics software was used to collect data in two compulsory exams (first one written and then oral) taken by the students. Discrepancies in the performance and grades of students in the two exams served as the ground truth in labelling possible collusion. We claim that user computer activities and application processes can help understand user behaviours in e-assessment. It is on this premise that we develop a machine learning model to predict collusion through user behaviour in e-assessment. Copyright © 2021 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved","Collusion; Computer Activity; Computer Processes; Computer-based Plagiarism; Plagiarism; User Behaviour","E-learning; Intellectual property; Collusion; Common knowledge; Computer activities; Computer process; Computer-based plagiarism; E - learning; E assessments; Plagiarism; Plagiarism checks; User behaviors; Behavioral research"
"Bakare A.M., Morley J.G., Simons R.R.","Developing a seabed resurvey strategy: A GIS approach to modelling seabed changes and resurvey risk","10.1016/j.compenvurbsys.2008.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449088701&doi=10.1016%2fj.compenvurbsys.2008.06.002&partnerID=40&md5=3bbf638779bca44db70f7164c18fd14b","There has been a steady transition towards the representation, analysis and modelling of dynamic spatio-temporal relationships in geographical information systems (GISs). These developments open up opportunities for investigating and modelling the dynamic relationships that occur in the coastal and marine environments and how these relate to human activities. Such a spatio-temporal approach is applied in this research to address an issue faced by the bodies responsible for maintaining navigation safety in territorial waters and this paper introduces the developed resurvey decision support system. The developed system models morphological change in response to hydrodynamic conditions and determines the seabed locations that require resurvey based on the modelled change and navigation characteristics. System validation tests indicate that the morphological modelling tool is under-predicting the magnitude and lateral extent of the change which then influences the locations that require resurvey. Additional sensitivity tests of the morphological modelling parameters highlight the influence of the parameters on the outputs and derived predictions. The achieved modelling results and resurvey decision indicate the applicability of utilising a GIS to model seabed change as an input into decision support systems for planning and management purposes. The results also suggest the applicability of the modelling and decision support methodology to similar problems in the coastal and marine environments. Crown Copyright © 2008.","Coastal; Decision support; GIS; Marine; Morphological modelling; Software coupling","Administrative data processing; Artificial intelligence; Data mining; Decision making; Decision support systems; Decision theory; Fluid dynamics; Fluid mechanics; Forecasting; Geographic information systems; Insulating materials; Management information systems; Model structures; Navigation; Planning; Research; Sensitivity analysis; Testing; Coastal; Decision support; Decision supports; Geographical information systems; GIS; Human activities; Hydrodynamic conditions; Marine; Marine environments; Morphological changes; Morphological modelling; Navigation safety; Sensitivi ty tests; Software coupling; Spatio-temporal approach; Spatio-temporal relationships; System modelling; System validation; Territorial waters; Risk assessment; coastal morphology; decision support system; GIS; hydrodynamics; modeling; navigation; remote sensing; seafloor; software; survey"
"Bakdi A., Kristensen N.B., Stakkeland M.","Multiple Instance Learning With Random Forest for Event Logs Analysis and Predictive Maintenance in Ship Electric Propulsion System","10.1109/TII.2022.3144177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123345354&doi=10.1109%2fTII.2022.3144177&partnerID=40&md5=4e34e8298d4192d74bafe292dc1141fe","In this article, a novel weakly supervised machine learning approach is proposed for intelligent predictive maintenance (IPdM). It employs balanced random forest and multiple instance learning based on event logs from ships' electric propulsion systems. The objectives are predicting failure likelihood, time to failure, and explainable predictions to ensure timely crew intervention. The IPdM approach uncovers, then learns, and classifies sequences of events that represent early causes or symptoms to forecast imminent failures. In particular, this article contributes effective solutions to irregular, imbalanced, and unlabeled data issues where conventional methods become obsolete. First, the events occur at irregular intervals; they include alarms, warnings, and operational information collected across multiple units and control systems. Second, the datasets exhibit extreme imbalance due to few failures and multiple failure modes; this entails biased predictions. Third, the training datasets are weakly labeled; only the failure timestamp is known without any expert input on prior causes or early symptoms. Temporal random indexing is proposed to transform textual log messages into a numerical lower dimensional space where timeseries analyses are applicable. Balanced random-forest models are developed for unbiased classification and regression. The overall approach learns recursively the ungiven data labels while training the base learners. The IPdM approach is validated through millions of events of multithousand types collected from two years of seagoing vessels. It successfully forecasts actual propulsion failures and performs better when compared with contemporary methods. © 2005-2012 IEEE.","Alarm system; balanced random forest (B-RF); event logs; event-driven predictive maintenance; failure prediction; imbalance; inexact labeling; inverter; liquified natural gas (LNG) carriers; multiple instance learning (MIL); ship electric propulsion system; temporal random indexing (TRI); time to failure (t2f); weakly supervised learning","Alarm systems; Decision trees; Indexing (of information); Maintenance; Ship propulsion; Ships; Supervised learning; Balanced random forest; Electric propulsion systems; Event logs; Event-driven; Eventdriven predictive maintenance; Failures prediction; Imbalance; Inexact labeling; Informatics; Inverte; Labelings; LNG carriers; Marine vehicles; Multiple-instance learning; Predictive maintenance; Radiofrequencies; Random forests; Random indexing; Ship electric propulsion system; Temporal random indexing; Time to failure; Weakly supervised learning; Forecasting"
"Baker M.R., Padmaja D.L., Puviarasi R., Mann S., Panduro-Ramirez J., Tiwari M., Samori I.A.","Implementing Critical Machine Learning (ML) Approaches for Generating Robust Discriminative Neuroimaging Representations Using Structural Equation Model (SEM)","10.1155/2022/6501975","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128801923&doi=10.1155%2f2022%2f6501975&partnerID=40&md5=0718811694f63c527808db6c7b28a84f","Critical ML or CML is a critical approach development of the standard ML (SML) procedure. Conventional ML (ML) is being used in radiology departments where complex neuroimages are discriminated using ML technology. Radiologists and researchers found that sole decision by the ML algorithms is not accurate enough to implement the treatment procedure. Thus, an intelligent decision is required further by the radiologists after evaluating the ML outcomes. The current research is based on the critical ML, where radiologists' critical thinking ability, IQ (intelligence quotient), and experience in radiology have been examined to understand how these factors affect the accuracy of neuroimaging discrimination. A primary quantitative survey has been carried out, and the data were analysed in IBM SPSS. The results showed that experience in works has a positive impact on neuroimaging discrimination accuracy. IQ and trained ML are also responsible for improving the accuracy as well. Thus, radiologists with more experience in that field are able to improve the discriminative and diagnostic capability of CML. © 2022 Mohammed Rashad Baker et al.",,"Neuroimaging; Radiation; Radiology; Conventional machines; Critical approach; Critical machine; Intelligence quotients; Learning machines; Learning procedures; Machine learning approaches; Machine-learning; Standard machines; Structural equation models; Machine learning; article; critical thinking; data analysis software; human; intelligence quotient; machine learning; neuroimaging; outcome assessment; quantitative analysis; radiologist; radiology; structural equation modeling; Algorithms; Artificial Intelligence; Humans; Machine Learning; Neuroimaging; Radiologists"
"Bakouregui A.S., Mohamed H.M., Yahia A., Benmokrane B.","Explainable extreme gradient boosting tree-based prediction of load-carrying capacity of FRP-RC columns","10.1016/j.engstruct.2021.112836","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111060314&doi=10.1016%2fj.engstruct.2021.112836&partnerID=40&md5=e76d330c4a18d99d34a6a0c5497a7088","This study presents a new approach for predicting the load-carrying capacity of reinforced concrete (RC) columns reinforced with fiber-reinforced polymer (FRP) bars with an eXtreme Gradient Boosting (XGBoost) algorithm. The proposed XGBoost model was developed based on a comprehensive database containing experimental data for 283 FRP-RC columns collected from the literature. The SHapley Additive exPlanations (SHAP) framework was used to interpret the output of the model. Furthermore, the efficiency and accuracy of the XGBoost model were evaluated and compared with design codes and equations in the literature. The results show that the proposed prediction model performed extremely well and was suitable for predicting the load-carrying capacity of FRP-RC columns. Moreover, the XGBoost model outperformed other numerical equations. For short columns, the mean R2 and MAPE values for the XGBoost model were 0.98% and 5.3%, respectively. In addition, the most significant input variables for predicting the maximum axial load-carrying capacity of FRP-RC columns were the eccentricity ratio, gross sectional area, compressive strength of concrete, slenderness ratio, and spacing or pitch of transversal reinforcement. Lastly, this study demonstrates the capability of machine learning models to predict the axial load-carrying capacity of FRP-RC columns. The proposed XGBoost model can provide an alternative method to existing mechanics-based models for design practice. © 2021 Elsevier Ltd","Columns; Concrete; Design codes; eXtreme gradient boosting; FRP bars; Machine learning; SHapley Additive exPlanations","Adaptive boosting; Additives; Columns (structural); Compressive strength; Forecasting; Load limits; Machine learning; Reinforced concrete; Design codes; Extreme gradient boosting; Fiber-reinforced polymer bar; Fiber-reinforced polymers; Fibre reinforced polymers; Gradient boosting; Load carrying; Machine-learning; Reinforced concrete column; Shapley additive explanation; Fiber reinforced plastics; algorithm; bearing capacity; column; compressive strength; loading; machine learning; polymer; prediction; reinforced concrete"
"Bala R., Ratnoo S.","A genetic algorithm approach for discovering tuned fuzzy classification rules with intra- and inter-class exceptions","10.1515/jisys-2015-0136","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964951721&doi=10.1515%2fjisys-2015-0136&partnerID=40&md5=76e746ac8b363c8848118306602e57e9","Fuzzy rule-based systems (FRBSs) are proficient in dealing with cognitive uncertainties like vagueness and ambiguity imperative to real-world decision-making situations. Fuzzy classification rules (FCRs) based on fuzzy logic provide a framework for a flexible human-like reasoning involving linguistic variables. Appropriate membership functions (MFs) and suitable number of linguistic terms - according to actual distribution of data - are useful to strengthen the knowledge base (rule base [RB]+ data base [DB]) of FRBSs. An RB is expected to be accurate and interpretable, and a DB must contain appropriate fuzzy constructs (type of MFs, number of linguistic terms, and positioning of parameters of MFs) for the success of any FRBS. Moreover, it would be fascinating to know how a system behaves in some rare/exceptional circumstances and what action ought to be taken in situations where generalized rules cease to work. In this article, we propose a three-phased approach for discovery of FCRs augmented with intra- and inter-class exceptions. A pre-processing algorithm is suggested to tune DB in terms of the MFs and number of linguistic terms for each attribute of a data set in the first phase. The second phase discovers FCRs employing a genetic algorithm approach. Subsequently, intra- and inter-class exceptions are incorporated in the rules in the third phase. The proposed approach is illustrated on an example data set and further validated on six UCI machine learning repository data sets. The results show that the approach has been able to discover more accurate, interpretable, and interesting rules. The rules with intra-class exceptions tell us about the unique objects of a category, and rules with inter-class exceptions enable us to take a right decision in the exceptional circumstances. © 2016 by De Gruyter.","exceptions discovery; Fuzzy classification rules (FCRs); inter-class exceptions, genetic algorithm; intra-class exceptions","Algorithms; Artificial intelligence; Classification (of information); Cognitive systems; Computation theory; Computational linguistics; Decision making; Fuzzy inference; Fuzzy logic; Fuzzy systems; Genetic algorithms; Knowledge based systems; Learning systems; Linguistics; Soft computing; Technology transfer; exceptions discovery; Fuzzy classification rule; Genetic algorithm approach; Inter class; Intra class; Linguistic variable; Pre-processing algorithms; UCI machine learning repository; Membership functions"
"Bala R., Agrawal R.K., Sardana M.","Relevant gene selection using normalized cut clustering with maximal compression similarity measure","10.1007/978-3-642-13672-6_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956301670&doi=10.1007%2f978-3-642-13672-6_9&partnerID=40&md5=d5620d13439d30b44285c51deb5ec605","Microarray cancer classification has drawn attention of research community for better clinical diagnosis in last few years. Microarray datasets are characterized by high dimension and small sample size. To avoid curse of dimensionality good feature selection methods are needed. Here, we propose a two stage algorithm for finding a small subset of relevant genes responsible for classification in high dimensional microarray datasets. In first stage of algorithm, the entire feature space is divided into k clusters using normalized cut. Similarity measure used for clustering is maximal information compression index. The informative gene is selected from each cluster using t-statistics and a pool of non redundant genes is created. In second stage a wrapper based forward feature selection method is used to obtain a set of optimal genes for a given classifier. The proposed algorithm is tested on three well known bdatasets from Kent Ridge Biomedical Data Repository. Comparison with other state of art methods shows that our proposed algorithm is able to achieve better classification accuracy with less number of features. © 2010 Springer-Verlag Berlin Heidelberg.","Cancer classification; Gene selection; Microarray; Normalized cut; Representative entropy","Biomedical data; Cancer classification; Classification accuracy; Clinical diagnosis; Curse of dimensionality; Feature selection methods; Feature space; Gene selection; High dimensions; High-dimensional; Informative genes; K cluster; Maximal information; Microarray data sets; Normalized cut; Normalized cuts; Representative entropy; Research communities; Similarity measure; Small Sample Size; State-of-art methods; T-statistics; Two-stage algorithm; Classification (of information); Data mining; Data reduction; Diagnosis; Diseases; Feature extraction; Genes; Lakes; Clustering algorithms"
"Balabaeva K., Kovalchuk S.","Neural Additive Models for Explainable Heart Attack Prediction","10.1007/978-3-031-08757-8_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134354104&doi=10.1007%2f978-3-031-08757-8_11&partnerID=40&md5=db436c1f49f7268b675dd404dcca1c96","Heart attack (HA) is a sudden health disorder when the flow of blood to the heart is blocked, causing damage to the heart. According to the World Health Organization (WHO), heart attack is one of the greatest causes of death and disability globally. Early recognition of the various warning signs of a HA can help reduce the severity. Different machine learning (ML) models have been developed to predict the heart attack. However, patients with arterial hypertension (AH) are especially prone to this disorder and have several features that distinguish them from other groups of patients. We apply these features to develop a special model for people suffering from AH. Moreover, we contribute to this field bringing more transparency to the modelling using interpretable machine learning. We also compare the patterns learned by methods with prior information used in heart attack scales and evaluate their efficiency. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Arterial hypertension; Decision Tree; Heart attack prediction; Heart attack risk; Interpretable machine learning; NAMs; XAI","Cardiology; Decision trees; Forecasting; Machine learning; Arterial hypertension; Attack prediction; Heart attack; Heart attack prediction; Heart attack risk; Interpretable machine learning; Machine-learning; NAM; XAI; Heart"
"Balabaeva K., Kovalchuk S.","Clustering results interpretation of continuous variables using Bayesian inference","10.3233/SHTI210204","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107238034&doi=10.3233%2fSHTI210204&partnerID=40&md5=0edb84b0f19db97ba3adad38a0bf99d2","The present study is devoted to interpretable artificial intelligence in medicine. In our previous work we proposed an approach to clustering results interpretation based on Bayesian Inference. As an application case we used clinical pathways clustering explanation. However, the approach was limited by working for only binary features. In this work, we expand the functionality of the method and adapt it for modelling posterior distributions of continuous features. To solve the task, we apply BEST algorithm to provide Bayesian t-testing and use NUTS algorithm for posterior sampling. The general results of both binary and continuous interpretation provided by the algorithm have been compared with the interpretation of two medical experts. © 2021 European Federation for Medical Informatics (EFMI) and IOS Press. © 2021 European Federation for Medical Informatics (EFMI) and IOS Press. All rights reserved.","Bayesian inference; BEST; Clinical pathways; Clustering interpretation; EXAI; Explainable artificial intelligence; Interpretable machine learning; K-Means; NUTS; Posterior sampling; XAI","algorithm; artificial intelligence; Bayes theorem; cluster analysis; Algorithms; Artificial Intelligence; Bayes Theorem; Cluster Analysis"
"Balabaeva K., Kovalchuk S.","Comparison of Efficiency, Stability and Interpretability of Feature Selection Methods for Multiclassification Task on Medical Tabular Data","10.1007/978-3-030-77967-2_51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111388956&doi=10.1007%2f978-3-030-77967-2_51&partnerID=40&md5=3ba222bfaa376d4e134ac4ed9316c14a","Feature selection is an important step of machine learning pipeline. Certain models may select features intrinsically without human interactions or additional algorithms applied. Such algorithms usually belong to neural networks class. Others require help of a researcher or feature selection algorithms. However, it is hard to know beforehand which variables contain the most relevant information and which may cause difficulties for a model to learn the correct relations. In that respect, researchers have been developing feature selection algorithms. To understand what methods perform better on tabular medical data, we have conducted a set of experiments to measure accuracy, stability and compare interpretation capacities of different feature selection approaches. Moreover, we propose an application of Bayesian Inference to the task of feature selection that may provide more interpretable and robust solution. We believe that high stability and interpretability are as important as classification accuracy especially in predictive tasks in medicine. © 2021, Springer Nature Switzerland AG.","Bayesian inference; eXAI; Explainable artificial intelligence; Feature selection; Kbest; Recursive feature elimination; XAI","Bayesian networks; Inference engines; Bayesian inference; Classification accuracy; Feature selection algorithm; Feature selection methods; Human interactions; Measure-accuracy; Multi-classification; Robust solutions; Feature extraction"
"Balabaeva K., Kovalchuk S.","Post-hoc Interpretation of Clinical Pathways Clustering using Bayesian Inference","10.1016/j.procs.2020.11.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098552178&doi=10.1016%2fj.procs.2020.11.028&partnerID=40&md5=a43d99723073a6745b1e9ea72e9d1c01","This study is dedicated to the domain of explainable artificial intelligence. We propose an approach to machine learning clustering results interpretation and apply it to clinical clusters interpretation. We use Bayesian inference to the post-hoc interpretation of clustering provided by K-means algorithm. We investigate what are the differences and similarities between clusters comparing posterior distributions of features. The proposed approach is not model-specific and may be used for interpretation of any clustering algorithm. Finally, we compare the results with a medical expert interpretation and analyze the differences and similarities. © 2020 Elsevier B.V.. All rights reserved.","bayesian inference; clinical pathways; clustering interpretation; eXAI; explainable artificial intelligence; interpretable machine learning; XAI","Artificial intelligence; Bayesian networks; Inference engines; Bayesian inference; Between clusters; Clinical pathways; Medical experts; Posterior distributions; K-means clustering"
"Balabaeva K., Kovalchuk S.","Comparison of Temporal and Non-Temporal Features Effect on Machine Learning Models Quality and Interpretability for Chronic Heart Failure Patients","10.1016/j.procs.2019.08.183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074543254&doi=10.1016%2fj.procs.2019.08.183&partnerID=40&md5=2bce130ec5138cd250347b8ed2f5b495","Chronic diseases are complex systems that can be described by various heteroscedastic data that varies in time. The goal of this work is to determine whether historical data helps to improve machine learning predictive models or is it more efficient to use the latest data describing the disease in particular moment in time. For simplicity we call features from the first group dynamic and features from the second one - static. We study the way both groups affect predictions quality and its interpretation. We set the experiments on data of chronic heart patients from Almazov Medical Research Center. From this data we extracted more than 300 features from patient comorbidity, anamnesis, analysis, etc. In terms of Chronic Heart Failure (CHF) modelling three different tasks have been selected: CHF identification as main diagnosis, CHF stage classification and diastolic blood pressure prediction. For each task several machine learning algorithms on three groups of features: static, dynamic and the whole feature set. The results show that, in general, models perform better on combination of temporal and non-temporal features. © 2018 Elsevier B.V. All rights reserved.","chronic heart failure; complex systems modelling; interpretable machine learning; machine learning; predictive modelling","Blood pressure; Cardiology; Computer aided diagnosis; Heart; Learning algorithms; Learning systems; Chronic heart failures; Diastolic blood pressures; Interpretability; Medical research; Predictive modelling; Predictive models; Systems modelling; Temporal features; Machine learning"
"Balaban E., Sweet A., Bajwa A., Maul W.A., Fulton C.E., Chicatelli A.","Transient region coverage in the propulsion IVHM technology experiment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744281492&partnerID=40&md5=c777d0987d14ae08c5a1d158d4cc2925","Over the last several years researchers at NASA Glenn and Ames Research Centers have developed a real-time fault detection and isolation system for propulsion subsystems of future space vehicles. The Propulsion IVHM Technology Experiment (PITEX), as it is called, follows the model-based diagnostic methodology and employs Livingstone, developed at NASA Ames, as its reasoning engine. The system has been tested on flight-like hardware through a series of nominal and fault scenarios. These scenarios have been developed using a highly detailed simulation of the X-34 flight demonstrator main propulsion system and include realistic failures involving valves, regulators, microswitches, and sensors. This paper focuses on one of the recent research and development efforts under PITEX - to provide more complete transient region diagnostic coverage. It describes the development of the transient monitors, the corresponding modeling methodology, and the interface software responsible for coordinating the flow of information between the quantitative monitors and the qualitative, discrete representation in Livingstone.",,"Fault detection; IVHM; Main propulsion system (IPS); Microswitches; Propulsion IVHM technology experiment (PITEX); Computer software; Constraint theory; Diagnosis; Interfaces (computer); Real time systems; Sensors; Spacecraft; Spacecraft propulsion; Artificial intelligence"
"Balachandar P., Michmizos K.P.","A Spiking Neural Network Emulating the Structure of the Oculomotor System Requires No Learning to Control a Biomimetic Robotic Head","10.1109/BioRob49111.2020.9224303","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095592448&doi=10.1109%2fBioRob49111.2020.9224303&partnerID=40&md5=460fb7ed303fbf090fafc231eab3d37a","Robotic vision introduces requirements for real-time processing of fast-varying, noisy information in a continuously changing environment. In a real-world environment, convenient assumptions, such as static camera systems and deep learning algorithms devouring high volumes of ideally slightlyvarying data are hard to survive. Leveraging on recent studies on the neural connectome associated with eye movements, we designed a neuromorphic oculomotor controller and placed it at the heart of our in-house biomimetic robotic head prototype. The controller is unique in the sense that (1) all data are encoded and processed by a spiking neural network (SNN), and (2) by mimicking the associated brain areas' topology, the SNN is biologically interpretable and requires no training to operate. Here, we report the robot's target tracking ability, demonstrate that its eye kinematics are similar to those reported in human eye studies and show that a biologically-constrained learning, although not required for the SNN's function, can be used to further refine its performance. This work aligns with our ongoing effort to develop energy-efficient neuromorphic SNNs and harness their emerging intelligence to control biomimetic robots with versatility and robustness. © 2020 IEEE.",,"Agricultural robots; Biomimetics; Controllers; Deep learning; Energy efficiency; Eye movements; Intelligent robots; Learning systems; Neural networks; Robotics; Target tracking; Biomimetic robotics; Changing environment; Oculomotor systems; Real world environments; Realtime processing; Spiking neural network(SNN); Spiking neural networks; Static camera systems; Learning algorithms"
"Balachander T., Batra A.K., Choudhary M.","Machine learning pipeline for an improved medical decision support",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084607333&partnerID=40&md5=263924909df95d8b180aa6c780911e8a","It has been projected that in recent years the amount of investment in automation is set to increase considerably while in the US alone it is set to reach $8 trillion. Given the rampant development in machine learning and artificial intelligence, there has been a gap between the advances in the field and the ethical and legal frameworks governing the field. This poses a problem which is set to grow in severity as more and more businesses and organization start adopting machine learning models. These models are essentially black boxes with even their developers having little knowledge as to how they came up with the results/decisions they did. In high stake decisions, this poses a serious problem as it leads to questioning of the trust ability of the model. There are a number of interpretability methods for different types of models and algorithms ranging from surrogate models to visual techniques for neural networks. However, there is often a trade-off between interpretability and model accuracy i.e., the more accurate a model tends to be, the less interpretable it becomes. Through this paper, we aim to demonstrate that interpretability of a model can actually be used to increase the model accuracy. We aim to achieve this by using the generated insights to update the model hyper parameters and comparing the accuracy scores before and after the update. We have used a breast cancer prediction dataset which we obtained from http://kaggle.com. Therefore, the aim of this paper is to create a machine learning pipeline which improves the model accuracy using insights from interpretability. © 2020 SERSC.","Interpretability; Model accuracy; Neural networks; Surrogate models",
"Balagopalan A., Zhang H., Hamidieh K., Hartvigsen T., Rudzicz F., Ghassemi M.","The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations","10.1145/3531146.3533179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133016678&doi=10.1145%2f3531146.3533179&partnerID=40&md5=b59d8b0a0d18cd0a06f762f3674c3b30","Machine learning models in safety-critical settings like healthcare are often ""blackboxes"": they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community. © 2022 Owner/Author.","explainability; fairness; machine learning","Health care; Safety engineering; Black box modelling; Black boxes; College admissions; Explainability; Fairness; Machine learning models; Machine-learning; Model prediction; Simple++; User trust models; Machine learning"
"Balagopalan A., Eyre B., Robin J., Rudzicz F., Novikova J.","Comparing Pre-trained and Feature-Based Models for Prediction of Alzheimer's Disease Based on Speech","10.3389/fnagi.2021.635945","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105666786&doi=10.3389%2ffnagi.2021.635945&partnerID=40&md5=acc215cab0e33923be6696896ed13bac","Introduction: Research related to the automatic detection of Alzheimer's disease (AD) is important, given the high prevalence of AD and the high cost of traditional diagnostic methods. Since AD significantly affects the content and acoustics of spontaneous speech, natural language processing, and machine learning provide promising techniques for reliably detecting AD. There has been a recent proliferation of classification models for AD, but these vary in the datasets used, model types and training and testing paradigms. In this study, we compare and contrast the performance of two common approaches for automatic AD detection from speech on the same, well-matched dataset, to determine the advantages of using domain knowledge vs. pre-trained transfer models. Methods: Audio recordings and corresponding manually-transcribed speech transcripts of a picture description task administered to 156 demographically matched older adults, 78 with Alzheimer's Disease (AD) and 78 cognitively intact (healthy) were classified using machine learning and natural language processing as “AD” or “non-AD.” The audio was acoustically-enhanced, and post-processed to improve quality of the speech recording as well control for variation caused by recording conditions. Two approaches were used for classification of these speech samples: (1) using domain knowledge: extracting an extensive set of clinically relevant linguistic and acoustic features derived from speech and transcripts based on prior literature, and (2) using transfer-learning and leveraging large pre-trained machine learning models: using transcript-representations that are automatically derived from state-of-the-art pre-trained language models, by fine-tuning Bidirectional Encoder Representations from Transformer (BERT)-based sequence classification models. Results: We compared the utility of speech transcript representations obtained from recent natural language processing models (i.e., BERT) to more clinically-interpretable language feature-based methods. Both the feature-based approaches and fine-tuned BERT models significantly outperformed the baseline linguistic model using a small set of linguistic features, demonstrating the importance of extensive linguistic information for detecting cognitive impairments relating to AD. We observed that fine-tuned BERT models numerically outperformed feature-based approaches on the AD detection task, but the difference was not statistically significant. Our main contribution is the observation that when tested on the same, demographically balanced dataset and tested on independent, unseen data, both domain knowledge and pretrained linguistic models have good predictive performance for detecting AD based on speech. It is notable that linguistic information alone is capable of achieving comparable, and even numerically better, performance than models including both acoustic and linguistic features here. We also try to shed light on the inner workings of the more black-box natural language processing model by performing an interpretability analysis, and find that attention weights reveal interesting patterns such as higher attribution to more important information content units in the picture description task, as well as pauses and filler words. Conclusion: This approach supports the value of well-performing machine learning and linguistically-focussed processing techniques to detect AD from speech and highlights the need to compare model performance on carefully balanced datasets, using consistent same training parameters and independent test datasets in order to determine the best performing predictive model. © Copyright © 2021 Balagopalan, Eyre, Robin, Rudzicz and Novikova.","Alzheimer's disease; BERT; dementia detection; feature engineering; MMSE regression; transfer learning","adult; aged; Alzheimer disease; Article; audio recording; Bidirectional Encoder Representations from Transformer; controlled study; demography; female; human; information processing; linguistics; machine learning; major clinical study; male; middle aged; Mini Mental State Examination; motivation; natural language processing; prediction; speech; task performance; transfer of learning; very elderly"
"Balagué C.","The challenge of responsible AI",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130206734&partnerID=40&md5=2e6e454bb62f4ae2063668dcfae56a7d","The dichotomic vision of artificial intelligence, which emerged during the last years, underlines both the benefits of machine learning in innovation and the negative consequences of AI on organizations and more globally on society. After an historical contextualization of digital transformation, this chapter first provides an understanding of the potential negative impacts of AI, second presents the concept of ethics and its application to AI systems, third describes different initiatives to tackle the issue of negative consequences of AI. It also illustrates the theoretical aspects of ethics in AI by analyzing current technologies presented at CES 2020 in Las Vegas. Finally, the chapter provides guidelines for managers to address the issue of ethics in AI, including metrics for corporate social responsibility. © Margherita Pagani and Renaud Champion 2021.",,
"Balaha H.M., Shaban A.O., El-Gendy E.M., Saafan M.M.","A multi-variate heart disease optimization and recognition framework","10.1007/s00521-022-07241-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129699010&doi=10.1007%2fs00521-022-07241-1&partnerID=40&md5=dafcf45b7eebd23cff9cb5ac56b67681","Cardiovascular diseases (CVD) are the most widely spread diseases all over the world among the common chronic diseases. CVD represents one of the main causes of morbidity and mortality. Therefore, it is vital to accurately detect the existence of heart diseases to help to save the patient life and prescribe a suitable treatment. The current evolution in artificial intelligence plays an important role in helping physicians diagnose different diseases. In the present work, a hybrid framework for the detection of heart diseases using medical voice records is suggested. A framework that consists of four layers, namely “Segmentation” Layer, “Features Extraction” Layer, “Learning and Optimization” Layer, and “Export and Statistics” Layer is proposed. In the first layer, a novel segmentation technique based on the segmentation of variable durations and directions (i.e., forward and backward) is suggested. Using the proposed technique, 11 datasets with 14,416 numerical features are generated. The second layer is responsible for feature extraction. Numerical and graphical features are extracted from the resulting datasets. In the third layer, numerical features are passed to 5 different Machine Learning (ML) algorithms, while graphical features are passed to 8 different Convolutional Neural Networks (CNN) with transfer learning to select the most suitable configurations. Grid Search and Aquila Optimizer (AO) are used to optimize the hyperparameters of ML and CNN configurations, respectively. In the last layer, the output of the proposed hybrid framework is validated using different performance metrics. The best-reported metrics are (1) 100% accuracy using ML algorithms including Extra Tree Classifier (ETC) and Random Forest Classifier (RFC) and (2) 99.17% accuracy using CNN. © 2022, The Author(s).","Aquila optimizer (AO); Convolutional neural network (CNN); Deep learning (DL); Heart disease; Machine learning (ML); Metaheuristic optimization","Convolution; Convolutional neural networks; Decision trees; Deep learning; Diseases; Extraction; Feature extraction; Heart; Multilayer neural networks; Optimization; Patient treatment; Aquila optimizer; Convolutional neural network; Deep learning; Heart disease; Machine learning; Metaheuristic optimization; Numerical features; Optimisations; Optimizers; Cardiology"
"Balaji K., Zhou Z., Rabiei M.","How big data analytics can help future regulatory issues in carbon capture and sequestration CCS projects","10.2118/195284-ms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066309646&doi=10.2118%2f195284-ms&partnerID=40&md5=a3dc9e3fff05ff9e7440eac70ed72f9c","In this age of data, there is a significant need for tracking and prediction of non-compliance of rules & regulations in various industries including the oil & gas sector. In this paper, we will be reviewing some of the anticipated regulatory issues in commercial implementations of carbon capture & sequestration (CCS) and discuss how machine learning and big data analytics can diminish future non-compliance incidents. With the rising awareness of advanced data-driven technologies such as ""Big Data Analytics"" and ""Machine Learning"", a contemporary approach to regulation and compliance is developing. This emerging approach, called ""Algorithmic Regulation"", defines an alternative framework for systematic collection of data (real-time or near real-time) and continuous generation of knowledge through intelligent computational algorithms in order to regulate a domain of activities. In this study, we will look at some of the major data management challenges in the pilot CCS operations with regards to rules and regulations. We will then discuss how an algorithmic regulatory framework can help in conducting CCS operations in a manner that are compliant with environmental, safety and health policies and regulations. Field operators collect a lot of data which needs to be formatted and modelled in a fashion acceptable to understand the operator's compliance with regulation. Generally, such compliance qualification criteria are verified using human intellect and basic querying software. In other industries, the idea of converting rules & regulations in a format understandable by machines is gaining momentum and great strides have been taken. The technological progresses made possible by data-driven analytical techniques can create a paradigm shift in the way rules and regulations are designed and implemented. Large-scale deployment of CCS projects is bound to bring with it a number of regulatory issues, making it a necessity to proactively explore and address the anticipated issues. These technologies can equip regulated entities as well as regulators with advanced tools for managing complexity in CCS projects. These improved solutions will help companies to better meet the regulatory data collection, reporting and governance requirements in large scale CCS operations. This paper looks into advanced data management and modeling techniques like ""algorithmic regulations"" to increase compliance in carbon capture and sequestration projects. The concept of handling rules and regulations in the form of big data will change the outlook and compliance management will become increasingly more agile and responsive. © 2019, Society of Petroleum Engineers","Artificial Intelligence; Carbon Capture; Machine Learning; Regulations & Policy; Sequestration",
"Balakir A., Yang A., Rosenbaum E.","An Interpretable Predictive Model for Early Detection of Hardware Failure","10.1109/IRPS45951.2020.9129615","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088359679&doi=10.1109%2fIRPS45951.2020.9129615&partnerID=40&md5=82497fc4060a325e7b0d69bec3fd29b4","This paper develops an accurate yet interpretable machine learning framework for predicting field failures from time-series diagnostic data with application to datacenter hard disk drive failure prediction. Interpretable models are accountable: model reasoning can be verified by a domain expert for critical reliability tasks. We develop an attention-augmented recurrent neural network that visualizes the temporal information used to generate predictions; visualizations correlate with physical expectations. Finally, we propose a clustering-based method for discovering failure modes. © 2020 IEEE.","Failure Prediction; Hard Disk Drives; Interpretable Prediction; Machine Learning; System Reliability","Digital storage; Forecasting; Recurrent neural networks; Reliability; Domain experts; Failure prediction; Hard Disk Drive; Hardware failures; Model reasonings; Predictive modeling; Reliability tasks; Temporal information; Predictive analytics"
"Balamurugan S.A.A., Mallick M.S.M., Chinthana G.","Improved prediction of dengue outbreak using combinatorial feature selector and classifier based on entropy weighted score based optimal ranking","10.1016/j.imu.2020.100400","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088950110&doi=10.1016%2fj.imu.2020.100400&partnerID=40&md5=14d65a93702da652d021cb893f16e1f7","The main objective of this work is to enhance the classification performance and to improve the accuracy of prediction for health care management systems. The proposed novel feature selection algorithm named Entropy Weighted Score based Optimal Ranking Algorithm (EWSORA) is shown to be an efficient and helpful algorithm for medical data analysis and prediction. The optimal feature subset selected by the proposed algorithm to easily identify the attributes (features) is responsible for the main cause of the disease. Under this analysis, the Dengue Dataset is framed by collecting the medical laboratory test reports of many patients as real-time samples from the various health centers of the Thanjavur zone of Tamilnadu. The observation is made on a real-time dataset with the proposed method, and the results obtained outperform the results of existing methods. © 2020","Classification; Dengue disease prediction; Feature selection; Healthcare; Machine learning","chymase; immunoglobulin E; accuracy; area under the curve; Article; artificial neural network; Bayesian learning; classifier; controlled study; data analysis; data mining; demography; dengue; entropy weighted score based optimal ranking algorithm; epidemic; feature selection algorithm; health care management; hospital patient; human; laboratory test; major clinical study; multilayer perceptron; outpatient; prediction; protein blood level; sensitivity and specificity; support vector machine"
"Bălan O., Cristea Ș., Moldoveanu A., Moise G., Leordeanu M., Moldoveanu F.","Towards a human-centered approach for vret systems: Case study for acrophobia","10.1007/978-3-030-49644-9_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089680958&doi=10.1007%2f978-3-030-49644-9_11&partnerID=40&md5=0311d7530ae6710fed433beaaa80ee42","This paper presents a human-centered methodology for designing and developing Virtual Reality Exposure Therapy (VRET) systems. By following the steps proposed by the methodology – Users analysis, Domain Analysis, Task Analysis and Representational Analysis, we developed a system for acrophobia therapy composed of 9 functional, interrelated modules which are responsible for patients, scenes, audio and graphics management, as well as for physiological monitoring and events triggering. The therapist visualizes in real time the patient’s biophysical signals and adapts the exposure scenario accordingly, as he can lower or increase the level of exposure. There are 3 scenes in the game, containing a ride by cable car, one by ski lift and a walk by foot in a mountain landscape. A reward system is implemented and emotion dimension ratings are collected at predefined points in the scenario. They will be stored and later used for constructing an automatic machine learning emotion recognition and exposure adaptation module. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2020.","Acrophobia; Exposure therapy; Human-centered; Phobia; Virtual reality",
"Bǎlan O., Cristea Ş., Moldoveanu A., Moise G., Leordeanu M., Moldoveanu F.","Towards a human-centered approach for VRET systems: Case study for acrophobia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091293126&partnerID=40&md5=40a401c2b6610c8293e5f61a4b3d798e","This paper presents a human-centered methodology for designing and developing Virtual Reality Exposure Therapy (VRET) systems. By following the steps proposed by the methodology - Users analysis, Domain Analysis, Task Analysis and Representational Analysis, we developed a system for acrophobia therapy composed of 9 functional, interrelated modules which are responsible for patients, scenes, audio and graphics management, as well as with physiological monitoring and event triggering. The therapist visualizes in real time the patient's biophysical signals and adapts the exposure scenario accordingly, as. he can lower or increase the level of exposure. There are 3 scenes in the game, depicting a ride by cable car, one by ski lift and a walk by foot in a mountain landscape. A reward system is implemented and emotion dimension ratings are collected at predefined points in the scenario. They will be stored and later used for constructing an automatic machine learning emotion recognition and exposure adaptation module. © ISD 2019. All rights reserved.","Acrophobia; Exposure therapy; Human-centered; Phobia; Virtual reality","Information use; Job analysis; Management information systems; Patient monitoring; Patient treatment; Turing machines; Adaptation module; Automatic machines; Domain analysis; Emotion recognition; Event-triggering; Physiological monitoring; Reward systems; Virtual reality exposure therapies; Information systems"
"Balashov Y.","The boundaries of meaning: a case study in neural machine translation","10.1080/0020174X.2022.2113429","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138396888&doi=10.1080%2f0020174X.2022.2113429&partnerID=40&md5=62a3f071ca65729145176a880fe2ec4d","The success of deep learning in natural language processing raises intriguing questions about the nature of linguistic meaning and ways in which it can be processed by natural and artificial systems. One such question has to do with subword segmentation algorithms widely employed in language modeling, machine translation, and other tasks since 2016. These algorithms often cut words into semantically opaque pieces, such as ‘period’, ‘on’, ‘t’, and ‘ist’ in ‘period|on|t|ist’. The system then represents the resulting segments in a dense vector space, which is expected to model grammatical relations among them. This representation may in turn be used to map ‘period|on|t|ist’ (English) to ‘par|od|ont|iste’ (French). Thus, instead of being modeled at the lexical level, translation is reformulated more generally as the task of learning the best bilingual mapping between the sequences of subword segments of two languages; and sometimes even between pure character sequences: ‘p|e|r|i|o|d|o|n|t|i|s|t’ → ‘p|a|r|o|d|o|n|t|i|s|t|e’. Such segmentations and alignments are at work in highly efficient end-to-end machine translation systems, despite their allegedly opaque nature. But do they have linguistic or philosophical plausibility? I attempt to cast light on this question, in the spirit of making artificial intelligence more transparent and explainable. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","computational linguistics; deep learning; neural machine translation; Opacity; subword segmentation",
"Balasubramanian J.B., Boes R.D., Gopalakrishnan V.","A novel approach to modeling multifactorial diseases using Ensemble Bayesian Rule classifiers","10.1016/j.jbi.2020.103455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086589677&doi=10.1016%2fj.jbi.2020.103455&partnerID=40&md5=6ac82be2301260b1dc0a38257b9d9e8c","Modeling factors influencing disease phenotypes, from biomarker profiling study datasets, is a critical task in biomedicine. Such datasets are typically generated from high-throughput ’omic’ technologies, which help examine disease mechanisms at an unprecedented resolution. These datasets are challenging because they are high-dimensional. The disease mechanisms they study are also complex because many diseases are multifactorial, resulting from the collective activity of several factors, each with a small effect. Bayesian rule learning (BRL) is a rule model inferred from learning Bayesian networks from data, and has been shown to be effective in modeling high-dimensional datasets. However, BRL is not efficient at modeling multifactorial diseases since it suffers from data fragmentation during learning. In this paper, we overcome this limitation by implementing and evaluating three types of ensemble model combination strategies with BRL— uniform combination (UC; same as Bagging), Bayesian model averaging (BMA), and Bayesian model combination (BMC)— collectively called Ensemble Bayesian Rule Learning (EBRL). We also introduce a novel method to visualize EBRL models, called the Bayesian Rule Ensemble Visualizing tool (BREVity), which helps extract interpret the most important rule patterns guiding the predictions made by the ensemble model. Our results using twenty-five public, high-dimensional, gene expression datasets of multifactorial diseases, suggest that, both EBRL models using UC and BMC achieve better predictive performance than BMA and other classic machine learning methods. Furthermore, BMC is found to be more reliable than UC, when the ensemble includes sub-optimal models resulting from the stochasticity of the model search process. Together, EBRL and BREVity provides researchers a promising and novel tool for modeling multifactorial diseases from high-dimensional datasets that leverages strengths of ensemble methods for predictive performance, while also providing interpretable explanations for its predictions. © 2020 Elsevier Inc.","Bayesian methods; Ensemble methods; Interpretability; Rule learning","Bayesian networks; Gene expression; Knowledge based systems; Bayesian model averaging; Biomarker profiling; Gene expression datasets; High dimensional datasets; Learning Bayesian networks; Machine learning methods; Multifactorial disease; Predictive performance; Learning systems; area under the curve; Article; Bayesian network; decision tree; Ensemble Bayesian Rule classifier; gene expression; learning algorithm; multifactorial genetic disorder; phenotype; prediction; priority journal; receiver operating characteristic; statistical analysis; statistical distribution; statistical model; Bayes theorem; machine learning; Bayes Theorem; Machine Learning"
"Balasubramanian K., Ananthamoorthy N.P., Ramya K.","An approach to classify white blood cells using convolutional neural network optimized by particle swarm optimization algorithm","10.1007/s00521-022-07279-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129297034&doi=10.1007%2fs00521-022-07279-1&partnerID=40&md5=260c4d0e13ae80121b65a0999b083fbd","Blood cell count is an important parameter in analysing a person’s health condition. White blood corpuscles (leukocytes) are responsible for deciding the immunity system of a person. White blood cells (WBC) are classified into: lymphocytes, monocytes and granulocytes. Granulocytes are sub-classified into: neutrophils, eosinophils, and basophils. These five types perform different functions in acting as a defence mechanism of the body. Manual investigation performed at laboratories for WBC count is prone to errors due to certain factors like human fatigue, inter-operability errors, etc. Further, there is serious issue in using trained data which cater to the changes in morphology of the white blood corpuscles, in order that trained classifiers could capitalize well. In a way to reduce misclassification rate, a methodology wherein a deep learning approach integrated with an evolutionary algorithm is proposed. Convolutional neural network (CNN) hyper-parameters were optimized using particle swarm optimization algorithm (PSO) to improve the network performance in classifying white blood cells into five types. The method is tested on merged LISC and BCCD datasets which achieved classification accuracy of 99.2% with 94.56% sensitivity, 98.78% specificity and 0.982 AUC. The results are compared with similar proven algorithms like genetic algorithms (GA), differential evolution (DE) and grey wolf optimization (GWO) algorithms. The experimental outcomes demonstrated PSO’s potential in optimizing the CNN hyper-parameters for white blood cell classification enhancing the sensitivity rate and serve a best second opinion in assessing blood cell count. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Blood; Classification; CNN; Leukocytes; PSO","Blood; Cells; Convolution; Convolutional neural networks; Cytology; Deep learning; Genetic algorithms; Particle swarm optimization (PSO); Blood; Blood cells; Cell count; Classifieds; Convolutional neural network; Health condition; Hyper-parameter; Leucocytes; Particle swarm optimization algorithm; White blood cells; Classification (of information)"
"Balasundaram R., Baskar N., Sankar R.S.","A new approach to generate dispatching rules for two machine flow shop scheduling using data mining","10.1016/j.proeng.2012.06.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900986673&doi=10.1016%2fj.proeng.2012.06.031&partnerID=40&md5=79f6082f3249604c8fa5d443c14153a2","This paper introduces a novel methodology for generating scheduling rules using data mining based approach to discover the dispatching sequence by applying learning algorithm directly to flow shop scheduling. Flow scheduling is one of the well-known combinatorial optimization problems. This paper considers the problem of finding schedule for two machine flow shop to minimize the make span using Decision Tree (DT) algorithm. This approach involves pre-processing of scheduling data into an appropriate data file, discovering the key scheduling concepts and representing of the data mining results in way that enables its use for scheduling. The advantages of DT's are that the dispatching rule is in the form of IF-Then else rules, which is easily understandable by the shop floor people. In decision tree based approach, the attribute selection greatly influences the predictive accuracy and hence this approach also considers creation of additional attributes. For two machine flow shop problem, the Johnson's Algorithm (JA) yields optimal solution. Hence the proposed approach is compared with Johnson's algorithm. The results show that both the methods yield the same result. The work is a complement to the traditional methods. © 2012 Published by Elsevier Ltd.","Data mining; Decision tree; Flow shop scheduling; Make span",
"Balazia M., Sojka P.","Learning robust features for gait recognition by Maximum Margin Criterion","10.1109/ICPR.2016.7899750","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996763089&doi=10.1109%2fICPR.2016.7899750&partnerID=40&md5=af3c03debbff5f609f755964eaf42d02","In the field of gait recognition from motion capture data, designing human-interpretable gait features is a common practice of many fellow researchers. To refrain from ad-hoc schemes and to find maximally discriminative features we may need to explore beyond the limits of human interpretability. This paper contributes to the state-of-the-art with a machine learning approach for extracting robust gait features directly from raw joint coordinates. The features are learned by a modification of Linear Discriminant Analysis with Maximum Margin Criterion so that the identities are maximally separated and, in combination with an appropriate classifier, used for gait recognition. Experiments on the CMU MoCap database show that this method outperforms eight other relevant methods in terms of the distribution of biometric templates in respective feature spaces expressed in four class separability coefficients. Additional experiments indicate that this method is a leading concept for rank-based classifier systems. © 2016 IEEE.",,"Discriminant analysis; Gait analysis; Learning algorithms; Learning systems; Additional experiments; Class separability; Classifier systems; Discriminative features; Linear discriminant analysis; Machine learning approaches; Maximum margin criterions; Motion capture data; Pattern recognition"
"Balazs K., Koczy L.T.","New parameterizable search space narrowing technique for adjusting between accuracy and interpretability in fuzzy systems","10.1109/CINTI.2012.6496783","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876907509&doi=10.1109%2fCINTI.2012.6496783&partnerID=40&md5=384fbb7bfb158a3a845056231dd8f877","It is well known that beyond the fact that fuzzy systems have favorable modeling capabilities from the viewpoint of accuracy, they also have outstanding inherent interpretability possibilities, which is a rather unique property among modeling architectures and which is a strong motivation for their research and application. This paper focuses on both mentioned property types and proposes a new technique for adjusting between accuracy and interpretability in modeling systems where fuzzy rule based architectures together with evolutionary algorithms are used for knowledge extraction. First, an inconsistency problem of conventional interpretable fuzzy systems is resolved. Then, a new search space narrowing technique for evolutionary algorithms is proposed, which can be applied for constructing interpretable fuzzy rule bases. Finally, the favorable properties of this new approach will be verified experimentally by carrying out simulation runs. © 2012 IEEE.","Fuzzy systems; Interpretability; Knowledge extraction","Fuzzy rule based; Interpretability; Interpretable fuzzy rules; Knowledge extraction; Modeling architecture; Modeling capabilities; Modeling systems; Research and application; Artificial intelligence; Extraction; Fuzzy rules; Information science; Fuzzy systems"
"Balbi M., Ristani A., Milanese G., Silva M., Eufrasia Ledda R., Milone F., Sartorio C., Tringali G., Sverzellati N.","The role of the radiologist in diagnosing the covid-19 infection. Parma experiences","10.23750/abm.v91i2.9564","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084787132&doi=10.23750%2fabm.v91i2.9564&partnerID=40&md5=fc123e9179fcff9e8b77fd93e3b73947","Summary. The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is a new virus responsible for the coronavirus disease 2019 (COVID-19), a respiratory disease that ranges from an asymptomatic or mild flulike illness to severe pneumonia, multiorgan failure, and death. Imaging might play an important role in clinical decision making by supporting rapid triage of patients with suspected COVID-19 and assessing supervening complications, such as super-added bacterial infection and thrombosis. Further studies will clarify the real impact of imaging on COVID-19 patients’ management and the potential role of radiology in future outbreaks. (www.actabiomedica.it). © Mattioli 1885.","COVID-19","Article; artificial intelligence; clinical decision making; comorbidity; computer assisted tomography; coronavirus disease 2019; disease severity; emergency health service; human; Italy; pandemic; prevalence; radiodiagnosis; radiologist; real time polymerase chain reaction; sensitivity and specificity; Betacoronavirus; Coronavirus infection; diagnostic imaging; virus pneumonia; x-ray computed tomography; Betacoronavirus; Coronavirus Infections; Humans; Italy; Pandemics; Pneumonia, Viral; Radiologists; Tomography, X-Ray Computed"
"Balcombe L., De Leo D.","Digital mental health challenges and the horizon ahead for solutions","10.2196/26811","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103667590&doi=10.2196%2f26811&partnerID=40&md5=33000c9c7bf101f0c3b42aebb04f3e48","The demand outstripping supply of mental health resources during the COVID-19 pandemic presents opportunities for digital technology tools to fill this new gap and, in the process, demonstrate capabilities to increase their effectiveness and efficiency. However, technology-enabled services have faced challenges in being sustainably implemented despite showing promising outcomes in efficacy trials since the early 2000s. The ongoing failure of these implementations has been addressed in reconceptualized models and frameworks, along with various efforts to branch out among disparate developers and clinical researchers to provide them with a key for furthering evaluative research. However, the limitations of traditional research methods in dealing with the complexities of mental health care warrant a diversified approach. The crux of the challenges of digital mental health implementation is the efficacy and evaluation of existing studies. Web-based interventions are increasingly used during the pandemic, allowing for affordable access to psychological therapies. However, a lagging infrastructure and skill base has limited the application of digital solutions in mental health care. Methodologies need to be converged owing to the rapid development of digital technologies that have outpaced the evaluation of rigorous digital mental health interventions and strategies to prevent mental illness. The functions and implications of human-computer interaction require a better understanding to overcome engagement barriers, especially with predictive technologies. Explainable artificial intelligence is being incorporated into digital mental health implementation to obtain positive and responsible outcomes. Investment in digital platforms and associated apps for real-time screening, tracking, and treatment offer the promise of cost-effectiveness in vulnerable populations. Although machine learning has been limited by study conduct and reporting methods, the increasing use of unstructured data has strengthened its potential. Early evidence suggests that the advantages outweigh the disadvantages of incrementing such technology. The limitations of an evidence-based approach require better integration of decision support tools to guide policymakers with digital mental health implementation. There is a complex range of issues with effectiveness, equity, access, and ethics (eg, privacy, confidentiality, fairness, transparency, reproducibility, and accountability), which warrant resolution. Evidence-informed policies, development of eminent digital products and services, and skills to use and maintain these solutions are required. Studies need to focus on developing digital platforms with explainable artificial intelligence–based apps to enhance resilience and guide the treatment decisions of mental health practitioners. Investments in digital mental health should ensure their safety and workability. End users should encourage the use of innovative methods to encourage developers to effectively evaluate their products and services and to render them a worthwhile investment. Technology-enabled services in a hybrid model of care are most likely to be effective (eg, specialists using these services among vulnerable, at-risk populations but not severe cases of mental ill health). © Luke Balcombe, Diego De Leo.","Challenges; COVID-19; Digital mental health implementation; Explainable artificial intelligence; Human-computer interaction; Hybrid model of care; Resilience; Technology","artificial intelligence; clinical outcome; confidentiality; coronavirus disease 2019; cost effectiveness analysis; decision support system; digital technology; ethics; evidence based medicine; health care access; health equity; high risk population; human; human computer interaction; investment; machine learning; mental disease; mental health; mental health care; mobile application; pandemic; policy; privacy; psychotherapy; reproducibility; Review; risk assessment; screening; vulnerable population; web-based intervention"
"Baldán F.J., Benítez J.M.","Multivariate times series classification through an interpretable representation","10.1016/j.ins.2021.05.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107161680&doi=10.1016%2fj.ins.2021.05.024&partnerID=40&md5=876173133ffca5a48eec973b8a2c71dc","Multivariate time series classification is a machine learning task with increasing importance due to the proliferation of information sources in different domains (economy, health, energy, crops, etc.). Univariate methods lack the ability to capture the relationships between the different variables that compose a multivariate time series and therefore cannot be directly extrapolated to multivariate environments. Despite the good performance and competitive results of the multivariate proposals published to date, they are hard to interpret due to their high complexity. In this paper, we propose a multivariate time series classification method based on an alternative representation of the time series, composed of a set of 41 descriptive time series features, in order to improve the interpretability of time series and results obtained. Our proposal uses traditional classifiers over the extracted features to look for relationships between the different variables that form a multivariate time series. We have selected four state-of-the-art algorithms as base classifiers to evaluate our method. We have tested our proposal on the complete University of East Anglia repository, obtaining highly interpretable results capable of explaining the relationships between the features that compose the time series and achieving performance results statistically indistinguishable from the best algorithms of the state-of-the-art. © 2021 The Author(s)","Classification; Complexity measures; Multivariate; Time series features; Time series interpretation","Time series; Complexity measures; Interpretable representation; Machine-learning; Multivariate; Multivariate time series; Multivariate time series classifications; Performance; Time series features; Time series interpretation; Classification (of information)"
"Baldassi C., Alemi-Neissi A., Pagan M., DiCarlo J.J., Zecchina R., Zoccolan D.","Shape Similarity, Better than Semantic Membership, Accounts for the Structure of Visual Object Representations in a Population of Monkey Inferotemporal Neurons","10.1371/journal.pcbi.1003167","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883374051&doi=10.1371%2fjournal.pcbi.1003167&partnerID=40&md5=1f0d10521f1a815b1519d63e46176450","The anterior inferotemporal cortex (IT) is the highest stage along the hierarchy of visual areas that, in primates, processes visual objects. Although several lines of evidence suggest that IT primarily represents visual shape information, some recent studies have argued that neuronal ensembles in IT code the semantic membership of visual objects (i.e., represent conceptual classes such as animate and inanimate objects). In this study, we investigated to what extent semantic, rather than purely visual information, is represented in IT by performing a multivariate analysis of IT responses to a set of visual objects. By relying on a variety of machine-learning approaches (including a cutting-edge clustering algorithm that has been recently developed in the domain of statistical physics), we found that, in most instances, IT representation of visual objects is accounted for by their similarity at the level of shape or, more surprisingly, low-level visual properties. Only in a few cases we observed IT representations of semantic classes that were not explainable by the visual similarity of their members. Overall, these findings reassert the primary function of IT as a conveyor of explicit visual shape information, and reveal that low-level visual properties are represented in IT to a greater extent than previously appreciated. In addition, our work demonstrates how combining a variety of state-of-the-art multivariate approaches, and carefully estimating the contribution of shape similarity to the representation of object categories, can substantially advance our understanding of neuronal coding of visual objects in cortex. © 2013 Baldassi et al.",,"Learning systems; Mammals; Multivariant analysis; Neurons; Semantics; Statistical Physics; Cortexes; Machine learning approaches; Multi variate analysis; Object representations; Shape information; Shape similarity; Visual areas; Visual information; Visual objects; Visual properties; Clustering algorithms; algorithm; animal cell; animal experiment; article; brain function; brain region; cluster analysis; computer analysis; controlled study; Haplorhini; inferior temporal cortex; machine learning; neurologic examination; nonhuman; semantics; stimulus response; synaptic transmission; task performance; visual cortex; visual discrimination; visual object processing; visual stimulation; Primates"
"Baldi P., Azencott C., Swamidass S.J.","Bridging the gap between neural network and kernel methods: Applications to drug discovery","10.3233/978-1-60750-692-8-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751660908&doi=10.3233%2f978-1-60750-692-8-3&partnerID=40&md5=71c5da3b51bb68e99899b3242564672a","We develop a hybrid machine learning architecture, the Influence Relevance Voter (IRV), where an initial geometry- or kernel- based step is followed by a feature-based step to derive the final prediction. While other implementations of the general idea are possible, we use a k-Nearest-Neighbor approach to implement the first step, and a Neural Network approach to implement the second step for a classification problem. In this version of the IRV, the rank and similarities of the k nearest neighbors of an input are used to compute their individual relevances. Relevances are combined multiplicatively with the class membership values to produce influences. Finally the influences of all the neighbors are aggregated to produce the final probabilistic prediction. IRVs have several advantages: they can be trained fast, they are easily interpretable and modifiable, and they are not prone to overfitting since they rely on extensive weight sharing across neighbors. The IRV approach is applied to the problem of predicting whether a given compound is active or not with respect to a particular biochemical assay in drug discovery and shown to perform well in comparison to other predictors. In addition, we also introduce and demonstrate a new approach, the Concentrated ROC (CROC), for assessing prediction performance in situations ranging from drug discovery to information retrieval, where ROC curves are not adequate, because only a very small subset of the top ranked positives is practically useful. The CROC approach uses a change of coordinates to smoothly magnify the relevant portion of the ROC curve. © 2011 The authors and IOS Press. All rights reserved.","Chemoinformatics; Drugs; Kernels; Neural Netowrks","Artificial intelligence; Forecasting; Learning systems; Membership functions; Motion compensation; Nearest neighbor search; Text processing; Chemoinformatics; Drugs; Hybrid machine learning; K-nearest neighbors; Kernels; Neural Netowrks; Prediction performance; Probabilistic prediction; Neural networks"
"Baldini G., Geneiatakis D.","A performance evaluation on distance measures in KNN for mobile malware detection","10.1109/CoDIT.2019.8820510","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072819070&doi=10.1109%2fCoDIT.2019.8820510&partnerID=40&md5=6b675fef2be1e43f859c96b97be826a7","Most of the related works on mobile malware detection for Android Operating System (OS) that are based on machine learning often use classifiers' default settings, and focus on opting either the optimal features or classifier. Even if this approach is understandable and it has proven to provide valuable results classifiers different hyper-parameters should be configured properly in order to achieve classifier's best performance. Thus, this paper investigates the performance of one of the most simple machine learning classifier, such as K Nearest Neighbor (KNN), considering its different hyper-parameters with emphasis on different distance measures. The authors have performed an extensive comparison using various well known distance measures over the Drebin data set. Results show that the proper choice of the distance measure can provide a significant enhancement to the classification accuracy. Specifically, the Euclidean distance that is mostly used for KNN is not the optimal option, instead other distance measures i.e., Hamming, CityBlock, can boost classifier's performance in the context of mobile malware detection. For instance, CityBlock can improve KNN false positive rate up to 33% in comparison to the Euclidean distance. © 2019 IEEE.","Machine learning; Mobile applications; Security","Clustering algorithms; Hamming distance; Learning systems; Machine learning; Machinery; Malware; Classification accuracy; Classifier's performance; Distance measure; Euclidean distance; False positive rates; K nearest neighbor (KNN); Mobile applications; Security; Nearest neighbor search"
"Baldock R.J.N., Maennel H., Neyshabur B.","Deep Learning Through the Lens of Example Difficulty",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126707225&partnerID=40&md5=f6f421989693bc5d805d747eaee620e0","Existing work on understanding deep learning often employs measures that compress all data-dependent information into a few numbers. In this work, we adopt a perspective based on the role of individual examples. We introduce a measure of the computational difficulty of making a prediction for a given input: the (effective) prediction depth. Our extensive investigation reveals surprising yet simple relationships between the prediction depth of a given input and the model’s uncertainty, confidence, accuracy and speed of learning for that data point. We further categorize difficult examples into three interpretable groups, demonstrate how these groups are processed differently inside deep models and showcase how this understanding allows us to improve prediction accuracy. Insights from our study lead to a coherent view of a number of separately reported phenomena in the literature: early layers generalize while later layers memorize; early layers converge faster and networks learn easy data and simple functions first. © 2021 Neural information processing systems foundation. All rights reserved.",,"Deep learning; Data dependent; Datapoints; Learn+; Prediction accuracy; Simple++; Through the lens; Uncertainty; Forecasting"
"Balemans D., Reiter P., Steckel J., Hellinckx P.","Resource efficient AI: Exploring neural network pruning for task specialization","10.1016/j.iot.2022.100599","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136693262&doi=10.1016%2fj.iot.2022.100599&partnerID=40&md5=0cedc0b53847b19b6e296f98bc352eeb","This paper explores the use of neural network pruning for transfer learning applications for more resource-efficient inference. The goal is to focus and optimize a neural network on a smaller specialized target task. With the advent of IoT, we have seen an immense increase in AI-based applications on mobile and embedded devices, such as wearables and other smart appliances. However, with the ever-increasing complexity and capabilities of machine learning algorithms, this push to the edge has led to new challenges due to the constraints imposed by the limited availability of resources on these devices. Some form of compression is needed to allow for state-of-the-art convolutional neural networks to run on edge devices. In this work, we adapt existing neural network pruning methods to allow them to specialize networks to only focus on a subset of what they were originally trained for. This is a transfer learning use-case where we optimize large pre-trained networks. This differs from standard optimization techniques by allowing the network to forget certain concepts and allow the network's footprint to be even smaller. We compare different pruning criteria, including one from the field of Explainable AI (XAI), to determine which technique allows for the smallest possible network while maintaining high performance on the target task. Our results show the benefits of using network specialization when executing neural networks on embedded devices both with and without GPU acceleration. © 2022 The Authors","Edge inference; Explainable AI; Machine learning; Neural network compression; Neural network pruning",
"Balemans D., Casteels W., Vanneste S., de Hoog J., Mercelis S., Hellinckx P.","Resource efficient sensor fusion by knowledge-based network pruning","10.1016/j.iot.2020.100231","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092914514&doi=10.1016%2fj.iot.2020.100231&partnerID=40&md5=929ffd6d412b256c3f217dc0d213f507","The perception of the environment is key for autonomous driving applications. To increase the accuracy of perception in different environmental contexts vehicles can rely on both camera and LiDAR sensors that provide complementary information about the same features. Therefore, a sensor fusion method can improve the detection accuracy by combining the information of both sensors. Recently, many sensor fusion methods have been proposed that rely on deep neural networks that typically require a lot of resources to be executed in real-time. Therefore, we propose a resource efficient sensor fusion approach with a new neural network optimization method called knowledge-based pruning. The general principle is to prune the neural network guided by the location of the knowledge within the network that is unveiled with explainable AI methods. More specifically, in this work we propose a pruning method that uses layer-wise relevance propagation (LRP) to localize the network knowledge. The considered sensor fusion method uses off-the-shelve pretrained networks which we optimize for our application using the LRP pruning method. This can be used as a form of transfer learning as a pretrained model is optimized to be applied for a subset of the tasks it was originally trained for. © 2020","Deep learning; Explainable AI; Neural network pruning; Sensor fusion",
"Baleshta C., White D., Reavie G., Cooper A., Taylor G., Skorburg J.A.G., Bruwaene D.V., Gignac S., Schmidt C., McDonald L., Thaine P., Ryan C., Luan R.","CARE-AI special session on AI ethics","10.1109/ISTAS52410.2021.9629130","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123163861&doi=10.1109%2fISTAS52410.2021.9629130&partnerID=40&md5=91bda72c64169a7f8f52998ad73f7710","This special session organized by the Centre for Advancing Responsible and Ethical Artificial Intelligence (CARE-AI) consists of two 90-minute parts, focusing on two groups at the frontline of AI Ethics: Students and start-up founders. Part 1 is a student-led AI Ethics paper presentation and critique: Two students from the Philosophy program will present original work, 'Analyzing Distrust in Human Interactions with AI,' and 'Enactivism and Modelling Human Behaviour in AI,' (20 min); each presentation will be followed by a prepared critique from a student in the Collaborative Specialization in AI (10 min) and a 15-minute general discussion with the audience. Part 2 is an AI Ethics start-up showcase: 5 Canadian start-up companies (whose products or services either present an AI Ethics dilemma or propose a solution) will present 5-minute pitches, which will each be followed by 5 minutes of expert commentary and 5 minutes of open discussion. © 2021 IEEE.","AI ethics; black boxing; Canada; human behaviour; machine learning; tech start-up","Behavioral research; Ethical technology; Machine learning; AI ethic; Black boxing; Canada; Frontline; Human behaviors; Humaninteraction; Specialisation; Start-up companies; Tech start-up; Students"
"Balkır E., Kiritchenko S., Nejadgholi I., Fraser K.C.","Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139410705&partnerID=40&md5=a284d448dc0618ddcc7b9e9787078d3b","Motivations for methods in explainable artificial intelligence (XAI) often include detecting, quantifying and mitigating bias, and contributing to making machine learning models fairer. However, exactly how an XAI method can help in combating biases is often left unspecified. In this paper, we briefly review trends in explainability and fairness in NLP research, identify the current practices in which explainability methods are applied to detect and mitigate bias, and investigate the barriers preventing XAI methods from being used more widely in tackling fairness issues. © 2022 Association for Computational Linguistics.",,"Artificial intelligence; Computational linguistics; Current practices; Machine learning models; Natural language processing systems"
"Ball J.E., Anderson D.T., Chan C.S.","Comprehensive survey of deep learning in remote sensing: Theories, tools, and challenges for the community","10.1117/1.JRS.11.042609","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032865390&doi=10.1117%2f1.JRS.11.042609&partnerID=40&md5=51c93993ddf0601eab19bf381ddf9762","In recent years, deep learning (DL), a rebranding of neural networks (NNs), has risen to the top in numerous areas, namely computer vision (CV), speech recognition, and natural language processing. Whereas remote sensing (RS) possesses a number of unique challenges, primarily related to sensors and applications, inevitably RS draws from many of the same theories as CV, e.g., statistics, fusion, and machine learning, to name a few. This means that the RS community should not only be aware of advancements such as DL, but also be leading researchers in this area. Herein, we provide the most comprehensive survey of state-of-the-art RS DL research. We also review recent new developments in the DL field that can be used in DL for RS. Namely, we focus on theories, tools, and challenges for the RS community. Specifically, we focus on unsolved challenges and opportunities as they relate to (i) inadequate data sets, (ii) human-understandable solutions for modeling physical phenomena, (iii) big data, (iv) nontraditional heterogeneous data sources, (v) DL architectures and learning algorithms for spectral, spatial, and temporal data, (vi) transfer learning, (vii) an improved theoretical understanding of DL systems, (viii) high barriers to entry, and (ix) training and optimizing the DL. © 2017 The Authors.","big data; computer vision; deep learning; hyperspectral; multispectral; remote sensing","Big data; Computer vision; Learning algorithms; Natural language processing systems; Remote sensing; Speech recognition; Surveys; Heterogeneous data sources; HyperSpectral; Multi-spectral; NAtural language processing; Neural networks (NNS); Physical phenomena; State of the art; Transfer learning; Deep learning"
"Ball R., Kruger H., Drevin L.","A Unified Approach to Anomaly Detection","10.1007/978-3-030-64580-9_24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101400825&doi=10.1007%2f978-3-030-64580-9_24&partnerID=40&md5=678281f2130d4e7048f9c9442f26f205","Anomalous actors are becoming increasingly sophisticated both in methodology and technical ability. Fortunately, the companies impacted by anomalous behaviour are also generating more data and insights on potential anomalous cases than ever before. In this paper, a unified approach to managing the complexity of constructing a useable anomaly detection system is presented. The unified approach is comprised of three algorithms, a Neural Architecture Search (NAS) implementation for autoencoders, an anomaly score threshold optimisation algorithm, and a Gaussian scaling function for anomaly scores. NAS is applied to a data set containing instances of credit card fraud. The NAS algorithm is used to simulate a population of 50 candidate deep learning architectures, with the best performing architecture being selected based on a balanced score, comprised of an average of the Area under the ROC curve, Average Precision and normalised Matthews Correlation Coefficient scores. The threshold optimisation algorithm is used to determine the appropriate threshold between the classes, for the purposes of producing the binary classification outcome of each architecture. The Gaussian scaling algorithm is applied to the raw anomaly scores of the optimal architecture into order to generate useable probability scores. Not only did the proposed unified approach simplify the process of selecting an optimal neural architecture whose output is interpretable by business practitioners and comparable with other probability score producing models, but it also contributes to anomaly detection in a transactional setting by eliminating subjective thresholds when classifying anomalous transactions. © 2020, Springer Nature Switzerland AG.","Anomaly detection; Artificial intelligence; Decision support systems; Evolutionary computations; Neural networks","Data Science; Deep learning; Learning systems; Optimization; Anomaly detection systems; Area under the ROC curve; Binary classification; Business practitioners; Correlation coefficient; Learning architectures; Neural architectures; Optimal architecture; Anomaly detection"
"Ballard S., Chappell K.M., Kennedy K.","Judgment call the game: Using value sensitive design and design fiction to surface ethical concerns related to technology","10.1145/3322276.3323697","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070638394&doi=10.1145%2f3322276.3323697&partnerID=40&md5=790567bcc479e15f920587f6fe5c3f9d","Artificial intelligence (AI) technologies are complex socio-technical systems that, while holding much promise, have frequently caused societal harm. In response, corporations, non-profits, and academic researchers have mobilized to build responsible AI, yet how to do this is unclear. Toward this aim, we designed Judgment Call, a game for industry product teams to surface ethical concerns using value sensitive design and design fiction. Through two industry workshops, we found Judgment Call to be effective for considering technology from multiple perspectives and identifying ethical concerns. This work extends value sensitive design and design fiction to ethical AI and demonstrates the game's effective use in industry. © 2019 Copyright is held by the owner/author(s).","Design ethics; Design fiction; Design method; Design tool; Ethical artificial intelligence; Product teams; Responsible artificial intelligence; Responsible innovation; Stakeholders; Value sensitive design","Artificial intelligence; Philosophical aspects; Design ethics; Design fictions; Design method; Design tool; Product teams; Stakeholders; Value sensitive design; Product design"
"Ballesteros A., Proenza J., Palmer P.","Towards a dynamic task allocation scheme for highly-reliable adaptive distributed embedded systems","10.1109/ETFA.2017.8247773","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044472699&doi=10.1109%2fETFA.2017.8247773&partnerID=40&md5=489b452c7497c76a5dc9ad1288d18508","An adaptive distributed embedded system is able to automatize some processes at the same time it modifies its behaviour autonomously and dynamically in response to changing operating conditions. To support adaptivity it is necessary that the underlying Distributed Embedded System (DES) is able to dynamically change the assignment of the processing and network resources. In this regard, the DFT4FTT project aims at providing a complete DES that can support applications with real-time, reliability and adaptivity requirements. This paper describes the first steps towards the design of the task allocation scheme used in the DFT4FTT architecture, responsible for dynamically distributing the workload among the nodes of the DES, taking into account the changes in the environment and in the system itself. This allocation scheme not only provides flexibility from a functional point of view, but also from a fault tolerance point of view. Moreover, its modular design makes it possible to tune the desired level of autonomy in the adaptivity, from a simple support for application reconfiguration to a complete automatic reconfiguration assisted with machine learning algorithms. © 2017 IEEE.",,"Factory automation; Fault tolerance; Learning algorithms; Learning systems; Application reconfiguration; Automatic reconfiguration; Changing operating conditions; Distributed embedded system; Dynamic task allocation; Functional points; Level of autonomies; Network resource; Embedded systems"
"Ballingall S., Sarvi M., Sweatman P.","Safety Assurance Concepts for Automated Driving Systems","10.4271/2020-01-0727","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083845038&doi=10.4271%2f2020-01-0727&partnerID=40&md5=2beabc796298ea59ee739837c6115829","Automated driving systems (ADSs) for road vehicles are being developed that can perform the entire dynamic driving task without a human driver in the loop. However, current regulatory frameworks for assuring vehicle safety may restrict the deployment of ADSs that can use machine learning to modify their functionality while in service. A review was undertaken to identify and assess key initiatives and research relevant to the safety assurance of adaptive safety-critical systems that use machine learning, and to highlight assurance concepts that could benefit from further research. The primary objective was to produce findings and recommendations that can inform policy and regulatory reform relating to ADS safety assurance. Due to the almost infinite number and combination of scenarios that an ADS could encounter, the review found much support for concepts that involve the use of simulation data as virtual evidence of safety compliance, with suggestions of a need to assure simulation tools and models. Real-world behavioural competency testing was also commonly proposed, although noting this concept has its limitations. The concept of whole-of-life assurance was identified, supported by various versions of dynamic runtime monitoring, verification and validation. Concerns regarding Artificial Intelligence (AI) robustness were raised, particularly regarding adversarial inputs and unmodelled scenarios that are essentially unknown unknowns. Further, the concept of explainable AI was highlighted as having potential to provide evidence from an ADS that could support safety assurance and regulatory compliance. While each of the identified assurance concepts should be considered when developing ADS safety assurance framework options, it is recommended that further research on each concept should be progressed. © 2020 SAE International. All Rights Reserved.",,"Deceleration; Machine learning; Regulatory compliance; Road vehicles; Automated driving systems; Behavioural competencies; Regulatory frameworks; Runtime Monitoring; Safety compliances; Safety critical systems; Verification-and-validation; Virtual evidences; Safety engineering"
"Balmer R.E., Levin S.L., Schmidt S.","Artificial Intelligence Applications in Telecommunications and other network industries","10.1016/j.telpol.2020.101977","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085073136&doi=10.1016%2fj.telpol.2020.101977&partnerID=40&md5=83c2073ab4bfb2aff7277bbacb57567a","Artificial intelligence applications in network industries have the potential to reduce network roll-out and operating costs, improve performance, enhance customer service, and support the development and introduction of new services. This paper identifies and analyzes the range of AI applications already in place and those expected to be in place in the near future. After analyzing the applications found in the literature, we have conducted interviews with senior executives responsible for AI in telecommunications, electricity, gas and water companies headquartered in North America and Europe, as well as with suppliers incorporating AI into their equipment. These interviews have provided information on which AI applications are actually being used in these network industries and which are expected to be implemented in the future. While some of these AI applications, like predictive maintenance, have already been implemented today by most operators, more complex applications, such as traffic management AIs for 5G, are being considered by only a few large operators. Finally, the paper explores the consequences of AI for regulation. At the least, regulators will need to understand AI in order to determine if AI applications raise regulatory concerns. At this point, however, as AI becomes increasingly integrated into network operations, regulatory issues are only beginning to emerge. © 2020","Artificial intelligence; Economics; Machine learning; Network industries; Regulation; Telecoms","5G mobile communication systems; Artificial intelligence; Gas industry; Oil field equipment; Operating costs; Service industry; AI applications; Complex applications; Customer services; Improve performance; Network industries; Network operations; Regulatory issues; Traffic management; Telecommunication industry"
"Balogh V., Diochnos D.I., Berend G., Turán G.","Understanding the semantic content of sparse word embeddings using a commonsense knowledge base",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105470944&partnerID=40&md5=4e61b93871c785aa05179efd2a4479b8","Word embeddings have developed into a major NLP tool with broad applicability. Understanding the semantic content of word embeddings remains an important challenge for additional applications. One aspect of this issue is to explore the interpretability of word embeddings. Sparse word embeddings have been proposed as models with improved interpretability. Continuing this line of research, we investigate the extent to which human interpretable semantic concepts emerge along the bases of sparse word representations. In order to have a broad framework for evaluation, we consider three general approaches for constructing sparse word representations, which are then evaluated in multiple ways. We propose a novel methodology to evaluate the semantic content of word embeddings using a commonsense knowledge base, applied here to the sparse case. This methodology is illustrated by two techniques using the ConceptNet knowledge base. The first approach assigns a commonsense concept label to the individual dimensions of the embedding space. The second approach uses a metric, derived by spreading activation, to quantify the coherence of coordinates along the individual axes. We also provide results on the relationship between the two approaches. The results show, for example, that in the individual dimensions of sparse word embeddings, words having high coefficients are more semantically related in terms of path lengths in the knowledge base than the ones having zero coefficients. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Knowledge based systems; Semantics; Commonsense knowledge base; Individual dimensions; Interpretability; Novel methodology; Semantic concept; Semantic content; Spreading activations; Word representations; Embeddings"
"Balogun V., Sarumi O.A., Obe O.O.","A Context-Sensitive Cloud-Based Data Analytic Mobile Alert and Optimal Route Discovery System for Rural and Urban ITS Penetration","10.1007/978-3-030-71454-3_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104486150&doi=10.1007%2f978-3-030-71454-3_3&partnerID=40&md5=0f975778d83c468ec9540193c26a4ddf","The rapid growth in the number of road users and poor road management have been deemed responsible for the upsurge in road congestions and fatalities in recent times. Many of the lives lost was due to inadequate or inefficient public-accessible alerts system and rerouting mechanisms during emergencies. The Intelligent Transportation System (ITS) was anticipated as a solution to the numerous road networks usage problems. Recently, some developed countries have implemented some forms of ITS initiatives. But the transition of the road networks to a fully integrated ITS has been slow and daunting due to the huge cost of implementation. The use of mobile devices as backbone infrastructure for ITS networks during public emergencies has been proposed. Despite the advantage of being a cheap alternative, low computing power of mobile devices limit their potentials to support the expected Big Data ITS traffic. In this paper, we propose a cloud-based context-sensitive ITS infrastructure that uses the cloud as a primary aggregator of traffic messages plus a hybrid Data Analytics algorithm. The algorithm combines the enhanced features of Apache-Spark and Kafka frameworks blended with collaborative filtering using the ensemble machine learning classifier. The novelty of our approach stems from its ability to provide load balancing routing services based on the users’ profiles, and avoid congestion-using the Dynamic Round Robin scheduling algorithm to reroute users with similar profiles. © 2021, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.","Cloud; Context-sensitive; Data analytics; ITS; Mobile alert; Road incidences","Balancing; Collaborative filtering; Data Analytics; Highway planning; Intelligent systems; Machine learning; Motor transportation; Roads and streets; Traffic control; Context sensitive; Developed countries; Dynamic round robin; Fully integrated; Intelligent transportation systems; Load-balancing routing; Public emergencies; Rural and urban; Intelligent vehicle highway systems"
"Balta M., Felea V.","Using Shannon entropy in ETL processes","10.1109/SYNASC.2007.41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-48049111599&doi=10.1109%2fSYNASC.2007.41&partnerID=40&md5=58af6c5f6d561d41ca3986d02017a599","The ETL (Extract, Transform and Load) processes are responsible for the extraction of the data from the external sources, transforming the data in order to satisfy the integration and cleanness needs and for loading the data into the data warehouse. In the data mining field, there is u special concern on using the metrics for efficient classification algorithms. One of these approaches is the one that uses metrics on partitions, based on the Shannon entropy, to study the degree of concentration of values. In this paper we show how this idea can be used in verification of the consistency of data loaded into the data warehouse by ETL processes. We calculate the Shannon entropy and Gini index on partitions induced by attribute sets and we show that these values can be used to signal a possible problem in the data extraction process. We also show how the choice of the set of attributes determining the partition can have a significant impact on the effectiveness of the method. © 2008 IEEE.",,"Data warehouses; Extraction; Metadata; Attribute sets; Classification algorithm; Data extraction; ETL process; External sources; Extract , transform and loads; Gini Index; Shannon entropy; Data mining"
"Balusamy B., Venkatakrishna P., Vaidhyanathan A., Ravikumar M., Munisamy N.D.","Enhanced security framework for data integrity using third-party auditing in the cloud system","10.1007/978-81-322-2135-7_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912571623&doi=10.1007%2f978-81-322-2135-7_4&partnerID=40&md5=730069b8503f463539d619b3d92659a3","Cloud computing is an evolving paradigm that has been resulted as an adoption of available technologies. Although cloud technology allows users to take more benefits from available infrastructures, and virtualization, which is an enabling technology provided by the cloud allows users to manage and use the resources in an efficient and easiest manner, it does not guarantee data integrity and security over the resources stored in the cloud. Though many security frameworks have been developed for the cloud, still there may be loss of data or loss of control over data uploaded into the cloud. And also, many solutions for data integrity by third-party auditor are available but they are semi-trustable, if third-party auditors (TPA) compromise unauthorized access over the resource in the cloud. Hence, our proposed scheme focuses on extended framework that guarantees data integrity by involving data owner to perform auditing on the outsourced data in the cloud. And so, our proposed scheme achieves data integrity and guarantees security to the data owners for their resource in the cloud to major extent. Therefore, this type of TPA approach creates awareness to the data owner of their resource and thereby guarantees data integrity for every resources stored in the cloud. © Springer India 2015.","Cloud service provider; Data integrity; Third-party auditor","Artificial intelligence; Evolutionary algorithms; Network security; Cloud service providers; Cloud technologies; Data integrity; Enabling technologies; Outsourced datum; Security frameworks; Third-party auditor; Unauthorized access; Information management"
"Balwani A.H., Dyer E.L.","Modeling Variability in Brain Architecture with Deep Feature Learning","10.1109/IEEECONF44664.2019.9048805","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083335087&doi=10.1109%2fIEEECONF44664.2019.9048805&partnerID=40&md5=72759f70d21dbbaafdbab619286ed7fa","The brain has long been divided into distinct areas based upon its local microstructure, or patterned composition of cells, genes, and proteins. While this taxonomy is incredibly useful and provides an essential roadmap for comparing two brains, there is also immense anatomical variability within areas that must be incorporated into models of brain architecture. In this work we leverage the expressive power of deep neural networks to create a data-driven model of intra-and inter-brain area variability. To this end, we train a convolutional neural network that learns relevant microstructural features directly from brain imagery. We then extract features from the network and fit a simple classifier to them, thus creating a simple, robust, and interpretable model of brain architecture. We further propose and show preliminary results for the use of features from deep neural networks in conjunction with unsupervised learning techniques to find fine-grained structure within brain areas. We apply our methods to micron-scale X-ray microtomography images spanning multiple regions in the mouse brain and demonstrate that our deep feature-based model can reliably discriminate between brain areas, is robust to noise, and can be used to reveal anatomically relevant patterns in neural architecture that the network wasn't trained to find. © 2019 IEEE.","brain architecture; convolutional neural networks; Deep learning; feature extraction; unsupervised learning","Brain; Brain mapping; Computer circuits; Convolutional neural networks; Deep neural networks; Learning systems; Mammals; Network architecture; Anatomical variability; Deep feature learning; Feature based modeling; Fine-grained structure; Microstructural features; Modeling variability; Neural architectures; X ray microtomography; Deep learning"
"Bamdev P., Grover M.S., Singla Y.K., Vafaee P., Hama M., Shah R.R.","Automated Speech Scoring System Under The Lens: Evaluating and interpreting the linguistic cues for language proficiency","10.1007/s40593-022-00291-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126275895&doi=10.1007%2fs40593-022-00291-5&partnerID=40&md5=f00becc4ac7b915f75687944c819cbd6","English proficiency assessments have become a necessary metric for filtering and selecting prospective candidates for both academia and industry. With the rise in demand for such assessments, it has become increasingly necessary to have the automated human-interpretable results to prevent inconsistencies and ensure meaningful feedback to the second language learners. Feature-based classical approaches have been more interpretable in understanding what the scoring model learns. Therefore, in this work, we utilize classical machine learning models to formulate a speech scoring task as both a classification and a regression problem, followed by a thorough study to interpret and study the relation between the linguistic cues and the English proficiency level of the speaker. First, we extract linguist features under five categories (fluency, pronunciation, content, grammar and vocabulary, and acoustic) and train models to grade responses. In comparison, we find that the regression-based models perform equivalent to or better than the classification approach. Second, we perform ablation studies to understand the impact of each of the feature and feature categories on the performance of proficiency grading. Further, to understand individual feature contributions, we present the importance of top features on the best performing algorithm for the grading task. Third, we make use of Partial Dependence Plots and Shapley values to explore feature importance and conclude that the best performing trained model learns the underlying rubrics used for grading the dataset used in this study. © 2022, International Artificial Intelligence in Education Society.","Automatic speech scoring; Classical models; Classification; Features extraction; Interpretability; Regression","Classification (of information); Linguistics; Regression analysis; Automatic speech; Automatic speech scoring; Classical modeling; Features extraction; Interpretability; Language proficiency; Learn+; Prospectives; Scoring systems; Second language learners; Grading"
"Bamler R., Mandt S.","Dynamic word embeddings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047013049&partnerID=40&md5=a8484ac8c0d25e94f9fb92ec6b7d7b53","We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec (Mikolov et al., 2013b). These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms-skipgram smoothing and skip-gram filtering-that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices. © 2017 International Machine Learning Society (IMLS). All rights reserved.",,"Artificial intelligence; Inference engines; Semantics; Context vector; Diffusion process; Predictive likelihoods; Probabilistic language; Semantic evolution; Static model; Variational inference; Word embedding; Learning systems"
"Banavalikar B., Bhat A., Joshi A., Talavar P., Hegade P.","Anveshan - A Model for Search","10.1016/j.procs.2020.04.256","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086630038&doi=10.1016%2fj.procs.2020.04.256&partnerID=40&md5=26aff4afdab4b85575eaee78a04bc8d4","Search and the engines performing them have witnessed significant transformations over time. While the web wants to be semantic, search demands to be customized, and every attached entity wants to learn; the challenges magnify leaving behind intricate woven questions to be addressed and resolved. The concept of machine learning challenges the designer to exploit the full potential of a machine capability. Through this paper, we propose a model - Anveshan, which uncovers the said challenges by inducing a learning model. The model carries hooks to establish the parameters, uses the user-defined initial weights, and the humongous data available on the web. It then prepares a list of results based on the considered parameters. The results learn and rearrange based on user clicks and preferences. The model also produces a bit string for the considered hooks, indices it, and makes it agent readable. The intention to create a universal bit string is to make it machine-understandable and contribute towards the semantic web. The paper discusses the model and then presents a specific case study in results and discussion. Anveshan works towards creating a learnable exploration space as per the user specifications. Anveshan is a fork-able model overlaid with a custom environment. The results are promising to guide towards a better search space. © 2020 The Authors. Published by Elsevier B.V.","agent; anveshan; hooks; semantic","Semantic Web; Bit-strings; Initial weights; Learning models; Machine capabilities; Search spaces; Learning systems"
"Bandi A., Yalamarthi S.","Towards Artificial Intelligence Empowered Security and Privacy Issues in 6G Communications","10.1109/ICSCDS53736.2022.9760857","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130091355&doi=10.1109%2fICSCDS53736.2022.9760857&partnerID=40&md5=2bbf5f51412ebb291c1a949c5b3683cd","Applications of wireless networks beyond 5G are vulnerable to various security and privacy concerns. This research aims to identify the security and privacy flaws beyond 5G network applications and their defense mechanisms. This research study has reviewed 44 research articles and presented the taxonomy of several concerns in security and privacy of artificial intelligence and machine learning empowered 6G applications. Furthermore, this study has identified the defense mechanism technologies for such issues. A few defense mechanism approaches are homographic encryption, physical layer authentication, biometric authentication, explainable, trustworthy, and superintelligent AI. The implications of this study are applicable to prevent attacks, eavesdropping, jamming, and other security and privacy concerns when developing wireless networks. © 2022 IEEE.","6G; blockchain; machine learning; privacy; quantum communication; security","Authentication; Blockchain; Machine learning; Network security; Quantum communication; Quantum cryptography; Secure communication; Wireless networks; 6g; Block-chain; Defence mechanisms; Network applications; Privacy; Privacy concerns; Research studies; Security; Security and privacy; Security and privacy issues; 5G mobile communication systems"
"Bandiera A., Bedini G., Chakravartula S.S.N., Massantini R., Moscetti R.","Computer vision for the development of smart drying processes [Impiego della visione artificiale per lo sviluppo di processi di disidratazione smart]",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130252212&partnerID=40&md5=1f7a36c1746571271d2f805f379f0309","Drying is a widely used process in the food industry. Its empirical execution can be responsible for high energy consumption and lowquality food. The aim of the study was to implement inline sensors (camera and load cells) in a cabinet dryer to monitor colour and moisture changes in carrot slices (5mm thick). Drying was carried out for 36 hours at 35 °C, 35 % R.H. and an air flow of 2.98 ms1. Inline acquisitions were performed every 5 min. The system successfully monitored changes in [i] luminance and hue angle, [ii] size and [iii] weight in the product. The linear relationship between size and weight was used to develop a PLS model to predict moisture content in the product. The model was characterised by performance metrics (RMSE 0.174; BIAS 0.170; AdjR2 0.99) which were comparable to the most common thin layer models (NetwonLew is; Page; Handerson &amp; Pabis; Logarithmic). The research lays the foundation for the development of dryers equipped with artificial intelligence to monitor and control the process through computer vision. © 2022 Chiriotti Editori. All rights reserved.","computer vision; inline monitoring; linear regression; thinlayer drying",
"Banditwattanawong T., Jankasem A.M.P., Masdisornchote M.","Hybrid data analytic technique for grading fairness","10.1108/DTA-01-2022-0047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129325673&doi=10.1108%2fDTA-01-2022-0047&partnerID=40&md5=77f2a1e3a8f5e108cb35882b339fece8","Purpose: Fair grading produces learning ability levels that are understandable and acceptable to both learners and instructors. Norm-referenced grading can be achieved by several means such as z score, K-means and a heuristic. However, these methods typically deliver the varied degrees of grading fairness depending on input score data. Design/methodology/approach: To attain the fairest grading, this paper proposes a hybrid algorithm that integrates z score, K-means and heuristic methods with a novel fairness objective function as a decision function. Findings: Depending on an experimented data set, each of the algorithm's constituent methods could deliver the fairest grading results with fairness degrees ranging from 0.110 to 0.646. We also pointed out key factors in the fairness improvement of norm-referenced achievement grading. Originality/value: The main contributions of this paper are four folds: the definition of fair norm-referenced grading requirements, a hybrid algorithm for fair norm-referenced grading, a fairness metric for norm-referenced grading and the fairness performance results of the statistical, heuristic and machine learning methods. © 2022, Emerald Publishing Limited.","Algorithm; Clustering; Decision function; Ensemble technique; Fair assessment; Fairness measurement; Heuristic; Hybrid technique; K-means; Norm-referenced achievement; Student grading; Z score",
"Banditwattanawong T., Masdisornchote M.","On Characterization of Norm-Referenced Achievement Grading Schemes toward Explainability and Selectability","10.1155/2021/8899649","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102062367&doi=10.1155%2f2021%2f8899649&partnerID=40&md5=753adc83677678b588e79433cff48669","Grading is the process of interpreting learning competence to inform learners and instructors of the current learning ability levels and necessary improvement. For norm-referenced grading, the instructors use a conventionally statistical method, z score. It is difficult for such a method to achieve explainable grade discrimination to resolve dispute between learners and instructors. To solve such difficulty, this paper proposes a simple and efficient algorithm for explainable norm-referenced grading. Moreover, the rise of artificial intelligence nowadays makes machine learning techniques attractive to the norm-referenced grading in general. This paper also investigates two popular clustering methods, K-means and partitioning around medoids. The experiment relied on the data sets of various score distributions and a metric, namely, Davies-Bouldin index. The comparative evaluation reveals that our algorithm overall outperforms the other three methods and is appropriate for all kinds of data sets in almost all cases. Our findings however lead to a practically useful guideline for the selection of appropriate grading methods including both clustering methods and z score. © 2021 Thepparit Banditwattanawong and Masawee Masdisornchote.",,
"Bandyopadhyay P., Dey C., Biswas P.","A Comparative Analysis Approach of Unsupervised Techniques to Explore Their Potentiality in Microarray Data.","10.1109/ICCCA49541.2020.9250833","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097655425&doi=10.1109%2fICCCA49541.2020.9250833&partnerID=40&md5=288e7356f6f622727dbac30846cb643c","Clustering is a very useful machine learning technique to find the underlying classification of unlabeled data. In computational biology, clustering techniques are extensively used to identify a group of biomolecules responsible for biological activity in animals. There are several types of knowledge acquisition techniques are used in Clustering analysis. Most of them are work best for a particular data type. Now, it is very difficult for researchers to choose appropriate clustering technique for a specific dataset. Therefore, we present a comprehensive comparative analysis of broadly classified clustering techniques over the biological dataset. Here we consider 4 types of datasets for our experiment. As a result of this comparative analysis, we found that using the partition base algorithm on all the data, the K-means clustering algorithm is giving the better results and in the case of the non-partition base algorithm, the Hierarchical clustering algorithm using complete linkage method is giving the better results than others. This study will further help us to find more efficient clustering technique that works well with all type of biological $dataset$. © 2020 IEEE.","Clustering; FCM; Hierarchical Clustering.; K-means; Microarray; Silhouette indexing","Bioactivity; Hierarchical clustering; Knowledge acquisition; Learning systems; Molecular biology; Clustering analysis; Clustering techniques; Comparative analysis; Complete linkage; Computational biology; Knowledge acquisition techniques; Machine learning techniques; Unsupervised techniques; K-means clustering"
"Bandyopadhyay S., Bhattacharyya M.","A novel method of studying the disease regulatory activities of microRNAs","10.2174/157489309789071101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450285401&doi=10.2174%2f157489309789071101&partnerID=40&md5=5bf97d1c4bec788cd99c24a8cc5c69b3","MicroRNAs (miRNAs) are small, non-coding RNAs that participate in the post-transcriptional regulation of messenger RNAs (mRNAs) by degrading or inhibiting translation. Some of the topical studies strongly suggest that the disorders in the normal activities of miRNAs might cause many diseases. Generally, such studies concern patient-specific expression profiles for the purposes like pruning, clustering or classification. This paper describes a novel relative coexpression measure to compute deviation in microarray expression profiles of diseased people over a set of people. This measure is used by an unsupervised algorithm of complexity O (n3 log n), where n denotes the number of miRNAs, to locate the group of miRNAs responsible for the specific disease. The results taken over the expression data of schizophrenic patients show efficiency in locating brain-enriched miRNAs, which have earlier established support to be associated with schizophrenia neuropathology. © 2009 Bentham Science Publishers Ltd.","Microarray; miRNA; p-value; Schizophrenia","microRNA; article; artificial neural network; fuzzy system; gene control; gene expression profiling; gene identification; genetic algorithm; human; machine learning; mathematical model; microarray analysis; neuropathology; nonhuman; priority journal; schizophrenia"
"Banerjee A., Lamrani I., Hossain S., Paudyal P., Gupta S.K.S.","AI enabled tutor for accessible training","10.1007/978-3-030-52237-7_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089612584&doi=10.1007%2f978-3-030-52237-7_3&partnerID=40&md5=01236bb96cc9169f2ecebc5188f45351","A significant number of jobs require highly skilled labor which necessitate training on pre-requisite knowledge. Examples include jobs in military, technical field such computer science, large scale fulfillment centers such as Amazon. Moreover, making such jobs accessible to the disabled population requires even more pre-requisite training such as knowledge of sign language. An artificial intelligent (AI) agent can potentially act as a tutor for such pre-requisite training. This will not only reduce resource requirements for such training but also decrease the time taken for making personnel job ready. In this paper, we develop an AI tutor that can teach users gestures that are required on the field as a pre-requisite. The AI tutor uses a model learning technique that learns the gestures performed by experts. It then uses a model comparison technique to compare a learner with the expert gesture and provides feedback for the learner to improve. © Springer Nature Switzerland AG 2020.","AI enabled tutor; ASL; Explainable AI","Learning systems; Personnel training; Artificial intelligent; Model comparison; Model learning; Pre-requisites; Resource requirements; Sign language; Skilled labor; Technical fields; Artificial intelligence"
"Banerjee A., Lamrani I., Paudyal P., Gupta S.","Generation of movement explanations for testing gesture based co-operative learning applications","10.1109/AITest.2019.00-15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067127184&doi=10.1109%2fAITest.2019.00-15&partnerID=40&md5=67986c6cf35da8efedcdb94d3460b747","The paper proposes an explanation framework for machine learning based gesture recognition systems to increase trust, and also provide users an interface to ask questions about the recognition result. Gestures have three components: a) handshape, b) location, and c) movement. Several techniques exist for handshape and location recognition explanation, but very limited analysis exists on explaining movement. The challenge is that modeling the movement between handshapes in a gesture require dynamic modeling of arm kinematics using differential equations. The arm models can be of various complexity, but many of them may not be explainable. Our approach in this paper, is to mine hybrid system models of gestures using a coalition of hand-shape recognition technology and explainable kinematic models. The hybrid dynamical systems are mined using video data collected from users. Change in dynamics of a test user is expressed using the parameters of the kinematic equations. The parameters are converted into human understandable explanations by experts in movement analysis. The novel outcome is the combination of fault detection in hybrid dynamical systems and machine learning to provide explanation for recognition of continuous events. We have applied our technique on 60 users for 20 ASL gestures. Results show that the mined parameters of the kinematic equations can represent each gesture with precision of 83 %, recall of 80 % and accuracy of 82 %. © 2019 IEEE.","Artificial intelligence; Explainable AI; Gesture recognition; Testing","Artificial intelligence; Differential equations; Fault detection; Hybrid systems; Kinematics; Machine learning; Testing; Cooperative learning; Gesture recognition system; Hand shape recognition; Hybrid dynamical systems; Hybrid system models; Kinematic equations; Location recognition; Movement analysis; Gesture recognition"
"Banerjee A., Chakrabarty M., Rakshit N., Bhowmick A.R., Ray S.","Environmental factors as indicators of dissolved oxygen concentration and zooplankton abundance: Deep learning versus traditional regression approach","10.1016/j.ecolind.2018.09.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054153694&doi=10.1016%2fj.ecolind.2018.09.051&partnerID=40&md5=047019b566cb8c715b0a3d514416cce9","Presence of optimal levels of dissolved oxygen (above critical level of 4.5 mg L−1) and presence of zooplankton community are indicators of good water quality of an aquatic ecosystem and also of the health of the same. Reservoirs being artificially created water bodies present hybrid systems containing features of both lotic and lentic systems and thus have unique organization that are representative of both rivers and lakes. Since any reservoir is primarily a fresh water system, presence of a large array of zooplankton (diverse community structure) implies its good health and also presence of optimal dissolved oxygen levels supports sustenance of life. In this study, artificial neural network modelling approach has been utilized to predict the level of dissolved oxygen and zooplankton abundance in the Bakreswar reservoir and their variation in relation to the environmental factors. Use of neural network modelling is exceedingly capable of determining correlation among apparently non correlated environmental data and in the current study these are capable of accurately predicting the variations in the levels of dissolved oxygen as well as the abundance of zooplankton. From this study, it has been observed that chemical factors like productivity, nitrates, salinity, pH, phosphates, total dissolved solids, etc. are mostly responsible for control of dissolved oxygen and zooplankton variation at certain points of the study site (stations 1 and 3) whereas at other points (station 2) physical factors like solar radiation, humidity, etc. are more effective. These models are capable of finding the important environmental controllers of such variations and prove to be a powerful alternative to traditional approaches like multiple regression analysis. © 2018 Elsevier Ltd","Artificial neural network; Bakreswar reservoir; Feed forward back propagation; India; Plankton; West Bengal","Aquatic ecosystems; Aquatic organisms; Backpropagation; Biochemical oxygen demand; Deep learning; Dissolution; Humidity control; Hybrid systems; Neural networks; Plankton; Regression analysis; Reservoirs (water); Water quality; Dissolved oxygen concentrations; Dissolved oxygen levels; Feed-forward back propagation; India; Multiple regression analysis; Neural network modelling; West Bengal; Zooplankton communities; Dissolved oxygen; abundance; aquatic ecosystem; artificial neural network; concentration (composition); environmental factor; multiple regression; plankton; reservoir; water quality; zooplankton; Bakreswar; India; India; West Bengal; West Bengal"
"Banerjee P., Barnwal R.P.","Methods and Metrics for Explaining Artificial Intelligence Models: A Review","10.1007/978-3-031-12807-3_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140708225&doi=10.1007%2f978-3-031-12807-3_4&partnerID=40&md5=e504db9c9997fb30d3da8cec60cdfda6","Deep learning (DL) solutions have been facing the long-standing problem of making the Explainable Artificial Intelligence (XAI) an integral part of the machine learning pipeline. In recent times, multiple deep learning approaches have been established for solving the enhanced complications aroused due to high predictive capacity. Though DL models demonstrate exceptionally high accuracy but the same comes with computationally complex and difficult to interpret black-box architectures. Several efforts are being made to develop the methods for making such high-precision black-box models explainable so that the trustworthiness and reliability of such models can be established. The chapter provides an overview of XAI, different methods of XAI, and metrics associated with those methods. Further, the chapter also discusses the motivational factors behind XAI, its applications, and its taxonomy. For clarity on the XAI implementation stage, Pre-model, In-model, and Post-model explainability are elaborated along with the model-agnostic and model-specific techniques. The chapter concludes with a brief discussion on a simple use-case of implementing the XAI method in a real-life problem followed by enumerating possible future research directions. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,
"Banerjee P., Banerjee S., Barnwal R.P.","Explaining deep-learning models using gradient-based localization for reliable tea-leaves classifications","10.1109/ICAECC54045.2022.9716699","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126732023&doi=10.1109%2fICAECC54045.2022.9716699&partnerID=40&md5=e9a985a76199fbd9612353be84a50ff2","In deep learning solutions there has been a lot of ambiguity about how to make explainability inclusive of a machine learning pipeline. Recently, several deep learning techniques have been introduced to solve increasingly complicated problems with higher predictive capacity. However, this predictive power comes at the cost of high computational complexity and difficult to interpret. While these models often produce very accurate predictions, we need to be able to explain the path followed by such models for decision making. Deep learning models, in general, predict with no or very less interpretable explanations. This lack of explainability makes such models blackbox. Explainable Artificial Intelligence (XAI) aims at transforming this black box approach into a more interpretable one. In this paper, we apply the well known Grad-CAM technique for the explainability of tea-leaf classification problem. The proposed method classifies tea-leaf-bud combinations using pre-trained deep learning models. We add classification explainability in our tea-leaf dataset using the pre-trained model as an input to the Grad-CAM technique to produce class-specific heatmap. We analyzed the results and working of the classification models for their reliability and effectiveness. © 2022 IEEE.","deep learning; explainable artificial intelligence; interpretability; reliability; tea-leaf","Cams; Decision making; Deep learning; Deep learning; Explainable artificial intelligence; Gradient based; Interpretability; Leaf classification; Learning models; Learning techniques; Localisation; Predictive capacity; Tea-leaves; Classification (of information)"
"Banerjee P., Dunkel M., Kemmler E., Preissner R.","SuperCYPsPred-a web server for the prediction of cytochrome activity","10.1093/NAR/GKAA166","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087320328&doi=10.1093%2fNAR%2fGKAA166&partnerID=40&md5=0e5d652e8feff0eda51d9e176b8266e0","Cytochrome P450 enzymes (CYPs)-mediated drug metabolism influences drug pharmacokinetics and results in adverse outcomes in patients through drug-drug interactions (DDIs). Absorption, distribution, metabolism, excretion and toxicity (ADMET) issues are the leading causes for the failure of a drug in the clinical trials. As details on their metabolism are known for just half of the approved drugs, a tool for reliable prediction of CYPs specificity is needed. The SuperCYPsPred web server is currently focused on five major CYPs isoenzymes, which includes CYP1A2, CYP2C19, CYP2D6, CYP2C9 and CYP3A4 that are responsible for more than 80% of the metabolism of clinical drugs. The prediction models for classification of the CYPs inhibition are based on well-established machine learning methods. The models were validated both on cross-validation and external validation sets and achieved good performance. The web server takes a 2D chemical structure as input and reports the CYP inhibition profile of the chemical for 10 models using different molecular fingerprints, along with confidence scores, similar compounds, known CYPs information of drugs-published in literature, detailed interaction profile of individual cytochromes including a DDIs table and an overall CYPs prediction radar chart (http://insilico-cyp.charite.de/SuperCYPsPred/). The web server does not require log in or registration and is free to use. © The Author(s) 2020. Published by Oxford University Press on behalf of Nucleic Acids Research.",,"cytochrome P450; cytochrome P450 1A2; cytochrome P450 2C19; cytochrome P450 2C9; cytochrome P450 2D6; cytochrome P450 3A4; antidepressant agent; cytochrome P450; cytochrome P450 inhibitor; isoenzyme; sertraline; Article; controlled study; cross validation; external validity; information processing; measurement accuracy; molecular fingerprinting; prediction; protein function; receiver operating characteristic; sensitivity and specificity; software; statistics; SuperCYPsPred software; chemistry; drug interaction; Internet; metabolism; Antidepressive Agents; Cytochrome P-450 Enzyme Inhibitors; Cytochrome P-450 Enzyme System; Drug Interactions; Internet; Isoenzymes; Sertraline; Software"
"Banerjee S., Lio P., Jones P.B., Cardinal R.N.","A class-contrastive human-interpretable machine learning approach to predict mortality in severe mental illness","10.1038/s41537-021-00191-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120948822&doi=10.1038%2fs41537-021-00191-y&partnerID=40&md5=af96ec254739cf76de53c845e714ac4b","Machine learning (ML), one aspect of artificial intelligence (AI), involves computer algorithms that train themselves. They have been widely applied in the healthcare domain. However, many trained ML algorithms operate as ‘black boxes’, producing a prediction from input data without a clear explanation of their workings. Non-transparent predictions are of limited utility in many clinical domains, where decisions must be justifiable. Here, we apply class-contrastive counterfactual reasoning to ML to demonstrate how specific changes in inputs lead to different predictions of mortality in people with severe mental illness (SMI), a major public health challenge. We produce predictions accompanied by visual and textual explanations as to how the prediction would have differed given specific changes to the input. We apply it to routinely collected data from a mental health secondary care provider in patients with schizophrenia. Using a data structuring framework informed by clinical knowledge, we captured information on physical health, mental health, and social predisposing factors. We then trained an ML algorithm and other statistical learning techniques to predict the risk of death. The ML algorithm predicted mortality with an area under receiver operating characteristic curve (AUROC) of 0.80 (95% confidence intervals [0.78, 0.82]). We used class-contrastive analysis to produce explanations for the model predictions. We outline the scenarios in which class-contrastive analysis is likely to be successful in producing explanations for model predictions. Our aim is not to advocate for a particular model but show an application of the class-contrastive analysis technique to electronic healthcare record data for a disease of public health significance. In patients with schizophrenia, our work suggests that use or prescription of medications like antidepressants was associated with lower risk of death. Abuse of alcohol/drugs and a diagnosis of delirium were associated with higher risk of death. Our ML models highlight the role of co-morbidities in determining mortality in patients with schizophrenia and the need to manage co-morbidities in these patients. We hope that some of these bio-social factors can be targeted therapeutically by either patient-level or service-level interventions. Our approach combines clinical knowledge, health data, and statistical learning, to make predictions interpretable to clinicians using class-contrastive reasoning. This is a step towards interpretable AI in the management of patients with schizophrenia and potentially other diseases. © 2021, The Author(s).",,"antidepressant agent; algorithm; area under the curve; Article; comorbidity; delirium; disease predisposition; human; machine learning; mental disease; mental health; mortality risk; prediction; prescription; receiver operating characteristic; secondary health care; standardized mortality ratio; survival analysis"
"Banerjee S., Raman K., Ravindran B.","Sequence neighborhoods enable reliable prediction of pathogenic mutations in cancer genomes","10.3390/cancers13102366","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105749318&doi=10.3390%2fcancers13102366&partnerID=40&md5=db259a281c9ec4801686f4b7a2adeb50","Identifying cancer-causing mutations from sequenced cancer genomes hold much promise for targeted therapy and precision medicine. “Driver” mutations are primarily responsible for cancer progression, while “passengers” are functionally neutral. Although several computational approaches have been developed for distinguishing between driver and passenger mutations, very few have concentrated on using the raw nucleotide sequences surrounding a particular mutation as potential features for building predictive models. Using experimentally validated cancer mutation data in this study, we explored various string-based feature representation techniques to incorporate information on the neighborhood bases immediately 5′ and 3′ from each mutated position. Density estimation methods showed significant distributional differences between the neighborhood bases surrounding driver and passenger mutations. Binary classification models derived using repeated cross-validation experiments provided comparable performances across all window sizes. Inte-grating sequence features derived from raw nucleotide sequences with other genomic, structural, and evolutionary features resulted in the development of a pan-cancer mutation effect prediction tool, NBDriver, which was highly efficient in identifying pathogenic variants from five independent validation datasets. An ensemble predictor obtained by combining the predictions from NBDriver with three other commonly used driver prediction tools (FATHMM (cancer), CONDEL, and Muta-tionTaster) significantly outperformed existing pan-cancer models in prioritizing a literature-curated list of driver and passenger mutations. Using the list of true positive mutation predictions derived from NBDriver, we identified a list of 138 known driver genes with functional evidence from various sources. Overall, our study underscores the efficacy of using raw nucleotide sequences as features to distinguish between driver and passenger mutations from sequenced cancer genomes. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Cancer driver mutations; Context of mutations; Machine learning; Missense mutations; Neighborhood sequences","nucleic acid base; accuracy; algorithm; Article; binary classification model; cancer driver mutation; Cancer Genome Interpreter Database; cancer mutation census; cancer patient; classifier; comparative study; context of mutation; controlled study; cross validation; density estimation method; experimental study; gene identification; gene mutation; gene sequence; genetic database; genome; glioblastoma; human; intermethod comparison; machine learning; methodology; missense mutation; molecular evolution; nucleotide sequence; oncogenomics; ovary cancer; pathogenic mutation; performance; prediction; Recurrent Driver Mutation; reliability; sequence neighborhood"
"Banerjee S., Dong M., Lee M.-H., Ohara N., Juhasz C., Asano E., Jeong J.-W.","Deep Relational Reasoning for the Prediction of Language Impairment and Postoperative Seizure Outcome Using Preoperative DWI Connectome Data of Children with Focal Epilepsy","10.1109/TMI.2020.3036933","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096879246&doi=10.1109%2fTMI.2020.3036933&partnerID=40&md5=30b17ca4a876371a75263bebfbb75110","Prolonged seizures in children with focal epilepsy (FE) may impair language functions and often reoccur after surgical intervention. This study is aimed at developing a novel deep relational reasoning network to investigate whether conventional diffusion-weighted imaging connectome analysis can be improved when predicting expressive and receptive scores of preoperative language impairments and classifying postoperative seizure outcomes (seizure freedom or recurrence) in individual FE children. To deeply reason the dependencies of axonal connections that are sparsely distributed in the whole brain, this study proposes the 'dilated CNN + RN', a dilated convolutional neural network (CNN) combined with a relation network (RN). The performance of the dilated CNN + RN was evaluated using whole brain connectome data from 51 FE children. It was found that when compared with other state-of-The-Art algorithms, the dilated CNN + RN led to an average improvement of 90.2% and 97.3% in predicting expressive and receptive language scores, and 2.2% and 4% improvement in classifying seizure freedom and seizure recurrence, respectively. These improvements were independent of the prefixed connectome densities. Also, the dilated CNN + RN could provide an explainable artificial intelligence (AI) model by computing gradient-based regression/classification activation maps. This mapping analysis revealed left superior-medial frontal cortex, bilateral hippocampi, and cerebellum as crucial hubs, facilitating important connections that were most predictive of language function and seizure refractoriness after surgery. © 1982-2012 IEEE.","convolutional neural network; Diffusion-weighted imaging; functional brain mapping; pediatric epilepsy surgery; relational reasoning","Forecasting; Neurology; Surgery; Activation maps; Diffusion weighted imaging; Frontal cortex; Language impairments; Mapping analysis; Relational reasoning; State-of-the-art algorithms; Surgical interventions; Convolutional neural networks; analytical error; Article; artificial intelligence; binary classification; brain mapping; calculation; cerebellum; child; childhood disease; comparative study; connectome; convolutional neural network; deep relational reasoning network; diffusion weighted imaging; dilated convolutional neural network; edge to node network; electrocorticography; extraoperative electrocorticography; focal epilepsy; human; language disability; major clinical study; medial frontal cortex; multi layer regressor; multilayer perceptron; network analysis; postoperative complication; prediction; random forest; relation network; school child; superior medial frontal cortex; support vector machine; tractography; whole brain streamline tractography; connectome; developmental language disorder; diagnostic imaging; focal epilepsy; seizure; Artificial Intelligence; Child; Connectome; Epilepsies, Partial; Humans; Language Development Disorders; Seizures"
"Banerjee S., Chattopadhyay T., Garain U.","A Wide Learning Approach for Interpretable Feature Recommendation for 1-D Sensor Data in IoT Analytics","10.1007/s11633-019-1185-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068762544&doi=10.1007%2fs11633-019-1185-8&partnerID=40&md5=d42b1398a3ab62e21ac4c470003e0061","This paper presents a state of the art machine learning-based approach for automation of a varied class of Internet of things (IoT) analytics problems targeted on 1-dimensional (1-D) sensor data. As feature recommendation is a major bottleneck for general IoT-based applications, this paper shows how this step can be successfully automated based on a Wide Learning architecture without sacrificing the decision-making accuracy, and thereby reducing the development time and the cost of hiring expensive resources for specific problems. Interpretation of meaningful features is another contribution of this research. Several data sets from different real-world applications are considered to realize the proof-of-concept. Results show that the interpretable feature recommendation techniques are quite effective for the problems at hand in terms of performance and drastic reduction in development time. © 2019, Institute of Automation, Chinese Academy of Sciences and Springer-Verlag GmbH Germany, part of Springer Nature.","automation; Feature engineering; Internet of things (IoT) analytics; interpretable learning; sensor data analysis","Automation; Decision making; Learning algorithms; Feature engineerings; Internet of Things (IOT); interpretable learning; Learning approach; Learning architectures; Recommendation techniques; Sensor data analysis; Specific problems; Internet of things"
"Banerjee S., Chattopadhyay T., Pal A., Garain U.","Automation of feature engineering for IoT analytics","10.1145/3231535.3231538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068734107&doi=10.1145%2f3231535.3231538&partnerID=40&md5=9f451a4ffc7f94e6a0a02016a08aaf6e","This paper presents an approach for automation of interpretable feature selection for Internet Of Things Analytics (IoTA) using machine learning (ML) techniques. Authors have conducted a survey over different people involved in different IoTA based application development tasks. The survey reveals that feature selection is the most time consuming and niche skill demanding part of the entire workflow. This paper shows how feature selection is successfully automated without sacrificing the decision making accuracy and thereby reducing the project completion time and cost of hiring expensive resources. Several pattern recognition principles and state of art (SoA) ML techniques are followed to design the overall approach for the proposed automation. Three data sets are considered to establish the proof-of-concept. Experimental results show that the proposed automation is able to reduce the time for feature selection to 2 days instead of 4 - 6 months which would have been required in absence of the automation. This reduction in time is achieved without any sacrifice in the accuracy of the decision making process. Proposed method is also compared against Multi Layer Perceptron (MLP) model as most of the state of the art works on IoTA uses MLP based Deep Learning. Moreover the feature selection method is compared against SoA feature reduction technique namely Principal Component Analysis (PCA) and its variants. The results obtained show that the proposed method is effective. © 2018 Authors.","feature engineering; information processing on sensor data; IoT analytics; sensor signal analytics","Automation; Behavioral research; Data handling; Decision making; Deep learning; Feature extraction; Principal component analysis; Surveys; Application development; Feature engineerings; Features selection; Information processing on sensor data; IoT analytic; Machine learning techniques; Multilayers perceptrons; Sensor signal analytic; Sensor signals; Sensors data; Internet of things"
"Banerjee S.S., Kalbarczyk Z.T., Iyer R.K.","AcMC2: Accelerated Markov Chain Monte Carlo for Probabilistic Models","10.1145/3297858.3304019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064637402&doi=10.1145%2f3297858.3304019&partnerID=40&md5=90a91357d018b36725bcdf5de2702c9d","Probabilistic models (PMs) are ubiquitously used across a variety of machine learning applications. They have been shown to successfully integrate structural prior information about data and effectively quantify uncertainty to enable the development of more powerful, interpretable, and efficient learning algorithms. This paper presents AcMC2, a compiler that transforms PMs into optimized hardware accelerators (for use in FPGAs or ASICs) that utilize Markov chain Monte Carlo methods to infer and query a distribution of posterior samples from the model. The compiler analyzes statistical dependencies in the PM to drive several optimizations to maximally exploit the parallelism and data locality available in the problem. We demonstrate the use of AcMC2 to implement several learning and inference tasks on a Xilinx Virtex-7 FPGA. AcMC2-generated accelerators provide a 47 - 100× improvement in runtime performance over a 6-core IBM Power8 CPU and a 8 - 18× improvement over an NVIDIA K80 GPU. This corresponds to a 753 - 1600× improvement over the CPU and 248 - 463× over the GPU in performance-per-watt terms. © 2019 Association for Computing Machinery.","Accelerator; Markov Chain Monte Carlo; Probabilistic Graphical Models; Probabilistic Programming","Digital storage; Field programmable gate arrays (FPGA); Graphics processing unit; Learning algorithms; Learning systems; Markov processes; Object oriented programming; Particle accelerators; Program compilers; Hardware accelerators; Machine learning applications; Markov chain Monte Carlo method; Markov Chain Monte-Carlo; Probabilistic graphical models; Probabilistic models; Probabilistic programming; Statistical dependencies; Monte Carlo methods"
"Banf M., Rhee S.Y.","Computational inference of gene regulatory networks: Approaches, limitations and opportunities","10.1016/j.bbagrm.2016.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001129911&doi=10.1016%2fj.bbagrm.2016.09.003&partnerID=40&md5=599271944e6fae46e095fcd651d49b47","Gene regulatory networks lie at the core of cell function control. In E. coli and S. cerevisiae, the study of gene regulatory networks has led to the discovery of regulatory mechanisms responsible for the control of cell growth, differentiation and responses to environmental stimuli. In plants, computational rendering of gene regulatory networks is gaining momentum, thanks to the recent availability of high-quality genomes and transcriptomes and development of computational network inference approaches. Here, we review current techniques, challenges and trends in gene regulatory network inference and highlight challenges and opportunities for plant science. We provide plant-specific application examples to guide researchers in selecting methodologies that suit their particular research questions. Given the interdisciplinary nature of gene regulatory network inference, we tried to cater to both biologists and computer scientists to help them engage in a dialogue about concepts and caveats in network inference. Specifically, we discuss problems and opportunities in heterogeneous data integration for eukaryotic organisms and common caveats to be considered during network model evaluation. This article is part of a Special Issue entitled: Plant Gene Regulatory Mechanisms and Networks, edited by Dr. Erich Grotewold and Dr. Nathan Springer. © 2016 Elsevier B.V.","Computational systems biology; Gene network evaluation; Gene regulatory network inference; Heterogeneous data integration; Machine learning","Article; gene expression; gene regulatory network; information science; machine learning; mathematical analysis; mathematical parameters; nonhuman; priority journal; algorithm; biology; gene regulatory network; genetics; plant; plant genome; procedures; transcriptome; Algorithms; Computational Biology; Gene Regulatory Networks; Genome, Plant; Plants; Transcriptome"
"Bang C.S., Ahn J.Y., Kim J.-H., Kim Y.Il., Choi IlJ., Shin W.G.","Establishing machine learning models to predict curative resection in early gastric cancer with undifferentiated histology: Development and usability study","10.2196/25053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104425739&doi=10.2196%2f25053&partnerID=40&md5=c88af9ef3a43c3ab4e362c6b49a0529c","Background: Undifferentiated type of early gastric cancer (U-EGC) is included among the expanded indications of endoscopic submucosal dissection (ESD); however, the rate of curative resection remains unsatisfactory. Endoscopists predict the probability of curative resection by considering the size and shape of the lesion and whether ulcers are present or not. The location of the lesion, indicating the likely technical difficulty, is also considered. Objective: The aim of this study was to establish machine learning (ML) models to better predict the possibility of curative resection in U-EGC prior to ESD. Methods: A nationwide cohort of 2703 U-EGCs treated by ESD or surgery were adopted for the training and internal validation cohorts. Separately, an independent data set of the Korean ESD registry (n=275) and an Asan medical center data set (n=127) treated by ESD were chosen for external validation. Eighteen ML classifiers were selected to establish prediction models of curative resection with the following variables: age; sex; location, size, and shape of the lesion; and whether ulcers were present or not. Results: Among the 18 models, the extreme gradient boosting classifier showed the best performance (internal validation accuracy 93.4%, 95% CI 90.4%-96.4%; precision 92.6%, 95% CI 89.5%-95.7%; recall 99.0%, 95% CI 97.8%-99.9%; and F1 score 95.7%, 95% CI 93.3%-98.1%). Attempts at external validation showed substantial accuracy (first external validation 81.5%, 95% CI 76.9%-86.1% and second external validation 89.8%, 95% CI 84.5%-95.1%). Lesion size was the most important feature in each explainable artificial intelligence analysis. Conclusions: We established an ML model capable of accurately predicting the curative resection of U-EGC before ESD by considering the morphological and ecological characteristics of the lesions. ©Chang Seok Bang, Ji Yong Ahn, Jie-Hyun Kim, Young-Il Kim, Il Ju Choi, Woon Geon Shin.","Artificial intelligence; Dissection; Early gastric cancer; Endoscopic submucosal dissection; Endoscopy; Gastric cancer; Machine learning; Undifferentiated","adult; Article; artificial intelligence; Bayesian learning; cancer surgery; classifier; cohort analysis; decision tree; deep learning; deep neural network; discriminant analysis; endoscopic submucosal dissection; endoscopy; female; gastrectomy; histopathology; human; human tissue; k nearest neighbor; logistic regression analysis; machine learning; major clinical study; male; model; prediction; random forest; register; stomach cancer; support vector machine; training; usability; validation process; gastroscopy; machine learning; retrospective study; stomach tumor; treatment outcome; Artificial Intelligence; Gastroscopy; Humans; Machine Learning; Retrospective Studies; Stomach Neoplasms; Treatment Outcome"
"Bang H., Selva D.","Leveraging logged intermediate design attributes for improved knowledge discovery in engineering design","10.1115/DETC2017-67835","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034780843&doi=10.1115%2fDETC2017-67835&partnerID=40&md5=5f745038e4efa3c86a5f2d17d243e37c","Despite years of research efforts developing methods and decision support tools, architecting complex engineered systems remains a challenging task. Improvements in computational power and optimization algorithms have made it possible to explore large design spaces, but making sense of such datasets is difficult due to their scale and complexity. Various knowledge discovery tools and data-driven methods have been developed in the past to help system designers analyze and make use of such complex data. However, most of the currently available tools do not fully exploit the data that is generated during design space exploration and instead consider the mapping between design decisions (inputs) and objectives (outputs) as a blackbox function. In this paper, we introduce a new method that utilizes not only the design inputs and outputs, but also intermediate variables that are generated during the evaluation of each design. The tool stores all intermediate variables in a database, and then feeds them into a data mining algorithm to extract useful and human understandable features in the form of if-then rules. We show how the use of intermediate variables leads to new insights that could not be discovered with the blackbox approach, and improved knowledge discovery in the sense of features that are more compact and/or with higher predictive power. The method is demonstrated on a real-world system architecting problem of a constellation of Earth observing satellites. © Copyright 2017 ASME.",,"Computer aided design; Decision support systems; Design; Knowledge management; Optimization; Black-box functions; Complex engineered systems; Computational power; Data mining algorithm; Decision support tools; Design space exploration; Earth observing satellite; Optimization algorithms; Data mining"
"Bang J., Kim C., Wu K., Sim A., Byna S., Kim S., Eom H.","HPC Workload Characterization Using Feature Selection and Clustering","10.1145/3391812.3396270","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089107500&doi=10.1145%2f3391812.3396270&partnerID=40&md5=1c51634eb7e43372e0aa69220984397d","Large high-performance computers (HPC) are expensive tools responsible for supporting thousands of scientific applications. However, it is not easy to determine the best set of configurations for workloads to best utilize the storage and I/O systems. Users typically use the default configurations provided by the system administrators, which typically results in poor performance. In an effort to identify application characteristics more important to I/O performance, we applied several machine learning techniques to characterize these applications. To identify the features that are most relevant to the I/O performance, we evaluate a number of different feature selection methods, e.g., Mutual information regression and F regression, and develop a novel feature selection method based on Min-max mutual information. These feature selection methods allow us to sift through a large set of the real-world workloads collected from NERSC's Cori supercomputer system, and identify the most important features. We employ a number of different clustering algorithms, including KMeans, Gaussian Mixture Model (GMM) and Ward linkage, and measure the cluster quality with Davies Boulder Index (DBI), Silhouette and a new Combined Score developed for this work. The cluster evaluation result shows that the test dataset could be best divided into three clusters, where cluster 1 contains mostly small jobs with operations on standard I/O units, cluster 2 consists of middle size parallel jobs dominated by read operations, and cluster 3 include large parallel jobs with heavy write operations. The cluster characteristics suggest that using parallel I/O library MPI IO and a large number of parallel cores are important to achieve high I/O throughput. © 2020 ACM.","clustering; feature selection; high performance computing; supercomputer; workload characterization","Clustering algorithms; Gaussian distribution; Large dataset; Learning systems; Statistical tests; Supercomputers; Telemetering equipment; Feature selection methods; Gaussian Mixture Model; High performance computers; Machine learning techniques; Mutual informations; Scientific applications; System administrators; Workload characterization; Feature extraction"
"Bang J.-S., Lee M.-H., Fazli S., Guan C., Lee S.-W.","Spatio-Spectral Feature Representation for Motor Imagery Classification Using Convolutional Neural Networks","10.1109/TNNLS.2020.3048385","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099724916&doi=10.1109%2fTNNLS.2020.3048385&partnerID=40&md5=c2832ab934ca4d0e52816e93e2d9b575","Convolutional neural networks (CNNs) have recently been applied to electroencephalogram (EEG)-based brain-computer interfaces (BCIs). EEG is a noninvasive neuroimaging technique, which can be used to decode user intentions. Because the feature space of EEG data is highly dimensional and signal patterns are specific to the subject, appropriate methods for feature representation are required to enhance the decoding accuracy of the CNN model. Furthermore, neural changes exhibit high variability between sessions, subjects within a single session, and trials within a single subject, resulting in major issues during the modeling stage. In addition, there are many subject-dependent factors, such as frequency ranges, time intervals, and spatial locations at which the signal occurs, which prevent the derivation of a robust model that can achieve the parameterization of these factors for a wide range of subjects. However, previous studies did not attempt to preserve the multivariate structure and dependencies of the feature space. In this study, we propose a method to generate a spatiospectral feature representation that can preserve the multivariate information of EEG data. Specifically, 3-D feature maps were constructed by combining subject-optimized and subject-independent spectral filters and by stacking the filtered data into tensors. In addition, a layer-wise decomposition model was implemented using our 3-D-CNN framework to secure reliable classification results on a single-trial basis. The average accuracies of the proposed model were 87.15% (±7.31), 75.85% (±12.80), and 70.37% (±17.09) for the BCI competition data sets IV_2a, IV_2b, and OpenBMI data, respectively. These results are better than those obtained by state-of-the-art techniques, and the decomposition model obtained the relevance scores for neurophysiologically plausible electrode channels and frequency domains, confirming the validity of the proposed approach. © 2012 IEEE.","Brain-computer interface (BCI); convolutional neural network (CNN); electroencephalography (EEG); explainable artificial intelligence (XAI); motor imagery (MI)","Biomedical signal processing; Brain computer interface; Convolution; Decoding; Electroencephalography; Image classification; Neuroimaging; Brain computer interfaces (BCIs); Classification results; Decomposition model; Electro-encephalogram (EEG); Feature representation; Motor imagery classification; Neuroimaging techniques; State-of-the-art techniques; Convolutional neural networks; algorithm; brain computer interface; electroencephalography; human; neuroimaging; procedures; Algorithms; Brain-Computer Interfaces; Electroencephalography; Humans; Neural Networks, Computer; Neuroimaging"
"Bang M., Park Y.W., Eom J., Ahn S.S., Kim J., Lee S.-K., Lee S.-H.","An interpretable radiomics model for the diagnosis of panic disorder with or without agoraphobia using magnetic resonance imaging","10.1016/j.jad.2022.02.072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125705666&doi=10.1016%2fj.jad.2022.02.072&partnerID=40&md5=6b4c23d6a029874adc99f4341b4c5427","Background: Early and accurate diagnosis of panic disorder with or without agoraphobia (PDA) is crucial to reducing disease burden and individual suffering. However, its diagnosis is challenging for lack of validated biomarkers. This study aimed to investigate whether radiomic features extracted from T1-weighted images (T1) of major fear-circuit structures (amygdala, insula, and anterior cingulate cortex [ACC]) could differentiate patients with PDA from healthy controls (HCs). Methods: The 213 participants (93 PDA, 120 HCs) were allocated to training (n = 149) and test (n = 64) sets after undergoing magnetic resonance imaging. Radiomic features (n = 1498) were extracted from T1 of the studied structures. Machine learning models were trained after feature selection and then validated in the test set. SHapley Additive exPlanations (SHAP) explored the model interpretability. Results: We identified 29 radiomic features to differentiate participants with PDA from HCs. The area under the curve, accuracy, sensitivity, and specificity of the best performing radiomics model in the test set were 0.84 (95% confidence interval: 0.74–0.95), 81.3%, 75.0%, and 86.1%, respectively. The SHAP model explanation suggested that the energy features extracted from the bilateral long insula gyrus and central sulcus of the insula and right ACC were highly associated with the risk of PDA. Limitations: This was a cross-sectional study with a relatively small sample size, and the causality of changes in radiomic features and their biological and clinical meanings remained to be elucidated. Conclusions: Our findings suggest that radiomic features from the fear-circuit structures could unveil hidden microstructural aberrations underlying the pathogenesis of PDA that could help identify PDA. © 2022 Elsevier B.V.","Biomarker; Fear circuit; Machine learning; Magnetic resonance imaging; Panic disorder; Radiomics","benzodiazepine; escitalopram; fluoxetine; lorazepam; paroxetine; sertraline; venlafaxine; accuracy; adult; agoraphobia; amygdala; anterior cingulate; Article; central sulcus; classifier; clinical feature; controlled study; cross-sectional study; feature extraction; feature selection; female; human; insula; machine learning; major clinical study; male; nuclear magnetic resonance imaging; panic; radiomics; sensitivity and specificity; treatment duration; diagnostic imaging; nuclear magnetic resonance imaging; panic; procedures; retrospective study; Agoraphobia; Cross-Sectional Studies; Humans; Machine Learning; Magnetic Resonance Imaging; Panic Disorder; Retrospective Studies"
"Bang S., Xie P., Lee H., Wu W., Xing E.","Explaining A Black-box By Using A Deep Variational Information Bottleneck Approach",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126364630&partnerID=40&md5=90b4fd9eb4bbf3e9087df50bc16f0a63","Interpretable machine learning has gained much attention recently. Briefness and comprehensiveness are necessary in order to provide a large amount of information concisely when explaining a black-box decision system. However, existing interpretable machine learning methods fail to consider briefness and comprehensiveness simultaneously, leading to redundant explanations. We propose the variational information bottleneck for interpretation, VIBI, a system-agnostic interpretable method that provides a brief but comprehensive explanation. VIBI adopts an information theoretic principle, information bottleneck principle, as a criterion for finding such explanations. For each instance, VIBI selects key features that are maximally compressed about an input (briefness), and informative about a decision made by a black-box system on that input (comprehensive). We evaluate VIBI on three datasets and compare with state-of-the-art interpretable machine learning methods in terms of both interpretability and fidelity evaluated by human and quantitative metrics. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Machine learning; Amount of information; Black box system; Black boxes; Decision systems; Information bottleneck; Information bottleneck principles; Key feature; Large amounts; Machine learning methods; State of the art; Information theory"
"Bangotra D.K., Singh Y., Selwal A., Kumar N., Singh P.K., Hong W.-C.","An intelligent opportunistic routing algorithm for wireless sensor networks and its application towards e-healthcare","10.3390/s20143887","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087794966&doi=10.3390%2fs20143887&partnerID=40&md5=5b917e8d94376d7702596cce61b25301","The lifetime of a node in wireless sensor networks (WSN) is directly responsible for the longevity of the wireless network. The routing of packets is the most energy-consuming activity for a sensor node. Thus, finding an energy-efficient routing strategy for transmission of packets becomes of utmost importance. The opportunistic routing (OR) protocol is one of the new routing protocol that promises reliability and energy efficiency during transmission of packets in wireless sensor networks (WSN). In this paper, we propose an intelligent opportunistic routing protocol (IOP) using a machine learning technique, to select a relay node from the list of potential forwarder nodes to achieve energy efficiency and reliability in the network. The proposed approach might have applications including e-healthcare services. As the proposed method might achieve reliability in the network because it can connect several healthcare network devices in a better way and good healthcare services might be offered. In addition to this, the proposed method saves energy, therefore, it helps the remote patient to connect with healthcare services for a longer duration with the integration of IoT services. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Energy efficiency; Naïve Bayes; Opportunistic routing (OR); Relay node; Reliability; Wireless sensor networks (WSN)","Energy efficiency; Health care; Internet protocols; Learning systems; Power management (telecommunication); Reliability; Routing algorithms; Routing protocols; E-healthcare services; Efficiency and reliability; Energy efficient routing; Healthcare services; ITS applications; Machine learning techniques; Network devices; Opportunistic routing; Sensor nodes; algorithm; human; machine learning; reproducibility; telemedicine; time factor; wireless communication; Algorithms; Humans; Machine Learning; Reproducibility of Results; Telemedicine; Time Factors; Wireless Technology"
"Banham A., Leemans S.J.J., Wynn M.T., Andrews R., Laupland K.B., Shinners L.","xPM: Enhancing exogenous data visibility","10.1016/j.artmed.2022.102409","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139729620&doi=10.1016%2fj.artmed.2022.102409&partnerID=40&md5=12e6a10a110f67c8ba473e24dae194f7","Process mining is a well-established discipline with applications in many industry sectors, including healthcare. To date, few publications have considered the context in which processes execute. Little consideration has been given as to how contextual data (exogenous data) can be practically included for process mining analysis, beyond including case or event attributes in a typical event log. We show that the combination of process data (endogenous) and exogenous data can generate insights not possible with standard process mining techniques. Our contributions are a framework for process mining with exogenous data and new analyses, where exogenous data and process behaviour are linked to process outcomes. Our new analyses visualise exogenous data, highlighting the trends and variations, to show where overlaps or distinctions exist between outcomes. We applied our analyses in a healthcare setting and show that clinicians could extract insights about differences in patients’ vital signs (exogenous data) relevant to clinical outcomes. We present two evaluations, using a publicly available data set, MIMIC-III, to demonstrate the applicability of our analysis. These evaluations show that process mining can integrate large amounts of physiologic data and interventions, with resulting discrimination and conversion to clinically interpretable information. © 2022 Elsevier B.V.","Exogenous data; MIMIC-III; Multi-perspective; Process mining","Data mining; Data visibility; Event logs; Exogenous data; Industry sectors; MIMIC-III; Mining techniques; Multi-perspective; Process data; Process mining; Standards process; Health care; adult; article; clinical outcome; human; mining; outcome assessment; visibility; vital sign"
"Baniecki H., Kretowicz W., Piatyszek P., Wisniewski J., Biecek P.","dalex: Responsible machine learning with interactive explainability and fairness in python",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116992900&partnerID=40&md5=6dfe6e3e0c3929b19068193241a3da78","In modern machine learning, we observe the phenomenon of opaqueness debt, which manifests itself by an increased risk of discrimination, lack of reproducibility, and deflated performance due to data drift. An increasing amount of available data and computing power results in the growing complexity of black-box predictive models. To manage these issues, good MLOps practice asks for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity for deeper model transparency comes from both scientific and social domains and is also caused by emerging laws and regulations on artificial intelligence. To facilitate the responsible development of machine learning models, we introduce dalex, a Python package which implements a model-agnostic interface for interactive explainability and fairness. It adopts the design crafted through the development of various tools for explainable machine learning; thus, it aims at the unification of existing solutions. This library’s source code and documentation are available under open license at https://python.drwhy.ai. © 2021 Hubert Baniecki, Wojciech Kretowicz, Piotr Piatyszek, Jakub Wisniewski, and Przemyslaw Biecek.","Explainability; Fairness; Interactivity; Interpretability; Responsible AI","Laws and legislation; Machine learning; Python; Black boxes; Computing power; Explainability; Fairness; Interactivity; Interpretability; Performance; Predictive models; Reproducibilities; Responsible AI; High level languages"
"Baniecki H., Biecek P.","Responsible Prediction Making of COVID-19 Mortality (Student Abstract)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104393665&partnerID=40&md5=b9d2c20bcf9735dd5f212d97c8a9ec46","For high-stakes prediction making, the Responsible Artificial Intelligence (RAI) is more important than ever. It builds upon Explainable Artificial Intelligence (XAI) to advance the efforts in providing fairness, model explainability, and accountability to the AI systems. During the literature review of COVID-19 related prognosis and diagnosis, we found out that most of the predictive models are not faithful to the RAI principles, which can lead to biassed results and wrong reasoning. To solve this problem, we show how novel XAI techniques boost transparency, reproducibility and quality of models. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved",,"AI systems; Fairness model; Literature reviews; Predictive models; Prognosis and diagnosis; Reproducibilities; Artificial intelligence"
"Banik D., Bhattacharjee D., Nasipuri M.","A Multi-Scale Patch-Based Deep Learning System for Polyp Segmentation","10.1007/978-981-15-2930-6_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079827171&doi=10.1007%2f978-981-15-2930-6_9&partnerID=40&md5=c459db2b30e2abda510606b9434c2d3e","Colorectal cancer (CRC) is one of the deadliest forms of cancer and is on the rise. Accurate segmentation of the precursor lesion, the polyp, can ensure the survival rate. Hence, there is a research trend to develop medical diagnosis support tool to assist clinicians. This study focuses on the development of deep learning-based CNN model for automated segmentation of polyp. As polyp varies frame to frame in terms of size, color, shape, and texture, segmentation is still an unsolved problem and a very challenging task. We have proposed a multi-scale patch-based CNN model for automatic segmentation of the polyp region. Local and global patches are extracted from each pixel of the input image and fed into two similar CNNs of which one is responsible for the extraction of local features and the other for extraction of global features that are being concatenated for accurately pixel label annotation of the polyp region. As in colonoscopy frames, there are some regions with the same intensity/texture as the polyp regions, so in the predicted segmentation map, some non-polyp regions are also considered as polyp regions, which are further refined by post-processing operation. The proposed model is evaluated on CVC-Clinic DB. The experimental result shows that our proposed method outperforms other baseline CNNs and state-of-the-art methods. © Springer Nature Singapore Pte Ltd 2020.","Colorectal cancer; Deep learning; Medical image segmentation; Polyp","Diagnosis; Diseases; Extraction; Image segmentation; Learning systems; Medical imaging; Pixels; Textures; Automated segmentation; Automatic segmentations; Colorectal cancer; Colorectal cancers (CRC); Medical diagnosis support; Polyp; Polyp segmentation; State-of-the-art methods; Deep learning"
"Banja J.D., Hollstein R.D., Bruno M.A.","When Artificial Intelligence Models Surpass Physician Performance: Medical Malpractice Liability in an Era of Advanced Artificial Intelligence","10.1016/j.jacr.2021.11.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125480211&doi=10.1016%2fj.jacr.2021.11.014&partnerID=40&md5=c5e225d659ba4aecab820fd0879d4a65","It seems inevitable that diagnostic and recommender artificial intelligence models will ultimately reach a point when they outperform human clinicians. Just as antibiotics displaced a host of medicinals for treating infections, the superior performance of such models will force their adoption. This article contemplates certain ethical and legal implications bearing on that adoption, especially because they involve a clinician's exposure to allegations of malpractice. The article discusses four relevant considerations: (1) the imperative of using explainable artificial intelligence models in clinical care, (2) specific strategies for diminishing liability when a clinician agrees or disagrees with a model's findings or recommendations but the patient nevertheless experiences a poor outcome, (3) relieving liability through legislation or regulation, and (4) comprehending such models as “persons” and therefore as potential defendants in legal proceedings. We conclude with observations on clinician–vendor relationships and argue that, although advanced artificial intelligence models have not yet arrived, clinicians must begin considering their implications now. © 2022 American College of Radiology","Artificial intelligence; ethics; legal; liability; medical malpractice","algorithm; Article; artificial intelligence; clinical reasoning; clinician; deep learning; health care organization; health care quality; human; law suit; legal liability; malpractice; manufacturing industry; medical device malfunction; medical liability; medical literature; performance; physician; policy; treatment outcome; artificial intelligence; legal liability; Artificial Intelligence; Humans; Liability, Legal; Malpractice; Physicians"
"Banjanović-Mehmedović L.","Artificial intelligence advancement in Service Robots Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114848785&partnerID=40&md5=cf3adddbe3299fe997470e159a85ca2b","Service robots have recently attracted a lot of attention of the public. Integrating with artificial intelligence and new technological issues like as embedded systems and virtual reality, modern service robots have great potential as they are capable of performing many sophisticated works of the human. The recent advances in an artificial intelligence have opened up new possibilities for technological progress in implementation of service robots in medical, field, defense, logistics, construction and demolition,rescue and security,underwater systems, inspection and maintenance, professional cleaning, production processes as well as in other branches of industry and human environment. This paper presents a comprehensive survey of artificial intelligence techniques, new challenges with explainable intelligence and examples of service robots applications. Advanced approaches of AI for service robot applications are reinforcement learning for imitation based robot learning, deep learning based human-robot interaction in home environment, optimal design of rehabilitation exoskeleton robots or explainable robotics in assistive robotics. AI with soft robotics and virtual reality have led to a new era of service robotics, which can adapt to the new needs of humans. © 2021 by Nova Science Publishers, Inc. All rights reserved.","Artificial intelligence; Collaborative robots; Deep learning; Explainable robotics; Fuzzy logic; Machine learning; Optimization algorithms; Soft robotics; Virtual reality",
"Bankar A., Padamwar K., Jahagirdar A.","Symptom analysis using a machine learning approach for early stage lung cancer","10.1109/ICISS49785.2020.9315904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100755904&doi=10.1109%2fICISS49785.2020.9315904&partnerID=40&md5=768be1381f9d7ee50e2647bb9421f6c9","The integration of the machine learning techniques in healthcare can be of huge benefit aimed at curing illness of millions of people. A lot of effort has been taken by researchers to detect and provide early-stage insights into cancer diagnosis. In the machine learning research community, various algorithms-KNN, SVM, Decision Trees and Random Forest have been applied to calculate the presence or decisiveness of cancer in correspondence with the symptoms shown by the patients. This paper aims to analyze the symptoms of the different age groups Youth, Working Class and Elderly. Tree-based algorithms like Decision Trees, Random Forest and XGBoost have been used to identify the underlying data patterns in order to calculate relative feature importances. It has been concluded that Coughing of Blood, Clubbing of Finger Nails, Genetic Risk, Passive Smoking and Snoring are the factors that are responsible for lung cancer in all the age groups in most of the cases. © 2020 IEEE.","Decision Trees; Exploratory Data Analysis; Feature Importance and Selection; Lung Cancer; Machine Learning; Random Forest; XGBoost","Biological organs; Decision trees; Diagnosis; Diseases; Random forests; Turing machines; Cancer diagnosis; Early-stage lung cancers; Genetic risks; Machine learning approaches; Machine learning research; Machine learning techniques; Passive smoking; Tree-based algorithms; Machine learning"
"Bann J., Irisarri G., Kirschen D., Miller B., Mokhtari S.","Integration of artificial intelligence applications in the EMS: issues and solutions",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029233547&partnerID=40&md5=fb474404ca64c20736ffb44d9c4333b9","This paper discusses the issues which must be addressed when integrating Artificial Intelligence (AI) and, in particular, expert system applications in an Energy Management System (EMS) environment. It is argued that these issues can be resolved by creating an environment which supports all the interfaces between the Artificial Intelligence (AI) applications and the EMS. This environment should also be responsible for maintaining a model of the power system common to all the AI applications. Once this environment has been created, AI applications can be easily 'plugged' into the EMS. The design of such an environment is described and case studies of its implementation are provided to illustrate its flexibility.",,"Computer simulation; Computer software; Electric power systems; Expert systems; Management information systems; SCADA systems; Systems analysis; User interfaces; Energy management systems; Topology processing; Artificial intelligence"
"Bannister J.J., Wilms M., Aponte J.D., Katz D.C., Klein O.D., Bernier F.P.J., Spritz R.A., Hallgrimsson B., Forkert N.D.","A Deep Invertible 3-D Facial Shape Model for Interpretable Genetic Syndrome Diagnosis","10.1109/JBHI.2022.3164848","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127812577&doi=10.1109%2fJBHI.2022.3164848&partnerID=40&md5=bd6cc20b495b40c30b8d022ead0bcf68","One of the primary difficulties in treating patients with genetic syndromes is diagnosing their condition. Many syndromes are associated with characteristic facial features that can be imaged and utilized by computer-assisted diagnosis systems. In this work, we develop a novel 3D facial surface modeling approach with the objective of maximizing diagnostic model interpretability within a flexible deep learning framework. Therefore, an invertible normalizing flow architecture is introduced to enable both inferential and generative tasks in a unified and efficient manner. The proposed model can be used (1) to infer syndrome diagnosis and other demographic variables given a 3D facial surface scan and (2) to explain model inferences to non-technical users via multiple interpretability mechanisms. The model was trained and evaluated on more than 4700 facial surface scans from subjects with 47 different syndromes. For the challenging task of predicting syndrome diagnosis given a new 3D facial surface scan, age, and sex of a subject, the model achieves a competitive overall top-1 accuracy of 71%, and a mean sensitivity of 43% across all syndrome classes. We believe that invertible models such as the one presented in this work can achieve competitive inferential performance while greatly increasing model interpretability in the domain of medical diagnosis. © 2013 IEEE.","3D shape model; Genetic syndrome; interpretable machine learning; normalizing flow","Computer aided diagnosis; Diseases; Three dimensional displays; 3D shape model; Facial surfaces; Genetic syndrome; Interpretable machine learning; Machine-learning; Normalizing flow; Shape; Solid modelling; Syndrome diagnosis; Three-dimensional display; Deep learning; accuracy; architecture; Article; child; computer assisted diagnosis; deep learning; diagnostic procedure; discriminant analysis; face; facies; genetic disorder; human; human experiment; machine learning; school child; sensitivity analysis; sex; surface property; three-dimensional imaging; computer assisted diagnosis; diagnostic imaging; procedures; Diagnosis, Computer-Assisted; Face; Humans"
"Bansal D., Khanna K., Chhikara R., Dua R.K., Malhotra R.","A superpixel powered autoencoder technique for detecting dementia","10.1111/exsy.12926","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121110621&doi=10.1111%2fexsy.12926&partnerID=40&md5=9693413878ec7ee1fdc75c2c201583ee","Dementia is a neurocognitive disorder responsible for decreasing the overall quality of life for patients. The disease has emerged as a worldwide health challenge in adults in the age group of 65 years or above. Deep learning has been successfully applied for the prediction of dementia using magnetic resonance imaging. In this paper, a superpixel-powered autoencoder technique has been proposed using a histogram of oriented gradients for extracting the relevant features. The proposed technique is capable of predicting and classifying three categories of dementia—normal, mild cognitive impairment and dementia subjects. The viability of the proposed method is established by comparing it with the other state of art models and the popular pre-trained networks including Squeezenet, Resnet50, Resnet18, Inceptionv3, Googlenet, VGG19 and Alexnet. The experimental results establish that the proposed model has performed significantly better than the state of art models and has outperformed the popular pre-trained networks. © 2021 John Wiley & Sons Ltd.","autoencoder; dementia; histogram of oriented gradient; magnetic resonance imaging; superpixel","Deep learning; Graphic methods; Magnetic resonance imaging; Superpixels; Age groups; ART model; Auto encoders; Cognitive impairment; Histogram of oriented gradients; Overall quality; Quality of life; Relevant features; Super pixels; Three categories; Neurodegenerative diseases"
"Bansal G., Nushi B., Kamar E., Horvitz E., Weld D.S.","Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130010234&partnerID=40&md5=a709626aba1d3e88fcea1680ed2c9558","AI practitioners typically strive to develop the most accurate systems, making an implicit assumption that the AI system will function autonomously. However, in practice, AI systems often are used to provide advice to people in domains ranging from criminal justice and finance to healthcare. In such AI-advised decision making, humans and machines form a team, where the human is responsible for making final decisions. But is the most accurate AI the best teammate? We argue “not necessarily” — predictable performance may be worth a slight sacrifice in AI accuracy. Instead, we argue that AI systems should be trained in a human-centered manner, directly optimized for team performance. We study this proposal for a specific type of human-AI teaming, where the human overseer chooses to either accept the AI recommendation or solve the task themselves. To optimize the team performance for this setting we maximize the team’s expected utility, expressed in terms of the quality of the final decision, cost of verifying, and individual accuracies of people and machines. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the most accuracy AI may not lead to highest team performance and show the benefit of modeling teamwork during training through improvements in expected team utility across datasets, considering parameters such as human skill and the cost of mistakes. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on AI optimization problems motivated by human-AI collaboration. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Behavioral research; AI systems; Criminal justice; Decision cost; Decisions makings; Expected utility; Final decision; Non-linear modelling; Performance; Real-world; Team performance; Decision making"
"Banswal D., Nagori M., Kshirsagar V.","Generating Homograph Models in Topic Modeling for Expediting User's Model Selection","10.1109/ICCCNT.2018.8493696","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056846237&doi=10.1109%2fICCCNT.2018.8493696&partnerID=40&md5=d846296cb9d70f4e37ae02556efd7385","The entire globe is responsible for generating large amounts of information every day due to the advent of diverse social media. Due to availability of such unstructured data, it becomes increasingly difficult to fetch relevant information that interests a user. This occurs due to the ever-present homograph words which imply multiple meanings when used under various contexts. Hence a need arises to develop an approach to organize such conflicting information generated due to homographs. Topic modeling is an approach through which we can organize the information. Twitter is one such social media site which greatly challenges the researchers to interpret accurate information. Given that a number of tweets may lead to conflicting and contradictory information as according to each user's interpretation. Hence the authors propose the use of Latent Dirichlet Allocation algorithm and generate all possible meaningful combinations through which the user's can analyze their peer's opinions by choosing the appropriate homograph models. © 2018 IEEE.","Information filtering; Pattern mining; Topic model; unstructured data","Data mining; Social networking (online); Statistics; Large amounts; Latent Dirichlet allocation; Model Selection; Pattern mining; Social media; Topic Modeling; Unstructured data; Information filtering"
"Bantilan N.","Themis-ml: A Fairness-Aware Machine Learning Interface for End-To-End Discrimination Discovery and Mitigation","10.1080/15228835.2017.1416512","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041003130&doi=10.1080%2f15228835.2017.1416512&partnerID=40&md5=06aa611eb41626fd1d396069b5c62362","As more industries integrate machine learning into socially sensitive decision processes like hiring, loan-approval, and parole-granting, we are at risk of perpetuating historical and contemporary socioeconomic disparities. This is a critical problem because on the one hand, organizations who use but do not understand the discriminatory potential of such systems will facilitate the widening of social disparities under the assumption that algorithms are categorically objective. On the other hand, the responsible use of machine learning can help us measure, understand, and mitigate the implicit historical biases in socially sensitive data by expressing implicit decision-making mental models in terms of explicit statistical models. In this article we specify, implement, and evaluate a “fairness-aware” machine learning interface called themis-ml, which is intended for use by individual data scientists and engineers, academic research teams, or larger product teams who use machine learning in production systems. © 2018 Taylor & Francis Group, LLC.","Application programming interface; discrimination discovery; fairness-aware machine learning; social bias; statistics",
"Banu R.K., Ravanan R.","A competency framework model to assess success pattern for Indian faculties a NLP based data mining approach",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020845603&partnerID=40&md5=b4bf7286757d47e0987d653409a1ffd8","Faculties who help us grow as people are responsible for imparting some of life’s most important lessons. We learn through them, through their commitment to excellence and through their ability to make us realize our own personal growth. The researchers look at the effectiveness by number of ways of assessing faculties. In our research work we analyzed and assessed the success pattern of college faculties based on Neuro-Linguistic Programming (NLP), a branch of Behavioral Psychology of the modem day. Using NLP Tools we pick up Behavior and Response Patterns in people in different life situations. The response patterns may vary in different contexts. Hence the patterns are checked in various contexts. The reports generated cut of this assessment helps to identity their core competencies and the areas of improvement for their professional growth. © 2016, Scientific Publishers. All rights reserved.","Competency; Data mining; NLP; Prefix span algorithm; Sequence pattern mining","algorithm; behavioral response; data mining; language; psychology; research work"
"Banville H., Wood S.U.N., Aimone C., Engemann D.-A., Gramfort A.","Robust learning from corrupted EEG with dynamic spatial filtering","10.1016/j.neuroimage.2022.118994","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125739940&doi=10.1016%2fj.neuroimage.2022.118994&partnerID=40&md5=5cc8511da550d7732715261350241144","Building machine learning models using EEG recorded outside of the laboratory setting requires methods robust to noisy data and randomly missing channels. This need is particularly great when working with sparse EEG montages (1–6 channels), often encountered in consumer-grade or mobile EEG devices. Neither classical machine learning models nor deep neural networks trained end-to-end on EEG are typically designed or tested for robustness to corruption, and especially to randomly missing channels. While some studies have proposed strategies for using data with missing channels, these approaches are not practical when sparse montages are used and computing power is limited (e.g., wearables, cell phones). To tackle this problem, we propose dynamic spatial filtering (DSF), a multi-head attention module that can be plugged in before the first layer of a neural network to handle missing EEG channels by learning to focus on good channels and to ignore bad ones. We tested DSF on public EEG data encompassing ∼4000 recordings with simulated channel corruption and on a private dataset of ∼100 at-home recordings of mobile EEG with natural corruption. Our proposed approach achieves the same performance as baseline models when no noise is applied, but outperforms baselines by as much as 29.4% accuracy when significant channel corruption is present. Moreover, DSF outputs are interpretable, making it possible to monitor the effective channel importance in real-time. This approach has the potential to enable the analysis of EEG in challenging settings where channel corruption hampers the reading of brain signals. © 2022","Deep learning; Electroencephalography; Machine learning; Mobile EEG; Noise robustness","adolescent; adult; aged; Article; artificial neural network; audio recording; child; controlled study; data accuracy; data analysis; deep learning; dynamic spatial filtering; electroencephalography; female; human; image analysis; information processing; major clinical study; male; neuroimaging; noise; spatial analysis; algorithm; brain; brain computer interface; machine learning; procedures; Algorithms; Brain; Brain-Computer Interfaces; Electroencephalography; Humans; Machine Learning; Neural Networks, Computer"
"Bany Muhammad M., Yeasin M.","Interpretable and parameter optimized ensemble model for knee osteoarthritis assessment using radiographs","10.1038/s41598-021-93851-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110427252&doi=10.1038%2fs41598-021-93851-z&partnerID=40&md5=d169835a5bcf059f4e36307bcdedeb1c","Knee osteoarthritis (KOA) is an orthopedic disorder with a substantial impact on mobility and quality of life. An accurate assessment of the KOA levels is imperative in prioritizing meaningful patient care. Quantifying osteoarthritis features such as osteophytes and joint space narrowing (JSN) from low-resolution images (i.e., X-ray images) are mostly subjective. We implement an objective assessment and quantification of KOA to aid practitioners. In particular, we developed an interpretable ensemble of convolutional neural network (CNN) models consisting of three modules. First, we developed a scale-invariant and aspect ratio preserving model to localize Knee joints. Second, we created multiple instances of ""hyperparameter optimized"" CNN models with diversity and build an ensemble scoring system to assess the severity of KOA according to the Kellgren–Lawrence grading (KL) scale. Third, we provided visual explanations of the predictions by the ensemble model. We tested our models using a collection of 37,996 Knee joints from the Osteoarthritis Initiative (OAI) dataset. Our results show a superior (13–27%) performance improvement compared to the state-of-the-art methods. © 2021, The Author(s).",,"computer simulation; diagnostic imaging; human; image processing; knee; knee osteoarthritis; machine learning; nerve cell network; pathophysiology; physiology; Computer Simulation; Humans; Image Processing, Computer-Assisted; Knee Joint; Machine Learning; Nerve Net; Osteoarthritis, Knee"
"Bany Muhammad M., Yeasin M.","Eigen-CAM: Visual Explanations for Deep Convolutional Neural Networks","10.1007/s42979-021-00449-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110446580&doi=10.1007%2fs42979-021-00449-3&partnerID=40&md5=257078d38af14a2b51346a1784700bee","The adoption of deep convolutional neural networks (CNN) is growing exponentially in wide varieties of applications due to exceptional performance that equals to or is better than classical machine learning as well as a human. However, such models are difficult to interpret, susceptible to overfit, and hard to decode failure. An increasing body of literature, such as class activation map (CAM), focused on understanding what representations or features a model learned from the data. This paper presents novel Eigen-CAM to enhance explanations of CNN predictions by visualizing principal components of learned representations from convolutional layers. The Eigen-CAM is intuitive, easy to use, computationally efficient, and does not require correct classification by the model. Eigen-CAM can work with all CNN models without the need to modify layers or retrain models. For the task of generating a visual explanation of CNN predictions, compared to state-of-the-art methods, Eigen-CAM is more consistent, class discriminative, and robust against classification errors made by dense layers. Empirical analyses and comparison with the best state-of-the-art methods show up to 12% improvement in weakly-supervised object localization, an average of 13% improvement in weakly-supervised segmentation, and at least 15% improvement in generic object proposal. © 2021, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. part of Springer Nature.","Class activation maps; Explainable AI; Salient features; Visual explanation of CNN; Weakly supervised localization",
"Banyal A., Sah A., Choudhury T.","Commitment of Traders Report: Angular-Based Graph Representation (Agriculture Contracts)","10.1007/978-981-15-2449-3_32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081399513&doi=10.1007%2f978-981-15-2449-3_32&partnerID=40&md5=25b9b502151e1e44aebdaa5bc5b254bd","In finance, a futures contract is a generalized legal agreement to buy or sell something at a predetermined price at a specified time later in the future, between parties anonymous to each other. The asset transacted is a commodity or financial instrument (in a general rule). The preordained price in which the assets are bought or sold is called forward price. The specified time when payment and delivery occurs is known as the delivery date. The Commodity Futures Trading Commission publishes the COT reports for the better understanding of the futures market. Particularly, the COT reports provide a breakdown of each Tuesday’s open interest for futures and options on futures markets in which 20 or more traders hold positions equivalent to or over the reporting levels established by the CFTC. COT reports released by CFTC are in either text or in spreadsheet format which are complicated and take some time to read and understand. Hence, here is an idea of developing an interactive graphical representation of all this data which will be filtered on various parameters such as desirable commodity, viewing date, different properties of the commodities, simple, understandable yet containing everything desired and also with a platform-independent way. © 2020, Springer Nature Singapore Pte Ltd.","COT; Futures; Long; Node; Open interest; Short","Artificial intelligence; Commerce; Contracts; Pattern recognition; Futures; Long; Node; Open interest; Short; Financial markets"
"Bao C., Yang Q., Gao X.-D., Lu Z.-Y., Zhang J.","Ant colony optimization with shortest distance biased dispatch for visiting constrained multiple traveling salesmen problem","10.1145/3520304.3528911","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136330811&doi=10.1145%2f3520304.3528911&partnerID=40&md5=7af616d72748bba9570408713f524932","The visiting constrained multiple traveling salesmen problem (VCMTSP) aims to minimize the total traveling cost of all salesmen by taking the accessibility of cities to salesmen into consideration. To solve this challenging problem, this paper devises a shortest distance biased dispatch (SDBD) scheme based on the accessibility of cities and a pheromone diffusion strategy for ant colony optimization (ACO). Specifically, this algorithm maintains a population of ant teams to construct feasible solutions. Each team maintains multiple ants with each ant responsible for constructing the route of one salesman to generate a feasible solution. During the solution construction of an ant team, the multiple ants construct the routes of all salesmen city by city in parallel based on the devised dispatch scheme. To further improve the solution quality, the 2-opt local search operation is integrated to further optimize the routes of all salesmen. Experiments conducted on several VCMTSP instances generated from the TSPLIB benchmark set demonstrate the effectiveness of the proposed algorithm. © 2022 Owner/Author.","ant colony optimization; combinatorial optimization; multiple traveling salesmen problem; solution construction; visiting constraints","Artificial intelligence; Combinatorial optimization; Constrained optimization; Traveling salesman problem; Diffusion strategies; Feasible solution; Local search operation; Multiple travelling salesmen problem; Problem instances; Solution construction; Solution quality; Visiting constraint; Ant colony optimization"
"Bao C., Bardhan I.R.","Performance of Accountable Care Organizations: Health Information Technology and Quality–Efficiency Trade-Offs","10.1287/isre.2021.1080","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134652572&doi=10.1287%2fisre.2021.1080&partnerID=40&md5=47e3c49e5823716df570ff0779e03ac5","Accountable care organizations (ACOs) were established under the Affordable Care Act to address systemic problems afflicting the U.S. healthcare system related to high costs and poor quality issues. ACOs represent groups of healthcare providers that are responsible for coordinating patient care with the goal of improving health outcomes for their patient population. To develop a better understanding of the role of health information technology (IT) in a value-based care environment, we study (a) whether there are potential trade-offs between ACO efficiency and quality and (b) whether effective use of health IT enables ACOs to balance competing efficiency and quality objectives. We test our models with a nationwide sample of ACO data using a two-stage approach based on data envelopment analysis and econometric estimation. We observe that efficient ACOs do not make trade-offs with respect to healthcare quality, compared with inefficient ACOs. Furthermore, we observe that hospitals that participated in ACOs, and used IT effectively for care coordination with other providers, exhibited a positive association between efficiency and quality. ACOs with higher levels of meaningful use achievement of health IT demonstrate better patient health outcomes because of greater information integration with other care providers. Our findings imply that value-based incentives alone are not sufficient to resolve trade-offs between healthcare quality and efficiency, and healthcare policy needs to incorporate appropriate incentives to foster effective IT use for health information sharing and care coordination between healthcare providers. © 2021 INFORMS","accountable care organization; data envelopment analysis; efficiency; health information technology; meaningful use; quality","Artificial intelligence; Commerce; Data envelopment analysis; Economic and social effects; Health care; Information use; Quality control; Accountable care organization; Health care providers; Health information technology; Health informations; Health outcomes; Healthcare quality; Meaningful use; Quality; Trade off; Value-based; Efficiency"
"Bao F., Deng Y., Du M., Ren Z., Wan S., Liang K.Y., Liu S., Wang B., Xin J., Chen F., Christiani D.C., Wang M., Dai Q.","Explaining the Genetic Causality for Complex Phenotype via Deep Association Kernel Learning","10.1016/j.patter.2020.100057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102966019&doi=10.1016%2fj.patter.2020.100057&partnerID=40&md5=1c92a81ca39cdf8aae732dbf26aa7c84","The genetic effect explains the causality from genetic mutations to the development of complex diseases. Existing genome-wide association study (GWAS) approaches are always built under a linear assumption, restricting their generalization in dissecting complicated causality such as the recessive genetic effect. Therefore, a sophisticated and general GWAS model that can work with different types of genetic effects is highly desired. Here, we introduce a deep association kernel learning (DAK) model to enable automatic causal genotype encoding for GWAS at pathway level. DAK can detect both common and rare variants with complicated genetic effects where existing approaches fail. When applied to four real-world GWAS datasets including cancers and schizophrenia, our DAK discovered potential casual pathways, including the association between dilated cardiomyopathy pathway and schizophrenia. Genetic mutations cause complex diseases in many different ways. Comprehensively identifying the genetic causality can lead to valuable insights into the development and treatment of diseases. However, existing genome-wide association study (GWAS) approaches are always built under linear assumption and simple disease models, restricting their generalization in discovering the complicated causality. DAK (deep association kernel learning) is a GWAS method that is constructed in a deep-learning framework and can simultaneously identify multiple types of genetic causalities without any modifications to the model. For biological contributions, the proposed approach enables the understanding of non-linear, complex genetic causalities and improves functional studies of the disease; for computational contributions, our method unifies kernel learning and association analysis in a joint explainable deep-learning framework. Genetic mutations are key factors for complex diseases. Comprehensively understanding the genetic contribution will improve the mechanism study and treatment of diseases. However, genetic causalities are complex and mutation specific. To extensively dissect the unknown genetic causality, we propose deep association kernel learning (DAK) that utilizes the power of deep learning to automatically infer complex, non-linear, various causal loci from gene sequence at pathway level. On four real datasets covering cancers and mental disease, we demonstrate that DAK can discover unseen yet meaningful suspicious pathways. © 2020 The Authors","association analysis; deep learning; disease causality; DSML 2: Proof-of-Concept: Data science output has been formulated, implemented, and tested for one domain/problem; genome-wide association studies; kernel learning","Diseases; Genes; Learning systems; Association analysis; Complex phenotype; Dilated cardiomyopathy; Genetic mutations; Genome-wide association studies; Kernel learning; Learning frameworks; Mechanism studies; Deep learning"
"Bao L., Krause N.M., Calice M.N., Scheufele D.A., Wirz C.D., Brossard D., Newman T.P., Xenos M.A.","Whose AI? How different publics think about AI and its social impacts","10.1016/j.chb.2022.107182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123417367&doi=10.1016%2fj.chb.2022.107182&partnerID=40&md5=563bfb1f4b089e06759dc0ecb95c1703","Effective public engagement with complex technologies requires a nuanced understanding of how different audiences make sense of and communicate disruptive technologies with immense social implications. Using latent class analysis (LCA) on nationally-representative survey data (N = 2,700), we examine public attitudes on different aspects of AI, and segment the U.S. population based on their AI-related risk and benefit perceptions. Our analysis reveals five segments: the negative, perceiving risks outweighing benefits; the ambivalent, seeing high risks and benefits; the tepid, perceiving slightly more benefits than risks; the ambiguous, perceiving moderate risks and benefits; and the indifferent, perceiving low risks and benefits. For societal debates surrounding a deeply disruptive issue like AI, our findings suggest potential opportunities for engagement by soliciting input from individuals in segments with varying levels of support for AI, as well as a way to widen representation of voices and ensure responsible innovation of AI. © 2022 Elsevier Ltd","Artificial intelligence; Benefit perceptions; Public opinion; Risk perceptions; Segmentation analysis","Population statistics; Risk assessment; Social aspects; Benefit perception; Disruptive technology; Latent class analysis; Public attitudes; Public engagement; Public opinions; Segmentation analysis; Social impact; Social implication; Survey data; Risk perception; adult; article; artificial intelligence; female; human; latent class analysis; major clinical study; male; public opinion; risk perception; voice"
"Bao L., Wang Z., Wu Z., Luo H., Yu J., Kang Y., Cao D., Hou T.","Kinome-wide polypharmacology profiling of small molecules by multi-task graph isomorphism network approach","10.1016/j.apsb.2022.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133863677&doi=10.1016%2fj.apsb.2022.05.004&partnerID=40&md5=a721c4abb33f210c2357b79c6514183e","Prediction of the interactions between small molecules and their targets play important roles in various applications of drug development, such as lead discovery, drug repurposing and elucidation of potential drug side effects. Therefore, a variety of machine learning-based models have been developed to predict these interactions. In this study, a model called auxiliary multi-task graph isomorphism network with uncertainty weighting (AMGU) was developed to predict the inhibitory activities of small molecules against 204 different kinases based on the multi-task Graph Isomorphism Network (MT-GIN) with the auxiliary learning and uncertainty weighting strategy. The calculation results illustrate that the AMGU model outperformed the descriptor-based models and state-of-the-art graph neural networks (GNN) models on the internal test set. Furthermore, it also exhibited much better performance on two external test sets, suggesting that the AMGU model has enhanced generalizability due to its great transfer learning capacity. Then, a naïve model-agnostic interpretable method for GNN called edges masking was devised to explain the underlying predictive mechanisms, and the consistency of the interpretability results for 5 typical epidermal growth factor receptor (EGFR) inhibitors with their structure‒activity relationships could be observed. Finally, a free online web server called KIP was developed to predict the kinome-wide polypharmacology effects of small molecules (http://cadd.zju.edu.cn/kip). © 2022 Chinese Pharmaceutical Association and Institute of Materia Medica, Chinese Academy of Medical Sciences","Artificial intelligence; Graph neural networks; Kinases; Kinome-wide polypharmacology; Machine learning",
"Bao P., Hong W., Li X.","Predicting Paper Acceptance via Interpretable Decision Sets","10.1145/3442442.3451370","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107615952&doi=10.1145%2f3442442.3451370&partnerID=40&md5=1266326104e39088977cbb105fa3353c","Measuring the quality of research work is an essential component of the scientific process. With the ever-growing rates of articles being submitted to top-Tier conferences, and the potential consistency and bias issues in the peer review process identified by scientific community, it is thus of great necessary and challenge to automatically evaluate submissions. Existing works mainly focus on exploring relevant factors and applying machine learning models to simply be accurate at predicting the acceptance of a given academic paper, while ignoring the interpretability power which is required by a wide range of applications. In this paper, we propose a framework to construct decision sets that consist of unordered if-Then rules for predicting paper acceptance. We formalize decision set learning problem via a joint objective function that simultaneously optimize accuracy and interpretability of the rules, rather than organizing them in a hierarchy. We evaluate the effectiveness of the proposed framework by applying it on a public scientific peer reviews dataset. Experimental results demonstrate that the learned interpretable decision sets by our framework performs on par with state-of-The-Art classification algorithms which optimize exclusively for predictive accuracy and much more interpretable than rule-based methods. © 2021 ACM.","acceptance prediction; interpretability; peer reviews","World Wide Web; Classification algorithm; Joint objective function; Machine learning models; Peer-review process; Predictive accuracy; Rule-based method; Scientific community; State of the art; Forecasting"
"Bao S., Wang T., Zhou L., Dai G., Sun G., Shen J.","Two-Layer Matrix Factorization and Multi-Layer Perceptron for Online Service Recommendation","10.3390/app12157369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136930172&doi=10.3390%2fapp12157369&partnerID=40&md5=4757fcbb8a91eabea460ecf08cd58079","Service recommendation is key to improving users’ online experience. The development of the Internet has accelerated the creation of many services, and whether users can obtain good experiences among the massive number of services mainly depends on the quality of service recommendation. It is commonly believed that deep learning has excellent nonlinear fitting ability in capturing the complex interactions between users and items. The advantage in learning intricacy relationships enables deep learning to become an important technology for present service recommendation. Recently, it is noticed that linear models can perform almost as well as the state-of-the-art deep learning models, suggesting that capturing linear relationships between users and items is also very important for recommender systems. Therefore, numerous deep learning systems combined with linear models have been proposed. However, existing models are incapable of considering the size of the embedding. When the embedding dimension is too large, it leads to overfitting and thus influences the model’s ability to capture linear relationships. In this paper, a neural network based on two-layer matrix factorization and multi-layer perceptron—Two-layer Matrix factorization and Multi-layer perceptron Neural Network (TMMNN)—is proposed. To solve the problem of overfitting caused by an oversized embedding dimension, multi-size embedding technology has been integrated into the model. Matrix factorization and the multi-layer perceptron are placed in the upper and lower layers respectively, and they both receive embedding vectors dynamically adjusted for dimensions. In the upper layer, the matrix factorization is responsible for receiving the embedding of users and items, capturing linear relationships, and then yielding the generated new vectors as input to the multi-layer perceptron in the lower layer. Compared to other previously proposed models, the experimental results on the standard datasets MovieLens 20M and MovieLens Latest show that the TMMNN model is evidently better in terms of prediction accuracy. © 2022 by the authors.","deep learning; linear model; matrix factorization; multi-size embedding; recommendation system",
"Bao S., Zhou X., Zhang L., Zhou J., To K.K.W., Wang B., Wang L., Zhang X., Song Y.-Q.","Prioritizing genes responsible for host resistance to influenza using network approaches","10.1186/1471-2164-14-816","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887878520&doi=10.1186%2f1471-2164-14-816&partnerID=40&md5=da1f0b21aae90bc7161e9236fb6a6912","Background: The genetic make-up of humans and other mammals (such as mice) affects their resistance to influenza virus infection. Considering the complexity and moral issues associated with experiments on human subjects, we have only acquired partial knowledge regarding the underlying molecular mechanisms. Although influenza resistance in inbred mice has been mapped to several quantitative trait loci (QTLs), which have greatly narrowed down the search for host resistance genes, only few underlying genes have been identified.Results: To prioritize a list of promising candidates for future functional investigation, we applied network-based approaches to leverage the information of known resistance genes and the expression profiles contrasting susceptible and resistant mouse strains. The significance of top-ranked genes was supported by different lines of evidence from independent genetic associations, QTL studies, RNA interference (RNAi) screenings, and gene expression analysis. Further data mining on the prioritized genes revealed the functions of two pathways mediated by tumor necrosis factor (TNF): apoptosis and TNF receptor-2 signaling pathways. We suggested that the delicate balance between TNF's pro-survival and apoptotic effects may affect hosts' conditions after influenza virus infection.Conclusions: This study considerably cuts down the list of candidate genes responsible for host resistance to influenza and proposed novel pathways and mechanisms. Our study also demonstrated the efficacy of network-based methods in prioritizing genes for complex traits. © 2013 Bao et al.; licensee BioMed Central Ltd.",,"tumor necrosis factor; tumor necrosis factor receptor 2; apoptosis; article; controlled study; gene; gene expression; gene function; gene identification; genetic analysis; genetic association; host resistance; host resistance gene; human; influenza; mouse strain; nonhuman; quantitative trait locus; RNA interference; signal transduction; survival; Mammalia; Mus; Orthomyxoviridae; Animals; Disease Resistance; Host-Pathogen Interactions; Humans; Influenza, Human; Mice; Orthomyxoviridae; Quantitative Trait Loci; Signal Transduction; Tumor Necrosis Factor-alpha"
"Bao W., King P., Zheng J., Smith B.E.","Expert capnogram analysis","10.1109/51.136134","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026832968&doi=10.1109%2f51.136134&partnerID=40&md5=2112cdcbdac2d7b1fb2d637e70bebf68","A real-time expert system that deciphers CO2 waveforms (capnograms) is described. The Capnogram Analyzer Expert System (CAES) was designed using both traditional pattern-recognition methods and an artificial intelligence (AI) approach for signal description and classification. The pattern-recognition technique is used to extract features from the digitized CO2 waveforms. The AI approach involves abstracting CO2 waveforms from numeric representation to higher-level symbolic representation, and a so-called reasoning step to analyze the symbolic data. The CAES consists of three essential components: segmentation, single breath cycle identification, and waveform classification. Each component is an expert in itself and is responsible for abstracting the waveform information from a lower level to a higher level using its own domain-specific knowledge base.",,"Carbon Dioxide; Expert Systems - Medical Applications; Pattern Recognition; Waveform Analysis; Capnogram Analyzer Expert System; Carbon Dioxide Waveforms; Biomedical Engineering; article; capnography; computer analysis; computer system; waveform"
"Bao Y., Tang Z., Li H.","Compressive-sensing data reconstruction for structural health monitoring: a machine-learning approach","10.1177/1475921719844039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064890756&doi=10.1177%2f1475921719844039&partnerID=40&md5=83e6fda6c6ad4bd5af5d15bedb28a934","Compressive sensing has been studied and applied in structural health monitoring for data acquisition and reconstruction, wireless data transmission, structural modal identification, and spare damage identification. The key issue in compressive sensing is finding the optimal solution for sparse optimization. In the past several years, many algorithms have been proposed in the field of applied mathematics. In this article, we propose a machine learning–based approach to solve the compressive-sensing data-reconstruction problem. By treating a computation process as a data flow, the solving process of compressive sensing–based data reconstruction is formalized into a standard supervised-learning task. The prior knowledge, i.e. the basis matrix and the compressive sensing–sampled signals, is used as the input and the target of the network; the basis coefficient matrix is embedded as the parameters of a certain layer; and the objective function of conventional compressive sensing is set as the loss function of the network. Regularized by l1-norm, these basis coefficients are optimized to reduce the error between the original compressive sensing–sampled signals and the masked reconstructed signals with a common optimization algorithm. In addition, the proposed network is able to handle complex bases, such as a Fourier basis. Benefiting from the nature of a multi-neuron layer, multiple signal channels can be reconstructed simultaneously. Meanwhile, the disassembled use of a large-scale basis makes the method memory-efficient. A numerical example of multiple sinusoidal waves and an example of field-test wireless data from a suspension bridge are carried out to illustrate the data-reconstruction ability of the proposed approach. The results show that high reconstruction accuracy can be obtained by the machine learning–based approach. In addition, the parameters of the network have clear meanings; the inference of the mapping between input and output is fully transparent, making the compressive-sensing data-reconstruction neural network interpretable. © The Author(s) 2019.","compressive sensing; interpretable machine learning; model-driven machine learning; sparse data reconstruction; Structural health monitoring","Compressed sensing; Damage detection; Data acquisition; Data transfer; Machine learning; Matrix algebra; Signal reconstruction; Structural health monitoring; Compressive sensing; Damage Identification; Machine learning approaches; Model-driven; Optimization algorithms; Reconstruction accuracy; Sparse data; Wireless data transmission; Data compression"
"Bao Y., Chen W.","Automated Concept Extraction in Internet-of-Things","10.1109/Cybermatics_2018.2018.00295","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067890446&doi=10.1109%2fCybermatics_2018.2018.00295&partnerID=40&md5=acc11a1a246d246c5090a1e4c63ff1a4","With the emergence of Internet-of-Things (IoT), we are witnessing rapid increases in the amount of sensory data, which are streaming, volatile, often real-Time, and heterogeneous in nature, when we enable interactions with the physical world by using sensors and intelligent objects. To help derive value-Add insights into the IoT systems and their users, we need methods to automatically extract machine-readable concepts from raw sensory data. To date, only limited research effort has been devoted to automatically extracting (machine-readable) concepts from large sets of heterogeneous multivariate time-series sensory data. In this paper, we propose a framework for real-Time automatic concept-extraction in the IoT environments. We enhance the symbolic aggregate approximation (SAX) algorithm into an optimized version (referred to as MultiSAX) for multivariate time-series sensory data-Thereby transforming the multivariate time-series sensory data into symbolic representations. By redefining the distance function and density function of the symbolic representations, we extend clustering algorithm and automatically group symbolic representations into different concepts, which can be further operated on by utilizing a rule-based mechanism to make the concepts machine understandable. Our evaluation results show that our proposed method, operating on heterogeneous multivariate sensory data, can achieve automatic concept-extraction with low construction error and low communications overhead. © 2018 IEEE.","concept extraction; heterogeneous multivariate time-series; Internet of Things; Sensory Data","Approximation algorithms; Blockchain; Clustering algorithms; Data mining; Extraction; Green computing; Metadata; Time series; Automatic concept extractions; Concept extraction; Internet of Things (IOT); Multivariate time series; Rule-based mechanisms; Sensory data; Symbolic aggregate approximation (SAX); Symbolic representation; Internet of things"
"Bao Y.Q., Liu D.W., Li H.","Output-only structural modal identification methods based on neural network","10.1201/9780429279119-447","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117564170&doi=10.1201%2f9780429279119-447&partnerID=40&md5=426e8871c1bd94581bd28381aa1e4d54","In this paper, we propose a neural network approach to identify the structural modal shapes of the output-only data. It takes advantage of the sparseness of the response signal in the time-frequency domain. We use the principle of unsupervised learning and takes advantage of sparseness of the response signal in the time-frequency domain, to get the structural modal shapes. A neural network is designed to get the modal shapes. The mixture signals, i.e., the structural response data, first transformed to the time-frequency domain, and then the most sparse point is found. Then we use a neural network to get the cluster center, which is the mode shapes. A numerical example of a simple structure is made to illustrate the modal shape identification ability of the proposed approach. © 2021 Taylor & Francis Group, London","Interpretable machine learning; Modal parameter identification; Neural network; Sparseness of the response signal; Structural health monitoring","Frequency domain analysis; Identification (control systems); Life cycle; Machine learning; Modal analysis; Interpretable machine learning; Modal identification; Modal shape; Modal-parameter identifications; Neural-networks; Output only; Sparseness of the response signal; Structural modals; Time frequency domain; Structural health monitoring"
"Bapat R., Mandya A., Liu X., Abraham B., Brown D.E., Kang H., Veeraraghavan M.","Identifying malicious botnet traffic using logistic regression","10.1109/SIEDS.2018.8374749","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049352600&doi=10.1109%2fSIEDS.2018.8374749&partnerID=40&md5=3e9c681995031054a20b3df1b3824544","An important source of cyber-attacks is malware, which proliferates in different forms such as botnets. The botnet malware typically looks for vulnerable devices across the Internet, rather than targeting specific individuals, companies or industries. It attempts to infect as many connected devices as possible, using their resources for automated tasks that may cause significant economic and social harm while being hidden to the user and device. Thus, it becomes very difficult to detect such activity. A considerable amount of research has been conducted to detect and prevent botnet infestation. In this paper, we attempt to create a foundation for an anomaly-based intrusion detection system using a statistical learning method to improve network security and reduce human involvement in botnet detection. We focus on identifying the best features to detect botnet activity within network traffic using a lightweight logistic regression model. The network traffic is processed by Bro, a popular network monitoring framework which provides aggregate statistics about the packets exchanged between a source and destination over a certain time interval. These statistics serve as features to a logistic regression model responsible for classifying malicious and benign traffic. Our model is easy to implement and simple to interpret. We characterized and modeled 8 different botnet families separately and as a mixed dataset. Finally, we measured the performance of our model on multiple parameters using F1 score, accuracy and Area Under Curve (AUC). © 2018 IEEE.","Botnet Detection; Cyber Security; Logistic Regression; Machine Learning","Botnet; Computer crime; Intrusion detection; Learning systems; Malware; Network security; Anomaly based intrusion detection systems; Botnet detections; Cyber security; Logistic Regression modeling; Logistic regressions; Multiple parameters; Network Monitoring; Statistical learning methods; Regression analysis"
"Baptista M.L., Goebel K., Henriques E.M.P.","Relation between prognostics predictor evaluation metrics and local interpretability SHAP values","10.1016/j.artint.2022.103667","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125490746&doi=10.1016%2fj.artint.2022.103667&partnerID=40&md5=e954d23f5b715102f4ce3be1000b3375","Maintenance decisions in domains such as aeronautics are becoming increasingly dependent on being able to predict the failure of components and systems. When data-driven techniques are used for this prognostic task, they often face headwinds due to their perceived lack of interpretability. To address this issue, this paper examines how features used in a data-driven prognostic approach correlate with established metrics of monotonicity, trendability, and prognosability. In particular, we use the SHAP model (SHapley Additive exPlanations) from the field of eXplainable Artificial Intelligence (XAI) to analyze the outcome of three increasingly complex algorithms: Linear Regression, Multi-Layer Perceptron, and Echo State Network. Our goal is to test the hypothesis that the prognostics metrics correlate with the SHAP model's explanations, i.e., the SHAP values. We use baseline data from a standard data set that contains several hundred run-to-failure trajectories for jet engines. The results indicate that SHAP values track very closely with these metrics with differences observed between the models that support the assertion that model complexity is a significant factor to consider when explainability is a consideration in prognostics. © 2022 The Author(s)","Local interpretability; Model-agnostic interpretability; Monotonicity; Prognosability; SHAP values; Trendability","Complex networks; Evaluation metrics; Interpretability; Local interpretability; Maintenance decisions; Model-agnostic interpretability; Monotonicity; Prognosability; SHAP value; Trendability; Systems engineering"
"Baptista Veiga E.A., Sousa Saraiva E.","Suitability of artificial intelligence techniques for the modeling and simulation of kinetic-electronic systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024943620&partnerID=40&md5=41fc309879614e614c06c9a8b83611f8","A methodology for modeling and simulating kinetic-electronic systems using artificial intelligence techniques is outlined. The methodology involves the coupling of numeric and semantic representations. The objective is to separate the simulation model responsible for the numeric computing from the knowledge system responsible for the semantic process. The semantic knowledge is processed by a Prolog module and all mathematical knowledge is processed using Fortran. The advantages of the proposed approach are demonstrated on a simple circuit example.",,"Artificial Intelligence; Electronic Circuits, Switching; Semiconductor Diodes; Thyristors; Prolog Knowledge Base; Prolog Language; Semantic Knowledge; Electric Networks, Switching"
"Baradaran M., Bergevin R.","Object Class Aware Video Anomaly Detection through Image Translation","10.1109/CRV55824.2022.00020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138487379&doi=10.1109%2fCRV55824.2022.00020&partnerID=40&md5=37282fa258778d3f54ad4c44710a394a","Semi-supervised video anomaly detection (VAD) methods formulate the task of anomaly detection as detection of deviations from the learned normal patterns. Previous works in the field (reconstruction or prediction-based methods) suffer from two drawbacks: 1) They focus on low-level features, and they (especially holistic approaches) do not effectively consider the object classes. 2) Object-centric approaches neglect some of the context information (such as location). To tackle these challenges, this paper proposes a novel two-stream object-aware VAD method that learns the normal appearance and motion patterns through image translation tasks. The appearance branch translates the input image to the target semantic segmentation map produced by Mask-RCNN, and the motion branch associates each frame with its expected optical flow magnitude. Any deviation from the expected appearance or motion in the inference stage shows the degree of potential abnormality. We evaluated our proposed method on the ShanghaiTech, UCSD-Pedl, and UCSD-Ped2 datasets and the results show competitive performance compared with state-of-the-art works. Most importantly, the results show that, as significant improvements to previous methods, detections by our method are completely explainable and anomalies are localized accurately in the frames. © 2022 IEEE.","deep learning; semantic segmentation; semi-supervised learning; video anomaly detection","Computer vision; Deep learning; Object detection; Semantic Segmentation; Semantics; Anomaly detection; Anomaly detection methods; Deep learning; Field reconstruction; Image translation; Object class; Semantic segmentation; Semi-supervised; Semi-supervised learning; Video anomaly detection; Anomaly detection"
"Barak L., Goldberg A.E.","Modeling the partial productivity of constructions",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028735004&partnerID=40&md5=7446c58b70eb574be3cd697ff7e7d8d6","People regularly produce novel sentences that sound nativelike (e.g., she googled us the information), while they also recognize that other novel sentences sound odd, even though they are interpretable (e.g., ? She explained us the information). This work offers a Bayesian, incremental model that learns clusters that correspond to grammatical constructions of different type and token frequencies. Without specifying in advance the number of constructions, their semantic contributions, nor whether any two constructions compete with one another, the model successfully generalizes when appropriate while identifying and suggesting an alternative when faced with overgeneralization errors. Results are consistent with recent psycholinguistic work that demonstrates that the existence of competing alternatives and the frequencies of those alternatives play a key role in the partial productivity of grammatical constructions. The model also goes beyond the psycholinguistic work in that it investigates a role for constructions' overall frequency. © Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Linguistics; Machine oriented languages; Productivity; Semantics; Bayesian; Grammatical construction; Incremental modeling; Overgeneralization; Learning systems"
"Baral S.K., Rath R.C., Goel R., Singh T.","Role of Digital Technology and Artificial Intelligence for Monitoring Talent Strategies to Bridge the Skill Gap","10.1109/MECON53876.2022.9751837","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129226908&doi=10.1109%2fMECON53876.2022.9751837&partnerID=40&md5=ad2307d5d9b89067255b4aeb15a36628","At present, digital technology and Artificial intelligence plays a vital role for monitoring the talents and bridge the skill gap needed by various organizations. Take the situation extremely seriously. Employers are experiencing a global skills deficit, which might limit intelligent technology's economic advantages. Long after today's talent shortages have passed, digital developments will continue to alter the need for talents. Incremental improvements to our education and business learning systems will not be enough to achieve such high levels of achievement in such a short period of time. As a result of the challenge, company leaders must reconsider how they train their workforces, from anticipating the talents their firms will require to assisting people in learning and applying new skills throughout their careers. Investing in people is both responsible and cost effective for executives attempting to achieve growth in a highly competitive and quickly changing corporate environment. In this invited paper, the researchers here have trying to the level best to find out the root causes of Skill Gap why occurs and what the main causes behind it? How will it be filled for enhancing the potentiality of workforce of an organization? In this connection, various models have been taken for skill-based learning, tools and techniques for proper study and analysis of the relevant causes/factors behind it, which are inhibit the skill gap in various constraints of organizational growth and development. © 2022 IEEE.","Artificial Intelligence; Bridging Skill Gap; Digital Technology; Work Force","Artificial intelligence; Cost effectiveness; Bridging skill gap; Company leaders; Cost effective; Digital technologies; Economic advantages; Incremental improvements; Intelligent technology; Short periods; Skills gaps; Work force; Learning systems"
"Baraldi A., Del Buono F., Paganelli M., Guerra F.","Using landmarks for explaining entity matching models","10.5441/002/edbt.2021.50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113713696&doi=10.5441%2f002%2fedbt.2021.50&partnerID=40&md5=f8f647a198c1b0c79e4b254857aa5e21","The state of the art approaches for performing Entity Matching (EM) rely on machine & deep learning models for inferring pairs of matching / non-matching entities. Although the experimental evaluations demonstrate that these approaches are effective, their adoption in real scenarios is limited by the fact that they are difficult to interpret. Explainable AI systems have been recently proposed for complementing deep learning approaches. Their application to the scenario offered by EM is still new and requires to address the specificity of this task, characterized by particular dataset schemas, describing a pair of entities, and imbalanced classes. This paper introduces Landmark Explanation, a generic and extensible framework that extends the capabilities of a post-hoc perturbation-based explainer over the EM scenario. Landmark Explanation generates perturbations that take advantage of the particular schemas of the EM datasets, thus generating explanations more accurate and more interesting for the users than the ones generated by competing approaches. © 2021 Copyright held by the owner/author(s).",,"Database systems; Deep learning; Entity matching; Experimental evaluation; Extensible framework; Imbalanced class; Learning approach; Learning models; On-machines; State-of-the-art approach; Learning systems"
"Baralis E., Garza P.","Associative text categorization exploiting negated words","10.1145/1141277.1141402","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751047913&doi=10.1145%2f1141277.1141402&partnerID=40&md5=204f6771f252ee5eb1f53058df6f571c","Associative classification has been recently applied to text document categorization. However, differently from classification of structured data, the quality of the generated classifier is rather low. This effect is mainly due to the poor precision of generated rules. To increase the precision of associative classifiers we propose the use of classification rules including negated words, i.e. words that the considered document should not contain. Rules are in the form ""If a document includes words A and B, but not word Z, then it belongs to class Ci"". Mining classification rules with negated words becomes quickly intractable when decreasing the support threshold. We tackle this problem by means of an opportunistic approach, where negated words are only generated to specialize rules that may wrongly classify training documents. Hence precision is increased, without losing recall. Experiments on the Reuters corpus show that our classifier based on negated words achieves good precision and recall results, while yielding an easily interpretable model typical of associative classifiers. Copyright 2006 ACM.","Association rules; Text classification","Classification (of information); Data mining; Data structures; Association rules; Reuters corpus; Text classification; Text processing"
"Baranwal M., Krishnan S., Oneka M., Frankel T., Rao A.","CGAT: Cell Graph ATtention Network for Grading of Pancreatic Disease Histology Images","10.3389/fimmu.2021.727610","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117337754&doi=10.3389%2ffimmu.2021.727610&partnerID=40&md5=f5a968e65995c3004b20a91e79fe046e","Early detection of Pancreatic Ductal Adenocarcinoma (PDAC), one of the most aggressive malignancies of the pancreas, is crucial to avoid metastatic spread to other body regions. Detection of pancreatic cancer is typically carried out by assessing the distribution and arrangement of tumor and immune cells in histology images. This is further complicated due to morphological similarities with chronic pancreatitis (CP), and the co-occurrence of precursor lesions in the same tissue. Most of the current automated methods for grading pancreatic cancers rely on extensive feature engineering involving accurate identification of cell features or utilising single number spatially informed indices for grading purposes. Moreover, sophisticated methods involving black-box approaches, such as neural networks, do not offer insights into the model’s ability to accurately identify the correct disease grade. In this paper, we develop a novel cell-graph based Cell-Graph Attention (CGAT) network for the precise classification of pancreatic cancer and its precursors from multiplexed immunofluorescence histology images into the six different types of pancreatic diseases. The issue of class imbalance is addressed through bootstrapping multiple CGAT-nets, while the self-attention mechanism facilitates visualization of cell-cell features that are likely responsible for the predictive capabilities of the model. It is also shown that the model significantly outperforms the decision tree classifiers built using spatially informed metric, such as the Morisita-Horn (MH) indices. © Copyright © 2021 Baranwal, Krishnan, Oneka, Frankel and Rao.","attention network; cell-graph; chronic pancreatitis; graph convolutional network (GCN); pancreas; PDAC (pancreatic ductal adenocarcinoma); spatial method","tumor marker; adult; algorithm; Article; artificial neural network; attention network; bootstrapping; chronic pancreatitis; computer assisted tomography; decision tree; electroencephalography; female; histology; human; immunofluorescence; learning algorithm; machine learning; major clinical study; male; middle aged; nerve potential; paired end sequencing; pancreas disease; pancreatic ductal carcinoma; predictive value; quantitative structure activity relation; regulatory T lymphocyte; speech discrimination; support vector machine; classification; pancreas disease; pathology; phenotype; theoretical model; Adult; Deep Learning; Female; Humans; Male; Middle Aged; Models, Theoretical; Pancreatic Diseases; Phenotype"
"Baranyi M., Nagy M., Molontay R.","Interpretable Deep Learning for University Dropout Prediction","10.1145/3368308.3415382","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094898209&doi=10.1145%2f3368308.3415382&partnerID=40&md5=497bdd2a6e050ae0b7dd4e76df130163","The early identification of college students at risk of dropout is of great interest and importance all over the world, since the early leaving of higher education is associated with considerable personal and social costs. In Hungary, especially in STEM undergraduate programs, the dropout rate is particularly high, much higher than the EU average. In this work, using advanced machine learning models such as deep neural networks and gradient boosted trees, we aim to predict the final academic performance of students at the Budapest University of Technology and Economics. The dropout prediction is based on the data that are available at the time of enrollment. In addition to the predictions, we also interpret our machine learning models with the help of state-of-the-art interpretable machine learning techniques such as permutation importance and SHAP values. The accuracy and AUC of the best-performing deep learning model are 72.4% and 0.771, respectively that slightly outperforms XGBoost, the cutting-edge benchmark model for tabular data. © 2020 ACM.","dropout prediction; explainable artificial intelligence; higher education; interpretable machine learning; neural networks","Deep neural networks; Education computing; Forecasting; Learning systems; Students; Academic performance; Benchmark models; Budapest University; College students; Higher education; Machine learning models; Machine learning techniques; Undergraduate program; Deep learning"
"Barata C., Celebi M.E., Marques J.S.","Explainable skin lesion diagnosis using taxonomies","10.1016/j.patcog.2020.107413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085040302&doi=10.1016%2fj.patcog.2020.107413&partnerID=40&md5=54f53f3e17323e806531e7da8bdcfd51","Deep neural networks have rapidly become an indispensable tool in many classification applications. However, the inclusion of deep learning methods in medical diagnostic systems has come at the cost of diminishing their explainability. This significantly reduces the safety of a diagnostic system, since the physician is unable to interpret and validate the output. Therefore, in this work we aim to address this major limitation and improve the explainability of a skin cancer diagnostic system. We propose to leverage two sources of information: (i) medical knowledge, in particular the taxonomic organization of skin lesions, which will be used to develop a hierarchical neural network; and (ii) recent advances in channel and spatial attention modules, which can identify interpretable features and regions in dermoscopy images. We demonstrate that the proposed approach achieves competitive results in two dermoscopy data sets (ISIC 2017 and 2018) and provides insightful information about its decisions, thus increasing the safety of the model. © 2020","Channel attention; Explainability; Hierarchical deep learning; Safety-critical CADS; Skin cancer; Spatial attention","Deep learning; Deep neural networks; Dermatology; Knowledge management; Learning systems; Medical imaging; Neural networks; Dermoscopy images; Diagnostic systems; Hierarchical neural networks; Indispensable tools; Learning methods; Medical diagnostics; Medical knowledge; Spatial attention; Diagnosis"
"Barati M., Bai Q., Liu Q.","Mining semantic association rules from RDF data","10.1016/j.knosys.2017.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021971477&doi=10.1016%2fj.knosys.2017.07.009&partnerID=40&md5=8762b7719f9fa7c02aae0a94181c53a9","The Semantic Web opens up new opportunities for the data mining research. Semantic Web data is usually represented in the RDF triple format (subject, predicate, object). Large RDF-style Knowledge Bases contain hundreds of millions of RDF triples that represent knowledge in a machine-understandable format. Association rule mining is one of the most effective techniques for detecting frequent patterns. In the context of Semantic Web data mining, most existing methods rely on users intervention that is time-consuming and error-prone due to a large amount of data. Meanwhile, rule quality factors (e.g. support and confidence) usually consider knowledge at the instance-level. Namely, these factors disregard the knowledge embedded at the schema-level. In this paper, we demonstrate that ignoring knowledge encoded at the schema-level negatively impacts the interpretation of discovered rules. We introduce an approach called SWARM (Semantic Web Association Rule Mining) that automatically mines Semantic Association Rules from RDF data. The main achievement of SWARM is to reveal common behavioural patterns associated with knowledge at the instance-level and schema-level. We discuss how to utilize knowledge encoded at the schema-level to add more semantics to the rules. We compare the semantic of rules discovered by SWRAM with one of the latest approaches in this field to show the importance of considering schema-level knowledge. Initial experiments performed on RDF-style Knowledge Bases demonstrate the effectiveness of the proposed approach. © 2017 Elsevier B.V.","Association rule mining; Knowledge discovery; Ontology; Semantic Web data","Association rules; Data mining; Ontology; Error prones; Large amounts; Quality factors; RDF data; RDF triples; Semantic associations; Style knowledge; Support and confidence; Semantic Web"
"Barbado A., Corcho Ó.","Interpretable machine learning models for predicting and explaining vehicle fuel consumption anomalies","10.1016/j.engappai.2022.105222","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135166491&doi=10.1016%2fj.engappai.2022.105222&partnerID=40&md5=a5e7f7006e55b54b25f866ef1e9dbf74","Identifying anomalies in the fuel consumption of vehicle fleets is crucial for optimizing consumption and reducing costs. However, this information alone is insufficient since fleet operators need to know the causes behind anomalous fuel consumption. Therefore, we combine unsupervised anomaly detection techniques, domain knowledge and interpretable Machine Learning models for explaining potential causes of abnormal fuel consumption in terms of feature relevance. The explanations are used for generating recommendations about fuel optimization that are adjusted according to two different user profiles: fleet managers and fleet operators. Results are evaluated over real-world data from telematics devices connected to diesel and petrol vehicles from different types of industrial vehicle fleets. We carry out an evaluation through model performance and Explainable AI metrics that compare the explanations in terms of representativeness, fidelity, stability, contrastiveness and consistency with prior beliefs. © 2022 The Author(s)","Explainable artificial intelligence; Explainable artificial intelligence metrics; Explainable boosting machine; Generalized additive models; Interpretable machine learning; Vehicle fuel consumption","Adaptive boosting; Anomaly detection; Domain Knowledge; Fleet operations; Fuel additives; Vehicles; Explainable artificial intelligence; Explainable artificial intelligence metric; Explainable boosting machine; Generalized additive model; Interpretable machine learning; Machine learning models; Machine-learning; Vehicle fleets; Vehicle fuel consumption; Vehicle fuels; Machine learning"
"Barbado A., Corcho Ó., Benjamins R.","Rule extraction in unsupervised anomaly detection for model explainability: Application to OneClass SVM[Formula presented]","10.1016/j.eswa.2021.116100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118512496&doi=10.1016%2fj.eswa.2021.116100&partnerID=40&md5=a28a9a275719475326ef0f6f65da96d6","OneClass SVM is a popular method for unsupervised anomaly detection. As many other methods, it suffers from the black box problem: it is difficult to justify, in an intuitive and simple manner, why the decision frontier is identifying data points as anomalous or non anomalous. This problem is being widely addressed for supervised models. However, it is still an uncharted area for unsupervised learning. In this paper, we evaluate several rule extraction techniques over OneClass SVM models, while presenting alternative designs for some of those algorithms. Furthermore, we propose algorithms for computing metrics related to eXplainable Artificial Intelligence (XAI) regarding the “comprehensibility”, “representativeness”, “stability” and “diversity” of the extracted rules. We evaluate our proposals with different data sets, including real-world data coming from industry. Consequently, our proposal contributes to extending XAI techniques to unsupervised machine learning models. © 2021 The Authors","Anomaly detection; Metrics; OneClass SVM; Rule extraction; Unsupervised learning; XAI","Extraction; Unsupervised learning; Anomaly detection; Black boxes; Datapoints; Extraction techniques; Metric; Oneclass SVM; Rules extraction; Simple++; Unsupervised anomaly detection; XAI; Anomaly detection"
"Barbalau A., Cosma A., Ionescu R.T., Popescu M.","A Generic and Model-Agnostic Exemplar Synthetization Framework for Explainable AI","10.1007/978-3-030-67661-2_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103291302&doi=10.1007%2f978-3-030-67661-2_12&partnerID=40&md5=228502b47f95ed293e4fc9cf8fd988c6","With the growing complexity of deep learning methods adopted in practical applications, there is an increasing and stringent need to explain and interpret the decisions of such methods. In this work, we focus on explainable AI and propose a novel generic and model-agnostic framework for synthesizing input exemplars that maximize a desired response from a machine learning model. To this end, we use a generative model, which acts as a prior for generating data, and traverse its latent space using a novel evolutionary strategy with momentum updates. Our framework is generic because (i) it can employ any underlying generator, e.g. Variational Auto-Encoders (VAEs) or Generative Adversarial Networks (GANs), and (ii) it can be applied to any input data, e.g. images, text samples or tabular data. Since we use a zero-order optimization method, our framework is model-agnostic, in the sense that the machine learning model that we aim to explain is a black-box. We stress out that our novel framework does not require access or knowledge of the internal structure or the training data of the black-box model. We conduct experiments with two generative models, VAEs and GANs, and synthesize exemplars for various data formats, image, text and tabular, demonstrating that our framework is generic. We also employ our prototype synthetization framework on various black-box models, for which we only know the input and the output formats, showing that it is model-agnostic. Moreover, we compare our framework (available at https://github.com/antoniobarbalau/exemplar ) with a model-dependent approach based on gradient descent, proving that our framework obtains equally-good exemplars in a shorter time. © 2021, Springer Nature Switzerland AG.","Black-box; Evolutionary algorithm; Exemplar generation; Explainable AI; Generative modelling; Prototype synthetization","Data mining; Deep learning; Evolutionary algorithms; Gradient methods; Turing machines; Adversarial networks; Black-box model; Evolutionary strategies; Generative model; Gradient descent; Internal structure; Learning methods; Machine learning models; Learning systems"
"Barbano C.A., Tartaglione E., Berzovini C., Calandri M., Grangetto M.","A Two-Step Radiologist-Like Approach for Covid-19 Computer-Aided Diagnosis from Chest X-Ray Images","10.1007/978-3-031-06427-2_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130969582&doi=10.1007%2f978-3-031-06427-2_15&partnerID=40&md5=4bd2c29bd9e552247ec77fae4c03b9cd","Thanks to the rapid increase in computational capability during the latest years, traditional and more explainable methods have been gradually replaced by more complex deep-learning-based approaches, which have in fact reached new state-of-the-art results for a variety of tasks. However, for certain kinds of applications performance alone is not enough. A prime example is represented by the medical field, in which building trust between the physicians and the AI models is fundamental. Providing an explainable or trustful model, however, is not a trivial task, considering the black-box nature of deep-learning based methods. While some existing methods, such as gradient or saliency maps, try to provide insights about the functioning of deep neural networks, they often provide limited information with regards to clinical needs. We propose a two-step diagnostic approach for the detection of Covid-19 infection from Chest X-Ray images. Our approach is designed to mimic the diagnosis process of human radiologists: it detects objective radiological findings in the lungs, which are then employed for making a final Covid-19 diagnosis. We believe that this kind of structural explainability can be preferable in this context. The proposed approach achieves promising performance in Covid-19 detection, compatible with expert human radiologists. Moreover, despite this work being focused Covid-19, we believe that this approach could be employed for many different CXR-based diagnosis. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Chest x-ray; Covid-19; Deep learning; Radiological findings","Computer aided diagnosis; Computer aided instruction; Application performance; Chest X-ray image; Chest x-rays; Computational capability; Covid-19; Deep learning; Learning-based approach; Medical fields; Radiological findings; State of the art; Deep neural networks"
"Barbara N.H., Bedding T.R., Fulcher B.D., Murphy S.J., Van Reeth T.","Classifying Kepler light curves for 12 000 A and F stars using supervised feature-based machine learning","10.1093/mnras/stac1515","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133692620&doi=10.1093%2fmnras%2fstac1515&partnerID=40&md5=49e43bc65ed9128c1f77003250cfb327","With the availability of large-scale surveys like Kepler and TESS, there is a pressing need for automated methods to classify light curves according to known classes of variable stars. We introduce a new algorithm for classifying light curves that compares 7000 time-series features to find those that most effectively classify a given set of light curves. We apply our method to Kepler light curves for stars with effective temperatures in the range 6500-10 000 K. We show that the sample can be meaningfully represented in an interpretable 5D feature space that separates seven major classes of light curves (δScuti stars, γDoradus stars, RR Lyrae stars, rotational variables, contact eclipsing binaries, detached eclipsing binaries, and non-variables). We achieve a balanced classification accuracy of 82 per cent on an independent test set of Kepler stars using a Gaussian mixture model classifier. We use our method to classify 12 000 Kepler light curves from Quarter 9 and provide a catalogue of the results. We further outline a confidence heuristic based on probability density to search our catalogue and extract candidate lists of correctly classified variable stars. © 2022 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society.","Asteroseismology; Binaries: eclipsing; Methods: data analysis; Stars: oscillations; Stars: variables: general","Classification (of information); Gaussian distribution; Machine learning; Asteroseismology; Binaries:eclipsing; Eclipsing binaries; Feature-based; Light curves; Machine-learning; Methods. Data analysis; Star oscillations; Stars:variables:general; Variable stars; Stars"
"Barbaresi A., Ceccarelli M., Menichetti G., Torreggiani D., Tassinari P., Bovo M.","Application of Machine Learning Models for Fast and Accurate Predictions of Building Energy Need","10.3390/en15041266","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124545222&doi=10.3390%2fen15041266&partnerID=40&md5=a42369ff35470098db12afb5a46bcfc1","Accurate prediction of building energy need plays a fundamental role in building design, despite the high computational cost to search for optimal energy saving solutions. An important advancement in the reduction of computational time could come from the application of machine learning models to circumvent energy simulations. With the goal of drastically limiting the number of simulations, in this paper we investigate the regression performance of different machine learning models, i.e., Support Vector Machine, Random Forest, and Extreme Gradient Boosting, trained on a small data-set of energy simulations performed on a case study building. Among the XX algorithms, the tree-based Extreme Gradient Boosting showed the best performance. Overall, we find that machine learning methods offer efficient and interpretable solutions, that could help academics and professionals in shaping better design strategies, informed by feature importance. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Building energy saving solutions; Building energy simulation; Machine learning; Optimisation algorithms","Adaptive boosting; Architectural design; Energy conservation; Support vector machines; Accurate prediction; Building energy; Building energy saving; Building energy saving solution; Building energy simulations; Energy needs; Energy simulation; Machine learning models; Optimization algorithms; Performance; Decision trees"
"Barbieri M., Fiorini S., Tomasi F., Barla A.","PALLADIO: A parallel framework for robust variable selection in high-dimensional data","10.1109/PyHPC.2016.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015294779&doi=10.1109%2fPyHPC.2016.007&partnerID=40&md5=9149312d1d442ea1e703b3eb7f84fb4d","The main goal of supervised data analytics is to model a target phenomenon given a limited amount of samples, each represented by an arbitrarily large number of variables. Especially when the number of variables is much larger than the number of available samples, variable selection is a key step as it allows to identify a possibly reduced subset of relevant variables describing the observed phenomenon. Obtaining interpretable and reliable results, in this highly indeterminate scenario, is often a non-trivial task. In this work we present PALLADIO, a framework designed for HPC cluster architectures, that is able to provide robust variable selection in high-dimensional problems. PALLADIO is developed in Python and it integrates CUDA kernels to decrease the computational time needed for several independent element-wise operations. The scalability of the proposed framework is assessed on synthetic data of different sizes, which represent realistic scenarios. © 2016 IEEE.","Machine learning; Parallel algorithms; Predictive models; Variable selection","Clustering algorithms; High level languages; Learning systems; Parallel algorithms; Computational time; High dimensional data; High-dimensional problems; Non-trivial tasks; Parallel framework; Predictive models; Realistic scenario; Variable selection; Digital storage"
"Barbieri S., Mehta S., Wu B., Bharat C., Poppe K., Jorm L., Jackson R.","Predicting cardiovascular risk from national administrative databases using a combined survival analysis and deep learning approach","10.1093/ije/dyab258","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129587585&doi=10.1093%2fije%2fdyab258&partnerID=40&md5=ad889e6f273384d7877f6ba236ddd0db","Background: Machine learning-based risk prediction models may outperform traditional statistical models in large datasets with many variables, by identifying both novel predictors and the complex interactions between them. This study compared deep learning extensions of survival analysis models with Cox proportional hazards models for predicting cardiovascular disease (CVD) risk in national health administrative datasets. Methods: Using individual person linkage of administrative datasets, we constructed a cohort of all New Zealanders aged 30-74 who interacted with public health services during 2012. After excluding people with prior CVD, we developed sex-specific deep learning and Cox proportional hazards models to estimate the risk of CVD events within 5 years. Models were compared based on the proportion of explained variance, model calibration and discrimination, and hazard ratios for predictor variables. Results: First CVD events occurred in 61 927 of 2 164 872 people. Within the reference group, the largest hazard ratios estimated by the deep learning models were for tobacco use in women (2.04, 95% CI: 1.99, 2.10) and chronic obstructive pulmonary disease with acute lower respiratory infection in men (1.56, 95% CI: 1.50, 1.62). Other identified predictors (e.g. hypertension, chest pain, diabetes) aligned with current knowledge about CVD risk factors. Deep learning outperformed Cox proportional hazards models on the basis of proportion of explained variance (R2: 0.468 vs 0.425 in women and 0.383 vs 0.348 in men), calibration and discrimination (all P <0.0001). Conclusions: Deep learning extensions of survival analysis models can be applied to large health administrative datasets to derive interpretable CVD risk prediction equations that are more accurate than traditional Cox proportional hazards models. © 2021 The Author(s) 2021. Published by Oxford University Press on behalf of the International Epidemiological Association.","Cardiovascular diseases; deep learning; health planning; machine learning; population health; primary prevention; risk assessment; survival analysis","amfebutamone; anticoagulant agent; antihypertensive agent; antilipemic agent; antithrombocytic agent; cilazapril; felodipine; furosemide; glyceryl trinitrate; insulin; ipratropium bromide; ipratropium bromide plus salbutamol; malathion; nicotine; quinapril; salbutamol; simvastatin; smoking cessation agent; tiotropium bromide; varenicline; algorithm; cardiovascular disease; chronic obstructive pulmonary disease; health risk; health services; public health; risk factor; adult; aged; Article; atrial fibrillation; cardiovascular disease; cardiovascular risk; chronic kidney failure; chronic obstructive lung disease; cohort analysis; comparative study; controlled study; data base; deep learning; diabetes mellitus; essential hypertension; female; follow up; general anesthesia; hazard ratio; human; major clinical study; male; New Zealander; non insulin dependent diabetes mellitus; predictor variable; proportional hazards model; public health; respiratory tract infection; sex difference; survival analysis; systemic disease; thorax pain; tobacco use; cardiovascular disease; risk assessment; risk factor; survival analysis; Cardiovascular Diseases; Deep Learning; Female; Heart Disease Risk Factors; Humans; Male; Proportional Hazards Models; Risk Assessment; Risk Factors; Survival Analysis"
"Barbieri S., Kemp J., Perez-Concha O., Kotwal S., Gallagher M., Ritchie A., Jorm L.","Benchmarking Deep Learning Architectures for Predicting Readmission to the ICU and Describing Patients-at-Risk","10.1038/s41598-020-58053-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078239943&doi=10.1038%2fs41598-020-58053-z&partnerID=40&md5=a38105bf1d569e1d1ee94c66db6e771b","To compare different deep learning architectures for predicting the risk of readmission within 30 days of discharge from the intensive care unit (ICU). The interpretability of attention-based models is leveraged to describe patients-at-risk. Several deep learning architectures making use of attention mechanisms, recurrent layers, neural ordinary differential equations (ODEs), and medical concept embeddings with time-aware attention were trained using publicly available electronic medical record data (MIMIC-III) associated with 45,298 ICU stays for 33,150 patients. Bayesian inference was used to compute the posterior over weights of an attention-based model. Odds ratios associated with an increased risk of readmission were computed for static variables. Diagnoses, procedures, medications, and vital signs were ranked according to the associated risk of readmission. A recurrent neural network, with time dynamics of code embeddings computed by neural ODEs, achieved the highest average precision of 0.331 (AUROC: 0.739, F1-Score: 0.372). Predictive accuracy was comparable across neural network architectures. Groups of patients at risk included those suffering from infectious complications, with chronic or progressive conditions, and for whom standard medical care was not suitable. Attention-based networks may be preferable to recurrent networks if an interpretable model is required, at only marginal cost in predictive accuracy. © 2020, The Author(s).",,"algorithm; Bayes theorem; chronic disease; communicable disease; complication; disease exacerbation; electronic medical record system; forecasting; hospital readmission; human; intensive care unit; odds ratio; risk; sensitivity and specificity; Algorithms; Bayes Theorem; Chronic Disease; Communicable Diseases; Deep Learning; Disease Progression; Forecasting; Humans; Intensive Care Units; Medical Records Systems, Computerized; Neural Networks, Computer; Odds Ratio; Patient Readmission; Risk; Sensitivity and Specificity"
"Barbiero P., Ciravegna G., Cirrincione G., Tonda A., Squillero G.","Generating Neural Archetypes to Instruct Fast and Interpretable Decisions","10.1007/978-3-030-38227-8_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080949785&doi=10.1007%2f978-3-030-38227-8_6&partnerID=40&md5=acd66c947a5c7b0cd5a5e886465550f1","In the field of artificial intelligence, agents learn how to take decisions by fitting their parameters on a set of samples called training set. Similarly, a core set is a subset of the training samples such that, if an agent exploits this set to fit its parameters instead of the whole training set, then the quality of the inferences does not change significantly. Relaxing the constraint that restricts the search for core sets to the available data, neural networks may be used to generate virtual samples, called archetype set, containing the same kind of information. This work illustrates the features of GH-ARCH, a recently proposed self-organizing hierarchical neural network for archetype discovery. Experiments show how the use of archetypes allows both ML agents to make fast and accurate predictions and human experts to make sense of such decisions by analyzing few important samples. © Springer Nature Switzerland AG 2020.","Archetypes; Big data; Classification; Coresets; Explain AI; GH-ARCH; Hierarchical clustering; Machine learning; Neural networks; Self-organization; Semi-supervised learning","Arches; Big data; Classification (of information); Complex networks; Hierarchical clustering; Learning algorithms; Learning systems; Neural networks; Semi-supervised learning; Accurate prediction; Archetypes; Core set; Hierarchical neural networks; Self organizations; Training sample; Training sets; Virtual sample; Core samples"
"Barboni P., Sestero D.","Flexible response choice using problem-solving plans and rhetorical relations",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961320811&partnerID=40&md5=6028bbb0fc01459b9d9ad288e2688c29","In this paper we present an architecture for choosing a flexible response in a natural language system involved in information-seeking tasks. Our work considers the crucial issue of choosing what information to provide and how to structure it, considering from the generation perspective a model of dialogue that was previously developed to study the recognition activity of an agent. In such a model the underlying reasoning activity of an agent is represented by means of problem-solving plans, that manage domain and linguistic actions. The correct choice of the information to provide depends on the ability to select the action that best suits the domain situation and an answer that conveys the information in a way appropriate to the user. For this reason we provide heuristic evaluation criteria that consider both participants' goals and some other context related factors impacting on the evaluation of the action to allow the problem-solving plans to make a choice among the available alternatives. Furthermore, we discuss how to convey the raw material in a suitable and understandable way by using different rhetorical relations. A detailed example illustrates how our approach models in a flexible way aspects of the interaction overlooked by previous systems. © Springer-Verlag Berlin Heidelberg 1997.",,"Artificial intelligence; Computational linguistics; Heuristic evaluation; Information seeking; Natural language systems; Related factors; Rhetorical relations; Problem solving"
"Barbosa L., Filgueiras J., Rocha G., Cardoso H.L., Reis L.P., Machado J.P., Caldeira A.C., Oliveira A.M.","Automatic Identification of Economic Activities in Complaints","10.1007/978-3-030-31372-2_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075891365&doi=10.1007%2f978-3-030-31372-2_21&partnerID=40&md5=b420d4819f7e347648b6aba8a275f686","In recent years, public institutions have undergone a progressive modernization process, bringing several administrative services to be provided electronically. Some institutions are responsible for analyzing citizen complaints, which come in huge numbers and are mainly provided in free-form text, demanding for some automatic way to process them, at least to some extent. In this work, we focus on the task of automatically identifying economic activities in complaints submitted to the Portuguese Economic and Food Safety Authority (ASAE), employing natural language processing (NLP) and machine learning (ML) techniques for Portuguese, which is a language with few resources. We formulate the task as several multi-class classification problems, taking into account the economic activity taxonomy used by ASAE. We employ features at the lexical, syntactic and semantic level using different ML algorithms. We report the results obtained to address this task and present a detailed analysis of the features that impact the performance of the system. Our best setting obtains an accuracy of 0.8164 using SVM. When looking at the three most probable classes according to the classifier’s prediction, we report an accuracy of 0.9474. © 2019, Springer Nature Switzerland AG.","Complaint analysis; Natural language processing; Text categorization; User-generated text","Automation; Learning systems; Natural language processing systems; Semantics; Speech processing; Support vector machines; Text processing; Administrative services; Complaint analysis; Economic activities; Multiclass classification problems; NAtural language processing; Public institution; Text categorization; User-generated; Economics"
"Barbosa L.N., Gemmell J.F., Horvath M., Heimfarth T.","Assessing distributed collaborative recommendations in different opportunistic network scenarios","10.1504/IJGUC.2020.110052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092397394&doi=10.1504%2fIJGUC.2020.110052&partnerID=40&md5=088bb9f7774bfdcb45d3207dbb40e3a8","Mobile devices are common throughout the world, even in countries with limited internet access and even when natural disasters disrupt access to a centralised infrastructure. This access allows for the exchange of information at an incredible pace and across vast distances. However, this wealth of information can frustrate users as they become inundated with irrelevant or unwanted data. Recommender systems help to alleviate this burden. In this work, we propose a recommender system where users share information via an opportunistic network. Each device is responsible for gathering information from nearby users and computing its own recommendations. An exhaustive empirical evaluation was conducted on two different data sets. Scenarios with different node densities, velocities and data exchange parameters were simulated. Our results show that in a relatively short time when a sufficient number of users are present, an opportunistic distributed recommender system achieves results comparable to that of a centralised architecture. © 2020 Inderscience Enterprises Ltd.","Decentralised recommender systems; Device-to-device communications; Machine learning; Mobile ad hoc networks; Opportunistic networks; Recommender systems; User-based collaborative filtering","Disasters; Electronic data interchange; Recommender systems; Collaborative recommendation; Empirical evaluations; Exchange of information; Exchange parameters; Internet access; Natural disasters; Opportunistic networks; Wealth of information; Information use"
"Barbosa R.P., Belo O.","An agent task force for stock trading","10.1007/978-3-642-19875-5_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053045141&doi=10.1007%2f978-3-642-19875-5_37&partnerID=40&md5=0ef9ee705afc87ba6789a2d2e6b1917a","In this article the authors present the simulated trading results of a system consisting of 60 intelligent agents, each being responsible for day trading a stock listed on the NYSE or the NASDAQ stock exchange. These agents were implemented according to an architecture that was previously applied to currency trading with interesting results. The performance of the stock trading agents, once integrated in a diversified investment system, showed similar promise. The trading simulation was done using out-of-sample price data for the period between February of 2006 and October of 2010. Throughout this period, the system's performance compared favorably with that of the buy-and-hold strategy, both in terms of return and maximum drawdown. These results indicate that agent technology might be of use for this particular practical application, a conclusion that should interest the investment industry. © 2011 Springer-Verlag Berlin Heidelberg.","Financial Data Mining; Intelligent Agent; Stock Trading","Agent technology; Buy-and-hold strategy; Currency trading; Financial data; Stock exchange; Stock trading; System's performance; Task force; Commerce; Economics; Multi agent systems; Intelligent agents"
"Barbosa R.P., Belo O.","Autonomous forex trading agents","10.1007/978-3-540-70720-2_30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-48949099217&doi=10.1007%2f978-3-540-70720-2_30&partnerID=40&md5=5617f0c47fd6f52f45387f8b8081a0ec","In this paper we describe an infrastructure for implementing hybrid intelligent agents with the ability to trade in the Forex Market without requiring human supervision. This infrastructure is composed of three modules. The ""Intuition Module"", implemented using an Ensemble Model, is responsible for performing pattern recognition and predicting the direction of the exchange rate. The ""A Posteriori Knowledge Module"", implemented using a Case-Based Reasoning System, enables the agents to learn from empirical experience and is responsible for suggesting how much to invest in each trade. The ""A Priori Knowledge Module"", implemented using a Rule-Based Expert System, enables the agents to incorporate non-experiential knowledge in their trading decisions. This infrastructure was used to develop an agent capable of trading the USD/JPY currency pair with a 6 hours timeframe. The agent's simulated and live trading results lead us to believe our infrastructure can be of practical interest to the traditional trading community. © 2008 Springer-Verlag.","Autonomy; Data mining; Forex trading; Hybrid agents","Administrative data processing; Agents; Artificial intelligence; Autonomous agents; Case based reasoning; Computer software; Data mining; Decision support systems; Electronic commerce; Expert systems; Feature extraction; Food processing; Information management; Intelligent agents; Management information systems; Medical applications; Medical computing; Mining; Pattern recognition; Search engines; A posteriori; A-priori; Autonomy; Case-based reasoning systems; E-commerce; Ensemble models; Exchange rates; Forex trading; Human supervision; Hybrid agents; Recognition and predicting; Rule-based expert system; Commerce"
"Barclay I., Preece A., Taylor I., Radha S.K., Nabrzyski J.","Providing assurance and scrutability on shared data and machine learning models with verifiable credentials","10.1002/cpe.6997","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127775023&doi=10.1002%2fcpe.6997&partnerID=40&md5=1a645e9df6520d4bff68e4e7e4e5dfec","Adopting shared data resources requires scientists to place trust in the originators of the data. When shared data is later used in the development of artificial intelligence (AI) systems or machine learning (ML) models, the trust lineage extends to the users of the system, typically practitioners in fields such as healthcare and finance. Practitioners rely on AI developers to have used relevant, trustworthy data, but may have limited insight and recourse. This article introduces a software architecture and implementation of a system based on design patterns from the field of self-sovereign identity. Scientists can issue signed credentials attesting to qualities of their data resources. Data contributions to ML models are recorded in a bill of materials (BOM), which is stored with the model as a verifiable credential. The BOM provides a traceable record of the supply chain for an AI system, which facilitates on-going scrutiny of the qualities of the contributing components. The verified BOM, and its linkage to certified data qualities, is used in the AI scrutineer, a web-based tool designed to offer practitioners insight into ML model constituents and highlight any problems with adopted datasets, should they be found to have biased data or be otherwise discredited. © 2022 John Wiley & Sons, Ltd.","accountability; AI ethics; data provenance; explainable AI; self-sovereign identity","Supply chains; Accountability; Artificial intelligence ethic; Artificial intelligence systems; Bill of materials; Data provenance; Data resources; Explainable artificial intelligence; Machine learning models; Self-sovereign identity; Shared data; Machine learning"
"Barda A.J., Horvat C.M., Hochheiser H.","A qualitative research framework for the design of user-centered displays of explanations for machine learning model predictions in healthcare","10.1186/s12911-020-01276-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092601699&doi=10.1186%2fs12911-020-01276-x&partnerID=40&md5=f34d0534504ebb2a21be35b7a1941497","Background: There is an increasing interest in clinical prediction tools that can achieve high prediction accuracy and provide explanations of the factors leading to increased risk of adverse outcomes. However, approaches to explaining complex machine learning (ML) models are rarely informed by end-user needs and user evaluations of model interpretability are lacking in the healthcare domain. We used extended revisions of previously-published theoretical frameworks to propose a framework for the design of user-centered displays of explanations. This new framework served as the basis for qualitative inquiries and design review sessions with critical care nurses and physicians that informed the design of a user-centered explanation display for an ML-based prediction tool. Methods: We used our framework to propose explanation displays for predictions from a pediatric intensive care unit (PICU) in-hospital mortality risk model. Proposed displays were based on a model-agnostic, instance-level explanation approach based on feature influence, as determined by Shapley values. Focus group sessions solicited critical care provider feedback on the proposed displays, which were then revised accordingly. Results: The proposed displays were perceived as useful tools in assessing model predictions. However, specific explanation goals and information needs varied by clinical role and level of predictive modeling knowledge. Providers preferred explanation displays that required less information processing effort and could support the information needs of a variety of users. Providing supporting information to assist in interpretation was seen as critical for fostering provider understanding and acceptance of the predictions and explanations. The user-centered explanation display for the PICU in-hospital mortality risk model incorporated elements from the initial displays along with enhancements suggested by providers. Conclusions: We proposed a framework for the design of user-centered displays of explanations for ML models. We used the proposed framework to motivate the design of a user-centered display of an explanation for predictions from a PICU in-hospital mortality risk model. Positive feedback from focus group participants provides preliminary support for the use of model-agnostic, instance-level explanations of feature influence as an approach to understand ML model predictions in healthcare and advances the discussion on how to effectively communicate ML model information to healthcare providers. © 2020 The Author(s).","Clinical decision support systems; Explainable artificial intelligence; In-hospital mortality; Machine learning; Pediatric intensive care units; User-computer interface","child; health care delivery; health care personnel; hospital mortality; human; information processing; machine learning; pediatric intensive care unit; psychology; qualitative research; Child; Delivery of Health Care; Focus Groups; Health Personnel; Hospital Mortality; Humans; Intensive Care Units, Pediatric; Machine Learning; Qualitative Research"
"Bardab S.N., Ahmed T.M., Mohammed T.A.A.","Data mining classification algorithms: An overview","10.21833/ijaas.2021.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105792061&doi=10.21833%2fijaas.2021.02.001&partnerID=40&md5=0d2bf0fbf684a19e7db81f28b063ce94","Data mining is also defined as the process of analyzing a quantity of data (usually a large amount) to find a logical relationship that summarizes the data in a new way that is understandable and useful to the owner of the data. This paper examines the various types of classification algorithms in Data Mining, their applications and categorically states the strengths and limitations of each type. The weaknesses found in each algorithm demonstrate how tasks cannot be performed well when only one type of algorithm is applied. For this reason, it is the view of the writer that further research needs to be carried out to explore the potential of combining several of these algorithms to solve machine learning problems. © 2020 The Authors.","Classification; Classifiers; Machine learning; Supervised learning",
"Barddal J.P., Enembreck F.","Learning regularized hoeffding trees from data streams","10.1145/3297280.3297334","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065674769&doi=10.1145%2f3297280.3297334&partnerID=40&md5=723f1527a183b0b50cc97ec79ed6dbc3","Learning from data streams is a hot topic in machine learning that targets the learning and update of predictive models as data becomes available for both training and query. Due to their simplicity and convincing results in a multitude of applications, Hoeffding Trees are, by far, the most widely used family of methods for learning decision trees from streaming data. Despite the aforementioned positive characteristics, Hoeffding Trees tend to continuously grow in terms of nodes as new data becomes available, i.e., they eventually split on all features available, and multiple times on the same feature; thus leading to unnecessary complexity. With this behavior, Hoeffding Trees lose the ability to be human-understandable and computationally efficient. To tackle these issues, we propose a regularization scheme for Hoeffding Trees that (i) uses a penalty factor to control the gain obtained by creating a new split node using a feature that has not been used thus far; and (ii) uses information from previous splits in the current branch to determine whether the gain observed indeed justifies a new split. The proposed scheme is combined with both standard and adaptive variants of Hoeffding Trees. Experiments using real-world, stationary and drifting synthetic data show that the proposed method prevents both original and adaptive Hoeffding Trees from unnecessarily growing while maintaining impressive accuracy rates. As a byproduct of the regularization process, significant improvements in processing time, model complexity, and memory consumption have also been observed, thus showing the effectiveness of the proposed regularization scheme. © 2019 Association for Computing Machinery.","Concept Drift; Data Stream Mining; Decision Tree; Regularization","Data mining; Decision trees; Learning systems; Computationally efficient; Concept drifts; Data stream mining; Memory consumption; Predictive models; Regularization; Regularization process; Regularization schemes; Trees (mathematics)"
"Bardos A., Mollas I., Bassiliades N., Tsoumakas G.","Local Explanation of Dimensionality Reduction","10.1145/3549737.3549770","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132943244&doi=10.1145%2f3549737.3549770&partnerID=40&md5=54565422cce4b9ddf901b9cfdeea911a","Dimensionality reduction (DR) is a popular method for preparing and analyzing high-dimensional data. Reduced data representations are less computationally intensive and easier to manage and visualize, while retaining a significant percentage of their original information. Aside from these advantages, these reduced representations can be difficult or impossible to interpret in most circumstances, especially when the DR approach does not provide further information about which features of the original space led to their construction. This problem is addressed by Interpretable Machine Learning, a subfield of Explainable Artificial Intelligence that addresses the opacity of machine learning models. However, current research on Interpretable Machine Learning has been focused on supervised tasks, leaving unsupervised tasks like Dimensionality Reduction unexplored. In this paper, we introduce LXDR, a technique capable of providing local interpretations of the output of DR techniques. Experiment results and two LXDR use case examples are presented to evaluate its usefulness. © 2022 ACM.","Black Box Models; Dimensionality Reduction; Interpretable Machine Learning; Local Interpretations; Model-Agnostic","Data reduction; Machine learning; Black box modelling; Data representations; Dimensionality reduction; High dimensional data; Interpretable machine learning; Local interpretation; Machine-learning; Model-agnostic; Reduced data; Reduced representation; Clustering algorithms"
"Bardozzo F., Priscoli M.D., Collins T., Forgione A., Hostettler A., Tagliaferri R.","Cross X-AI: Explainable Semantic Segmentation of Laparoscopic Images in Relation to Depth Estimation","10.1109/IJCNN55064.2022.9892345","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140793297&doi=10.1109%2fIJCNN55064.2022.9892345&partnerID=40&md5=a240ad380ad50063ec77daeffe3d6c65","In this work, two deep learning models, trained to segment the liver and perform depth reconstruction, are compared and analysed with their post-hoc explanation interplay. The first model (a U-Net) is designed to perform liver semantic segmentation over different subjects and scenarios. Particularly, the image pixels representing the liver are classified and separated by the surrounding pixels. Meanwhile, with the second model, a depth estimation is performed to regress the z-position of each pixel (relative depths). In general, these two models apply a sort of classification task which can be explained for each model individually and that can be combined to show additional relations and insights between the most relevant learned features. In detail, this work shows how post-hoc explainable AI systems (X-AI) based on Grad CAM and Grad CAM++ can be compared by introducing Cross X-AI (CX-AI). Typically the post-hoc explanation maps provide different visual explanations of their decisions based on the two proposed approaches. Our results show that the Grad Cam++ segmentation explanation maps present cross-learning strategies similar to disparity explanations (and vice versa). © 2022 IEEE.","Deep Learning; Explainable AI; Liver segmentation","Cams; Computer vision; Deep learning; Learning systems; Pixels; Semantics; Classification tasks; Classifieds; Deep learning; Depth Estimation; Depth reconstruction; Explainable AI; Image pixels; Learning models; Liver segmentation; Semantic segmentation; Semantic Segmentation"
"Bareiss R., Slator B.M.","The Evolution of a Case-Based Computatoinal Approach to Knowledge Representation, Classification, and Learning","10.1016/S0079-7421(08)60139-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957027066&doi=10.1016%2fS0079-7421%2808%2960139-5&partnerID=40&md5=8d6b43fc017265a3c8827942a12ae9f7","This chapter discusses research on artificial intelligence systems for performing classification in weak-theory domains. The research focuses on expert problem-solving. The first system, Protos, is a general purpose knowledge acquisition and classification system which has been evaluated in the domain of clinical audiology. The second system, ORCA, extends the Protos approach with strategies for incremental data gathering and classification of very complex problem situations. It is being applied in a business consulting domain. The chapter explores that many real-world domains are characterized by a lack of reliable general principles. In these “weak-theory’’ domains, knowledge is incomplete, uncertain, and even contradictory. Real-world learners and classifiers in such domains operate under constraints that are inherent to the problems they are solving. These are constraints in (1)the representational demands of concepts in weak-theory domains, (2) the conditions under which classification is typically performed, and (3)the experience that is actually available for learning. Concepts in real world, weak-theory domains are inherently polymorphic- their instances vary greatly in observable features, and this variability is not explainable in terms of general domain knowledge. © 1993, Academic Press Inc.",,
"Barenji A.V., Guo H., Wang Y., Li Z., Rong Y.","Toward blockchain and fog computing collaborative design and manufacturing platform: Support customer view","10.1016/j.rcim.2020.102043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088819435&doi=10.1016%2fj.rcim.2020.102043&partnerID=40&md5=5190ac9f343033a8499cd06c541eca7e","Overview of current manufacturing enterprises show that successful global manufacturing enterprise has great collaboration among designer, manufacturer, and customer, which effect on reducing production life cycle and improving customer satisfaction. Recently, several past and ongoing research projects have conducted to enable the collaborative platform to develop effective collaboration with the manufacturing section, design section, and customer views. However, trustable collaboration and how to utilize customer views efficiently is still a challenge. Therefore, this research proposed a blockchain-enabled fog computing-based collaborative design and manufacturing platform to develop triple communication and cooperation among the manufacturing section, design section, and customers in a trustable environment. In the proposed platform, the machine-learning method is used for clustering and categorizing customer-views, and fog computing-based integration between subsystems via blockchain technology is proposed to improve the data integrity and security problem. The proposed platform explained based on key technology, data integrity, key requirement, and illustrative case study. In this respect, we presented the design and manufacturing of bicycles based on customer requests. © 2020","Blockchain technology; Collaborative design; Data integration; Fog computing; Machine learning; Manufacturing platform","Agricultural robots; Blockchain; Customer satisfaction; Engineering education; Fog; Fog computing; Industrial research; Learning systems; Life cycle; Sales; Collaborative design and manufacturing; Collaborative platform; Global manufacturing enterprise; Key technologies; Machine learning methods; Manufacturing enterprise; Production life; Security problems; Manufacture"
"Barfield W., Pagallo U.","Research handbook on the law of artificial intelligence","10.4337/9781786439055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076036154&doi=10.4337%2f9781786439055&partnerID=40&md5=8bcb0f85b795797789f4cd751c4a252c","The field of artificial intelligence (AI) has made tremendous advances in the last two decades, but as smart as AI is now, it is getting smarter and becoming more autonomous. This raises a host of challenges to current legal doctrine, including whether AI/algorithms should count as 'speech', whether AI should be regulated under antitrust and criminal law statutes, and whether AI should be considered as an agent under agency law or be held responsible for injuries under tort law. This book contains chapters from US and international law scholars on the role of law in an age of increasingly smart AI, addressing these and other issues that are critical to the evolution of the field. © The Editors and Contributors Severally 2018.",,
"Barford L.","Material Value Ethics in a Model Process for Values-Based Design","10.1109/MTS.2021.3101830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114690663&doi=10.1109%2fMTS.2021.3101830&partnerID=40&md5=ec82253eb9968b42108a74f9c3dca8fd","Over recent years, it has become increasingly apparent that engineering activities must proceed with consideration of the human values that they potentially affect. The pervasiveness of communication and information technology (CIT) systems in nearly every aspect of work and personal life gives the system providers influence over human thought, feeling, and behavior with a magnitude not widely foreseen until a few years ago. Increasingly complex software, including applications of artificial intelligence that is poorly explainable, are integrated with the physical world through sensors, actuators, control systems, and Internet of Things (IoT)-enabled products. Such systems can pose direct threats to the physical safety of human beings and can negatively impact the environment. The design, deployment, and operation of transportation systems and chemical, nuclear, and other dangerous industrial plants have always required attention to human values such as safety in addition to economic values. However, recent events such as the Boeing 737 Max crashes and data breaches of financial information impacting broad swathes of nations' populations have brought public attention to the importance of including human values such as safety and privacy in the design, test, and deployment of CIT and hybrid software/control/actuation systems. This is especially the case for autonomous systems that can operate without - or can override - human decisions and control inputs [1]. © 1982-2012 IEEE.",,"Accident prevention; Application programs; Artificial intelligence; Internet of things; Man machine systems; Population statistics; Software testing; Autonomous systems; Complex software; Economic values; Engineering activities; Financial information; Internet of Things (IOT); Modeling process; Transportation system; Privacy by design"
"Bargagli-Stoffi F.J., DE WITTE K., Gnecco G.","HETEROGENEOUS CAUSAL EFFECTS WITH IMPERFECT COMPLIANCE: A BAYESIAN MACHINE LEARNING APPROACH","10.1214/21-AOAS1579","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134570223&doi=10.1214%2f21-AOAS1579&partnerID=40&md5=01262d8a2fa4c30278c7375bbf3e294b","This paper introduces an innovative Bayesian machine learning algorithm to draw interpretable inference on heterogeneous causal effects in the presence of imperfect compliance (e.g., under an irregular assignment mechanism). We show, through Monte Carlo simulations, that the proposed Bayesian Causal Forest with Instrumental Variable (BCF-IV) methodology outperforms other machine learning techniques tailored for causal inference in discovering and estimating the heterogeneous causal effects while controlling for the familywise error rate (or, less stringently, for the false discovery rate) at leaves’ level. BCF-IV sheds a light on the heterogeneity of causal effects in instrumental variable scenarios and, in turn, provides the policy-makers with a relevant tool for targeted policies. Its empirical application evaluates the effects of additional funding on students’ performances. The results indicate that BCF-IV could be used to enhance the effectiveness of school funding on students’ performance. © Institute of Mathematical Statistics, 2022.","Causal inference; heterogeneous effects; instrumental variable; interpretable machine learning; school funding; students’ performance",
"Bargal S.A., Zunino A., Petsiuk V., Zhang J., Murino V., Sclaroff S., Saenko K.","Beyond the Visual Analysis of Deep Model Saliency","10.1007/978-3-031-04083-2_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128902517&doi=10.1007%2f978-3-031-04083-2_13&partnerID=40&md5=4121573a8fe502279eda683f0b115d54","Increased explainability in machine learning is traditionally associated with lower performance, e.g. a decision tree is more explainable, but less accurate than a deep neural network. We argue that, in fact, increasing the explainability of a deep classifier can improve its generalization. In this chapter, we survey a line of our published work that demonstrates how spatial and spatiotemporal visual explainability can be obtained, and how such explainability can be used to train models that generalize better on unseen in-domain and out-of-domain samples, refine fine-grained classification predictions, better utilize network capacity, and are more robust to network compression. © 2022, The Author(s).","Deep learning; Explainability; Interpretability; Saliency","Deep neural networks; Classification prediction; Deep learning; Explainability; Fine grained; Generalisation; Interpretability; Performance; Saliency; Train model; Visual analysis; Decision trees"
"Bargiela A., Pedrycz W.","A model of granular data: A design problem with the Tchebyschev FCM","10.1007/s00500-003-0339-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-13844298042&doi=10.1007%2fs00500-003-0339-2&partnerID=40&md5=f4aa2cf24319197342893472009ee708","In this study, we propose a model of granular data emerging through a summarization and processing of numeric data. This model supports data analysis and contributes to further interpretation activities. The structure of data is revealed through the FCM equipped with the Tchebyschev (l∞) metric. The paper offers a novel contribution of a gradient-based learning of the prototypes developed in the l∞-based FCM. The l∞ metric promotes development of easily interpretable information granules, namely hyperboxes. A detailed discussion of their geometry is provided. In particular, we discuss a deformation effect of the hyperbox-shape of granules due to an interaction between the granules. It is shown how the deformation effect can be quantified. Subsequently, we show how the clustering gives rise to a two-level topology of information granules: The core part of the topology comes in the form of hyperbox information granules. A residual structure is expressed through detailed, yet difficult to interpret, membership grades. Illustrative examples including synthetic data are studied. © Springer-Verlag 2003.","Deformation effect in clustering; FCM; Geometry; Hyperboxes; Information granulation through clustering; L∞metric (distance)","Algorithms; Chebyshev approximation; Cognitive systems; Data mining; Data reduction; Data structures; Fuzzy sets; Geometry; Mathematical models; Neural networks; Optimization; Deformation effect-in-clustering; Fuzzy cognitive maps; Hyperboxes; Information granulation-through-clustering; Data processing"
"Bargiela A., Pedrycz W.","A model of granular data: A design problem with the Tchebyschev -based clustering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036454142&partnerID=40&md5=7062cae6452a52d6f421843a81a4ae8b","We introduce a model of granular data emerging through a summarization and processing of numeric data. It supports data analysis and casts it in the setting of data mining. The structure of data is revealed through the FCM equipped with the Tchebyschev (l∞) metric. The study offers a novel contribution to a gradient-based learning of the prototypes developed in the l∞-based data space. The l∞ metric promotes a development of easily interpretable information granules, namely hyperboxes. A detailed discussion of their geometry is provided. In particular, we discuss a deformation effect of the hyperbox-shape of granules due to an interaction between the granules. We also show how the clustering gives rise to a two-level topology of information granules. A core part of the topology comes in the form of hyperbox information granules. A residual structure is expressed through detailed, yet difficult to interpret, membership grades. Illustrative examples including synthetic data are studied.","Data mining; Deformation effect in clustering; FCM; Geometry of data space; Hyperboxes; Information granulation through clustering; L∞ metric (distance)","Clustering; Granular data; Hyperboxes; Tchebyschev metric; Algorithms; Data mining; Data reduction; Data structures; Fuzzy sets; Heuristic methods; Learning systems; Neural networks; Artificial intelligence"
"Bargiela A., Pedrycz W., Hirota K.","Logic-based granular prototyping","10.1109/CMPSAC.2002.1045169","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036391947&doi=10.1109%2fCMPSAC.2002.1045169&partnerID=40&md5=0caa014d1e3fd425e384a5e9e0af276c","A fuzzy logic based similarity measure is introduced as a criterion for the identification of structure in data. An important characteristic of the proposed approach is that cluster prototypes are formed and evaluated in the course of the optimization without any a-priori assumptions about the number of clusters. The intuitively straightforward compound optimization criterion of maximizing the overall similarity between data and the prototypes while minimizing the similarity between the prototypes has been adopted. It is shown that the partitioning of the pattern space obtained in the course of the optimization is more intuitive than the one obtained for the standard FCM. The local properties of clusters (in terms of the ranking order of features in the multi-dimensional pattern space) are captured by the weight vector associated with each cluster prototype. The weight vector is then used for the construction of interpretable information granules.","Clustering; Data mining; Granular prototyping; Logic-based optimization","Fuzzy C-means; Granular prototype; Similarity measure; Weight vector; Algorithms; Approximation theory; Constraint theory; Fuzzy sets; Iterative methods; Lagrange multipliers; Mathematical models; Membership functions; Optimization; Probability; Vectors; Data structures"
"Barić D., Fumić P., Horvatić D., Lipic T.","Benchmarking attention-based interpretability of deep learning in multivariate time series predictions","10.3390/e23020143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100333557&doi=10.3390%2fe23020143&partnerID=40&md5=3ef0463c23decdf4f846ff4d112bf330","The adaptation of deep learning models within safety-critical systems cannot rely only on good prediction performance but needs to provide interpretable and robust explanations for their decisions. When modeling complex sequences, attention mechanisms are regarded as the established approach to support deep neural networks with intrinsic interpretability. This paper focuses on the emerging trend of specifically designing diagnostic datasets for understanding the inner workings of attention mechanism based deep learning models for multivariate forecasting tasks. We design a novel benchmark of synthetically designed datasets with the transparent underlying generating process of multiple time series interactions with increasing complexity. The benchmark enables empirical evaluation of the performance of attention based deep neural networks in three different aspects: (i) prediction performance score, (ii) interpretability correctness, (iii) sensitivity analysis. Our analysis shows that although most models have satisfying and stable prediction performance results, they often fail to give correct interpretability. The only model with both a satisfying performance score and correct interpretability is IMV-LSTM, capturing both autocorrelations and crosscorrelations between multiple time series. Interestingly, while evaluating IMV-LSTM on simulated data from statistical and mechanistic models, the correctness of interpretability increases with more complex datasets. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Attention mechanism; Interpretability; Multivariate time series; Synthetically designed datasets",
"Barile B., Marzullo A., Stamile C., Durand-Dubief F., Sappey-Marinier D.","Ensemble Learning for Multiple Sclerosis Disability Estimation Using Brain Structural Connectivity","10.1089/brain.2020.1003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131770949&doi=10.1089%2fbrain.2020.1003&partnerID=40&md5=e2fb4e7e15ff09897ad209bdf804c36b","Background: Multiple sclerosis (MS) is an autoimmune inflammatory disease of the central nervous system characterized by demyelination and neurodegeneration processes. It leads to different clinical courses and degrees of disability that need to be anticipated by the neurologist for personalized therapy. Recently, machine learning (ML) techniques have reached a high level of performance in brain disease diagnosis and/or prognosis, but the decision process of a trained ML system is typically nontransparent. Using brain structural connectivity data, a fully automatic ensemble learning model, augmented with an interpretable model, is proposed for the estimation of MS patients' disability, measured by the Expanded Disability Status Scale (EDSS). Materials and Methods: An ensemble of four boosting-based models (GBM, XGBoost, CatBoost, and LightBoost) organized following a stacking generalization scheme was developed using diffusion tensor imaging (DTI)-based structural connectivity data. In addition, an interpretable model based on conditional logistic regression was developed to explain the best performances in terms of white matter (WM) links for three classes of EDSS (low, medium, and high). Results: The ensemble model reached excellent level of performance (root mean squared error of 0.92 ± 0.28) compared with single-based models and provided a better EDSS estimation using DTI-based structural connectivity data compared with conventional magnetic resonance imaging measures associated with patient data (age, gender, and disease duration). Used for interpretation of the estimation process, the counterfactual method showed the importance of certain brain networks, corresponding mainly to left hemisphere WM links, connecting the left superior temporal with the left posterior cingulate and the right precuneus gray matter regions, and the interhemispheric WM links constituting the corpus callosum. Also, a better accuracy estimation was found for the high disability class. Conclusion: The combination of advanced ML models and sensitive techniques such as DTI-based structural connectivity demonstrated to be useful for the estimation of MS patients' disability and to point out the most important brain WM networks involved in disability. An ensemble of ""boosting""machine learning (ML) models was more performant than single models to estimate disability in multiple sclerosis. Diffusion tensor imaging (DTI)-based structural connectivity led to better performance than conventional magnetic resonance imaging. An interpretable model, based on counterfactual perturbation, highlighted the most relevant white matter fiber links for disability estimation. These findings demonstrated the clinical interest of combining DTI, graph modeling, and ML techniques. © Copyright 2022, Mary Ann Liebert, Inc., publishers 2022.","brain structural connectivity; disability estimation; machine learning; multiple sclerosis","brain; diagnostic imaging; diffusion tensor imaging; human; machine learning; multiple sclerosis; nuclear magnetic resonance imaging; procedures; white matter; Brain; Diffusion Tensor Imaging; Humans; Machine Learning; Magnetic Resonance Imaging; Multiple Sclerosis; White Matter"
"Barkan O., Hauon E., Caciularu A., Katz O., Malkiel I., Armstrong O., Koenigstein N.","Grad-SAM: Explaining Transformers via Gradient Self-Attention Maps","10.1145/3459637.3482126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119180055&doi=10.1145%2f3459637.3482126&partnerID=40&md5=1c60408568495ce02220c4805775824e","Transformer-based language models significantly advanced the state-of-the-art in many linguistic tasks. As this revolution continues, the ability to explain model predictions has become a major area of interest for the NLP community. In this work, we present Gradient Self-Attention Maps (Grad-SAM) - a novel gradient-based method that analyzes self-attention units and identifies the input elements that explain the model's prediction the best. Extensive evaluations on various benchmarks show that Grad-SAM obtains significant improvements over state-of-the-art alternatives. © 2021 ACM.","bert; deep learning; explainable & interpretable ai; nlp; self-attention; transformers; transparent machine learning","Natural language processing systems; Bert; Deep learning; Explainable & interpretable ai; Language model; Model prediction; Nlp; Self-attention; State of the art; Transformer; Transparent machine learning; Deep learning"
"Barkan O., Armstrong O., Hertz A., Caciularu A., Katz O., Malkiel I., Koenigstein N.","GAM: Explainable Visual Similarity and Classification via Gradient Activation Maps","10.1145/3459637.3482430","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119175338&doi=10.1145%2f3459637.3482430&partnerID=40&md5=d015e2664b825a6b82dd77faae95d88f","We present Gradient Activation Maps (GAM) - a machinery for explaining predictions made by visual similarity and classification models. By gleaning localized gradient and activation information from multiple network layers, GAM offers improved visual explanations, when compared to existing alternatives. The algorithmic advantages of GAM are explained in detail, and validated empirically, where it is shown that GAM outperforms its alternatives across various tasks and datasets. © 2021 ACM.","deep learning; explainable & interpretable ai; saliency maps","Chemical activation; Deep learning; Network layers; Activation maps; Classification models; Deep learning; Explainable & interpretable ai; Localised; Multiple networks; Saliency map; Similarity models; Visual classification; Visual similarity; Machinery"
"Barker-Plummer D., Dale R., Cox R.","Impedance Effects of Visual and Spatial Content upon Language-to-Logic Translation Accuracy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959001805&partnerID=40&md5=184ff8b4594cfead6fbec9408c303757","There is a body of work that suggests that those elements of the cognitive architecture responsible for processing, on the one hand, visual information (essentially visual properties of objects), and, on the other hand, spatial information (spatial relationships between objects), may compete with each other for resources. In this paper, we explore whether and to what degree the processing of visual and spatial information interferes with the task of translation from natural language into logic, a skill that students often find difficult to master. Using a large corpus of student data, we determine correlations between difficulty and the particular properties used in the sentences, with implications for pedagogical design. © CogSci 2011.","educational data mining; first-order logic; instructional design; logic teaching and learning; visual impedance; visuospatial reasoning; visuospatial working memory","Computer circuits; Data mining; Formal logic; Teaching; Translation (languages); Visual languages; Educational data mining; First order logic; Instructional designs; Logic teaching and learning; Teaching and learning; Visual impedance; Visual information; Visuospatial reasoning; Visuospatial working memory; Working memory; Students"
"Barlaug N.","LEMON: Explainable Entity Matching","10.1109/TKDE.2022.3200644","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137610233&doi=10.1109%2fTKDE.2022.3200644&partnerID=40&md5=8fa94f5a724dd147adf9b6f877dd6c0a","State-of-the-art entity matching (EM) methods are hard to interpret, and there is significant value in bringing explainable AI to EM. Unfortunately, most popular explainability methods do not work well out of the box for EM and need adaptation. In this paper, we identify three challenges of applying local post hoc feature attribution methods to entity matching: cross-record interaction effects, non-match explanations, and variation in sensitivity. We propose our novel model-agnostic and schema-flexible method LEMON that addresses all three challenges by (i) producing dual explanations to avoid cross-record interaction effects, (ii) introducing the novel concept of attribution potential to explain how two records could have matched, and (iii) automatically choosing explanation granularity to match the sensitivity of the matcher and record pair in question. Experiments on public datasets demonstrate that the proposed method is more faithful to the matcher and does a better job of helping users understand the decision boundary of the matcher than previous work. Furthermore, user studies show that the rate at which human subjects can construct counterfactual examples after seeing an explanation from our proposed method increases from 54&#x0025; to 64&#x0025; for matches and from 15&#x0025; to 49&#x0025; for non-matches compared to explanations from a standard adaptation of LIME. IEEE","Adaptation models; Data integration; Data models; Deep learning; entity matching; entity resolution; explainability; Machine learning; machine learning; Predictive models; Sensitivity; Standards","Citrus fruits; Deep learning; Learning systems; Lime; Sensitivity analysis; Adaptation models; Deep learning; Entity matching; Entity resolutions; Explainability; Interaction effect; Machine-learning; Predictive models; Sensitivity; Data integration"
"Barlaug N.","Tailoring Entity Matching for Industrial Settings","10.1145/3340531.3418514","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095865150&doi=10.1145%2f3340531.3418514&partnerID=40&md5=81fb57c0d710649e643fa7a81d54fbeb","Entity matching has received significant attention from the research community over many years. Despite some limited success, most state-of-the-art methods see no widespread usage in industry. In this paper, we present the author's PhD research, which aims at identifying issues that hold techniques and methods developed by the research community back from use in industry, and look at how they might be adapted to address those issues. In our proposed approach, we implement a modular framework, which will be used for real-world user testing and quantitative experiments of our adapted methods. We will have three main contributions from our research: 1) We develop a modular framework for interactive entity matching combining intra- and inter-session iterations. 2) We show how active learning methods for entity matching can be adapted to learn not only classification of matches but also classification of which records are of interest to the user jointly, and how it compares to current methods. 3) We show how deep learning can be used to synthesize interpretable rules for entity matching, and how it compares to traditional methods. © 2020 ACM.","data integration; data matching; entity matching; entity resolution","Deep learning; Knowledge management; Active learning methods; Entity matching; Industrial settings; Interpretable rules; Modular framework; Quantitative experiments; Research communities; State-of-the-art methods; Learning systems"
"Barlow C.","Oncology Research: Clinical Trial Management Systems, Electronic Medical Record, and Artificial Intelligence","10.1016/j.soncn.2020.151005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082874477&doi=10.1016%2fj.soncn.2020.151005&partnerID=40&md5=108943af18ac9337d23e5b04d276770a","Objective: To discuss the implications of electronic systems and regulations regarding the use of electronic systems implemented during the conduct of a clinical trial and identify the impact of such platforms on oncology nurses’ responsible for providing care to the research participant. Data Sources: Peer-reviewed journal articles, internet, book chapters, and white papers. Conclusion: Electronic systems are being increasingly used in the conduct of clinical research. Electronic systems enable the capability to streamline data transfer, remote enrollment capabilities, greater transparency of the trial conduct, improved research documentation, and clearer audit trails. The oncology nurse is at the center of implementation of electronic systems to support the conduct of clinical research and enables safe and effective care to the research participant. Implications for Nursing Practice: Oncology nurses are vital to the successful outcome of clinical research studies and are key members of the clinical research team. Electronic systems move beyond traditional data collection in clinical trials with multiple benefits. Such systems may enhance the successful completion and adherence of the clinical trial and maintain the safety of the individual consented to research trial. © 2020 Elsevier Inc.","artificial intelligence (AI); Clinical Trial Management System (CTMS); electronic medical record (EMR); integration; research nurse; research team","adult; artificial intelligence; clinical research; electronic medical record; human; Internet; nursing practice; oncology nurse; review"
"Barmpounakis S., Demestichas P.","Framework for Trustworthy AI/ML in B5G/6G","10.1109/6GNet54646.2022.9830408","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136129595&doi=10.1109%2f6GNet54646.2022.9830408&partnerID=40&md5=09a853d2281b6ff59f741dd49eab57ba","The world already moves towards the 6G era. AI/ML mechanisms will become structural components of the system and operate in a native manner. As the systems get more complex and intelligent, mechanisms for ensuring trust in those operations become critical. In this paper we take some first steps in the discussion on trustworthy AI towards 6G. Our discussion justifies the need for a framework and highlights that a comprehensive approach needs to be taken, for protecting the input of the AI mechanisms, for achieving an explainable operation, and for guaranteeing proper outputs. Architectural design principles, as well as resource allocation aspects are touched upon, and the important future steps are designated. © 2022 IEEE.","Artificial Intelligence; Explainability; Machine Learning; Trust; Wireless communications","Complex mechanisms; Design Principles; Design resources; Explainability; Intelligent mechanisms; Machine-learning; Resources allocation; Structural component; Trust; Wireless communications; Machine learning"
"Barnard A.S.","Explainable prediction of N-V-related defects in nanodiamond using neural networks and Shapley values","10.1016/j.xcrp.2021.100696","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123021737&doi=10.1016%2fj.xcrp.2021.100696&partnerID=40&md5=4d63584df21d1f2c44d6e82753bc7bb9","Although the negatively charged nitrogen-vacancy (N-V−) defect in nanodiamonds is desirable for a variety of biomedical applications, a range of other defect complexes involving nitrogen and/or vacancies can also exist, depending on their relative stability. Using machine learning, a re-usable model is developed to predict the likelihood of a particular defect complex being stable at a given depth below reconstructed or hydrogen-passivated surfaces. A neural network is used to generate a system of equations that can be easily implemented in any workflow, and explainable artificial intelligence (XAI) methods are used to provide insights into which structural features and defect configurations are most responsible for the model prediction. It is found that, although the number of nitrogen atoms present in the defect is the most important feature determining the defect likelihood, the most influential data instances are the unlikely defects, providing a type of baseline for comparison. © 2021 The Author(s)",,
"Barnard P., MacAluso I., Marchetti N., Dasilva L.A.","Resource Reservation in Sliced Networks: An Explainable Artificial Intelligence (XAI) Approach","10.1109/ICC45855.2022.9838766","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137263741&doi=10.1109%2fICC45855.2022.9838766&partnerID=40&md5=72dd27dfa7b168898fd7642a190ee167","The growing complexity of wireless networks has sparked an upsurge in the use of artificial intelligence (AI) within the telecommunication industry in recent years. In network slicing, a key component of 5G that enables network operators to lease their resources to third-party tenants, AI models may be employed in complex tasks, such as short-term resource reservation (STRR). When AI is used to make complex resource management decisions with financial and service quality implications, it is important that these decisions be understood by a human-in-the-loop. In this paper, we apply state-of-the-art techniques from the field of Explainable AI (XAI) to the problem of STRR. Using real-world data to develop an AI model for STRR, we demonstrate how our XAI methodology can be used to explain the real-time decisions of the model, to reveal trends about the model's general behaviour, as well as aid in the diagnosis of potential faults during the model's development. In addition, we quantitatively validate the faithfulness of the explanations across an extensive range of XAI metrics to ensure they remain trustworthy and actionable. © 2022 IEEE.","Explainable Artificial Intelligence (XAI); Network Resource Management (RM)","5G mobile communication systems; Artificial intelligence; Decision making; Decision support systems; Human resource management; Natural resources management; Quality of service; Resource allocation; Telecommunication industry; Explainable artificial intelligence (XAI); In networks; Intelligence models; Network operator; Network resource management; Network slicing; Resource reservations; Telecommunications industry; Third parties; Complex networks"
"Barnathan M., Megalooikonomou V., Faloutsos C., Faro S., Mohamed F.B.","TWave: High-order analysis of functional MRI","10.1016/j.neuroimage.2011.06.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051789881&doi=10.1016%2fj.neuroimage.2011.06.043&partnerID=40&md5=181fedae48314313d0a324b52cacb45e","The traditional approach to functional image analysis models images as matrices of raw voxel intensity values. Although such a representation is widely utilized and heavily entrenched both within neuroimaging and in the wider data mining community, the strong interactions among space, time, and categorical modes such as subject and experimental task inherent in functional imaging yield a dataset with ""high-order"" structure, which matrix models are incapable of exploiting. Reasoning across all of these modes of data concurrently requires a high-order model capable of representing relationships between all modes of the data in tandem. We thus propose to model functional MRI data using tensors, which are high-order generalizations of matrices equivalent to multidimensional arrays or data cubes. However, several unique challenges exist in the high-order analysis of functional medical data: naïve tensor models are incapable of exploiting spatiotemporal locality patterns, standard tensor analysis techniques exhibit poor efficiency, and mixtures of numeric and categorical modes of data are very often present in neuroimaging experiments. Formulating the problem of image clustering as a form of Latent Semantic Analysis and using the WaveCluster algorithm as a baseline, we propose a comprehensive hybrid tensor and wavelet framework for clustering, concept discovery, and compression of functional medical images which successfully addresses these challenges. Our approach reduced runtime and dataset size on a 9.3. GB finger opposition motor task fMRI dataset by up to 98% while exhibiting improved spatiotemporal coherence relative to standard tensor, wavelet, and voxel-based approaches. Our clustering technique was capable of automatically differentiating between the frontal areas of the brain responsible for task-related habituation and the motor regions responsible for executing the motor task, in contrast to a widely used fMRI analysis program, SPM, which only detected the latter region. Furthermore, our approach discovered latent concepts suggestive of subject handedness nearly 100× faster than standard approaches. These results suggest that a high-order model is an integral component to accurate scalable functional neuroimaging. © 2011 Elsevier Inc.","Clustering; FMRI; Latent Semantic Analysis; Parallel Factor Analysis; Tensors; Wavelets","adult; article; automation; computer program; controlled study; frontal lobe; functional magnetic resonance imaging; habituation; handedness; human; human experiment; image analysis; intermethod comparison; motor cortex; motor performance; normal human; priority journal; wavelet analysis; Adult; Algorithms; Cluster Analysis; Data Interpretation, Statistical; Data Mining; Diffusion Tensor Imaging; Factor Analysis, Statistical; Female; Fuzzy Logic; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Models, Statistical; Principal Component Analysis; Wavelet Analysis"
"Barnett A.J., Sharma V., Gajjar N., Fang J., Schwartz F.R., Chen C., Lo J.Y., Rudin C.","Interpretable Deep Learning Models for Better Clinician-AI Communication in Clinical Mammography","10.1117/12.2612372","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131881767&doi=10.1117%2f12.2612372&partnerID=40&md5=9708adaf59e2b7e2909c2e4120599a6f","There is increasing interest in using deep learning and computer vision to help guide clinical decisions, such as whether to order a biopsy based on a mammogram. Existing networks are typically black box, unable to explain how they make their predictions. We present an interpretable deep-learning network which explains its predictions in terms of BI-RADS features mass shape and mass margin. Our model predicts mass margin and mass shape, then uses the logits from those interpretable models to predict malignancy, also using an interpretable model. The interpretable mass margin model explains its predictions using a prototypical parts model. The interpretable mass shape model predicts segmentations, fits an ellipse, then determines shape based on the goodness of fit and eccentricity of the fitted ellipse. While including mass shape logits in the malignancy prediction model did not improve performance, we present this technique as part of a framework for better clinician-AI communication. 2022 SPIE. © 2022 SPIE. All rights reserved.","Cancer; Deep Learning; Interpretability; Mammography; Masses","Deep learning; Mammography; BI-RADS; Black boxes; Clinical decision; Clinical mammography; Deep learning; Interpretability; Learning models; Learning network; Mass; Shape Modelling; Forecasting"
"Barnett A.J., Schwartz F.R., Tao C., Chen C., Ren Y., Lo J.Y., Rudin C.","A case-based interpretable deep learning model for classification of mass lesions in digital mammography","10.1038/s42256-021-00423-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121359698&doi=10.1038%2fs42256-021-00423-x&partnerID=40&md5=f19dbce9a04118f56cf1260de7b61d0b","Interpretability in machine learning models is important in high-stakes decisions such as whether to order a biopsy based on a mammographic exam. Mammography poses important challenges that are not present in other computer vision tasks: datasets are small, confounding information is present and it can be difficult even for a radiologist to decide between watchful waiting and biopsy based on a mammogram alone. In this work we present a framework for interpretable machine learning-based mammography. In addition to predicting whether a lesion is malignant or benign, our work aims to follow the reasoning processes of radiologists in detecting clinically relevant semantic features of each image, such as the characteristics of the mass margins. The framework includes a novel interpretable neural network algorithm that uses case-based reasoning for mammography. Our algorithm can incorporate a combination of data with whole image labelling and data with pixel-wise annotations, leading to better accuracy and interpretability even with a small number of images. Our interpretable models are able to highlight the classification-relevant parts of the image, whereas other methods highlight healthy tissue and confounding information. Our models are decision aids—rather than decision makers—and aim for better overall human–machine collaboration. We do not observe a loss in mass margin classification accuracy over a black box neural network trained on the same data. © 2021, The Author(s), under exclusive licence to Springer Nature Limited.",,"Classification (of information); Decision making; Decision support systems; Deep learning; E-learning; Mammography; RNA; Semantics; Case based; Digital mammography; Interpretability; Learning models; Machine learning models; Mammographic; Mass lesion; Neural networks algorithms; Reasoning process; Semantic features; Biopsy"
"Baron B., Musolesi M.","Interpretable Machine Learning for Privacy-Preserving Pervasive Systems","10.1109/MPRV.2019.2918540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078215110&doi=10.1109%2fMPRV.2019.2918540&partnerID=40&md5=6dabdfd245fcfdf4673e360b5c033ba7","Our everyday interactions with pervasive systems generate traces that capture various aspects of human behavior and enable machine learning algorithms to extract latent information about users. In this paper, we propose a machine learning interpretability framework that enables users to understand how these generated traces violate their privacy. © 2002-2012 IEEE.",,"Behavioral research; Learning algorithms; Human behaviors; Interpretability; Latent information; Pervasive systems; Privacy preserving; Machine learning"
"Baron S., Lazzarini N., Bacardit J.","Characterising the influence of rule-based knowledge representations in biological knowledge extraction from transcriptomics data","10.1007/978-3-319-55849-3_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017567274&doi=10.1007%2f978-3-319-55849-3_9&partnerID=40&md5=ee4983fa53404bac46ea1e48395346c5","Currently, there is a wealth of biotechnologies (e.g. sequencing, proteomics, lipidomics) able to generate a broad range of data types out of biological samples. However, the knowledge gained from such data sources is constrained by the limitations of the analytics techniques. The state-of-the-art machine learning algorithms are able to capture complex patterns with high prediction capacity. However, often it is very difficult if not impossible to extract human-understandable knowledge out of these patterns. In recent years evolutionary machine learning techniques have shown that they are competent methods for biological/biomedical data analytics. They are able to generate interpretable prediction models and, beyond just prediction models, they are able to extract useful knowledge in the form of biomarkers or biological networks. The focus of this paper is to thoroughly characterise the impact that a core component of the evolutionary machine learning process, its knowledge representations, has in the process of extracting biologically-useful knowledge out of transcriptomics datasets. Using the FuNeL evolutionary machine learning-based network inference method, we evaluate several variants of rule knowledge representations on a range of transcriptomics datasets to quantify the volume and complementarity of the knowledge that each of them can extract. Overall we show that knowledge representations, often considered a minor detail, greatly impact on the downstream biological knowledge extraction process. © Springer International Publishing AG 2017.","Biological knowledge extraction; Evolutionary machine learning; Rule knowledge representations","Artificial intelligence; Extraction; Forecasting; Knowledge representation; Learning algorithms; Molecular biology; Biological networks; Biological samples; Core components; Knowledge extraction; Machine learning techniques; Network inference; Prediction model; State of the art; Learning systems"
"Baronti F., Starita A.","Hypothesis testing with classifier systems for rule-based risk prediction","10.1007/978-3-540-71783-6_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049061698&doi=10.1007%2f978-3-540-71783-6_3&partnerID=40&md5=2f586dc8b91053d8984687023709b26f","Analysis of medical datasets has some specific requirements not always fulfilled by standard Machine Learning methods. In particular, heterogeneous and missing data must be tolerated, the results should be easily interpretable. Moreover, with genetic data, often the combination of two or more attributes leads to non-linear effects not detectable for each attribute on its own. We present a new ML algorithm, HCS, taking inspiration from learning classifier systems, decision trees and statistical hypothesis testing. We show the results of applying this algorithm to a well-known benchmark dataset, and to HNSCC, a dataset studying the connection between smoke and genetic patterns to the development of oral cancer. © Springer-Verlag Berlin Heidelberg 2007.",,"Algorithms; Data reduction; Data structures; Decision trees; Genetic programming; Learning systems; Statistical methods; Benchmark datasets; Classifier systems; Genetic data; Classification (of information)"
"Barr Kumarakulasinghe N., Blomberg T., Liu J., Saraiva Leao A., Papapetrou P.","Evaluating local interpretable model-agnostic explanations on clinical machine learning classification models","10.1109/CBMS49503.2020.00009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091135245&doi=10.1109%2fCBMS49503.2020.00009&partnerID=40&md5=202b6fb9f7aa6051c6e2f8f3eaffc321","The usage of black-box classification models within the healthcare field is highly dependent on being interpretable by the receiver. Local Interpretable Model-Agnostic Explanation (LIME) provides a patient-specific explanation for a given classification, thus enhancing the possibility for any complex classifier to serve as a safety aid within a clinical setting. However, research on if the explanation provided by LIME is relevant to clinicians is limited and there is no current framework for how an evaluation of LIME is to be performed. To evaluate the clinical relevance of the explanations provided by LIME, this study has investigated how physician's independent explanations for classified observations compare with explanations provided by LIME. Additionally, the clinical relevance and the experienced reliance on the explanations provided by LIME have been evaluated by field experts. The results indicate that the explanation provided by LIME is clinically relevant and has a very high concordance with the explanations provided by physicians. Furthermore, trust and reliance on LIME are fairly high amongst clinicians. The study proposes a framework for further research within the area. © 2020 IEEE.","Classification model; Clinical decision support system; Local interpretable model-agnostic explanation; Machine learning; Patient-specific explanation","Machine learning; Black boxes; Classification models; Clinical settings; Machine learning classification; Patient specific; Lime"
"Barrance E., Kazim E., Hilliard A., Trengove M., Zannone S., Koshiyama A.","Overview and commentary of the CDEI's extended roadmap to an effective AI assurance ecosystem","10.3389/frai.2022.932358","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136560221&doi=10.3389%2ffrai.2022.932358&partnerID=40&md5=4a5c0b0d8d11175f593b14c5d12e429f","In recent years, the field of ethical artificial intelligence (AI), or AI ethics, has gained traction and aims to develop guidelines and best practices for the responsible and ethical use of AI across sectors. As part of this, nations have proposed AI strategies, with the UK releasing both national AI and data strategies, as well as a transparency standard. Extending these efforts, the Centre for Data Ethics and Innovation (CDEI) has published an AI Assurance Roadmap, which is the first of its kind and provides guidance on how to manage the risks that come from the use of AI. In this article, we provide an overview of the document's vision for a “mature AI assurance ecosystem” and how the CDEI will work with other organizations for the development of regulation, industry standards, and the creation of AI assurance practitioners. We also provide a commentary of some key themes identified in the CDEI's roadmap in relation to (i) the complexities of building “justified trust”, (ii) the role of research in AI assurance, (iii) the current developments in the AI assurance industry, and (iv) convergence with international regulation. Copyright © 2022 Barrance, Kazim, Hilliard, Trengove, Zannone and Koshiyama.","artificial intelligence (AI); compliance; ethics; governance; regulation; roadmap; standards",
"Barratt M.D., Langowski J.J.","Validation and subsequent development of the derek skin sensitization rulebase by analysis of the BgVV list of contact allergens","10.1021/ci980204n","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033091308&doi=10.1021%2fci980204n&partnerID=40&md5=2ebea1d7af61ab51048f88a99910d9e5","The DEREK knowledge-based computer system contains a subset of approximately 50 rules describing chemical substructures (toxophores) responsible for skin sensitization. This rulebase, based originally on Unilever historical in-house guinea pig maximization test data, has been subject to extensive validation and is undergoing refinement as the next stage of its development. As part of an ongoing program of validation and testing, the predictive ability of the sensitization rule set has been assessed by processing the structures of the 84 chemical substances in the list of contact allergens issued by the BgVV (German Federal Institute for Health Protection of Consumers). This list of chemicals is important because the biological data for each of the chemicals have been carefully scrutinized and peer reviewed, a key consideration in an area of toxicology in which much unreliable and potentially misleading data have been published. The existing DEREK rulebase for skin sensitization identified toxophores for skin sensitization in the structures of 71 out of the 84 chemicals (85%). The exercise highlighted areas of chemistry where further development of the rulebase was required, either by extension of the scope of existing rules or by generation of new rules where a sound mechanistic rationale for the biological activity could be established. Chemicals likely to be acting as photoallergens were identified, and new rules for photoallergenicity have subsequently been written. At the end of the exercise, the refined rulebase was able to identify toxophores for skin sensitization for 82 of the 84 chemicals in the BgVV list.",,"allergen; hydroxylamine; peroxide; animal; article; artificial intelligence; chemistry; drug effect; drug screening; guinea pig; immunology; photoallergy; reproducibility; skin; skin allergy; structure activity relation; Allergens; Animals; Artificial Intelligence; Dermatitis, Allergic Contact; Dermatitis, Photoallergic; Drug Evaluation, Preclinical; Guinea Pigs; Hydroxylamine; Peroxides; Reproducibility of Results; Skin; Structure-Activity Relationship"
"Barraza J.F., Droguett E.L., Martins M.R.","Towards interpretable deep learning: A feature selection framework for prognostics and health management using deep neural networks","10.3390/s21175888","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114116540&doi=10.3390%2fs21175888&partnerID=40&md5=6e9010d9e8531f8414e23720f55b12ea","In the last five years, the inclusion of Deep Learning algorithms in prognostics and health management (PHM) has led to a performance increase in diagnostics, prognostics, and anomaly detection. However, the lack of interpretability of these models results in resistance towards their deployment. Deep Learning‐based models fall within the accuracy/interpretability tradeoff, which means that their complexity leads to high performance levels but lacks interpretability. This work aims at addressing this tradeoff by proposing a technique for feature selection embedded in deep neural networks that uses a feature selection (FS) layer trained with the rest of the network to evaluate the input features’ importance. The importance values are used to determine which will be considered for deployment of a PHM model. For comparison with other techniques, this paper introduces a new metric called ranking quality score (RQS), that measures how performance evolves while following the corresponding ranking. The proposed framework is exemplified with three case studies involving health state diagnostics and prognostics and remaining useful life prediction. Results show that the proposed technique achieves higher RQS than the compared techniques, while maintaining the same performance level when compared to the same model but without an FS layer. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Deep neural networks; Feature selection; Interpretable AI; Prognostics and health management","Anomaly detection; Deep neural networks; Feature extraction; Health; Learning algorithms; Multilayer neural networks; Case-studies; Diagnostics and prognostics; Input features; Interpretability; Performance level; Prognostics and health managements; Remaining useful life predictions; Selection framework; Deep learning"
"Barredo Arrieta A., Gil-Lopez S., Laña I., Bilbao M.N., Del Ser J.","On the post-hoc explainability of deep echo state networks for time series forecasting, image and video classification","10.1007/s00521-021-06359-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111830181&doi=10.1007%2fs00521-021-06359-y&partnerID=40&md5=0de7ea507506ec64059c6db0132535af","Since their inception, learning techniques under the reservoir computing paradigm have shown a great modeling capability for recurrent systems without the computing overheads required for other approaches, specially deep neural networks. Among them, different flavors of echo state networks have attracted many stares through time, mainly due to the simplicity and computational efficiency of their learning algorithm. However, these advantages do not compensate for the fact that echo state networks remain as black-box models whose decisions cannot be easily explained to the general audience. This issue is even more involved for multi-layered (also referred to as deep) echo state networks, whose more complex hierarchical structure hinders even further the explainability of their internals to users without expertise in machine learning or even computer science. This lack of explainability can jeopardize the widespread adoption of these models in certain domains where accountability and understandability of machine learning models is a must (e.g., medical diagnosis, social politics). This work addresses this issue by conducting an explainability study of echo state networks when applied to learning tasks with time series, image and video data. Among these tasks, we stress on the latter one (video classification) which, to the best of our knowledge, has never been tackled before with echo state networks in the related literature. Specifically, the study proposes three different techniques capable of eliciting understandable information about the knowledge grasped by these recurrent models, namely potential memory, temporal patterns and pixel absence effect. Potential memory addresses questions related to the effect of the reservoir size in the capability of the model to store temporal information, whereas temporal patterns unveil the recurrent relationships captured by the model over time. Finally, pixel absence effect attempts at evaluating the effect of the absence of a given pixel when the echo state network model is used for image and video classification. The benefits of the proposed suite of techniques are showcased over three different domains of applicability: time series modeling, image and, for the first time in the related literature, video classification. The obtained results reveal that the proposed techniques not only allow for an informed understanding of the way these models work, but also serve as diagnostic tools capable of detecting issues inherited from data (e.g., presence of hidden bias). © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Echo state networks; Explainable artificial intelligence; Randomization-based machine learning; Reservoir computing","Classification (of information); Computational efficiency; Deep neural networks; Diagnosis; Image classification; Learning algorithms; Learning systems; Pixels; Time series; Time series analysis; Hierarchical structures; Machine learning models; Reservoir Computing; Temporal information; Three different techniques; Time series forecasting; Time series modeling; Video classification; Recurrent neural networks"
"Barredo Arrieta A., Díaz-Rodríguez N., Del Ser J., Bennetot A., Tabik S., Barbado A., Garcia S., Gil-Lopez S., Molina D., Benjamins R., Chatila R., Herrera F.","Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI","10.1016/j.inffus.2019.12.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077515399&doi=10.1016%2fj.inffus.2019.12.012&partnerID=40&md5=720e37936410af916e3efe40346dbeed","In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability. © 2019","Accountability; Comprehensibility; Data Fusion; Deep Learning; Explainable Artificial Intelligence; Fairness; Interpretability; Machine Learning; Privacy; Responsible Artificial Intelligence; Transparency","Data fusion; Data privacy; Deep learning; Deep neural networks; Expert systems; Taxonomies; Transparency; Accountability; Comprehensibility; Fairness; Interpretability; Literature analysis; Machine learning models; Reference material; Rule-based models; Learning systems"
"Barredo-Arrieta A., Del Ser J.","Plausible Counterfactuals: Auditing Deep Learning Classifiers with Realistic Adversarial Examples","10.1109/IJCNN48605.2020.9206728","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093831442&doi=10.1109%2fIJCNN48605.2020.9206728&partnerID=40&md5=9f853cfc3395e8118548c6e16f13ebe1","The last decade has witnessed the proliferation of Deep Learning models in many applications, achieving unrivaled levels of predictive performance. Unfortunately, the black-box nature of Deep Learning models has posed unanswered questions about what they learn from data. Certain application scenarios have highlighted the importance of assessing the bounds under which Deep Learning models operate, a problem addressed by using assorted approaches aimed at audiences from different domains. However, as the focus of the application is placed more on non-expert users, it results mandatory to provide the means for him/her to trust the model, just like a human gets familiar with a system or process: by understanding the hypothetical circumstances under which it fails. This is indeed the angular stone for this research work: to undertake an adversarial analysis of a Deep Learning model. The proposed framework constructs counterfactual examples by ensuring their plausibility, e.g. there is a reasonable probability that a human could generate them without resorting to a computer program. Therefore, this work must be regarded as valuable auditing exercise of the usable bounds a certain model is constrained within, thereby allowing for a much greater understanding of the capabilities and pitfalls of a model used in a real application. To this end, a Generative Adversarial Network (GAN) and multi-objective heuristics are used to furnish a plausible attack to the audited model, efficiently trading between the confusion of this model, the intensity and plausibility of the generated counterfactual. Its utility is showcased within a human face classification task, unveiling the enormous potential of the proposed framework. © 2020 IEEE.","Counterfactuals; Deep Learning; Explainable Artificial Intelligence; Generative Adversarial Networks; Meta-heuristics; Multiobjective Optimization","Learning systems; Neural networks; Adversarial networks; Application scenario; Counterfactuals; Different domains; Learning classifiers; Multi objective; Predictive performance; Real applications; Deep learning"
"Barredo-Arrieta A., Lana I., Del Ser J.","What Lies Beneath: A Note on the Explainability of Black-box Machine Learning Models for Road Traffic Forecasting","10.1109/ITSC.2019.8916985","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076798818&doi=10.1109%2fITSC.2019.8916985&partnerID=40&md5=9b25d7704a43d8aa522e730460104c49","Traffic flow forecasting is widely regarded as an essential gear in the complex machinery underneath Intelligent Transport Systems, being a critical component of avant-garde Automated Traffic Management Systems. Research in this area has stimulated a vibrant activity, yielding a plethora of new forecasting methods contributed to the community on a yearly basis. Efforts in this domain are mainly oriented to the development of prediction models featuring with ever-growing levels of performances and/or computational efficiency. After the swerve towards Artificial Intelligence that gradually took place in the modeling sphere of traffic forecasting, predictive schemes have ever since adopted all the benefits of applied machine learning, but have also incurred some caveats. The adoption of highly complex, black-box models has subtracted comprehensibility to forecasts: even though they perform better, they are more obscure to ITS practitioners, which hinders their practicality. In this paper we propose the adoption of explainable Artificial Intelligence (xAI) tools that are currently being used in other domains, in order to extract further knowledge from black-box traffic forecasting models. In particular we showcase the utility of xAI to unveil the knowledge extracted by Random Forests and Recurrent Neural Networks when predicting real traffic. The obtained results are insightful and suggest that the traffic forecasting model should be analyzed from more points of view beyond that of prediction accuracy or any other regression score alike, due to the treatment each algorithm gives to input variables: even with the same nominal score value, some methods can take advantage of inner knowledge that others instead disregard. © 2019 IEEE.",,"Advanced traffic management systems; Complex networks; Computational efficiency; Data mining; Decision trees; Highway traffic control; Intelligent systems; Machine learning; Machinery; Recurrent neural networks; Applied machine learning; Forecasting methods; Intelligent transport systems; Machine learning models; Prediction accuracy; Traffic flow forecasting; Traffic Forecasting; Traffic management systems; Forecasting"
"Barreto G.A., Araújo A.F.","Unsupervised learning and temporal context to recall complex robot trajectories.","10.1016/S0129-0657(01)00046-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035260060&doi=10.1016%2fS0129-0657%2801%2900046-1&partnerID=40&md5=dff9a53051fb64b182a88e1adf5f32fa","An unsupervised neural network is proposed to learn and recall complex robot trajectories. Two cases are considered: (i) A single trajectory in which a particular arm configuration (state) may occur more than once, and (ii) trajectories sharing states with each other. Ambiguities occur in both cases during recall of such trajectories. The proposed model consists of two groups of synaptic weights trained by competitive and Hebbian learning laws. They are responsible for encoding spatial and temporal features of the input sequences, respectively. Three mechanisms allow the network to deal with repeated or shared states: local and global context units, neurons disabled from learning, and redundancy. The network reproduces the current and the next state of the learned sequences and is able to resolve ambiguities. The model was simulated over various sets of robot trajectories in order to evaluate learning and recall, trajectory sampling effects and robustness.",,"algorithm; article; artificial intelligence; computer simulation; methodology; robotics; Algorithms; Artificial Intelligence; Computer Simulation; Robotics"
"Barreto S.C., Lambert J.A., de Barros Vidal F.","Using Synthetic Images for Deep Learning Recognition Process on Automatic License Plate Recognition","10.1007/978-3-030-21077-9_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068330481&doi=10.1007%2f978-3-030-21077-9_11&partnerID=40&md5=62123a2e4c61eaaa0db894cd5605abc9","The Automatic License Plate Recognition has been the subject of several studies, given its applicability in real world situations (e.g. toll collection, identification of vehicles in parking lots or even for safety issues in vehicle control that cross borders between countries). In this work, we propose an analysis of the influence to retraining a plate recognition model and a deep neural network for object detection, using synthetic plates image databases from the Brazilian licence plates. The proposed data set uses variations of rotation, size and noise to evaluate the robustness. Thus, the influence of the use of synthetic plates images on the accuracy of systems responsible for locating real plates, segmenting the characters and recognizing them was evaluated and in the tests performed there was an increase in accuracy (considering a system trained with real plates) of three stages: character segmentation, letter recognition and number recognition ($$2.54\%$$, $$1.09\%$$ and $$2,49\%$$ respectively). It stands out the accuracy of $$62.47\%$$ (in the number recognition step) obtained by a neural network trained exclusively with synthetic data and tested on real plates. © 2019, Springer Nature Switzerland AG.","Deep neural networks; License plate recognition; Synthetic images","Automatic vehicle identification; Control system synthesis; Deep neural networks; Image segmentation; Object detection; Object recognition; Optical character recognition; Automatic license plate recognition; Character segmentation; Letter recognition; License plate recognition; Number recognition; Real world situations; Recognition process; Synthetic images; License plates (automobile)"
"Barrett H., Rose D.C.","Perceptions of the Fourth Agricultural Revolution: What’s In, What’s Out, and What Consequences are Anticipated?","10.1111/soru.12324","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090341643&doi=10.1111%2fsoru.12324&partnerID=40&md5=2c9d33345bc80d87d6b7aa75e0fbd4f3","Technological advancement is seen as one way of sustainably intensifying agriculture. Scholars argue that innovation needs to be responsible, but it is difficult to anticipate the consequences of the ‘fourth agricultural revolution’ without a clear sense of which technologies are included and excluded. The major aims of this article were to investigate which technologies are being associated with the fourth agricultural revolution, as well as to understand how this revolution is being perceived, whether positive or negative consequences are given equal attention, and what type of impacts are anticipated. To this end, we undertook a content analysis of UK media and policy documents alongside interviews of farmers and advisers. We found that the fourth agricultural revolution is associated with emergent, game-changing technologies, at least in media and policy documents. In these sources, the benefits to productivity and the environment were prioritised with less attention to social consequences, but impacts were overwhelmingly presented positively. Farmers and advisers experienced many benefits of technologies and some predicted higher-tech futures. It was clear, however, that technologies create a number of negative consequences. We reflect on these findings and provide advice to policy-makers about how to interrogate the benefits, opportunities, and risks afforded by agricultural technologies. © 2020 The Authors. Sociologia Ruralis published by John Wiley & Sons Ltd on behalf of European Society for Rural Sociology.","adaptive capacity; agriculture 4.0; artificial intelligence; automation; data analytics; fourth agricultural revolution; gene editing; responsible innovation; robotics; technology","agricultural development; agricultural policy; innovation; intensive agriculture; perception; policy approach; policy making; technological development; United Kingdom"
"Barrett N., Weber-Jahnke J.H., Thai V.","Engineering natural language processing solutions for structured information from clinical text: Extracting sentinel events from palliative care consult letters","10.3233/978-1-61499-289-9-594","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894365226&doi=10.3233%2f978-1-61499-289-9-594&partnerID=40&md5=7e680a5f85d7c53059436643ba77b141","Despite a trend to formalize and codify medical information, natural language communications still play a prominent role in health care workflows, in particular when it comes to hand-overs between providers. Natural language processing (NLP) attempts to bridge the gap between informal, natural language information and coded, machine-interpretable data. This paper reports on a study that applies an advanced NLP method for the extraction of sentinel events in palliative care consult letters. Sentinel events are of interest to predict survival and trajectory for patients with acute palliative conditions. Our NLP method combines several novel characteristics, e.g., the consideration of topological knowledge structures sourced from an ontological terminology system (SNOMED CT). The method has been applied to the extraction of different types of sentinel events, including simple facts, temporal conditions, quantities, and degrees. A random selection of 215 anonymized consult letters was used for the study. The results of the NLP extraction were evaluated by comparison with coded sentinel event data captured independently by clinicians. The average accuracy of the automated extraction was 73.6%. © 2013 IMIA and IOS Press.","Letter; Natural Language Processing; Palliative Care; Sentinel Event","Extraction; Automated extraction; Knowledge structures; Letter; Medical information; Natural language communication; Palliative care; Sentinel Event; Structured information; Natural language processing systems; automated pattern recognition; Canada; classification; data mining; electronic medical record; natural language processing; nomenclature; palliative therapy; patient referral; procedures; sentinel surveillance; Systematized Nomenclature of Medicine; Alberta; Data Mining; Medical Records Systems, Computerized; Natural Language Processing; Palliative Care; Pattern Recognition, Automated; Referral and Consultation; Sentinel Surveillance; Systematized Nomenclature of Medicine; Terminology as Topic"
"Barrey E., Mucher E., Jeansoule N., Larcher T., Guigand L., Herszberg B., Chaffaux S., Guérin G., Mata X., Benech P., Canale M., Alibert O., Maltere P., Gidrol X.","Gene expression profiling in equine polysaccharide storage myopathy revealed inflammation, glycogenesis inhibition, hypoxia and mitochondrial dysfunctions","10.1186/1746-6148-5-29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-73349142391&doi=10.1186%2f1746-6148-5-29&partnerID=40&md5=45fe5e9ae943c09299014ed0ab24aaac","Background: Several cases of myopathies have been observed in the horse Norman Cob breed. Muscle histology examinations revealed that some families suffer from a polysaccharide storage myopathy (PSSM). It is assumed that a gene expression signature related to PSSM should be observed at the transcriptional level because the glycogen storage disease could also be linked to other dysfunctions in gene regulation. Thus, the functional genomic approach could be conducted in order to provide new knowledge about the metabolic disorders related to PSSM. We propose exploring the PSSM muscle fiber metabolic disorders by measuring gene expression in relationship with the histological phenotype.Results: Genotypying analysis of GYS1 mutation revealed 2 homozygous (AA) and 5 heterozygous (GA) PSSM horses. In the PSSM muscles, histological data revealed PAS positive amylase resistant abnormal polysaccharides, inflammation, necrosis, and lipomatosis and active regeneration of fibers. Ultrastructural evaluation revealed a decrease of mitochondrial number and structural disorders. Extensive accumulation of an abnormal polysaccharide displaced and partially replaced mitochondria and myofibrils. The severity of the disease was higher in the two homozygous PSSM horses.Gene expression analysis revealed 129 genes significantly modulated (p < 0.05). The following genes were up-regulated over 2 fold: IL18, CTSS, LUM, CD44, FN1, GST01. The most down-regulated genes were the following: mitochondrial tRNA, SLC2A2, PRKCα, VEGFα. Data mining analysis showed that protein synthesis, apoptosis, cellular movement, growth and proliferation were the main cellular functions significantly associated with the modulated genes (p < 0.05). Several up-regulated genes, especially IL18, revealed a severe muscular inflammation in PSSM muscles. The up-regulation of glycogen synthase kinase-3 (GSK3β) under its active form could be responsible for glycogen synthase (GYS1) inhibition and hypoxia-inducible factor (HIF1α) destabilization.Conclusion: The main disorders observed in PSSM muscles could be related to mitochondrial dysfunctions, glycogenesis inhibition and the chronic hypoxia of the PSSM muscles. © 2009 Barrey et al; licensee BioMed Central Ltd.",,"Equidae; glycogen; polysaccharide; animal; animal disease; anoxia; article; female; gene expression profiling; gene expression regulation; genotype; horse; horse disease; inflammation; male; metabolism; mitochondrion; muscle disease; pathology; pathophysiology; phenotype; skeletal muscle; Animals; Anoxia; Female; Gene Expression Profiling; Gene Expression Regulation; Genotype; Glycogen; Horse Diseases; Horses; Inflammation; Male; Mitochondria; Muscle, Skeletal; Muscular Diseases; Phenotype; Polysaccharides"
"Barri A., Dooms A., Jansen B., Schelkens P.","A locally adaptive system for the fusion of objective quality measures","10.1109/TIP.2014.2316379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900411658&doi=10.1109%2fTIP.2014.2316379&partnerID=40&md5=48a8c334f1ff4870792fec941fa69585","Objective measures to automatically predict the perceptual quality of images or videos can reduce the time and cost requirements of end-to-end quality monitoring. For reliable quality predictions, these objective quality measures need to respond consistently with the behavior of the human visual system (HVS). In practice, many important HVS mechanisms are too complex to be modeled directly. Instead, they can be mimicked by machine learning systems, trained on subjective quality assessment databases, and applied on predefined objective quality measures for specific content or distortion classes. On the downside, machine learning systems are often difficult to interpret and may even contradict the input objective quality measures, leading to unreliable quality predictions. To address this problem, we developed an interpretable machine learning system for objective quality assessment, namely the locally adaptive fusion (LAF). This paper describes the LAF system and compares its performance with traditional machine learning. As it turns out, the LAF system is more consistent with the input measures and can better handle heteroscedastic training data. Copyright © 2014 IEEE.","machine learning; measure fusion; Objective quality assessment","Forecasting; Quality of service; Human visual systems; Measure fusion; Objective quality assessment; Objective quality measures; Perceptual quality; Quality monitoring; Quality prediction; Subjective quality assessments; Learning systems"
"Barria-Pineda J., Akhuseyinoglu K., Želem-Ćelap S., Brusilovsky P., Milicevic A.K., Ivanovic M.","Explainable Recommendations in a Personalized Programming Practice System","10.1007/978-3-030-78292-4_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126206406&doi=10.1007%2f978-3-030-78292-4_6&partnerID=40&md5=b8e302ce4bce4e627b9a585b595d83c2","This paper contributes to the research on explainable educational recommendations by investigating explainable recommendations in the context of personalized practice system for introductory Java programming. We present the design of two types of explanations to justify recommendation of next learning activity to practice. The value of these explainable recommendations was assessed in a semester-long classroom study. The paper analyses the observed impact of explainable recommendations on various aspects of student behavior and performance. © 2021, Springer Nature Switzerland AG.","Educational recommender systems; Explanations","Artificial intelligence; Computers; Recommender systems; Classroom study; Educational recommende system; Explanation; Java programming; Learning Activity; Paper analysis; Practice systems; Programming practices; Student performance; Students' behaviors; Computer programming"
"Barricelli B.R., Casiraghi E., Gliozzo J., Petrini A., Valtolina S.","Human Digital Twin for Fitness Management","10.1109/ACCESS.2020.2971576","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081154665&doi=10.1109%2fACCESS.2020.2971576&partnerID=40&md5=d106a044f974a5ba89e66c54553c937a","Our research work describes a team of human Digital Twins (DTs), each tracking fitness-related measurements describing an athlete's behavior in consecutive days (e.g. food income, activity, sleep). After collecting enough measurements, the DT firstly predicts the physical twin performance during training and, in case of non-optimal result, it suggests modifications in the athlete's behavior. The athlete's team is integrated into SmartFit, a software framework for supporting trainers and coaches in monitoring and manage athletes' fitness activity and results. Through IoT sensors embedded in wearable devices and applications for manual logging (e.g. mood, food income), SmartFit continuously captures measurements, initially treated as the dynamic data describing the current physical twins' status. Dynamic data allows adapting each DT's status and triggering the DT's predictions and suggestions. The analyzed measurements are stored as the historical data, further processed by the DT to update (increase) its knowledge and ability to provide reliable predictions. Results show that, thanks to the team of DTs, SmartFit computes trustable predictions of the physical twins' conditions and produces understandable suggestions which can be used by trainers to trigger optimization actions in the athletes' behavior. Though applied in the sport context, SmartFit can be easily adapted to other monitoring tasks. © 2013 IEEE.","Counterfactual explanations; Digital twins; Internet of Things; Machine learning; Smart health; Sociotechnical design; Wearables","Computer programming; Dynamics; Forecasting; Health; Internet of things; Learning systems; Sleep research; Sports; Wearable technology; Counterfactual explanations; Historical data; Monitoring tasks; Optimal results; Socio-technical designs; Software frameworks; Wearable devices; Wearables; Digital twin"
"Barrientos F., Sainz G.","Interpretable knowledge extraction from emergency call data based on fuzzy unsupervised decision tree","10.1016/j.knosys.2011.01.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052392939&doi=10.1016%2fj.knosys.2011.01.014&partnerID=40&md5=d0c1838a6ea66ae55bee81dedfe7ed65","Nowadays, call centers are common in different areas of activity providing customer services, medical attention, security services, etc. Each type of call center has its own particularities but all call centers have to plan the availability of resources at each time period to support the incoming calls. The emergency call centers are a special case with extra restrictions. In this context, this work is devoted to providing support for the decision making about resource planning of an emergency call center in order to reach its mandatory quality of service. This is carried out by the extraction of interpretable knowledge from the activity data collected by an emergency call center. A linguistic prediction, categorization and description of the days based on the call center activity and information permits the workload for each category of day to be known. This has been generated by a fuzzy version of an unsupervised decision tree (FUDT), merging decision trees and clustering. This involves quality indexes to reach an adequate trade-off between the tree complexity and the category quality in order to guarantee interpretability and performance. This unsupervised approach deals correctly with the real management of this type of centers generating and preserving expert knowledge. © 2011 Elsevier B.V. All rights reserved.","Decision support systems; Emergency call center; Fuzzy clustering; Fuzzy rules; Interpretability; Unsupervised decision trees","Artificial intelligence; Data mining; Decision support systems; Decision trees; Economic and social effects; Extraction; Fuzzy clustering; Fuzzy inference; Fuzzy rules; Linguistics; Quality of service; Trees (mathematics); Customer services; Emergency calls; Interpretability; Knowledge extraction; Resource planning; Security services; Unsupervised approaches; Unsupervised decision tree; Decision making"
"Barriga J.J.A., Yoo S.G.","Malware detection and evasion with machine learning techniques: A survey",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036655096&partnerID=40&md5=7fbdb8ef8f9363a2326f79fe560a7375","Malware has become a powerful and sophisticated tool used by malicious users to compromise and harm systems, and its evasion ability has improved considerably, getting to the point of becoming completely undetectable. On the other hand, machine learning has evolved tremendously in last years and it has become a standard in many IT solutions including the data processing field. Likewise, cryptography also has growth in popularity in providing confidentiality and integrity to important information. Even though those technologies are being widely used for trustable IT solutions, they also are used by malicious applications such as ransomware, which uses the cryptography as its infecting mechanism and the machine learning as its evasion technique. In this aspect, this paper makes a survey of existing researches regarding to malware detection and evasion by examining possible scenarios where malware could take advantage of machine learning and cryptography to improve its evasion techniques and infection impact. © Research India Publications.","Detection; Evasion; Machine learning malware; Malware; Obfuscation",
"Barron M.E., Thies A.B., Espinoza J.A., Barott K.L., Hamdoun A., Tresguerres M.","A vesicular Na+/Ca2+ exchanger in coral calcifying cells","10.1371/journal.pone.0205367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055768154&doi=10.1371%2fjournal.pone.0205367&partnerID=40&md5=3fb94a57d97e4b6c22b0ae5042fbeced","The calcium carbonate skeletons of corals provide the underlying structure of coral reefs; however, the cellular mechanisms responsible for coral calcification remain poorly understood. In osteoblasts from vertebrate animals, a Na+/Ca2+ exchanger (NCX) present in the plasma membrane transports Ca2+ to the site of bone formation. The aims of this study were to establish whether NCX exists in corals and its localization within coral cells, which are essential first steps to investigate its potential involvement in calcification. Data mining identified genes encoding for NCX proteins in multiple coral species, a subset of which were more closely related to NCXs from vertebrates (NCXA). We cloned NCXA from Acropora yongei (AyNCXA), which, unexpectedly, contained a peptide signal that targets proteins to vesicles from the secretory pathway. AyNCXA subcellular localization was confirmed by heterologous expression of fluorescently tagged AyNCXA protein in sea urchin embryos, which localized together with known markers of intracellular vesicles. Finally, immunolabeling of coral tissues with specific antibodies revealed AyNCXA was present throughout coral tissue. AyNCXA was especially abundant in calcifying cells, where it exhibited a subcellular localization pattern consistent with intracellular vesicles. Altogether, our results demonstrate AyNCXA is present in vesicles in coral calcifying cells, where potential functions include intracellular Ca2+ homeostasis and Ca2+ transport to the growing skeleton as part of an intracellular calcification mechanism. © 2018 Barron et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"isoprotein; NCX protein; unclassified drug; calcium carbonate; isoprotein; sodium calcium exchange protein; sodium-calcium exchanger 1; Acropora; Acropora yongei; animal cell; animal tissue; Article; calcification; calcium transport; cell function; cell vacuole; cellular distribution; controlled study; coral; embryo; fluorescence analysis; gene identification; genetic code; homeostasis; immune response; intracellular membrane; molecular cloning; nonhuman; osteoblast; protein expression; protein localization; protein targeting; sea urchin embryo; signal transduction; sodium calcium exchange; vertebrate; animal; Anthozoa; bone mineralization; classification; fluorescence microscopy; genetics; growth, development and aging; metabolism; nonmammalian embryo; phylogeny; sea urchin; transmission electron microscopy; Animals; Anthozoa; Calcification, Physiologic; Calcium Carbonate; Cloning, Molecular; Embryo, Nonmammalian; Microscopy, Electron, Transmission; Microscopy, Fluorescence; Phylogeny; Protein Isoforms; Sea Urchins; Sodium-Calcium Exchanger"
"Barros E., Boullenger B.","Quantitative conformance assessment in CO2 storage reservoirs under geological uncertainties using convolutional neural network classifiers","10.3997/2214-4609.202021062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101692409&doi=10.3997%2f2214-4609.202021062&partnerID=40&md5=d0a954d3cd19972ac853b2aa5b9aa3c3","Responsible use of geological reservoirs for storage purposes requires that operators demonstrate that their assets can be managed safely or, in other words, in conformance with the intended plan and targets. Smart monitoring plans provide sufficient evidence of the reservoir behavior to improve the understanding of the system and support decision making regarding subsequent development, operational and monitoring activities. In previous work we introduced a model-based workflow to objectively quantify the usefulness of monitoring within the context of conformance verification in CO2 storage under geological uncertainties, to support the design of effective monitoring strategies. Now we investigate the use of convolutional neural networks to render conformance classification more practical and swift within the workflow. The approach was applied to a case study based on a real storage aquifer and showed to be suitable for conformance classification based on time-series pressure measurements and 2D time-lapse images of the CO2 plume. The results obtained indicated that both types of data can, in time, provide sufficient evidence for accurately inferring the chances of future migration of CO2 to undesired areas of the reservoir. These promising results confirm the suitability of machine learning techniques to further improve workflows for quantitative conformance assessment under uncertainties. © Geoscience and Engineering in Energy Transition Conference, GET 2020.All right reserved.",,"Aquifers; Carbon dioxide; Convolution; Decision making; Digital storage; Geology; Learning systems; Conformance assessment; Geological reservoirs; Geological uncertainty; Machine learning techniques; Monitoring activities; Monitoring strategy; Smart monitoring; Time lapse images; Convolutional neural networks"
"Barros P., Tanevska A., Cruz F., Sciutti A.","Moody Learners-Explaining Competitive Behaviour of Reinforcement Learning Agents","10.1109/ICDL-EpiRob48136.2020.9278125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095521006&doi=10.1109%2fICDL-EpiRob48136.2020.9278125&partnerID=40&md5=409559e3f8ce32de6344116b5dc4ca80","Designing the decision-making processes of artificial agents that are involved in competitive interactions is a challenging task. In a competitive scenario, the agent does not only have a dynamic environment but also is directly affected by the opponents' actions. Observing the Q-values of the agent is usually a way of explaining its behavior, however, it does not show the temporal-relation between the selected actions. We address this problem by proposing the Moody framework that creates an intrinsic representation for each agent based on the Pleasure/Arousal model. We evaluate our model by performing a series of experiments using the competitive multiplayer Chef's Hat card game and discuss how by observing the intrinsic state generated by our model allows us to obtain a holistic representation of the competitive dynamics within the game. © ICDL-EpiRob 2020. All rights reserved.","Explainable artificial intelligence; Intrinsic confidence; Reinforcement learning","Agricultural robots; Decision making; Intelligent agents; Robotics; Artificial agents; Competitive dynamics; Competitive interactions; Decision making process; Dynamic environments; Multiplayers; Reinforcement learning agent; Temporal relation; Reinforcement learning"
"Barros R.C., Basgalupp M.P., De Carvalho A.C.P.L.F., Freitas A.A.","A hyper-heuristic evolutionary algorithm for automatically designing decision-tree algorithms","10.1145/2330163.2330335","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864679587&doi=10.1145%2f2330163.2330335&partnerID=40&md5=6d2aaf6aaf7dc26492c687858985aaf5","Decision tree induction is one of the most employed methods to extract knowledge from data, since the representation of knowledge is very intuitive and easily understandable by humans. The most successful strategy for inducing decision trees, the greedy top-down approach, has been continuously improved by researchers over the years. This work, following recent breakthroughs in the automatic design of machine learning algorithms, proposes a hyper-heuristic evolutionary algorithm for automatically generating decision-tree induction algorithms, named HEAD-DT. We perform extensive experiments in 20 public data sets to assess the performance of HEAD-DT, and we compare it to traditional decision-tree algorithms such as C4.5 and CART. Results show that HEAD-DT can generate algorithms that significantly outperform C4.5 and CART regarding predictive accuracy and F-Measure. © 2012 ACM.","decision trees; evolutionary algorithms; hyper-heuristics","Automatic design; Decision-tree algorithm; Decision-tree induction; F-measure; Hyper-heuristics; Hyperheuristic; Predictive accuracy; Public data; Top-down approach; Decision trees; Knowledge representation; Learning algorithms; Evolutionary algorithms"
"Barros R.C., Basgalupp M.P., De Carvalho A.C.P.L.F., Freitas A.A.","Towards the automatic design of decision tree induction algorithms","10.1145/2001858.2002050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051950481&doi=10.1145%2f2001858.2002050&partnerID=40&md5=ba74ba51f44ed5cc274fe8640275f6eb","Decision tree induction is one of the most employed methods to extract knowledge from data, since the representation of knowledge is very intuitive and easily understandable by humans. The most successful strategy for inducing decision trees, the greedy top-down approach, has been continuously improved by researchers over the years. This work, following recent breakthroughs in the automatic design of machine learning algorithms, proposes two different approaches for automatically generating generic decision tree induction algorithms. Both approaches are based on the evolutionary algorithms paradigm, which improves solutions based on metaphors of biological processes. We also propose guidelines to design interesting fitness functions for these evolutionary algorithms, which take into account the requirements and needs of the end-user. © 2011 ACM.","automatic design; decision tree induction; evolutionary algorithms","Automatic design; Biological process; Decision tree induction; End users; Fitness functions; Generic decisions; Top-down approach; Data mining; Decision trees; Design; Knowledge representation; Learning algorithms; Plant extracts; Trees (mathematics); Evolutionary algorithms"
"Barros R.C., Ruiz D.D., Basgalupp M.P.","Evolutionary model trees for handling continuous classes in machine learning","10.1016/j.ins.2010.11.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650537896&doi=10.1016%2fj.ins.2010.11.010&partnerID=40&md5=ed28ffb1f25133111b9f89f99786b57a","Model trees are a particular case of decision trees employed to solve regression problems. They have the advantage of presenting an interpretable output, helping the end-user to get more confidence in the prediction and providing the basis for the end-user to have new insight about the data, confirming or rejecting hypotheses previously formed. Moreover, model trees present an acceptable level of predictive performance in comparison to most techniques used for solving regression problems. Since generating the optimal model tree is an NP-Complete problem, traditional model tree induction algorithms make use of a greedy top-down divide-and-conquer strategy, which may not converge to the global optimal solution. In this paper, we propose a novel algorithm based on the use of the evolutionary algorithms paradigm as an alternate heuristic to generate model trees in order to improve the convergence to globally near-optimal solutions. We call our new approach evolutionary model tree induction (E-Motion). We test its predictive performance using public UCI data sets, and we compare the results to traditional greedy regression/model trees induction algorithms, as well as to other evolutionary approaches. Results show that our method presents a good trade-off between predictive performance and model comprehensibility, which may be crucial in many machine learning applications. © 2010 Elsevier Inc. All rights reserved.","Continuous classes; Evolutionary algorithms; Genetic programming; Machine learning; Model trees","Continuous classes; Data sets; Divide and conquer; End users; Evolutionary approach; Evolutionary models; Global optimal solutions; Induction algorithms; Machine learning applications; Machine-learning; Model trees; Near-optimal solutions; New approaches; Novel algorithm; NP complete problems; Optimal model; Predictive performance; Regression problem; Techniques used; Topdown; Computational complexity; Convergence of numerical methods; Decision trees; Genetic algorithms; Genetic programming; Heuristic algorithms; Learning systems; Optimal systems; Optimization; Problem solving; Learning algorithms"
"Barros R.C., Basgalupp M.P., Ruiz D.D., De Carvalho A.C.P.L.F., Freitas A.A.","Evolutionary model tree induction","10.1145/1774088.1774327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954692000&doi=10.1145%2f1774088.1774327&partnerID=40&md5=d50932a16cb99d9382bc483b8eb3ef15","Model trees are a particular case of decision trees employed to solve regression problems. They have the advantage of presenting an interpretable output with an acceptable level of predictive performance. Since generating optimal model trees is a NP-Complete problem, the traditional model tree induction algorithms make use of a greedy heuristic, which may not converge to the global optimal solution. We propose the use of the evolutionary algorithms paradigm (EA) as an alternate heuristic to generate model trees in order to improve the convergence to global optimal solutions. We test the predictive performance of this new approach using public UCI datasets, and compare the results with traditional greedy regression/model trees induction algorithms. © 2010 ACM.","data mining; evolutionary algorithms; model trees; multi-objective optimisation","Data sets; Evolutionary models; Global optimal solutions; Greedy heuristics; Induction algorithms; Model trees; Multiobjective optimisation; New approaches; NP complete problems; Optimal model; Predictive performance; Regression problem; Convergence of numerical methods; Data mining; Decision trees; Heuristic algorithms; Mathematical models; Multiobjective optimization; Optimal systems; Evolutionary algorithms"
"Barros T.M., Silva I., Guedes L.A.","Determination of dropout student profile based on correspondence analysis technique","10.1109/TLA.2019.8931146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076711513&doi=10.1109%2fTLA.2019.8931146&partnerID=40&md5=ce84b7fbbcebf6a1545e45310901a8f6","The purpose of this paper is to analize how the Correspondence Analysis Techinique can be used to enhance the Exploratory Data Analysis on the drop-out for a set of educational data with a majority of the categorical type. In order to do these analisys, the independence calculation was implemented from the chi-square test, and the heatmap and perceptual map were generated with the indexes on the socioeconomic data of students and academic performance in Portuguese and Mathematics subjects. As a result of this paper is presented the relations of attraction and repulsion between socioeconomic attributes and drop-out, and the profile of the student most vulnerable to evasion is drawn. Some characteristics are: students who have already failed at least once, who do not live with their parents, elementary school in public school, low level of education of the financial responsible. The studies were carry out using a real academic database of students of the secondary education with training in professional education through technical courses with duration of four years, in the face-to-face modality of the Federal Institute of Rio Grande do Norte (IFRN) in Brazil. © 2003-2012 IEEE.","correspondence analysis; data visualization; education data mining","Data mining; Data visualization; Drops; Statistical tests; Academic performance; Correspondence analysis; Elementary schools; Exploratory data analysis; Level of educations; Professional education; Socio-economic data; Student profiles; Students"
"Barrows B.A., Le Vie L.R., Ecker J.E., Allen B.D.","Analyzing natural language context in human-machine teaming using supervised machine learning","10.2514/6.2020-1113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091904743&doi=10.2514%2f6.2020-1113&partnerID=40&md5=bb8c0a926fb145296e0c1750ebe28b17","Building a foundation for trustworthiness and trust verification in multi-asset teaming is the research challenge of Autonomy Teaming and TRAjectories for Complex Trusted Operational Reliability (ATTRACTOR). The Design Reference Mission (DRM) for ATTRACTOR is a search and rescue mission objective governed by a multi-member team consisting of human and machine operators. A crucial component to the effort is the communication between humans and autonomous agents throughout both planning and execution stages of the mission. Intuitive communication methods and modalities are posited as critical enablers for certifying trust and trustworthiness. This paper reports on the data collection and analysis conducted in support of the Human Informed Natural-language GANs Evaluation (HINGE) project to attain explainable and trusted communication between human-machine assets. Two identically curated image description datasets were acquired for HINGE, both consisting of two unique input modalities (typed vs. verbal) and retrieved in two distinct contexts (general vs. specific). The gathered datasets were assessed and compared using Parts-of-Speech (POS) features, sentence similarity metrics, and linguistic analysis. Then, the datasets were modeled and tested separately and in combination with one another using machine learning algorithms. The comparison and testing results reveal a superior dataset, by which a preferred context and input is understood, for generating image representations of missing persons using a Generative Adversarial Network (GAN). © 2020, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.",,"Autonomous agents; Aviation; Hinges; Learning systems; Statistical tests; Supervised learning; Syntactics; Adversarial networks; Design reference missions; Image representations; Operational reliability; Planning and execution; Supervised machine learning; Trust and trustworthiness; Trusted communications; Learning algorithms"
"Barszcz T., Zabaryłło M.","AUTOMATIC IDENTIFICATION OF MALFUNCTIONS OF LARGE TURBOMACHINERY DURING TRANSIENT STATES WITH GENETIC ALGORITHM OPTIMIZATION","10.24425/mms.2022.138551","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127546357&doi=10.24425%2fmms.2022.138551&partnerID=40&md5=fe0fb5818b5ed0deb4e3b13b9c18f639","Turbines and generators operating in the power generation industry are a major source of electrical energy worldwide. These are critical machines and their malfunctions should be detected in advance in order to avoid catastrophic failures and unplanned shutdowns. A maintenance strategy which enables to detect malfunctions at early stages of their existence plays a crucial role in facilities using such types of machinery. The best source of data applied for assessment of the technical condition are the transient data measured during start-ups and coast-downs. Most of the proposed methods using signal decomposition are applied to small machines with a rolling element bearing in steady-state operation with a shaft considered as a rigid body. The machines examined in the authors’ research operate above their first critical rotational speed interval and thus their shafts are considered to be flexible and are equipped with a hydrodynamic sliding bearing. Such an arrangement introduces significant complexity to the analysis of the machine behavior, and consequently, analyzing such data requires a highly skilled human expert. The main novelty proposed in the paper is the decomposition of transient vibration data into components responsible for particular failure modes. The method is automated and can be used for identification of turbogenerator malfunctions. Each parameter of a particular decomposed function has its physical representation and can help the maintenance staff to operate the machine properly. The parameters can also be used by the managing personnel to plan overhauls more precisely. The method has been validated on real-life data originating from a 200 MW class turbine. The real-life field data, along with the data generated by means of the commercial software utilized in GE’s engineering department for this particular class of machines, was used as the reference data set for an unbalanced response during the transients in question. © 2022 Polish Academy of Sciences. All rights reserved.","fault detection; genetic algorithm; machine learning; transient; turbine generator","Automation; Fault detection; Machine learning; Plant shutdowns; Roller bearings; Signal processing; Turbogenerators; Automatic identification; Catastrophic failures; Critical machine; Electrical energy; Faults detection; Genetic-algorithm optimizations; Maintenance strategies; Power generation industries; Transient state; Unplanned shutdowns; Genetic algorithms"
"Barth D., Cohen-Boulakia B., Ehounou W.","Distributed Reinforcement Learning for the Management of a Smart Grid Interconnecting Independent Prosumers","10.3390/en15041440","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124908250&doi=10.3390%2fen15041440&partnerID=40&md5=39c7ecfd0dc091a41077c20c005e537a","In the context of an eco-responsible production and distribution of electrical energy at the local scale of an urban territory, we consider a smart grid as a system interconnecting different prosumers, which all retain their decision-making autonomy and defend their own interests in a comprehensive system where the rules, accepted by all, encourage virtuous behavior. In this paper, we present and analyze a model and a management method for smart grids that is shared between different kinds of independent actors, who respect their own interests, and that encourages each actor to behavior that allows, as much as possible, an energy independence of the smart grid from external energy suppliers. We consider here a game theory model, in which each actor of the smart grid is a player, and we investigate distributed machine-learning algorithms to allow decision-making, thus, leading the game to converge to stable situations, in particular to a Nash equilibrium. We propose a Linear Reward Inaction algorithm that achieves Nash equilibria most of the time, both for a single time slot and across time, allowing the smart grid to maximize its energy independence from external energy suppliers. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Energy management; Energy optimization; Game theory; Nash equilibria; Reinforcement learning; Smart grid","Decision making; Decision theory; Electric power transmission networks; Learning algorithms; Reinforcement learning; Smart power grids; Distribution of electrical energy; Energy independence; Energy optimization; Energy suppliers; External energy; Local scale; Nash equilibria; Reinforcement learnings; Smart grid; Urban territories; Game theory"
"Barth S.W., Norton S.W.","Knowledge Engineering within a Generalized Bayesian Framework","10.1016/B978-0-444-70396-5.50015-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012642573&doi=10.1016%2fB978-0-444-70396-5.50015-7&partnerID=40&md5=673d64034c6119c3c041d9141aa92daa","During the ongoing debate over the representation of uncertainty in Artificial Intelligence, Cheeseman, Lemmer, Pearl, and others have argued that probability theory, and in particular the Bayesian theory, should be used as the basis for the inference mechansims of Expert Systems dealing with uncertainty. In order to pursue the issue in a practical setting, sophisticated tools for knowledge engineering are needed that allow flexible and understandable interaction with the underlying knowledge representation schemes. This paper describes a Generalized Bayesian framework for building expert systems which function in uncertain domains, using algorithms proposed by Lemmer. It is neither rule-based nor frame-based, and requires a new system of knowledge engineering tools. The framework we describe provides a knowledge-based system architecture with an inference engine, explanation capability, and a unique aid for building consistent knowledge bases. © 1988, Elsevier B.V.",,
"Barthélemy M., Elie N., Pellissier L., Wolfender J.-L., Stien D., Touboul D., Eparvier V.","Structural identification of antibacterial lipids from Amazonian palm tree endophytes through the molecular network approach","10.3390/ijms20082006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065420723&doi=10.3390%2fijms20082006&partnerID=40&md5=bc39b15c8b78510271984a77ee26d208","A library of 197 endophytic fungi and bacteria isolated from the Amazonian palm tree Astrocaryum sciophilum was extracted and screened for antibacterial activity against methicillin-resistant Staphylococcus aureus (MRSA). Four out of five antibacterial ethyl acetate extracts were also cytotoxic for the MRC-5 cells line. Liquid chromatography coupled to tandem mass spectrometry (UPHLC-HRMS/MS) analyses combined with molecular networking data processing were carried out to allow the identification of depsipeptides and cyclopeptides responsible for the cytotoxicity in the dataset. Specific ion clusters from the active Luteibacter sp. extract were also highlighted using an MRSA activity filter. A chemical study of Luteibacter sp. was conducted leading to the structural characterization of eight fatty acid exhibiting antimicrobial activity against MRSA in the tens of µg/mL range. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Antibacterial; Astrocaryum sciophilum; Cytotoxicity; Endophytes; Fatty acids; Molecular networking","13 methyltetradecanoic acid; 15 methyl 9z hexadecenoic acid; 2 hydroxy 13 methyltetradecanoic acid; 3 hydroxy 13 methyltetradecanoic acid; 3 hydroxy 14 methylpentadecanoic acid; 3 hydroxy 15 methylhexadecanoic acid; 9z hexadecenoic acid; antiinfective agent; beta hydroxypalmitic acid; docetaxel; lipid; unclassified drug; antiinfective agent; depsipeptide; fatty acid; lipid; antibacterial activity; Arecaceae; Article; Astrocaryum sciophilum; atmospheric pressure chemical ionization mass spectrometry; bacterium isolation; carbon nuclear magnetic resonance; controlled study; cytotoxicity; drug synthesis; electrospray; endophyte; endophytic bacteria; endophytic fungus; Gram negative bacterium; human; human cell; ion chromatography; liquid chromatography-mass spectrometry; Luteibacter; machine learning; methicillin resistant Staphylococcus aureus; minimum inhibitory concentration; molecular network; MRC-5 cell line; nonhuman; protein analysis; protein structure; proton nuclear magnetic resonance; ultra performance liquid chromatography; Arecaceae; chemistry; drug effect; Gammaproteobacteria; high performance liquid chromatography; microbial sensitivity test; microbiology; molecular model; Staphylococcus infection; tandem mass spectrometry; tree; Anti-Bacterial Agents; Arecaceae; Chromatography, High Pressure Liquid; Depsipeptides; Endophytes; Fatty Acids; Gammaproteobacteria; Humans; Lipids; Methicillin-Resistant Staphylococcus aureus; Microbial Sensitivity Tests; Models, Molecular; Staphylococcal Infections; Tandem Mass Spectrometry; Trees"
"Bartl A., Spanakis G.","A retrieval-based dialogue system utilizing utterance and context embeddings","10.1109/ICMLA.2017.00011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048494731&doi=10.1109%2fICMLA.2017.00011&partnerID=40&md5=8c58d69c1ae6a9fe50be702a2378e9d7","Finding semantically rich and computer-understandable representations for textual dialogues, utterances and words is crucial for dialogue systems (or conversational agents), as their performance mostly depends on understanding the context of conversations. In recent research approaches, responses have been generated utilizing a decoder architecture, given the distributed vector representation (embedding) of the current conversation. In this paper, the utilization of embeddings for answer retrieval is explored by using Locality-Sensitive Hashing Forest (LSH Forest), an Approximate Nearest Neighbor (ANN) model, to find similar conversations in a corpus and rank possible candidates. Experimental results on the well-known Ubuntu Corpus (in English) and a customer service chat dataset (in Dutch) show that, in combination with a candidate selection method, retrieval-based approaches outperform generative ones and reveal promising future research directions towards the usability of such a system. © 2017 IEEE.","Deep Learning; Dialogue Systems; Information Retrieval","Deep learning; Embeddings; Forestry; Information retrieval; Machine learning; Nearest neighbor search; Speech processing; Approximate nearest neighbors (ANN); Candidate selection; Conversational agents; Decoder architecture; Dialogue systems; Future research directions; Locality sensitive hashing; Vector representations; Search engines"
"Bartler A., Hinderer D., Yang B.","Grad-LAM: Visualization of deep neural networks for unsupervised learning","10.23919/Eusipco47968.2020.9287730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099287494&doi=10.23919%2fEusipco47968.2020.9287730&partnerID=40&md5=4f894e08f943704df7975c032886ab74","Nowadays, the explainability of deep neural networks is an essential part of machine learning. In the last years, many methods were developed to visualize important regions of an input image for the decision of the deep neural network. Since almost all methods are designed for supervised trained models, we propose in this work a visualization technique for unsupervised trained autoencoders called Gradient-weighted Latent Activation Mapping (Grad-LAM). We adapt the idea of Grad-CAM and propose a novel weighting based on the knowledge of the autoencoder's decoder. Our method will help to get insights into the highly nonlinear mapping of an input image to a latent space. We show that the visualization maps of Grad-LAM are meaningful on simple datasets like MNIST and the method is even applicable to real-world datasets like ImageNet. © 2021 European Signal Processing Conference, EUSIPCO. All rights reserved.","Deep visualization; Explainable artificial intelligence; Grad-LAM; Transparency; Unsupervised learning","Deep learning; Deep neural networks; Learning systems; Mapping; Signal processing; Visualization; Activation mapping; Autoencoders; Input image; Nonlinear mappings; Real-world datasets; Visualization technique; Neural networks"
"Bartocci E., Deshmukh J., Gigler F., Mateis C., Nickovic D., Qin X.","Mining Shape Expressions from Positive Examples","10.1109/TCAD.2020.3012240","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096039786&doi=10.1109%2fTCAD.2020.3012240&partnerID=40&md5=46fc2db40f6504fe5f7fa19071586ab6","Shape expressions (SEs) is a novel specification language that was recently introduced to express behavioral patterns over real-valued signals observed during the execution of cyber-physical systems. An SE is a regular expression composed of arbitrary parameterized shapes, such as lines, exponential curves, and sinusoids as atomic symbols with symbolic constraints on the shape parameters. SEs enable a natural and intuitive specification of complex temporal patterns over possibly noisy data. In this article, we propose a novel method for mining a broad and interesting fragment of SEs from time-series data using a combination of techniques from linear regression, unsupervised clustering, and learning finite automata from positive examples. The learned SE for a given dataset provides an explainable and intuitive model of the observed system behavior. We demonstrate the applicability of our approach on two case studies from different application domains and experimentally evaluate the implemented specification mining procedure. © 1982-2012 IEEE.","Computational and artificial intelligence; computer science; computers and information processing; data mining; formal languages; learning automata; learning systems; pattern recognition","Embedded systems; Specification languages; Behavioral patterns; Exponential curves; Intuitive modeling; Positive examples; Regular expressions; Shape parameters; Specification mining; Unsupervised clustering; Specifications"
"Bartoszewicz J.M., Seidel A., Renard B.Y.","Interpretable detection of novel human viruses from genome sequencing data","10.1093/nargab/lqab004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123224512&doi=10.1093%2fnargab%2flqab004&partnerID=40&md5=9db1890c6db29713f58f8a890650d2e1","Viruses evolve extremely quickly, so reliable methods for viral host prediction are necessary to safeguard biosecurity and biosafety alike. Novel human-infecting viruses are difficult to detect with standard bioinformatics workflows. Here, we predict whether a virus can infect humans directly from next-generation sequencing reads. We show that deep neural architectures significantly outperform both shallow machine learning and standard, homology-based algorithms, cutting the error rates in half and generalizing to taxonomic units distant from those presented during training. Further, we develop a suite of interpretability tools and show that it can be applied also to other models beyond the host prediction task. We propose a new approach for convolutional filter visualization to disentangle the information content of each nucleotide from its contribution to the final classification decision. Nucleotide-resolution maps of the learned associations between pathogen genomes and the infectious phenotype can be used to detect regions of interest in novel agents, for example, the SARS-CoV-2 coronavirus, unknown before it caused a COVID-19 pandemic in 2020. All methods presented here are implemented as easy-to-install packages not only enabling analysis of NGS datasets without requiring any deep learning skills, but also allowing advanced users to easily train and explain new models for genomics. © 2021 The Author(s).",,"nucleotide; algorithm; Article; benchmarking; bioinformatics; controlled study; convolutional neural network; coronavirus disease 2019; deep learning; deep neural network; DNA sequence; gene sequence; high throughput sequencing; host range; human; nonhuman; nucleotide sequence; pathogenicity; phenotype; position weight matrix; prediction; priority journal; RNA virus; SARS-related coronavirus; taxonomy; virus detection; virus genome"
"Baru C.","Data in the 21st Century","10.1007/978-3-319-70942-0_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037859363&doi=10.1007%2f978-3-319-70942-0_1&partnerID=40&md5=f67b1c78548e1f42c86251e8f62b391d","The past couple of decades have witnessed exponential growth in data, due to the penetration of information technology across all aspects of science and society; the increasing ease with which we are able to collect more data; and the growth of Internet-scale, planet-wide Web-based and mobile services—leading to the notion of “big data”. While the emphasis so far has been on developing technologies to manage the volume, velocity, and variety of the data, and to exploit available data assets via machine learning techniques, going forward the emphasis must also be on translational data science and the responsible use of all of these data in real-world applications. Data science in the 21st century must provide trust in the data and provide responsible and trustworthy techniques and systems by supporting the notions of transparency, interpretability, and reproducibility. The future offers exciting opportunities for transdisciplinary research and convergence among disciplines—computer science, statistics, mathematics, and the full range of disciplines that impact all aspects of society. Econometrics and economics can find an important role in this convergence of ideas. © Springer International Publishing AG 2018.","Convergence; Data science; Responsible data management; Translational data science; Trustworthiness",
"Bârzan H., Ichim A.-M., Moca V.V., Mureşan R.C.","Time-Frequency Representations of Brain Oscillations: Which One Is Better?","10.3389/fninf.2022.871904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129236618&doi=10.3389%2ffninf.2022.871904&partnerID=40&md5=10c709e9bb25d6e3a835e59dbc322a3b","Brain oscillations are thought to subserve important functions by organizing the dynamical landscape of neural circuits. The expression of such oscillations in neural signals is usually evaluated using time-frequency representations (TFR), which resolve oscillatory processes in both time and frequency. While a vast number of methods exist to compute TFRs, there is often no objective criterion to decide which one is better. In feature-rich data, such as that recorded from the brain, sources of noise and unrelated processes abound and contaminate results. The impact of these distractor sources is especially problematic, such that TFRs that are more robust to contaminants are expected to provide more useful representations. In addition, the minutiae of the techniques themselves impart better or worse time and frequency resolutions, which also influence the usefulness of the TFRs. Here, we introduce a methodology to evaluate the “quality” of TFRs of neural signals by quantifying how much information they retain about the experimental condition during visual stimulation and recognition tasks, in mice and humans, respectively. We used machine learning to discriminate between various experimental conditions based on TFRs computed with different methods. We found that various methods provide more or less informative TFRs depending on the characteristics of the data. In general, however, more advanced techniques, such as the superlet transform, seem to provide better results for complex time-frequency landscapes, such as those extracted from electroencephalography signals. Finally, we introduce a method based on feature perturbation that is able to quantify how much time-frequency components contribute to the correct discrimination among experimental conditions. The methodology introduced in the present study may be extended to other analyses of neural data, enabling the discovery of data features that are modulated by the experimental manipulation. Copyright © 2022 Bârzan, Ichim, Moca and Mureşan.","electroencephalography; explainable AI; machine learning; neural oscillations; neurophysiology; time-frequency representation","animal experiment; article; brain; controlled study; electroencephalography; human; machine learning; male; mouse; neurophysiology; noise; nonhuman; oscillation; quantitative analysis; visual stimulation"
"Bar-Zur R., Abu-Hanna A., Eyal I., Tamar A.","WeRLman: To Tackle Whale (Transactions), Go Deep (RL)","10.1145/3534056.3535005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133371260&doi=10.1145%2f3534056.3535005&partnerID=40&md5=e9caef31a492db70c840a448b502eb8c","Blockchain technology is responsible for the emergence of cryptocurrencies, such as Bitcoin and Ethereum. The security of a blockchain protocol relies on the incentives of its participants. Selfish mining is a form of deviation from the protocol where a participant can gain more than her fair share. Previous analyses of selfish mining make easing, non-realistic assumptions. We introduce a more realistic model with varying block rewards in the form of transaction fees. However, this comes at the cost of an intractable state space. To solve the complex model, we introduce WeRLman, a novel method based on deep Reinforcement Learning (deep RL). Using WeRLman, we show reward variability can significantly hurt blockchain security. © 2022 Owner/Author.","bitcoin; blockchain; deep Q networks; deep reinforcement learning; fees; security; selfish mining; transaction fees","Bitcoin; Blockchain; Deep learning; Learning systems; Network security; Block-chain; Deep Q network; Deep reinforcement learning; Fair share; Fee; Realistic model; Reinforcement learnings; Security; Selfish mining; Transaction fee; Reinforcement learning"
"Bas E., Egrioglu E., Karahasan O.","A Pi-Sigma artificial neural network based on sine cosine optimization algorithm","10.1007/s41066-021-00297-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117964140&doi=10.1007%2fs41066-021-00297-9&partnerID=40&md5=5ffa034d4e8e6d8ee900490aebc99763","Pi-Sigma artificial neural network, which is a special artificial neural network model, is an artificial neural network that can be thought as a combination of different types of neuron models. Since Pi-Sigma artificial neural networks contain both additive and multiplicative structures, it can also be expressed as an artificial neural network model in which both multiplicative neuron model and perceptron are used together. One of the most important success criteria of the Pi-Sigma artificial neural networks is the optimization algorithm used in the training of the network, as in many artificial neural networks. The optimization algorithms used in the training of the network are divided into two parts as derivative-based algorithms and artificial intelligence optimization method-based algorithms. The artificial intelligence optimization algorithms have been used frequently in recent years compared with derivative-based algorithms due to many advantages. As known, one of the success criteria of an artificial neural network model is its training algorithm. Although many optimization algorithms are used in the training of Pi-Sigma artificial neural networks in the literature, the sine cosine algorithm, which is simpler, understandable, and easy to use, has not yet been used compared with many artificial intelligence optimization algorithms. In this study, the sine cosine algorithm is used for the first time in the training of Pi-Sigma artificial neural networks. The motivation of the paper is the about the evaluation of sine cosine algorithm performance in the training of Pi-Sigma artificial neural networks. The reason for the preference of using sine cosine algorithm is that the algorithm has not some specific operators that many artificial optimizations have and it uses the advantages of sine cosine functions easily. The performance of the proposed method is compared with many methods that are frequently used in the forecasting literature, especially many artificial neural networks models that use different optimization algorithms in the training process. Some popular time series, which are frequently used in the forecasting literature, are used in the analysis process, and as a result of the analysis process, it is concluded that the proposed method has better performance than other methods. © 2021, The Author(s), under exclusive licence to Springer Nature Switzerland AG.","Forecasting; Optimization; Pi-Sigma artificial neural networks; Sine cosine algorithm","Cosine transforms; Neural networks; Optimization; Time series analysis; Analysis process; Artificial neural network modeling; Multiplicative neuron model; Network-based; Neuron modeling; Optimisations; Optimization algorithms; Performance; Pi-sigma artificial neural network; Sine-cosine algorithm; Forecasting"
"Bas J., Zou Z., Cirillo C.","An interpretable machine learning approach to understanding the impacts of attitudinal and ridesourcing factors on electric vehicle adoption","10.1080/19427867.2021.2009098","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121120433&doi=10.1080%2f19427867.2021.2009098&partnerID=40&md5=a73bfb2515040583cef65d00c75087ec","The global electric vehicle (EV) market has been experiencing an impressive growth in recent times. Understanding consumer preferences on this cleaner, more eco-friendly mobility option could help guide public policy toward accelerating EV adoption and sustainable transportation systems. Previous studies suggest the strong influence of individual and external factors on EV adoption decisions. In this study, we apply machine learning techniques on EV stated preference survey data to predict EV adoption using attitudinal factors, ridesourcing factors (e.g., frequency of Uber/Lyft rides), as well as underlying sociodemographic and vehicle factors. To overcome machine learning models’ low interpretability, we adopt the innovative Local Interpretable Model-Agnostic Explanations (LIME) method to elaborate each factor’s contribution to the predicting outcomes. Besides what was found in previous EV preference literature, we find that the frequent usage of ridesourcing, knowledge about EVs, and awareness of environmental protection are important factors in explaining high willingness of adopting EVs. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","attitudes; Electric vehicles; local interpretable model-agnostic explanations (lime); machine learning; ridesourcing",
"Başağaoğlu H., Chakraborty D., Do Lago C., Gutierrez L., Şahinli M.A., Giacomoni M., Furl C., Mirchi A., Moriasi D., Şengör S.S.","A Review on Interpretable and Explainable Artificial Intelligence in Hydroclimatic Applications","10.3390/w14081230","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128853137&doi=10.3390%2fw14081230&partnerID=40&md5=a5cd1381c022db28fbd7931d2375db07","This review focuses on the use of Interpretable Artificial Intelligence (IAI) and eXplainable Artificial Intelligence (XAI) models for data imputations and numerical or categorical hydroclimatic predictions from nonlinearly combined multidimensional predictors. The AI models considered in this paper involve Extreme Gradient Boosting, Light Gradient Boosting, Categorical Boosting, Extremely Randomized Trees, and Random Forest. These AI models can transform into XAI models when they are coupled with the explanatory methods such as the Shapley additive explanations and local interpretable model-agnostic explanations. The review highlights that the IAI models are capable of unveiling the rationale behind the predictions while XAI models are capable of discovering new knowledge and justifying AI-based results, which are critical for enhanced accountability of AI-driven predictions. The review also elaborates the importance of domain knowledge and interventional IAI modeling, potential advantages and disadvantages of hybrid IAI and non-IAI predictive modeling, unequivocal importance of balanced data in categorical decisions, and the choice and performance of IAI versus physics-based modeling. The review concludes with a proposed XAI framework to enhance the interpretability and explainability of AI models for hydroclimatic applications. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","explainable artificial intelligence; explanatory methods; hydroclimatic applications; multidimensional data; nonlinearity","Decision trees; Domain Knowledge; Data imputation; Explainable artificial intelligence; Explanatory method; Gradient boosting; Hydroclimatic; Hydroclimatic application; Intelligence models; Light gradients; Multidimensional data; Nonlinearity; Forecasting; additive; artificial intelligence; data set; hydrometeorology; nonlinearity"
"Basaj D., Oleszkiewicz W., Sieradzki I., Górszczak M., Rychalska B., Trzcinski T., Zielinski B.","Explaining Self-Supervised Image Representations with Visual Probing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124176278&partnerID=40&md5=9335a6170b1f53c760bfadf8a78eae47","Recently introduced self-supervised methods for image representation learning provide on par or superior results to their fully supervised competitors, yet the corresponding efforts to explain the self-supervised approaches lag behind. Motivated by this observation, we introduce a novel visual probing framework for explaining the self-supervised models by leveraging probing tasks employed previously in natural language processing. The probing tasks require knowledge about semantic relationships between image parts. Hence, we propose a systematic approach to obtain analogs of natural language in vision, such as visual words, context, and taxonomy. We show the effectiveness and applicability of those analogs in the context of explaining self-supervised representations. Our key findings emphasize that relations between language and vision can serve as an effective yet intuitive tool for discovering how machine learning models work, independently of data modality. Our work opens a plethora of research pathways towards more explainable and transparent AI. © 2021 International Joint Conferences on Artificial Intelligence. All rights reserved.",,"Image representation; Learning algorithms; Natural language processing systems; Visual languages; Image representations; Machine learning models; Natural languages; Semantic relationships; Supervised methods; Visual word; Word contexts; Semantics"
"Basak A., Schmidt K.M., Mengshoel O.J.","From data to interpretable models: machine learning for soil moisture forecasting","10.1007/s41060-022-00347-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137235392&doi=10.1007%2fs41060-022-00347-8&partnerID=40&md5=40051dd54a36b5d5122e5887b7cd4ec1","Soil moisture is critical to agricultural business, ecosystem health, and certain hydrologically driven natural disasters. Monitoring data, though, is prone to instrumental noise, wide ranging extrema, and nonstationary response to rainfall where ground conditions change. Furthermore, existing soil moisture models generally forecast poorly for time periods greater than a few hours. To improve such forecasts, we introduce two data-driven models, the Naive Accumulative Representation (NAR) and the Additive Exponential Accumulative Representation (AEAR). Both of these models are rooted in deterministic, physically based hydrology, and we study their capabilities in forecasting soil moisture over time periods longer than a few hours. Learned model parameters represent the physically based unsaturated hydrological redistribution processes of gravity and suction. We validate our models using soil moisture and rainfall time series data collected from a steep gradient, post-wildfire site in southern California. Data analysis is complicated by rapid landscape change observed in steep, burned hillslopes in response to even small to moderate rain events. The proposed NAR and AEAR models are, in forecasting experiments, shown to be competitive with several established and state-of-the-art baselines. The AEAR model fits the data well for three distinct soil textures at variable depths below the ground surface (5, 15, and 30 cm). Similar robust results are demonstrated in controlled, laboratory-based experiments. Our AEAR model includes readily interpretable hydrologic parameters and provides more accurate forecasts than existing models for time horizons of 10–24 h. Such extended periods of warning for natural disasters, such as floods and landslides, provide actionable knowledge to reduce loss of life and property. © 2022, The Author(s).","Data analysis; Interpretable machine learning; Model optimization and fitting; Monitoring; Post-fire landslides; Soil moisture forecasting","Data handling; Disasters; Ecosystems; Forecasting; Information analysis; Landslides; Machine learning; Moisture control; Rain; Textures; Exponentials; Interpretable machine learning; Machine-learning; Model fitting; Model optimization; Optimization and fittings; Post-fire; Post-fire landslide; Representation model; Soil moisture forecasting; Soil moisture"
"Basak S.C., Vracko M., Witzmann F.A.","Mathematical nanotoxicoproteomics: Quantitative characterization of effects of multi-walled carbon nanotubes (MWCNT) and TiO2 nanobelts (TiO2-nb) on protein expression patterns in human intestinal cells","10.2174/1573409912666160824145722","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995917647&doi=10.2174%2f1573409912666160824145722&partnerID=40&md5=32b2ddd6b146d6defa97ec515d1386f9","Background: Various applications of nanosubstances in industrial and consumer goods sectors are growing rapidly because of their useful chemical and physical properties. Objectives: Assessment of hazard posed by exposure to nanosubstances is essential for the protection of human and ecological health. Methods: We analyzed the proteomics patterns of Caco-2/HT29-MTX cells in co-culture exposed for three and twenty four hours to two kinds of nanoparticles: multi-walled carbon nanotubes (MWCNT) and TiO2 nanobelts (TiO2-NB). For each nanosubstance cells were exposed to two concentrations of the material before carrying out proteomics analyses: 10 μg and 100 μg. In each case over 3000 proteins were identified. A mathematically based similarity index, which measures the changes in abundances of cellular proteins that are highly affected by exposure to the nanosubstances, was used to characterize toxic effects of the nanomaterials. Results: We identified 8 and 25 proteins, which are most highly affected by MWCNT and TiO2-NB, respectively. These proteins may be responsible for specific response of cells to the nanoparticles. Further 14 reported proteins are affected by either of the two nanoparticles and they are probably related to nonspecific toxic response of the cells. Conclusion: The similarity methods proposed in this paper may be useful in the management and visualization of the large amount of data generated by proteomics technologies. © 2016 Bentham Science Publishers.","Aspect ratio; Big data; Caco cell; Multi-walled carbon nanotubes (MWCNT); Optical devices; Photocatalyst; Proteomics; Superconductor materials; TiO2 nanobelts (TiO2-NB)","Aspect ratio; Big data; Cells; Cytology; Data visualization; Molecular biology; Multiwalled carbon nanotubes (MWCN); Nanobelts; Nanoparticles; Proteins; Yarn; Aspect-ratio; Caco cell; Intestinal cells; Multi-walled carbon nanotube; Multi-walled-carbon-nanotubes; Protein expression patterns; Proteomics; Quantitative characterization; Superconductor materials; TiO2 nanobelt; Titanium dioxide; multi walled nanotube; nanoribbon; titanium dioxide nanoparticle; transcriptome; biological marker; carbon nanotube; nanoparticle; protein; titanium; titanium dioxide; Article; controlled study; health hazard; human; human cell; intestine cell; mass spectrometry; mathematical analysis; metabolomics; nanotoxicology; protein expression; proteomics; toxicity; biological model; Caco-2 cell line; colon; data mining; drug effects; HT-29 cell line; information science; metabolism; nanotechnology; procedures; protein database; risk assessment; time factor; toxicology; Biomarkers; Caco-2 Cells; Colon; Data Mining; Databases, Protein; HT29 Cells; Humans; Informatics; Models, Biological; Nanoparticles; Nanotechnology; Nanotubes, Carbon; Proteins; Proteomics; Risk Assessment; Time Factors; Titanium; Toxicology"
"Basant N., Gupta S., Singh K.P.","A three-tier QSAR modeling strategy for estimating eye irritation potential of diverse chemicals in rabbit for regulatory purposes","10.1016/j.yrtph.2016.03.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963680052&doi=10.1016%2fj.yrtph.2016.03.014&partnerID=40&md5=2fed2702769bb4dd8afc7964eecceb35","Experimental determination of the eye irritation potential (EIP) of chemicals is not only tedious, time and resource intensive, it involves cruelty to test animals. In this study, we have established a three-tier QSAR modeling strategy for estimating the EIP of chemicals for the use of pharmaceutical industry and regulatory agencies. Accordingly, a qualitative (binary classification: irritating, non-irritating), semi-quantitative (four-category classification), and quantitative (regression) QSAR models employing the SDT, DTF, and DTB methods were developed for predicting the EIP of chemicals in accordance with the OECD guidelines. Structural features of chemicals responsible for eye irritation were extracted and used in QSAR analysis. The external predictive power of the developed QSAR models were evaluated through the internal and external validation procedures recommended in QSAR literature. In test data, the two and four category classification QSAR models (DTF, DTB) rendered accuracy of &gt;93%, while the regression QSAR models (DTF, DTB) yielded correlation (R2) of &gt;0.92 between the measured and predicted EIPs. Values of various statistical validation coefficients derived for the test data were above their respective threshold limits (except rm2 in DTF), thus put a high confidence in this analysis. The applicability domain of the constructed QSAR models were defined using the descriptors range and leverage approaches. The QSAR models in this study performed better than any of the previous studies. The results suggest that the developed QSAR models can reliably predict the EIP of diverse chemicals and can be useful tools for screening of candidate molecules in the drug development process. © 2016 Elsevier Inc..","Eye irritation potential; Qualitative and semi-quantitative QSAR; Regression QSAR; Structural diversity; Structural features; Three-tier strategy","acetic acid derivative; acrylic acid derivative; alcohol derivative; aldehyde; aromatic compound; bromine derivative; ester derivative; ether derivative; fatty acid; halide; heterocyclic compound; hydrocarbon; inorganic compound; ketone; methacrylic acid derivative; nitrile; organophosphate; pesticide; soap; sulfur derivative; surfactant; toxic substance; triacylglycerol; irritant agent; Article; chemical structure; eye irritation; measurement accuracy; nonhuman; prediction; priority journal; qualitative analysis; quantitative analysis; quantitative structure activity relation; rabbit; validation process; animal; animal testing alternative; chemically induced; chemistry; classification; data mining; eye disease; factual database; molecular model; procedures; regression analysis; risk assessment; statistical model; toxicity testing; Animal Testing Alternatives; Animals; Data Mining; Databases, Factual; Eye Diseases; Irritants; Models, Molecular; Models, Statistical; Molecular Structure; Quantitative Structure-Activity Relationship; Rabbits; Regression Analysis; Risk Assessment; Toxicity Tests"
"Basant N., Gupta S., Singh K.P.","QSAR modeling for predicting reproductive toxicity of chemicals in rats for regulatory purposes","10.1039/c6tx00083e","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976470537&doi=10.1039%2fc6tx00083e&partnerID=40&md5=541ec002aeea41ce08d17c56c93031be","The experimental determination of multi-generation reproductive toxicity of chemicals involves high costs and a large number of animal studies over a long period of time. Computational toxicology offers possibilities to overcome such difficulties. In this study, we have established ensemble machine learning (EML) based quantitative structure-activity relationship models for predicting the reproductive toxicity potential (LOAEL) of structurally diverse chemicals in accordance with the OECD guidelines. Accordingly, decision tree forest (DTF) and decision tree boost (DTB) QSAR models were developed using a novel dataset composed of the toxicity endpoints for 334 chemicals. Relevant structural features of chemicals responsible for toxicity potential were identified and used in QSAR modeling. The generalization and prediction abilities of the constructed QSAR models were evaluated by internal and external validation procedures and by deriving several stringent statistical criteria parameters. In the test set, the two models (DTF and DTB) yielded R2 of 0.856 and 0.945, between the experimental and predicted endpoint toxicity values. The models were also evaluated for predictive use through the most recent criteria based on root mean squared error (RMSE) and mean absolute error (MAE). The values of various statistical validation coefficients derived for the test data were above their respective threshold limits and thus put a high confidence in this analysis. The applicability domains of the constructed QSAR models were defined using the leverage and standardization approaches. The results suggest that the proposed QSAR models can reliably predict the reproductive toxicity potential of diverse chemicals and can be useful tools for screening new chemicals for safety assessment. © The Royal Society of Chemistry 2016.",,"organic compound; Article; data processing; decision tree; decision tree boost; decision tree forest; machine learning; mean absolute error; nonhuman; Organisation for Economic Co-operation and Development; practice guideline; priority journal; quantitative structure activity relation; rat; reproductive toxicity; risk assessment; root mean squared error; statistical parameters"
"Basant N., Gupta S., Singh K.P.","Predicting acetyl cholinesterase enzyme inhibition potential of ionic liquids using machine learning approaches: An aid to green chemicals designing","10.1016/j.molliq.2015.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930960508&doi=10.1016%2fj.molliq.2015.06.001&partnerID=40&md5=3d7a76135b852cc8519eb66d04edb7d0","The ionic liquids (ILs) constitute a group of novel chemicals that have potential industrial applications. Designing of safer ILs is among the priorities of the chemists and toxicologists today. Computational approaches have been considered appropriate methods for prior safety assessment of the chemicals. The present study is an attempt to investigate the chemical attributes of a wide variety of ILs towards their inhibitory potential of acetyl cholinesterase enzyme (AChE) through the development of predictive qualitative and quantitative structure-activity relationship (SAR) models in light of the OECD principles. Here, machine learning based cascade correlation network (CCN) and support vector machine (SVM) SAR models were established for qualitative and quantitative prediction of the AChE inhibition potential of ILs. Diversity and nonlinearity of the considered dataset were evaluated. The CCN and SVM models were constructed using simple descriptors and validated with external data. Predictive power of these SAR models was established through deriving several stringent parameters recommended for QSAR studies. The developed SAR models exhibited better statistical confidence than those in the previously reported studies. The models identified the structural elements of the ILs responsible for the AChE inhibition, and hence could be useful tools in designing of safer and green ILs. © 2015 Published by Elsevier B.V.","AChE inhibition; Ionic liquids; Molecular descriptors; SARs; Structural diversity","Artificial intelligence; Chemicals; Computational chemistry; Enzyme inhibition; Enzymes; Industrial chemicals; Learning systems; Liquids; Support vector machines; Cascade correlation network; Computational approach; Machine learning approaches; Molecular descriptors; Quantitative prediction; Quantitative structure activity relationship; SARs; Structural diversity; Ionic liquids"
"Bascur O.A., O’Rourke J.","Measuring, managing, and transforming data for operational insights","10.1016/B978-0-12-820027-8.00006-X","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124660944&doi=10.1016%2fB978-0-12-820027-8.00006-X&partnerID=40&md5=34eb8e3d5aee3b2fb0270cc7136c426f","The process industries face many pressures, such as meeting production targets, minimizing costs, and maximizing product quality. In addition, manufacturers (process industries such as refineries, processing plants, metal producers, paper mills, pharmaceutical plants, or oil and gas producers) must adhere to increasing governmental safety and environmental compliance regulations. This chapter discusses how most of these issues can be addressed through intelligent use of real-time operating data continuously collected from their production facilities. It advocates the use of a data-driven strategy to optimize operational efficiency and maintenance and enable business personnel to quickly and easily take corrective action when abnormal conditions occur. By taking corrective actions in real time, unnecessary costs can be avoided, bottom line profitability is increased, employee safety is maintained, and responsible environmental stewardship is achieved. The strategy uses real-time data infrastructure software as the integration layer between production process control systems (operational technology), business systems, and advanced offline analytics (information technology). © 2020 Elsevier Inc. All rights reserved.","Artificial intelligence; Data infrastructure; Digital plant template; Digital transformation; Machine learning; Operational intelligence; PI vision; Smart manufacturing; Unit template",
"Basgalupp M.P., Barros R.C., de Sá A.G.C., Pappa G.L., Mantovani R.G., de Carvalho A.C.P.L.F., Freitas A.A.","An extensive experimental evaluation of automated machine learning methods for recommending classification algorithms","10.1007/s12065-020-00463-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089682130&doi=10.1007%2fs12065-020-00463-z&partnerID=40&md5=32280eb369c63525679bc913ba31d7a1","This paper presents an experimental comparison among four automated machine learning (AutoML) methods for recommending the best classification algorithm for a given input dataset. Three of these methods are based on evolutionary algorithms (EAs), and the other is Auto-WEKA, a well-known AutoML method based on the combined algorithm selection and hyper-parameter optimisation (CASH) approach. The EA-based methods build classification algorithms from a single machine learning paradigm: either decision-tree induction, rule induction, or Bayesian network classification. Auto-WEKA combines algorithm selection and hyper-parameter optimisation to recommend classification algorithms from multiple paradigms. We performed controlled experiments where these four AutoML methods were given the same runtime limit for different values of this limit. In general, the difference in predictive accuracy of the three best AutoML methods was not statistically significant. However, the EA evolving decision-tree induction algorithms has the advantage of producing algorithms that generate interpretable classification models and that are more scalable to large datasets, by comparison with many algorithms from other learning paradigms that can be recommended by Auto-WEKA. We also observed that Auto-WEKA has shown meta-overfitting, a form of overfitting at the meta-learning level, rather than at the base-learning level. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Algorithm recommendation; Automated machine learning; Classification; Evolutionary algorithms; Meta-learning","Bayesian networks; Classification (of information); Decision trees; Evolutionary algorithms; Large dataset; Learning algorithms; Parameter estimation; Bayesian network classification; Classification algorithm; Classification models; Controlled experiment; Decision tree induction; Evolutionary algorithms (EAs); Experimental comparison; Experimental evaluation; Machine learning"
"Bashar M.A., Li Y., Shen Y., Albathan M.","Interpreting discovered patterns in terms of ontology concepts","10.1109/WI-IAT.2014.67","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931676984&doi=10.1109%2fWI-IAT.2014.67&partnerID=40&md5=00df2849fd87dd68b30cca826d974b2f","Semantic Web offers many possibilities for future Web technologies. Therefore, it is a need to search for ways that can bring the huge amount of unstructured documents from current Web to Semantic Web automatically. One big challenge in searching for such ways is how to understand patterns by both humans and machine. To address this issue, we present an innovative model which interprets patterns to high level concepts. These concepts can explain the patterns' meanings in a human understandable way while improving the information filtering performance. The model is evaluated by comparing it against one state-of-the-art benchmark model using standard Reuters dataset. The results show that the proposed model is successful. The significance of this model is three fold. It gives a way to interpret text mining output, provides a technique to find concepts relevant to the whole set of patterns which is an essential feature to understand the topic, and to some extent overcomes information mismatch and overload problems of existing models. This model will be very useful for knowledge based applications. © 2014 IEEE.",,"Data mining; Information filtering; Knowledge based systems; Benchmark models; Essential features; Innovative models; Knowledge-based applications; Ontology concepts; Overload problems; State of the art; Unstructured documents; Semantic Web"
"Bashayreh M., Sibai F.N., Tabbara A.","Artificial intelligence and legal liability: towards an international approach of proportional liability based on risk sharing","10.1080/13600834.2020.1856025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097173847&doi=10.1080%2f13600834.2020.1856025&partnerID=40&md5=879f7604e45c02433d1448270e853034","This paper critically examines the allocation of liability when autonomous artificial intelligence (AI) systems cause accidents. Problems of applying existing principles of legal liability in AI environment are addressed. This paper argues that the sharing of risk as a basis for proportionate liability should be a basis for a new liability regime to govern future autonomous machines. It is argued that this approach favors the reality of parties’ consent to taking the risk of unpredictable AI behavior over the technicality of existing principles of legal liability. The suggested approach also encourages transparency and responsible decisions of developers and owners of AI systems. A flowchart to clarify possible outcomes of applying the suggested approach is provided. The paper also discusses the need for harmonization of national laws and international cooperation regarding AI incidents crossing national borders to ensure predictability of legal rules governing the liability ensuing from AI applications. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Artificial Intelligence; autonomous machines; legal liability; proportionate liability; risk sharing",
"Basilico E., Johnsen T.","Smart(er) investing: How academic insights propel the savvy investor","10.1007/978-3-030-26692-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088482520&doi=10.1007%2f978-3-030-26692-9&partnerID=40&md5=c93360fbfeec7310f119e95915f7e20d","This book identifies and discusses the most successful investing practices with an emphasis on the academic articles that produced them and why this research led to popular adoption and growth in $AUM. Investors are bombarded with ideas and prescriptions for successful investing every day. Given the steady stream of information on stock tips, sector timing, asset allocation, etc., how do investors decide? How do they judge the quality and reliability of the investment advice they are given on a day-to-day basis? This book identifies which academic articles turned investment ideas were the most innovative and influential in the practice of investment management. Each article is discussed in terms of the asset management process: strategy, portfolio construction, portfolio implementation, and risk management. Some examples of topics covered are factor investing, the extreme growth of trading instruments like Exchange Traded Funds, multi-asset investing, socially responsible investing, big data, and artificial intelligence. This book analyzes a curated selection of peer-reviewed academic articles identified among those published by the scientific investment community. The book briefly describes each of the articles, how and why each one changed the way we think about investing in that specific asset class, and provides insights as to the nuts and bolts of how to take full advantage of this successful investment idea. It is as timely as it is informative and will help each investor to focus on the most successful strategies, ideas, and implementation that provide the basis for the efficient accumulation and management of wealth. © The Editor(s) (if applicable) and The Author(s), under exclusive licence to Springer Nature Switzerland AG 2019.",,
"Baskar J., Yan C., Lindgren H.","Instrument-oriented approach to detecting and representing human activity for supporting executive functions and learning","10.1145/3121283.3121305","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033499439&doi=10.1145%2f3121283.3121305&partnerID=40&md5=38190f6a997e0d78c12b492db2e99341","The goal of this study is to develop a computer-interpretable model for activity detection and representation, based on existing informal models of how humans perform activity. Appropriate detection of purposeful human activity is an essential functionality of active assistive technology aiming at providing tailored support to individuals for improving activity performance and completion. The main contribution is the design of a model for detection and representation of human activities based on three categories of instruments, which is implemented as two generic and supplementary terminology models: an event ontology and a core ontology. The core ontology is extended for each new knowledge domain into a domain ontology. The model builds the base for personalization of services generated by the cooperative reasoning performed by a human collaborating with an intelligent and social software agent. Ongoing and future work includes user studies in the different application domains. © 2017 Association for Computing Machinery.","Activity recognition; Activity theory; Assistive technology; Decision-support systems; Human-agent interaction; Knowledge representation; Ontology; User modelling","Artificial intelligence; Decision support systems; Decision theory; Knowledge representation; Ontology; Software agents; Activity recognition; Activity Theory; Assistive technology; Human agent interactions; User Modelling; Ergonomics"
"Baskar T., Kannan M.","Classification Rule Diagram to Diagnosis the Tuberculosis Based on Symptoms","10.1007/978-981-16-7952-0_36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130342872&doi=10.1007%2f978-981-16-7952-0_36&partnerID=40&md5=ac73058df2a99d81c1c098f47a395129","Data mining is a vast area in research and applications where it is applicable for many real-time problems hence it proves to be beneficial for large data handling. Especially, when it comes to medical diagnosis, using data mining algorithms always proves to be exemplary where it helps to quickly diagnose diseases using available data based on classification algorithms. For example, diagnosing tuberculosis can be done using knowledge discovery patterns on uncovering various symptoms and associated patterns of behavior which is cost-efficient instead of diagnosing using various medical tests. In this paper, we are employing a classification diagram which is a key element of a much more elaborative process called knowledge discovery, to provide some indicators that are mainly responsible for the occurrence of special disease—tuberculosis and symptoms that are highly correlated with the positive examination test based on various rule-based conditions. Classification rules can be used to infer the defining characteristics based on information relevance of different test components and hence discover hidden knowledge, unexpected patterns, and new rules from the database by using the classification rule diagram to diagnose tuberculosis. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Classification rule diagram; Data mining; Knowledge discovery; Medical diagnosis",
"Başoğlu Kabran F., Ünlü K.D.","A two-step machine learning approach to predict S&P 500 bubbles","10.1080/02664763.2020.1823947","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091464751&doi=10.1080%2f02664763.2020.1823947&partnerID=40&md5=92e53412d630a481fd2d4eb46e1c9b68","In this paper, we are interested in predicting the bubbles in the S&P 500 stock market with a two-step machine learning approach that employs a real-time bubble detection test and support vector machine (SVM). SVM as a nonparametric binary classification technique is already a widely used method in financial time series forecasting. In the literature, a bubble is often defined as a situation where the asset price exceeds its fundamental value. As one of the early warning signals, prediction of bubbles is vital for policymakers and regulators who are responsible to take preemptive measures against the future crises. Therefore, many attempts have been made to understand the main factors in bubble formation and to predict them in their earlier phases. Our analysis consists of two steps. The first step is to identify the bubbles in the S&P 500 index using a widely recognized right-tailed unit root test. Then, SVM is employed to predict the bubbles by macroeconomic indicators. Also, we compare SVM with different supervised learning algorithms by using k-fold cross-validation. The experimental results show that the proposed approach with high predictive power could be a favourable alternative in bubble prediction. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Bubbles; early warning; machine learning; macroeconomic indicators; support vector machines",
"Bassano C., Carotenuto A., Ferretti M., Pietronudo M.C., Coskun H.E.","Smart university for sustainable governance in smart local service systems","10.1007/978-3-319-94229-2_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049696500&doi=10.1007%2f978-3-319-94229-2_20&partnerID=40&md5=55c18ad7741e02158bd80232dc28c009","From the Service Science perspective, our work tends to provide a conceptual and methodological contribution to affirm the role of the university as the responsible agent for the growth and development of a local area. In this sense, the purpose is to promote a harmonious growth of the whole local service system, focused on academic quality and accountability through the approach to Social Responsibility, also involving government, business and society. The University as a place for higher education and research, at the same time, represents a privileged space of convergence of different growth perspectives of the local actors. This convergence should be seen as a “place” to share and develop a common sense of value that is smart, ethically, socially, and economically sustainable, i.e. a Smart Local Service System (S-LSS). © Springer International Publishing AG, part of Springer Nature 2019.","Service science; Smart local service systems; Sustainable governance; University","Artificial intelligence; Human engineering; Systems engineering; Business and societies; Higher education and researches; Local service; Methodological contributions; Service science; Social responsibilities; Sustainable governance; University; Social aspects"
"Bassel G.W., Glaab E., Marquez J., Holdsworth M.J., Bacardit J.","Functional network construction in arabidopsis using rule-based machine learning on large-scale data sets","10.1105/tpc.111.088153","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054969308&doi=10.1105%2ftpc.111.088153&partnerID=40&md5=2482aa746ce693240bccdddc812b152c","The author responsible for distribution of materials integral to the findings presented in this article in accordance with the policy described in the Instructions for Authors (www.plantcell.org) is: George W. Bassel (george.bassel@nottingham.ac.uk). The meta-analysis of large-scale postgenomics data sets within public databases promises to provide important novel biological knowledge. Statistical approaches including correlation analyses in coexpression studies of gene expression have emerged as tools to elucidate gene function using these data sets. Here, we present a powerful and novel alternative methodology to computationally identify functional relationships between genes from microarray data sets using rulebased machine learning. This approach, termed ""coprediction,"" is based on the collective ability of groups of genes cooccurring within rules to accurately predict the developmental outcome of a biological system. We demonstrate the utility of coprediction as a powerful analytical tool using publicly available microarray data generated exclusively from Arabidopsis thaliana seeds to compute a functional gene interaction network, termed Seed Co-Prediction Network (SCoPNet). SCoPNet predicts functional associations between genes acting in the same developmental and signal transduction pathways irrespective of the similarity in their respective gene expression patterns. Using SCoPNet, we identified four novel regulators of seed germination (ALTERED SEED GERMINATION5, 6, 7, and 8), and predicted interactions at the level of transcript abundance between these novel and previously described factors influencing Arabidopsis seed germination. An online Web tool to query SCoPNet has been developed as a community resource to dissect seed biology and is available at http://www.vseed.nottingham.ac.uk/.© 2011 American Society of Plant Biologists.",,"Arabidopsis; Arabidopsis thaliana"
"Bassetti T., Luvison A.","Apology of scientific reason - IV: Dilemmas of choice and ethics of artificial intelligence (AI) [Apologia della ragione scientifica - IV: Dilemmi di scelta ed etica dell'IA]",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049873059&partnerID=40&md5=2cf967359b8872e31275f75c58b693dd","Science and technology are two major drivers for development in the most advanced countries. Methodologically, critical thinking, motivated reasoning and cognitive reflection provide the conceptual framework and operational toolbox for decision making under uncertainty. The inherent limits (i.e., biases) of human bounded rationality can be overcome, so that scientific rationality is enhanced even more. Two cases are discussed to support this assumption. The first is the Newcomb's paradox, a logical and philosophical thought experiment entailing a game between two players, one of whom claims to be able to predict the future. The solution to this brainteaser is based on elementary algebra involving simple probabilities. By starting from the trolley problem, the focus of the second case is on ethical issues of artificial intelligence (AI), e.g., autonomous systems, such as driverless cars, or other disruptive and pervasive AI applications. Here, the term accountability applies to a designer who considers the question of how intelligent systems should be imbued with ethical values. The underlying argument is that the two cultures - humanities and techno-science - thanks to the many intersection and cross-fertilization points, are both faces of the same coin, i.e., interdisciplinary knowledge. This type of knowledge should belong to the education and training background of any leader, executive, or opinion maker, responsible for facing the incumbent challenges of the digital society. © 2019 Associazione Italiana per l'Informatica e il Calcolo Automatico. All rights reserved.","AI and machine ethics; Critical decision making; Logical conundrums; Newcomb's paradox revisited; Science and humanities; Trolley problem","Behavioral research; Decision making; Decision support systems; Intelligent systems; Decision making under uncertainty; Education and training; Logical conundrums; Newcomb's paradox revisited; Science and humanities; Science and Technology; Scientific rationalities; Trolley problem; Philosophical aspects"
"Bassim S., Genard B., Gauthier-Clerc S., Moraga D., Tremblay R.","Ontogeny of bivalve immunity: Assessing the potential of next-generation sequencing techniques","10.1111/raq.12064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944194216&doi=10.1111%2fraq.12064&partnerID=40&md5=ef6acfff58d28aa1abe4e55096de703f","Living organisms are constantly evolving to secure their survival via adaptations at the molecular and cellular level. Most marine bivalves have microscopic planktonic larval stages until settlement to the benthic environment. These pelagic stages are generally more sensitive than their adult counterparts to environmental and pathogen threats. Adaptive capacities could improve survival of these early stages. Recent advancements in data mining and pipeline analysis should shed light on the currently unknown processes that occur during these first stages. Existing data on early stages are fragmented compared with the abundance of information available for adult. Exploring diversity through aquaculture and lessening the impact of common issues, for example, massive mortalities of larvae, especially within the current conditions of a changing climate, ultimately rests on our knowledge of the molecular processes responsible for phenotypic plasticity. Although it is somewhat difficult to assess immune mechanisms by tracking circulating immunocytes in larvae, studies on the development of immune processes are now feasible at the transcript level. Next-generation techniques offer outstanding solutions for wide-range transcriptome analysis. We present a short review of the early ontogeny of the immune system in marine bivalves, with particular focus on next-generation sequencing applications. Like all reviews of this nature, there is a trade-off between the depth of the coverage and the number of subjects discussed. We will thus restrict the scope to bivalve immunity and focus on the central concepts across a wide range of topics, that is, the ontogeny of immunity and advancements in molecular studies. © 2014 Wiley Publishing Asia Pty Ltd.","Bivalve; Immune defence; Next-generation sequencing; Ontogeny; Transcriptomics",
"Bassiouny R., Mohamed A., Umapathy K., Khan N.","An Interpretable Object Detection-Based Model for the Diagnosis of Neonatal Lung Diseases Using Ultrasound Images","10.1109/EMBC46164.2021.9630169","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122500594&doi=10.1109%2fEMBC46164.2021.9630169&partnerID=40&md5=8ea2c112ff68f02d14cf2d12849d5a77","Over the last few decades, Lung Ultrasound (LUS) has been increasingly used to diagnose and monitor different lung diseases in neonates. It is a noninvasive tool that allows a fast bedside examination while minimally handling the neonate. Acquiring a LUS scan is easy, but understanding the artifacts concerned with each respiratory disease is challenging. Mixed artifact patterns found in different respiratory diseases may limit LUS readability by the operator. While machine learning (ML), especially deep learning can assist in automated analysis, simply feeding the ultrasound images to an ML model for diagnosis is not enough to earn the trust of medical professionals. The algorithm should output LUS features that are familiar to the operator instead. Therefore, in this paper we present a unique approach for extracting seven meaningful LUS features that can be easily associated with a specific pathological lung condition: Normal pleura, irregular pleura, thick pleura, A- lines, Coalescent B-lines, Separate B-lines and Consolidations. These artifacts can lead to early prediction of infants developing later respiratory distress symptoms. A single multi-class region proposal-based object detection model faster-RCNN (fRCNN) was trained on lower posterior lung ultrasound videos to detect these LUS features which are further linked to four common neonatal diseases. Our results show that fRCNN surpasses single stage models such as RetinaNet and can successfully detect the aforementioned LUS features with a mean average precision of 86.4%. Instead of a fully automatic diagnosis from images without any interpretability, detection of such LUS features leave the ultimate control of diagnosis to the clinician, which can result in a more trustworthy intelligent system. © 2021 IEEE.","faster RCNN; Lung Ultrasound; Object detection models; RetinaNet","Biological organs; Deep learning; Diagnosis; Feature extraction; Intelligent systems; Medical imaging; Object recognition; Pulmonary diseases; Ultrasonic applications; Automated analysis; Fast RCNN; Lung ultrasound; Machine learning models; Noninvasive tools; Object detection model; Objects detection; Retinanet; Ultrasound images; Ultrasound scans; Object detection; diagnostic imaging; echography; human; lung; lung disease; newborn; newborn disease; thorax; Humans; Infant, Newborn; Infant, Newborn, Diseases; Lung; Lung Diseases; Thorax; Ultrasonography"
"Bassiri A., Malek M.R., Alesheikh A.A.","Rough spatio-temporal topological relationships","10.1142/9789812799470_0021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049185212&doi=10.1142%2f9789812799470_0021&partnerID=40&md5=4555dc55462af30aad080758e9da56bc","In order to model natural phenomena and their relationships, fuzziness and vagueness should be considered. Topological relations are one of the most important definable characters between two spatio-temporal objects. This paper proposes a new way for modeling and constructing topological relationships between spatio-temporal objects under the umbrella of rough topological relationships in space and time. And as a case study, land covers as spatio-temporal objects considered. Results show that rough set theory models uncertainty of spatio-temporal objects in a transparent and explainable way.",,"Artificial intelligence; Computation theory; Topology; Land cover; Natural phenomena; Space and time; Spatio temporal; Spatio-temporal objects; Topological relations; Topological relationships; Rough set theory"
"Basso F., Cifuentes Á., Cuevas-Pavincich F., Pezoa R., Varas M.","Assessing influential factors for lane change behavior using full real-world vehicle-by-vehicle data","10.1080/19427867.2021.1998876","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118434574&doi=10.1080%2f19427867.2021.1998876&partnerID=40&md5=ab0c4e98ec1d4e5b321d41ce8458ce20","Understanding the underlying reasons for potential human risky driving behaviors is crucial for improving road safety. Recent technologies allow the analysis of driving behaviors at a microscopic level, allowing a naturalistic observation of such phenomenon through information collected unobtrusively. This paper assesses the factors that influence discretionary lane changes on an urban highway in Santiago, Chile, employing an interpretable machine learning approach. We use full real-world vehicle-by-vehicle data gathered from Automatic Vehicle Identification technology and individual public information of the vehicles and their owners, which allows us to understand patterns that might have different characteristics when performed in simulated environments. We provide insights about the causes that increase the likelihood of lane changes. For example, we find that: (i) the older the car, the less likely it is to change lane, (ii) younger drivers change lane more often, and (iii) motorcycles drivers are the most likely to change lane. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Automatic Vehicle Identification; Drivers’ behaviors; Interpretable Machine Learning; lane change; safety","Behavioral research; Machine learning; Motor transportation; Change lane; Driver's behavior; Driving behaviour; Influential factors; Interpretable machine learning; Lane change; Machine-learning; Real-world; Risky driving; Road safety; Vehicles"
"Bastas G., Gkiokas A., Katsouros V., Maragos P.","CONVOLUTIONAL NETWORKS for VISUAL ONSET DETECTION in the CONTEXT of BOWED STRING INSTRUMENT PERFORMANCES",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122070401&partnerID=40&md5=c0b465ea6eae554140945e4d14814e0e","In this work, we employ deep learning methods for visual onset detection. We focus on live music performances involving bowed string instruments. In this context, we take as a source of meaningful information the sequence of movements of the performers’ body and especially the bowing motion of the (right) hand. Body skeletons for each video frame are extracted through OpenPose and are then used as input for Temporal Convolutional Neural Networks (TCNs). TCNs prove capable of handling such temporal information by conditioning outputs on an adequately long history (i.e. variable receptive field), ensuring highly parallelizable lightweight computations and a multitude of trainable parameters that provide robustness. As another source of information for our task, we consider the more subtle movements of the (left) hand fingers which are responsible for pitch changes. Detections in this case rely directly on pixel data from specifically chosen regions of interest. Here, a 2D Convolutional Neural Network (CNN) is applied on the input in order to learn the features to be fed to the TCN. The models were trained and evaluated on single-player string recordings from the University of Rochester Multi-Modal Music Performance (URMP) Dataset. We show that these two approaches provide some complementary information. Copyright: © 2021 the Authors.",,"Convolution; Deep learning; Music; Bowed string; Convolutional neural network; Instrument performance; Learning methods; Music performance; Onset detection; Receptive fields; String instruments; Temporal information; Video frame; Convolutional neural networks"
"Bastos J.A., Matos S.M.","Explainable models of credit losses","10.1016/j.ejor.2021.11.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121121861&doi=10.1016%2fj.ejor.2021.11.009&partnerID=40&md5=90ac25196ae25ffae28bb5dc2eaf5d2d","Credit risk management is an area where regulators expect banks to have transparent and auditable risk models, which would preclude the use of more accurate black-box models. Furthermore, the opaqueness of these models may hide unknown biases that may lead to unfair lending decisions. In this study, we show that banks do not have to sacrifice prediction accuracy at the cost of model transparency to be compliant with regulatory requirements. We illustrate this by showing that the predictions of credit losses given by a black-box model can be easily explained in terms of their inputs. Because black-box models are better at uncovering complex patterns in the data, banks should consider the determinants of credit losses suggested by these models in lending decisions and pricing of credit exposures. © 2021 Elsevier B.V.","Explainable machine learning; Forecasting; Loss given default; Recovery rates; Risk management","Costs; Machine learning; Risk assessment; Risk management; Black box modelling; Credit risk management; Explainable machine learning; Loss given defaults; Management IS; Model transparency; Prediction accuracy; Recovery rate; Risk modeling; Risks management; Forecasting"
"Bastos L.M., Rice C.W., Tomlinson P.J., Mengel D.","Untangling soil-weather drivers of daily N2O emissions and fertilizer management mitigation strategies in no-till corn","10.1002/saj2.20292","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113308826&doi=10.1002%2fsaj2.20292&partnerID=40&md5=b57b162ae1fa3f00f4d7c08432e1cd67","Fertilizer N management can mitigate N2O emissions but complex soil-weather conditions modulate the mitigation potential. Conditional inference tree (CIT) is a machine learning method able to untangle complex interactions while providing an interpretable model. The goals of this study were (a) to assess the effect of N fertilizer on N2O emissions, and to use CIT to identify (b) the main soil-weather drivers of daily N2O hot moments and (c) fertilizer management options to mitigate them. The study was conducted in 2 yr in no-till corn (Zea mays L.) with seven combinations of N source and placement tested. Daily N2O emissions were measured with vented chambers, and soil temperature and water-filled pore space (WFPS) were measured near the chambers on the same days of gas sampling. Overall, 2013 was drier with lower N2O emissions than 2014. Cumulative N2O losses differed across treatments and years, with broadcast emitting more in 2014 than in 2013, and only subsurface-banded fertilizer with a nitrification inhibitor (NI) consistently abated N2O losses. The main hot moment conditions were within ∼80 d of fertilizer application when soil temperature &gt;15 °C and WFPS &gt;57%. Under these conditions, NI abated losses by 50% compared with fertilizer alone. The machine learning approach used here could be used in larger datasets to elucidate environment-specific drivers of N2O hot moments and potential fertilizer mitigation practices under different soil, weather, and management conditions. © 2021 The Authors. Soil Science Society of America Journal published by Wiley Periodicals LLC on behalf of Soil Science Society of America",,"Gas emissions; Machine learning; Soils; Temperature; Turing machines; Conditional inference; Fertilizer applications; Fertilizer management; Machine learning approaches; Machine learning methods; Mitigation strategy; Nitrification inhibitor; Water-filled pore space; Fertilizers; emission control; fertilizer application; machine learning; maize; management practice; nitrous oxide; soil temperature; zero tillage; Zea mays"
"Bastos N.S., Adamatti D.F., Billa C.Z.","Discovering Patterns in Brain Signals Using Decision Trees","10.1155/2016/6391807","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988733927&doi=10.1155%2f2016%2f6391807&partnerID=40&md5=7428dbd198b5997ed02bf6e42db4c312","Even with emerging technologies, such as Brain-Computer Interfaces (BCI) systems, understanding how our brains work is a very difficult challenge. So we propose to use a data mining technique to help us in this task. As a case of study, we analyzed the brain's behaviour of blind people and sighted people in a spatial activity. There is a common belief that blind people compensate their lack of vision using the other senses. If an object is given to sighted people and we asked them to identify this object, probably the sense of vision will be the most determinant one. If the same experiment was repeated with blind people, they will have to use other senses to identify the object. In this work, we propose a methodology that uses decision trees (DT) to investigate the difference of how the brains of blind people and people with vision react against a spatial problem. We choose the DT algorithm because it can discover patterns in the brain signal, and its presentation is human interpretable. Our results show that using DT to analyze brain signals can help us to understand the brain's behaviour. © 2016 Narusci S. Bastos et al.",,"Behavioral research; Brain; Data mining; Decision trees; Forestry; Interfaces (computer); Blind people; Brain signals; Emerging technologies; Spatial activity; Spatial problems; Brain computer interface"
"Bastos P., Lopes I., Pires L.","A maintenance prediction system using data mining techniques",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051692690&partnerID=40&md5=3c07b0a5194b328117c8ba92b5cacd17","In the last years we have assisted to several and deep changes in industrial companies, mainly due to market dynamics and the need to converge with a globalized and impatient world. These changes are transversal to the entire company also impacting on company maintenance function. In an attempt to eliminate faults and keep systems running without interruption, companies incorporated tools into their Information and Communication Technologies (ICT) systems. The benefits are clear in terms of resulting quality and in costs reduction, particularly those related with the data processing time and accuracy of the resulting knowledge. In their daily routine, companies produce and store endless and complex quantities of data of different nature, increasing the difficulty of use in real time. In this sense, considering the relevance of data collected on industrial plants, namely in its maintenance activities, it is intended with this paper to present a functional architecture of a predictive maintenance system, using data mining techniques on data gathered from manufacturing units globally dispersed. Data Mining will identify behavior patterns, allowing a more accurate early detection of faults in machines. The remote data collection is based on an intricate system of distributed agents, which, given its nature, will be responsible for remote data collection through the functional architecture. © 2012 Newswood Limited. All rights reserved.","Agents; Data mining; E-collaboration; Maintenance; Management","Agents; Computer architecture; Data handling; Data mining; Industrial plants; Information services; Maintenance; Management; e-Collaboration; Functional architecture; Industrial companies; Information and Communication Technologies; Maintenance activity; Maintenance functions; Manufacturing units; Predictive maintenance; Data acquisition"
"Basu D., Parui S., Choudhury T., Ghosh U.","In-Heal: Intelligent Healthcare Architecture using AI-based Priority Scheduling Mechanism in vSDN-driven Edge Network","10.1109/CONECCT52877.2021.9622601","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123345654&doi=10.1109%2fCONECCT52877.2021.9622601&partnerID=40&md5=750bf0dc9bdb36acf6f8a2a2889983d0","The advancements of Edge Computing, Artificial Intelligence, vSDN (virtualized Software Defined Networking) are significantly boosting up the upcoming smart healthcare systems, but the advanced medical facilities are severely affected by the lack of proper systematic architectures. The chronological up-gradation of the latest technology is not amalgamated according to the existing infrastructure and that is why the fatal cases of medical negligence can not be overlooked, specially during any pandemic or epidemic situation. In this work, we have proposed a novel AI-driven network architecture that will proficiently improve the supervision systems inside smart medical care units. vSDN-driven edge servers are responsible for continuous monitoring of patient inflow and outflow inside any hospital and simultaneously provide the health status of the patients using an AI-based Risk Factor (RF) Scale. An intelligent priority scheduling mechanism is designed to selective filter the health conditions and identify the critical patients with immediate medical assistance. Our analysis shows that the proposed method provides better patient-to-doctor mapping using various available media and significantly reduce the cases of medical negligence inside any medical care facility. This dynamic architecture can be easily up-gradable even for the multi-vendor scenario. Our approach is also efficient enough to control any sudden surge of cases that the entire world has recently witnessed due to COVID-19. We have used random real-time patient flow data to validate our claims. © 2021 IEEE.","e-Health; Edge Network; priority scheduling; smart healthcare facility; vSDN","Health care; Health risks; Network architecture; E health; Edge computing; EDGE Networks; Ehealth; Healthcare facility; Priority scheduling; Scheduling mechanism; Smart healthcare facility; Software-defined networkings; Virtualized software defined networking; Scheduling"
"Basu S., Munafo A., Ben-Amor A.-F., Roy S., Girard P., Terranova N.","Predicting disease activity in patients with multiple sclerosis: An explainable machine-learning approach in the Mavenclad trials","10.1002/psp4.12796","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129616273&doi=10.1002%2fpsp4.12796&partnerID=40&md5=44a1d64ba1d39ccb0982d0d3a7cc643a","Multiple sclerosis (MS) is among the most common autoimmune disabling neurological conditions of young adults and affects more than 2.3 million people worldwide. Predicting future disease activity in patients with MS based on their pathophysiology and current treatment is pivotal to orientate future treatment. In this respect, we used machine learning to predict disease activity status in patients with MS and identify the most predictive covariates of this activity. The analysis is conducted on a pooled population of 1935 patients enrolled in three cladribine tablets clinical trials with different outcomes: relapsing–remitting MS (from CLARITY and CLARITY-Extension trials) and patients experiencing a first demyelinating event (from the ORACLE-MS trial). We applied gradient-boosting (from XgBoost library) and Shapley Additive Explanations (SHAP) methods to identify patients' covariates that predict disease activity 3 and 6 months before their clinical observation, including patient baseline characteristics, longitudinal magnetic resonance imaging readouts, and neurological and laboratory measures. The most predictive covariates for early identification of disease activity in patients were found to be treatment duration, higher number of new combined unique active lesion count, higher number of new T1 hypointense black holes, and higher age-related MS severity score. The outcome of this analysis improves our understanding of the mechanism of onset of disease activity in patients with MS by allowing their early identification in clinical settings and prompting preventive measures, therapeutic interventions, or more frequent patient monitoring. © 2022 Merck Serono S.A. CPT: Pharmacometrics & Systems Pharmacology published by Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology and Therapeutics.",,"alanine aminotransferase; albumin; alkaline phosphatase; aspartate aminotransferase; bilirubin; calcium; cladribine; creatine kinase; creatinine; potassium; protein; sodium; uric acid; cladribine; immunosuppressive agent; accuracy; algorithm; Article; basophil count; black hole; brain stem; controlled study; disease activity; eosinophil count; Expanded Disability Status Scale; glucose urine level; human; leukocyte count; lymphocyte; machine learning; major clinical study; multiple sclerosis; nuclear magnetic resonance imaging; onset age; patient monitoring; prediction; prevalence; receiver operating characteristic; sensitivity and specificity; T1 weighted imaging; urea nitrogen blood level; urinalysis; urine pH; machine learning; multiple sclerosis; young adult; Cladribine; Humans; Immunosuppressive Agents; Machine Learning; Multiple Sclerosis; Multiple Sclerosis, Relapsing-Remitting; Young Adult"
"Basu S., You X., Feizi S.","On second-order group influence functions for black-box predictions",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105148276&partnerID=40&md5=05a5904171cd40e82fbc4437209117de","With the rapid adoption of machine learning systems in sensitive applications, there is an increasing need to make black-box models explainable. Often we want to identify an influential group of training samples in a particular test prediction for a given machine learning model. Existing influence functions tackle this problem by using firstorder approximations of the effect of removing a sample from the training set on model parameters. To compute the influence of a group of training samples (rather than an individual point) in model predictions, the change in optimal model parameters after removing that group from the training set can be large. Thus, in such cases, the first-order approximation can be loose. In this paper, we address this issue and propose second-order influence functions for identifying influential groups in test-Time predictions. For linear models, across different sizes and types of groups, we show that using the proposed second-order influence function improves the correlation between the computed influence values and the ground truth ones. We also show that second-order influence functions could be used with optimization techniques to improve the selection of the most influential group for a test-sample. © ICML 2020. All rights reserved.",,"Forecasting; Method of moments; Sampling; Subsidence; Different sizes; First-order approximations; Influence functions; Machine learning models; Model parameters; Model prediction; Optimization techniques; Sensitive application; Machine learning"
"Basu S., Kumbier K., Brown J.B., Yu B.","Iterative random forests to discover predictive and stable high-order interactions","10.1073/pnas.1711236115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042180619&doi=10.1073%2fpnas.1711236115&partnerID=40&md5=989b50e350dde37031028490a5905934","Genomics has revolutionized biology, enabling the interrogation of whole transcriptomes, genome-wide binding sites for proteins, and many other molecular processes. However, individual genomic assays measure elements that interact in vivo as components of larger molecular machines. Understanding how these high-order interactions drive gene expression presents a substantial statistical challenge. Building on random forests (RFs) and random intersection trees (RITs) and through extensive, biologically inspired simulations, we developed the iterative random forest algorithm (iRF). iRF trains a feature-weighted ensemble of decision trees to detect stable, high-order interactions with the same order of computational cost as the RF. We demonstrate the utility of iRF for high-order interaction discovery in two prediction problems: enhancer activity in the early Drosophila embryo and alternative splicing of primary transcripts in human-derived cell lines. In Drosophila, among the 20 pairwise transcription factor interactions iRF identifies as stable (returned in more than half of bootstrap replicates), 80% have been previously reported as physical interactions. Moreover, third-order interactions, e.g., between Zelda (Zld), Giant (Gt), and Twist (Twi), suggest high-order relationships that are candidates for follow-up experiments. In human-derived cells, iRF rediscovered a central role of H3K36me3 in chromatin-mediated splicing regulation and identified interesting fifth- and sixth-order interactions, indicative of multivalent nucleosomes with specific roles in splicing regulation. By decoupling the order of interactions from the computational cost of identification, iRF opens additional avenues of inquiry into the molecular mechanisms underlying genome biology. © 2018 National Academy of Sciences. All Rights Reserved.","Genomics; High-order interaction; Interpretable machine learning; Random forests; Stability","histone H3; transcription factor; transcriptome; algorithm; alternative RNA splicing; Article; chromatin; decision tree; Drosophila; enhancer region; gene; gene activity; genetic regulation; genome; genomics; Gt gene; human; human cell; iterative random forest; molecular interaction; nonhuman; nucleosome; priority journal; random forest; Twi gene; Zld gene; alternative RNA splicing; animal; biological model; biology; gene expression regulation; gene regulatory network; genetics; genome-wide association study; Algorithms; Alternative Splicing; Animals; Computational Biology; Drosophila; Gene Expression Regulation, Developmental; Gene Regulatory Networks; Genome-Wide Association Study; Models, Genetic"
"Basuki S., Kusuma S.F.","Automatic question generation for 5W-1H open domain of indonesian questions by using syntactical template-based features from academic textbooks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049447813&partnerID=40&md5=864846918d39ab9420aef298d91c8815","The measuring of education quality in school can be conducted by delivering the examination to the students. Composing questions in the examination process to measure students’ achievement in the school teaching and learning process can be difficult and time consuming. To solve this problem, this research proposes Automatic Question Generation (AQG) method to generate Open Domain Indonesian Question by using syntactical approach. Open Domain questions are questions covering many domains of knowledge. The challenge of generating the questions is how to identify the types of declarative sentences that are potential to be transformed into questions and how to develop the method for generating question automatically. In realizing the method, this research incorporates four stages, namely: the identification of declarative sentence for 8 coarse-class and 19 fine-class sentences, the classification of features for coarse-class sentence and the classification rules for fine-class sentence, the identification of question patterns, and the extraction of sentence’s components as well as the rule generation of questions. The coarse-class classification was carried out based on a machine learning with syntactical features of the sentence, namely: Part of Speech (POS) Tag, the presence of punctuation, the availability of specific verbs, sequence of words, etc. The fine-class classification was carried out based on a set of rules. According to the implementation and experiment, the findings show that the accuracy of coarse-class classification reaches 83.26% by using the SMO classifier and the accuracy of proposed fine-class classification reaches 92%. The generated questions are categorized into three types, namely: TRUE, UNDERSTANDABLE, and FALSE. The accuracy of generated TRUE and UNDERSTANDABLE questions reaches 88.66%. Thus, the obtained results show that the proposed method is prospective to implement in the real situation. © 2005 - ongoing JATIT & LLS.","Automatic Question Generation (AQG); Coarse-class classification; Fine-class classification; Open domain question; Syntactical approach",
"Bataille C.P., von Holstein I.C.C., Laffoon J.E., Willmes M., Liu X.-M., Davies G.R.","A bioavailable strontium isoscape for Western Europe: A machine learning approach","10.1371/journal.pone.0197386","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047828662&doi=10.1371%2fjournal.pone.0197386&partnerID=40&md5=bbe5c321dd0f5e316d3cba04eab628c3","Strontium isotope ratios (87Sr/86Sr) are gaining considerable interest as a geolocation tool and are now widely applied in archaeology, ecology, and forensic research. However, their application for provenance requires the development of baseline models predicting surficial 87Sr/86Sr variations (“isoscapes”). A variety of empirically-based and process-based models have been proposed to build terrestrial 87Sr/86Sr isoscapes but, in their current forms, those models are not mature enough to be integrated with continuous-probability surface models used in geographic assignment. In this study, we aim to overcome those limitations and to predict 87Sr/86Sr variations across Western Europe by combining process-based models and a series of remote-sensing geospatial products into a regression framework. We find that random forest regression significantly outperforms other commonly used regression and interpolation methods, and efficiently predicts the multi-scale patterning of 87Sr/86Sr variations by accounting for geological, geomorphological and atmospheric controls. Random forest regression also provides an easily interpretable and flexible framework to integrate different types of environmental auxiliary variables required to model the multi-scale patterning of 87Sr/86Sr variability. The method is transferable to different scales and resolutions and can be applied to the large collection of geospatial data available at local and global levels. The isoscape generated in this study provides the most accurate 87Sr/86Sr predictions in bioavailable strontium for Western Europe (R2 = 0.58 and RMSE = 0.0023) to date, as well as a conservative estimate of spatial uncertainty by applying quantile regression forest. We anticipate that the method presented in this study combined with the growing numbers of bioavailable 87Sr/86Sr data and satellite geospatial products will extend the applicability of the 87Sr/86Sr geo-profiling tool in provenance applications. © 2018 Bataille et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"radioisotope; strontium 86; strontium 87; unclassified drug; strontium; Strontium-86; Strontium-87; Article; bioavailability; controlled study; Denmark; France; Germany; Great Britain; machine learning; measurement accuracy; Netherlands; prediction; process optimization; radioactivity; random forest; algorithm; atmosphere; climate; environmental monitoring; Europe; geography; geology; procedures; regression analysis; statistical model; Algorithms; Atmosphere; Climate; Environmental Monitoring; Europe; Geography; Geology; Linear Models; Machine Learning; Regression Analysis; Strontium Isotopes"
"Bataineh A.A.","A comparative analysis of nonlinear machine learning algorithms for breast cancer detection","10.18178/ijmlc.2019.9.3.794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067024460&doi=10.18178%2fijmlc.2019.9.3.794&partnerID=40&md5=80e5fd7481446e1e262f9ffa62ad4c14","Breast cancer is a form of invasive cancer and one of the most common health problems for women that is globally responsible for a large number of deaths. Accurately classifying and categorizing breast cancer subtype is an essential task. Automated techniques based on artificial intelligence can significantly save time and reduce error. In this paper, a performance comparison between five nonlinear machine learning algorithms viz Multilayer Perceptron (MLP), K-Nearest Neighbors (KNN), Classification and Regression Trees (CART), Gaussian Nave Bayes (NB) and Support Vector Machines (SVM) on the Wisconsin Breast Cancer Diagnostic (WBCD) dataset is conducted. The primary objective is to evaluate the performance in classifying data with respect to efficiency and effectiveness of each algorithm in terms of classification test accuracy, precision, and recall. © 2019 International Association of Computer Science and Information Technology.","Artificial intelligence; Classification and regression trees (CART); Gaussian naïve bayes (NB); K-nearest neighbors (KNN); Machine learning; Multilayer perceptron (MLP); Support vector machines (SVM)",
"Batarseh F.A., Perini D., Wani Q., Freeman L.","Explainable Artificial Intelligence for Technology Policy Making Using Attribution Networks","10.1007/978-3-031-08421-8_43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135067345&doi=10.1007%2f978-3-031-08421-8_43&partnerID=40&md5=57bdee25b66adc88e9504e17192fce04","In this manuscript, we propose an alternative to conventional policy making procedures. The presented policy pipeline leverages intelligent methods that factor for causal relations and economic factors to produce explainable outcomes. Attribution-based methods for analyzing the effects of technology policies are deployed for all American states. Legal codes are analyzed using natural language processing methods to detect similarity, and K-nearest neighbor (Knn) is applied to group laws by influence on state’s technological descriptors, such as broadband and internet use. Additionally, we classify which laws are excitatory and which ones are inhibitory regarding the overall quality of technology services. Our pipeline allows for explaining the ‘goodness of a policy’ using task-based and end-to-end learning; a notion that has not been explored prior. Data are collected from multiple state statutes, intelligent models are developed, experimental work is performed, and the results are presented and discussed. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Attributions; Data-driven law; Public policy; XAI","Decision making; Natural language processing systems; Nearest neighbor search; Public policy; Attribution; Causal relations; Data driven; Data-driven law; Economic factors; Intelligent method; Natural languages; Policy making; Technology policy; XAI; Pipelines"
"Batarseh F.A., Freeman L., Huang C.-H.","A survey on artificial intelligence assurance","10.1186/s40537-021-00445-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104988842&doi=10.1186%2fs40537-021-00445-7&partnerID=40&md5=133ef6a606fa4249eda696ab1158f466","Artificial Intelligence (AI) algorithms are increasingly providing decision making and operational support across multiple domains. AI includes a wide (and growing) library of algorithms that could be applied for different problems. One important notion for the adoption of AI algorithms into operational decision processes is the concept of assurance. The literature on assurance, unfortunately, conceals its outcomes within a tangled landscape of conflicting approaches, driven by contradicting motivations, assumptions, and intuitions. Accordingly, albeit a rising and novel area, this manuscript provides a systematic review of research works that are relevant to AI assurance, between years 1985 and 2021, and aims to provide a structured alternative to the landscape. A new AI assurance definition is adopted and presented, and assurance methods are contrasted and tabulated. Additionally, a ten-metric scoring system is developed and introduced to evaluate and compare existing methods. Lastly, in this manuscript, we provide foundational insights, discussions, future directions, a roadmap, and applicable recommendations for the development and deployment of AI assurance. © 2021, The Author(s).","AI assurance; Data Engineering; Explainable AI (XAI); Validation and verification",
"Batchelor C., Corbett P., Day A., White J., Boyle J.","Chapter 4: Deep Learning and Chemical Data","10.1039/9781788016841-00045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097492102&doi=10.1039%2f9781788016841-00045&partnerID=40&md5=b514f7ee2708aff281a8d49b5c66ec3b","Deep learning, machine learning that uses multilayer neural networks, has been responsible for significant advances in performance on standard tasks in image processing, machine translation and speech recognition in recent years. This is at least in part due to breakthroughs in algorithms for training neural networks and the widespread availability of GPUs. In this chapter we outline some tasks that use chemical data and practical examples of how deep learning has been used. The first of these is to infer facts about chemical structures from the corresponding NMR spectra. The second and third are based on chemical text. We describe entries to the BioCreative series of text-mining competitions, both by some of the present authors and outside the group - identifying chemical names and chemical-protein interactions. We discuss different appraoches to deep learning, a more traditional approach based on careful feature selection and engineering, and ones where a very simple underlying representation is chosen and the feature set is learnt by the system. We also compare the performance of deep learning algorithms to human annotators. © The Royal Society of Chemistry 2021.",,"algorithm; chemical interaction; competition; conformation; controlled study; deep learning; feature selection; human; human experiment; mining; nuclear magnetic resonance"
"Batet M., Valls A., Gibert K.","Performance of ontology-based semantic similarities in clustering","10.1007/978-3-642-13208-7_36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955359974&doi=10.1007%2f978-3-642-13208-7_36&partnerID=40&md5=9903927001295b1f7006d8d02278d978","Clustering was usually applied on numerical and categorical information. However, textual information is acquiring an increasing importance with the appearance of methods for textual data mining. This paper proposes the use of classical clustering algorithms with a mixed function that combines numerical, categorical and semantic features. The content of the semantic features is extracted from textual data. This paper analyses and compares the behavior of different existing semantic similarity functions that use WordNet as background ontology. The different partitions obtained with the clustering algorithm are compared to human classifications in order to see which one approximates better the human reasoning. Moreover, the interpretability of the obtained clusters is discussed. The results show that those similarity measures that provide better results when compared using a standard benchmark also provide better and more interpretable partitions. © 2010 Springer-Verlag.","Clustering; ontologies; semantic similarity","Human reasoning; Interpretability; Ontology-based; Semantic features; Semantic similarity; Similarity measure; Textual data; Textual information; Wordnet; Artificial intelligence; Natural language processing systems; Ontology; Semantics; Soft computing; Clustering algorithms"
"Batista A.R., Bertolini D., Costa Y.M.G., Pereira L.F.M., Pereira R.M., Teixeira L.O.","An Evaluation of Segmentation Techniques for Covid-19 Identification in Chest X-Ray","10.1007/978-3-030-93420-0_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124252512&doi=10.1007%2f978-3-030-93420-0_5&partnerID=40&md5=91ed324d07b87c66b04284b63b32b2e9","COVID-19 is a highly contagious disease caused by the SARS-CoV-2 virus. Due to its high impact on society, several efforts have been made to design practical ways to support COVID-19 diagnosis. In this context, automated solutions based on chest x-rays (CXR) images and deep learning are among the popular ones. Although these techniques achieved exciting results in the literature, the use of regions that do not support pneumonia diagnosis, i.e., regions outside the lung area, may bias the recognition model. A strategy to avoid this issue is to use segmentation techniques to isolate the lung area before the classification process. In this work, we investigate the impact of three CNN segmentation architectures on COVID-19 identification: U-Net, MultiResUnet, and BCDU-NET. We also investigate which portions of the CXR most influence each model’s predictions, using Explainable Artificial Intelligence. The BCDU-NET architecture achieved a Jaccard Index of 0.91 and a Dice Coefficient of 0.95. In the best scenario, lung segmentation improved the COVID-19 identification F1-Score by about 6.6%. © 2021, Springer Nature Switzerland AG.","COVID-19; Explainable artificial intelligence; Semantic segmentation","Biological organs; Deep learning; Diseases; SARS; Semantic Segmentation; Semantics; Automated solutions; Chest X-ray image; Contagious disease; COVID-19; Evaluation of segmentation; Explainable artificial intelligence; High impact; Recognition models; Segmentation techniques; Semantic segmentation; Diagnosis"
"Batista V.F.L., Pintado F.P., Gil A.B., Rodríguez S., Moreno M.N.","A system for multi-label classification of learning objects","10.1007/978-3-642-19644-7_55","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052934150&doi=10.1007%2f978-3-642-19644-7_55&partnerID=40&md5=f7a9223db9cca34b00fffb1848fb672d","The rapid evolution within the context of e-learning is closely linked to international efforts on the standardization of Learning Object (LO), which provides ubiquitous access to multiple and distributed educational resources in many repositories. This article presents a system that enables the recovery and classification of LO and provides individualized help with selecting learning materials to make the most suitable choice among many alternatives. For this classification, it is used a special multi-label data mining designed for the LO ranking tasks. According to each position, the system is responsible for presenting the results to the end user. The learning process is supervised, using two major tasks in supervised learning from multi-label data: multi-label classification and label ranking. © 2011 Springer-Verlag Berlin Heidelberg.","label ranking; learning object; multi-label classification; multi-label data mining","Educational resource; End users; Learning materials; Learning objects; Learning process; Multi-label; Ubiquitous access; Data mining; E-learning; Soft computing; Ubiquitous computing; Distributed computer systems"
"Batmanghelich N., Dong A., Taskar B., Davatzikos C.","Regularized tensor factorization for multi-modality medical image classification","10.1007/978-3-642-23626-6_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-82255164591&doi=10.1007%2f978-3-642-23626-6_3&partnerID=40&md5=9b9b24c4f9e80f9162a0eae69e69bedd","This paper presents a general discriminative dimensionality reduction framework for multi-modal image-based classification in medical imaging datasets. The major goal is to use all modalities simultaneously to transform very high dimensional image to a lower dimensional representation in a discriminative way. In addition to being discriminative, the proposed approach has the advantage of being clinically interpretable. We propose a framework based on regularized tensor decomposition. We show that different variants of tensor factorization imply various hypothesis about data. Inspired by the idea of multi-view dimensionality reduction in machine learning community, two different kinds of tensor decomposition and their implications are presented. We have validated our method on a multi-modal longitudinal brain imaging study. We compared this method with a publically available classification software based on SVM that has shown state-of-the-art classification rate in number of publications. © 2011 Springer-Verlag.","Basis Learning; Classification; Multi-Modality; Multi-view Learning; Optimization; Tensor factorization","Basis Learning; Brain imaging; Classification rates; Data sets; Dimensional representation; Dimensionality reduction; High-dimensional images; Image-based classification; Machine learning communities; Medical image classification; Multi-modal; Multi-modality; Multi-view learning; Multi-views; Software-based; Tensor decomposition; Tensor factorization; Classification (of information); Factorization; Medical computing; Optimization; Tensors; Medical imaging; aged; algorithm; article; automated pattern recognition; brain; brain mapping; cognitive defect; computer program; diagnostic imaging; diffusion tensor imaging; factual database; human; image processing; methodology; nuclear magnetic resonance imaging; pathology; statistical model; Aged; Algorithms; Brain; Brain Mapping; Cognition Disorders; Databases, Factual; Diagnostic Imaging; Diffusion Tensor Imaging; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Models, Statistical; Pattern Recognition, Automated; Software"
"Batmanghelich N.K., Taskar B., Davatzikos C.","Generative-discriminative basis learning for medical imaging","10.1109/TMI.2011.2162961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855382390&doi=10.1109%2fTMI.2011.2162961&partnerID=40&md5=62db8a23560ce9cdda4d56f66e111b2e","This paper presents a novel dimensionality reduction method for classification in medical imaging. The goal is to transform very high-dimensional input (typically, millions of voxels) to a low-dimensional representation (small number of constructed features) that preserves discriminative signal and is clinically interpretable. We formulate the task as a constrained optimization problem that combines generative and discriminative objectives and show how to extend it to the semi-supervised learning (SSL) setting. We propose a novel large-scale algorithm to solve the resulting optimization problem. In the fully supervised case, we demonstrate accuracy rates that are better than or comparable to state-of-the-art algorithms on several datasets while producing a representation of the group difference that is consistent with prior clinical reports. Effectiveness of the proposed algorithm for SSL is evaluated with both benchmark and medical imaging datasets. In the benchmark datasets, the results are better than or comparable to the state-of-the-art methods for SSL. For evaluation of the SSL setting in medical datasets, we use images of subjects with mild cognitive impairment (MCI), which is believed to be a precursor to Alzheimer's disease (AD), as unlabeled data. AD subjects and normal control (NC) subjects are used as labeled data, and we try to predict conversion from MCI to AD on follow-up. The semi-supervised extension of this method not only improves the generalization accuracy for the labeled data (AD/NC) slightly but is also able to predict subjects which are likely to converge to AD. © 2011 IEEE.","Basis learning; classification; feature construction; generative-discriminative learning; machine learning; matrix factorization; morphological pattern analysis; optimization; semi-supervised learning; sparsity","Basis learning; feature construction; generative-discriminative learning; Machine-learning; Matrix factorizations; Morphological patterns; Semi-supervised learning; sparsity; Classification (of information); Constrained optimization; Learning algorithms; Optimization; Supervised learning; Medical imaging; algorithm; Alzheimer disease; article; artificial intelligence; automated pattern recognition; diagnostic imaging; discriminant analysis; factual database; human; methodology; neuroimaging; nuclear magnetic resonance imaging; pathology; Algorithms; Alzheimer Disease; Artificial Intelligence; Databases, Factual; Diagnostic Imaging; Discriminant Analysis; Humans; Magnetic Resonance Imaging; Neuroimaging; Pattern Recognition, Automated"
"Batra R., Mahajan M.","Deep Learning Models: An Understandable Interpretable Approach","10.1007/978-981-16-6186-0_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127836116&doi=10.1007%2f978-981-16-6186-0_10&partnerID=40&md5=db39b61debcb6089250d67736421100c","In many of applications which are mainly related to image, text, audio, video, graphics etc. mainly consist of task like classification and prediction. Classification and prediction tasks are to be done in different manner. The best level accuracy can be achieved by deep neural networks. The networks mainly works likes black box function approximations that make a good prediction of output for a given value of input. In various applications like medical diagnosis, planning and control where a high level of trust needed on machine output, deep neural networks proved to be best. In order to evaluate uncertainty of particular output value statistical matrices can be used. However concept of trust on a particular output also rely on visibility that a human is also en route towards working of machine. In the same context, neural networks provides well founded concepts which are human understandable for an output which describes perception about inner working. Interpretability of deep learning models is an immovable concept. As human understanding exists at different levels, there also exist a multitude of different dimensions that collectively define the concept of interpretability of deep learning models. Interpretability can be described either in terms of network parameters or in term of input attributes. This chapter mainly describes the dimensions which are convenient for interpretability of a model. Based on these dimensions, classification of prior work can be done. By means of describing the interpretability, analysis of gap can be done that must be filled up for gaining more interpretability of model. Interpretability of a model can be easily described by understanding model transparency. Model transparency can be further achieved by calculating the model simulatability, computability and transparency of algorithm used. Similarly, model functionality can be expressed in terms of visualization of parameters used in deep learning model, textual description in terms of input/output values and explanation of mapping of input values to the trusted output values. This chapter also describes the various challenges that come across while federation of models or on the time of using different approaches for attaining interpretability of models. Major challenges may be in form of fairness of models and accountability of models which are to be describes in detail in this chapter. Interpretability is more generous term to be used in deep learning model than that of explain ability. Interpretability of model can be related to Bayesian approach as the concept can be extended to Bayesian model. Both Bayesian framework and deep learning frame work are complement to each other. Model transparency and model functionality are two main terms used for interpretability. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Deep learning; Functionality; Interpretability; Model; Neural networks","Deep neural networks; Diagnosis; Forecasting; Text processing; Transparency; Audio videos; Classification tasks; Deep learning; Functionality; Image texts; Interpretability; Learning models; Model transparency; Neural-networks; Output values; Bayesian networks"
"Batsis G., Partsinevelos P., Stavrakakis G.","A deep learning and gis approach for the optimal positioning of wave energy converters","10.3390/en14206773","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117335353&doi=10.3390%2fen14206773&partnerID=40&md5=6367068504c1be5cd46262c36b80b2a9","Renewable Energy Sources provide a viable solution to the problem of ever-increasing climate change. For this reason, several countries focus on electricity production using alternative sources. In this paper, the optimal positioning of the installation of wave energy converters is examined taking into account geospatial and technical limitations. Geospatial constraints depend on Land Use classes and seagrass of the coastal areas, while technical limitations include meteorological conditions and the morphology of the seabed. Suitable installation areas are selected after the exclusion of points that do not meet the aforementioned restrictions. We implemented a Deep Neural Network that operates based on heterogeneous data fusion, in this case satellite images and time series of meteorological data. This fact implies the definition of a two-branches architecture. The branch that is trained with image data provides for the localization of dynamic geospatial classes in the potential installation area, whereas the second one is responsible for the classification of the region according to the potential wave energy using wave height and period time series. In making the final decision on the suitability of the potential area, a large number of static land use data play an important role. These data are combined with neural network predictions for the optimizing positioning of the Wave Energy Converters. For the sake of completeness and flexibility, a Multi-Task Neural Network is developed. This model, in addition to predicting the suitability of an area depending on seagrass patterns and wave energy, also predicts land use classes through Multi-Label classification process. The proposed methodology is applied in the marine area of the city of Sines, Portugal. The first neural network achieves 98.7% Binary Classification accuracy, while the Multi-Task Neural Network 97.5% in the same metric and 93.5% in the F1 score of the Multi-Label classification output. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep neural networks; Renewable energy sources; Sentinel satellite imagery; Spatial planning; Wave energy converters","Classification (of information); Climate change; Data fusion; Geographic information systems; Land use; Natural resources; Plants (botany); Satellite imagery; Time series; Wave energy conversion; Geo-spatial; Land use class; Neural-networks; Optimal positioning; Renewable energy source; Seagrasses; Sentinel satellite imagery; Spatial planning; Technical limitations; Wave energy converters; Deep neural networks"
"Battineni G., Chintalapudi N., Zacharewicz G.","Process Mining in Clinical Practice: Model Evaluations in the Central Venous Catheter Installation Training","10.3390/a15050153","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130378427&doi=10.3390%2fa15050153&partnerID=40&md5=68048b2b88d325b248195274791fedd4","An acknowledgment of feedback is extremely helpful in medical training, as it may improve student skill development and provide accurate, unbiased feedback. Data are generated by hundreds of complicated and variable processes within healthcare including treatments, lab results, and internal logistics. Additionally, it is crucial to analyze medical training data to improve opera-tional processes and eliminate bottlenecks. Therefore, the use of process mining (PM) along with conformance checking allows healthcare trainees to gain knowledge about instructor training. Re-searchers find it challenging to analyze the conformance between observations from event logs and predictions from models with artifacts from the training process. To address this conformance check, we modeled student activities and performance patterns in the training of Central Venous Catheter (CVC) installation. This work aims to provide medical trainees with activities with easy and interpretable outcomes. The two independent techniques for mining process models were fuzzy (i.e., for visualizing major activities) and inductive (i.e., for conformance checking at low threshold noise levels). A set of 20 discrete activity traces was used to validate conformance checks. Results show that 97.8% of the fitness of the model and the movement of the model occurred among the nine activities. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","CVC; fuzzy mining; inductive mining; medical training; Petri net; process mining","Catheters; Health care; Medical computing; Petri nets; Students; Central venoi catheter; Clinical practices; Conformance checking; Fuzzy mining; Inductive mining; Medical training; Model evaluation; Process mining; Skills development; Student skills; Data mining"
"Battineni G., Sagaro G.G., Chinatalapudi N., Amenta F.","Applications of machine learning predictive models in the chronic disease diagnosis","10.3390/jpm10020021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083107877&doi=10.3390%2fjpm10020021&partnerID=40&md5=0663bda85ba7e450c917decad8f7e74a","This paper reviews applications of machine learning (ML) predictive models in the diagnosis of chronic diseases. Chronic diseases (CDs) are responsible for a major portion of global health costs. Patients who suffer from these diseases need lifelong treatment. Nowadays, predictive models are frequently applied in the diagnosis and forecasting of these diseases. In this study, we reviewed the state-of-the-art approaches that encompass ML models in the primary diagnosis of CD. This analysis covers 453 papers published between 2015 and 2019, and our document search was conducted from PubMed (Medline), and Cumulative Index to Nursing and Allied Health Literature (CINAHL) libraries. Ultimately, 22 studies were selected to present all modeling methods in a precise way that explains CD diagnosis and usage models of individual pathologies with associated strengths and limitations. Our outcomes suggest that there are no standard methods to determine the best approach in real-time clinical practice since each method has its advantages and disadvantages. Among the methods considered, support vector machines (SVM), logistic regression (LR), clustering were the most commonly used. These models are highly applicable in classification, and diagnosis of CD and are expected to become more important in medical practice in the near future. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Accuracy; Chronic diseases; Disease classification; Pathologies; Prediction models","age; Alzheimer disease; artificial neural network; chronic disease; chronic obstructive lung disease; decision making; dementia; depression; diagnostic accuracy; disease severity; feature extraction; fibromyalgia; hepatitis B; human; k nearest neighbor; logistic regression analysis; machine learning; memory; prediction; quality control; Review; subjective memory; support vector machine; systematic review"
"Bauckhage C., Sifa R., Dong T.","Prototypes Within Minimum Enclosing Balls","10.1007/978-3-030-30493-5_36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072948172&doi=10.1007%2f978-3-030-30493-5_36&partnerID=40&md5=cedd3d57fcf3cf27e51126b93ec4fe32","We revisit the kernel minimum enclosing ball problem and show that it can be solved using simple recurrent neural networks. Once solved, the interior of a ball can be characterized in terms of a function of a set of support vectors and local minima of this function can be thought of as prototypes of the data at hand. For Gaussian kernels, these minima can be naturally found via a mean shift procedure and thus via another recurrent neurocomputing process. Practical results demonstrate that prototypes found this way are descriptive, meaningful, and interpretable. © Springer Nature Switzerland AG 2019.",,"Artificial intelligence; Computer science; Computers; Gaussian kernels; Local minimums; Mean shift; Minimum enclosing ball; Neurocomputing; Simple recurrent neural networks; Support vector; Recurrent neural networks"
"Bauckhage C., Ojeda C., Schücker J., Sifa R., Wrobel S.","Informed machine learning through functional composition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053622545&partnerID=40&md5=9044d51f3cc4309e9e619122ae79d6ee","Addressing general problems with applied machine learning, we sketch an approach towards informed learning. The general idea is to treat data driven learning not as a parameter estimation problem but as a problem of sequencing predefined operations. We show by means of an example that this allows for incorporating expert knowledge and leads to traceable or explainable decision making systems. © 2018 CEUR-WS. All Rights Reserved.",,"Artificial intelligence; Decision making; Applied machine learning; Data driven; Decision-making systems; Expert knowledge; Functional compositions; Parameter estimation problems; Learning systems"
"Bauckhage C., Sifa R.","k-maxoids clustering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944313520&partnerID=40&md5=9171d49c018d229b205adf2ca760a8ec","We explore the idea of clustering according to extremal rather than to central data points. To this end, we introduce the notion of the maxoid of a data set and present an algorithm for k-maxoids clustering which can be understood as a variant of classical k-means clustering. Exemplary results demonstrate that extremal cluster prototypes are more distinctive and hence more interpretable than central ones. Copyright © 2015 by the papers authors.",,"Artificial intelligence; Clustering algorithms; Information management; Knowledge management; Learning systems; Search engines; Classical k-means; Cluster prototype; Data points; Data set; Extremal; Data mining"
"Baudoin Y.","Robotic assistance to counter improvised cbe (Chemical, biological, explosive) threats","10.13180/clawar.2020.24-26.08.p03","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091277984&doi=10.13180%2fclawar.2020.24-26.08.p03&partnerID=40&md5=af0a519415b17db5167f96d4a44ee716","A natural or human-caused ‘disaster’ usually is a vast area where massive and diverse evidences must be located, collected and transferred for analysis to a distant reference laboratory. Every single detail may be crucial for the identification of the causes and/or the criminal action of groups responsible of a potential risk caused by improvised chemical, biological and explosive devices. Nevertheless, the main difficulty is to determine which kind of evidence to look for. A detailed knowledge of similar events/contamination process could be helpful results of a threat analysis (prediction) will be useful (preparation of the plot, logistics, targeting, action, post-action steps-prevention). Cooperative multi-robot systems based on UAV and UGV equipped with modular sensors and intervention tools (detection, measurements) will carry out the screening, searching and collection of samples in the “hot zones” where the first responders could be exposed to dangerous and hazardous agents. Artificial intelligence techniques applied to the acquired data will contribute to improve detection and classification of evidentiary material and discrimination of mixed evidence, all of them critical for forensic investigations. Data fusion and artificial intelligent techniques will also be applied for predicting the dispersion patterns of some biological agents, providing in this way, a useful decision making tool for first-responders. © CLAWAR Association Ltd.",,"Agricultural robots; Aircraft detection; Data fusion; Decision making; Mobile robots; Multipurpose robots; Robotics; Screening; Artificial intelligence techniques; Artificial intelligent techniques; Biological agents; Cooperative multi-robot systems; Decision making tool; Dispersion patterns; Explosive devices; Forensic investigation; Explosives"
"Bauer M., Buchtala O., Horeis T., Kern R., Sick B., Wagner R.","Technical data mining with evolutionary radial basis function classifiers","10.1016/j.asoc.2008.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58849108463&doi=10.1016%2fj.asoc.2008.07.007&partnerID=40&md5=9436dd8f3842fec0af1b2c9eaa64ce22","This article deals with two key problems of data mining, the automation of the data mining process and the integration of human domain experts. We show how an evolutionary algorithm (EA) can be used to optimize radial basis function (RBF) neural networks used for classification tasks. First, input features will be chosen from a set of possible input features (feature selection). Second, the number of hidden neurons is adapted (model selection). It is known that interpretable (fuzzy-type) rule sets may be extracted from RBF networks. We show how appropriate training algorithms for RBF networks and penalty terms in the fitness function of the EA may improve the understandability of the extracted rules. The properties of our approach are set out by means of two industrial application examples (process identification and quality control). © 2008 Elsevier B.V. All rights reserved.","Data mining; Evolutionary algorithm; Feature selection; Interpretability; Knowledge extraction; Model selection; Radial basis function neural network; Understandability","Algorithms; Attitude control; Customer satisfaction; Data mining; Evolutionary algorithms; Feature extraction; Feedforward neural networks; Function evaluation; Functions; Image segmentation; Industrial applications; Information management; Knowledge based systems; Knowledge management; Mining; Neural networks; Probability density function; Quality assurance; Quality function deployment; Total quality management; Evolutionary algorithm; Feature selection; Interpretability; Knowledge extraction; Model selection; Radial basis function neural network; Understandability; Radial basis function networks"
"Bäuerle A., Cabrera Á.A., Hohman F., Maher M., Koski D., Suau X., Barik T., Moritz D.","Symphony: Composing Interactive Interfaces for Machine Learning","10.1145/3491102.3502102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130538783&doi=10.1145%2f3491102.3502102&partnerID=40&md5=c5aac9a87af836b8cfeba5fb622b4b64","Interfaces for machine learning (ML), information and visualizations about models or data, can help practitioners build robust and responsible ML systems. Despite their benefits, recent studies of ML teams and our interviews with practitioners (n=9) showed that ML interfaces have limited adoption in practice. While existing ML interfaces are effective for specific tasks, they are not designed to be reused, explored, and shared by multiple stakeholders in cross-functional teams. To enable analysis and communication between different ML practitioners, we designed and implemented Symphony, a framework for composing interactive ML interfaces with task-specific, data-driven components that can be used across platforms such as computational notebooks and web dashboards. We developed Symphony through participatory design sessions with 10 teams (n=31), and discuss our findings from deploying Symphony to 3 production ML projects at Apple. Symphony helped ML practitioners discover previously unknown issues like data duplicates and blind spots in models while enabling them to share insights with other stakeholders. © 2022 Owner/Author.","AI; computational notebooks; documentation; interactive programming; Machine learning; visualization","Machine learning; Computational notebook; Cross-functional teams; Documentation; Interactive interfaces; Interactive machine learning; Interactive programming; Machine learning systems; Machine-learning; Multiple stakeholders; Specific tasks; Visualization"
"Baum K., Mantel S., Schmidt E., Speith T.","From Responsibility to Reason-Giving Explainable Artificial Intelligence","10.1007/s13347-022-00510-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125292638&doi=10.1007%2fs13347-022-00510-w&partnerID=40&md5=ce9734dc8fe83435d448e0939457d5cc","We argue that explainable artificial intelligence (XAI), specifically reason-giving XAI, often constitutes the most suitable way of ensuring that someone can properly be held responsible for decisions that are based on the outputs of artificial intelligent (AI) systems. We first show that, to close moral responsibility gaps (Matthias 2004), often a human in the loop is needed who is directly responsible for particular AI-supported decisions. Second, we appeal to the epistemic condition on moral responsibility to argue that, in order to be responsible for her decision, the human in the loop has to have an explanation available of the system’s recommendation. Reason explanations are especially well-suited to this end, and we examine whether—and how—it might be possible to make such explanations fit with AI systems. We support our claims by focusing on a case of disagreement between human in the loop and AI system. © 2022, The Author(s).","Decision support systems; Explainable artificial intelligence; Moral responsibility; Reason explanations; Reasons; Responsibility gap",
"Baum T., Herbold S., Schneider K.","GIMO: A multi-objective anytime rule mining system to ease iterative feedback from domain experts","10.1016/j.eswax.2020.100040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090830401&doi=10.1016%2fj.eswax.2020.100040&partnerID=40&md5=0c2a7764b294fd08cc3c164779f2860d","Data extracted from software repositories is used intensively in Software Engineering research, for example, to predict defects in source code. In our research in this area, with data from open source projects as well as an industrial partner, we noticed several shortcomings of conventional data mining approaches for classification problems: (1) Domain experts’ acceptance is of critical importance, and domain experts can provide valuable input, but it is hard to use this feedback. (2) Evaluating the quality of the model is not a matter of calculating AUC or accuracy. Instead, there are multiple objectives of varying importance with hard to quantify trade-offs. Furthermore, the performance of the model cannot be evaluated on a per-instance level in our case, because it shares aspects with the set cover problem. To overcome these problems, we take a holistic approach and develop a rule mining system that simplifies iterative feedback from domain experts and can incorporate the domain-specific evaluation needs. A central part of the system is a novel multi-objective anytime rule mining algorithm. The algorithm is based on the GRASP-PR meta-heuristic but extends it with ideas from several other approaches. We successfully applied the system in the industrial context. In the current article, we focus on the description of the algorithm and the concepts of the system. We make an implementation of the system available. © 2020 The Authors","Explainable artificial intelligence; Human-in-the-loop; Interpretable artificial intelligence; Multi-objective; Rule mining; Set cover","Economic and social effects; Electronic trading; Industrial research; Iterative methods; Knowledge acquisition; Mining machinery; Open source software; Conventional data mining; Holistic approach; Industrial context; Industrial partners; Multiple-objectives; Open source projects; Rule mining algorithms; Software repositories; Data mining"
"Baumann P.F.M., Hothorn T., Rügamer D.","Deep Conditional Transformation Models","10.1007/978-3-030-86523-8_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115721948&doi=10.1007%2f978-3-030-86523-8_1&partnerID=40&md5=7c3d47166851a9ff71683099f5db3b7b","Learning the cumulative distribution function (CDF) of an outcome variable conditional on a set of features remains challenging, especially in high-dimensional settings. Conditional transformation models provide a semi-parametric approach that allows to model a large class of conditional CDFs without an explicit parametric distribution assumption and with only a few parameters. Existing estimation approaches within this class are, however, either limited in their complexity and applicability to unstructured data sources such as images or text, lack interpretability, or are restricted to certain types of outcomes. We close this gap by introducing the class of deep conditional transformation models which unifies existing approaches and allows to learn both interpretable (non-)linear model terms and more complex neural network predictors in one holistic framework. To this end we propose a novel network architecture, provide details on different model definitions and derive suitable constraints as well as network regularization terms. We demonstrate the efficacy of our approach through numerical experiments and applications. © 2021, Springer Nature Switzerland AG.","Deep learning; Distributional regression; Normalizing flows; Semi-structured regression; Transformation models","Complex networks; Deep learning; Distribution functions; Linear transformations; Metadata; Conditional transformations; Cumulative distribution function; Deep learning; Distributional regression; High-dimensional; Normalizing flow; Semi-structured; Semi-structured regression; Sets of features; Transformation modeling; Network architecture"
"Baumeister T., Finkbeiner B., Torfah H.","Explainable Reactive Synthesis","10.1007/978-3-030-59152-6_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093859223&doi=10.1007%2f978-3-030-59152-6_23&partnerID=40&md5=fd968b31a9f7420ad287171490d532e5","Reactive synthesis transforms a specification of a reactive system, given in a temporal logic, into an implementation. The main advantage of synthesis is that it is automatic. The main disadvantage is that the implementation is usually very difficult to understand. In this paper, we present a new synthesis process that explains the synthesized implementation to the user. The process starts with a simple version of the specification and a corresponding simple implementation. Then, desired properties are added one by one, and the corresponding transformations, repairing the implementation, are explained in terms of counterexample traces. We present SAT-based algorithms for the synthesis of repairs and explanations. The algorithms are evaluated on a range of examples including benchmarks taken from the SYNTCOMP competition. © 2020, Springer Nature Switzerland AG.","Reactive synthesis; SAT-based synthesis; Temporal logic","Artificial intelligence; Computer science; Computers; Reactive synthesis; Reactive system; SAT-based; Synthesis process; Specifications"
"Baumgarte F., Keller R., Röhrich F., Valett L., Zinsbacher D.","Revealing influences on carsharing users’ trip distance in small urban areas","10.1016/j.trd.2022.103252","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126362293&doi=10.1016%2fj.trd.2022.103252&partnerID=40&md5=e62f12a58619148a94c5139ca58dd85d","Carsharing is an essential part of the transformation towards sustainable mobility in smaller urban areas. To expand their services and the positive social and environmental benefits, carsharing operators must understand their users' travel behavior. To accelerate this understanding, we analyze usage data of a station-based carsharing service from a small city in Germany with machine learning and explainable artificial intelligence to reveal influencing factors on the trip distance. The resulting four overarching groups are personal characteristics, time-related, car-related, and environmental features. We further analyze the driving distance of several subgroups split by personal and time-related features. Our findings highlight the importance of time-related features for the trip distance of carsharing users in all subgroups. We also discuss the influence of non-time-related features on the user groups. With these results, we derive valuable insights for research and carsharing operators by understanding patterns in individual user behavior in smaller urban areas. © 2022 Elsevier Ltd","Carsharing; Explainable artificial intelligence; Feature importance; Machine learning; Trip distance; User behavior","Behavioral research; Carsharing; Environmental benefits; Explainable artificial intelligence; Feature importance; Social and environmental; Social benefits; Sustainable mobility; Trip distances; Urban areas; User behaviors; Machine learning; artificial intelligence; machine learning; transportation development; travel behavior; urban area; urban transport; Germany"
"Baumung W., Baumung V.","Application of machine learning and vision for real-time condition monitoring and acceleration of product development cycles","10.1016/j.promfg.2020.11.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100766330&doi=10.1016%2fj.promfg.2020.11.012&partnerID=40&md5=e9d241a3ee130c2b2dd54e5584b5f9b8","Development work within an experimental environment, in which certain properties are investigated and optimized, requires many test runs and is therefore often associated with long execution times, costs and risks. This can affect product, material and technology development in industry and research. New digital driver technologies offer the possibility to automate complex manual work steps in a cost-effective way, to increase the relevance of the results and to accelerate the processes many times over. In this context, this article presents a low-cost, modular and open-source machine vision system for test execution and evaluates it on the basis of a real industrial application. For this purpose a methodology for the automated execution of the load intervals, the process documentation and for the evaluation of the generated data by means of machine learning to classify wear levels. The software and the mechanical structure are designed to be adaptable to different conditions, components and for a variety of tasks in industry and research. The mechanical structure is required for tracking the test object and represents a motion platform with independent positioning by machine vision operators or machine learning. An evaluation of the state of the test object is performed by the transfer learning after the initial documentation run. The manual procedure for classifying the visually recorded data on the state of the test object is described for the training material. This leads to an increased resource efficiency on the material as well as on the personnel side since on the one hand the significance of the tests performed is increased by the continuous documentation and on the other hand the responsible experts can be assigned time efficiently. The presence and know-how of the experts are therefore only required for defined and decisive events during the execution of the experiments. Furthermore, the generated data are suitable for later use as an additional source of data for predictive maintenance of the developed object. © 2020 The Authors. Published by Elsevier B.V.","Industrial application; Machine learning; Machine vision",
"Baur T., Heimerl A., Lingenfelser F., Wagner J., Valstar M.F., Schuller B., André E.","eXplainable Cooperative Machine Learning with NOVA","10.1007/s13218-020-00632-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088013979&doi=10.1007%2fs13218-020-00632-3&partnerID=40&md5=ef814029423772ba8e9d71d099cf5c9b","In the following article, we introduce a novel workflow, which we subsume under the term “explainable cooperative machine learning” and show its practical application in a data annotation and model training tool called NOVA. The main idea of our approach is to interactively incorporate the ‘human in the loop’ when training classification models from annotated data. In particular, NOVA offers a collaborative annotation backend where multiple annotators join their workforce. A main aspect is the possibility of applying semi-supervised active learning techniques already during the annotation process by giving the possibility to pre-label data automatically, resulting in a drastic acceleration of the annotation process. Furthermore, the user-interface implements recent eXplainable AI techniques to provide users with both, a confidence value of the automatically predicted annotations, as well as visual explanation. We show in an use-case evaluation that our workflow is able to speed up the annotation process, and further argue that by providing additional visual explanations annotators get to understand the decision making process as well as the trustworthiness of their trained machine learning models. © 2020, The Author(s).","Annotation; Cooperative machine learning; Explainable AI","Classification (of information); Decision making; Machine learning; Petroleum reservoir evaluation; Annotation; Classification models; Cooperative machine learning; Data annotation; Explainable AI; Human-in-the-loop; Machine-learning; Model training; Training tools; Work-flows; User interfaces"
"Baur T., Clausen S., Heimerl A., Lingenfelser F., Lutz W., André E.","NOVA: A Tool for Explanatory Multimodal Behavior Analysis and Its Application to Psychotherapy","10.1007/978-3-030-37734-2_47","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080886676&doi=10.1007%2f978-3-030-37734-2_47&partnerID=40&md5=d2769618b15d67781c08749d70ce39cf","In this paper, we explore the benefits of our next-generation annotation and analysis tool NOVA in the domain of psychotherapy. The NOVA tool has been developed, tested and applied in behaviour studies for several years and psychotherapy sessions offer a great way to expand areas of application into a challenging yet promising field. In such scenarios, interactions with patients are often rated by questionnaires and the therapist’s subjective rating, yet a qualitative analysis of the patient’s non-verbal behaviours can only be performed in a limited way as this is very expensive and time-consuming. A main aspect of NOVA is the possibility of applying semi-supervised active learning where Machine Learning techniques are already used during the annotation process by giving the possibility to pre-label data automatically. Furthermore, NOVA provides therapists with a confidence value of the automatically predicted annotations. This way, also non-ML experts get to understand whether they can trust their ML models for the problem at hand. © 2020, Springer Nature Switzerland AG.","Annotation tools; Cooperative Machine Learning; Explainable AI; Psychotherapy","Semi-supervised learning; Surveys; Annotation tool; Machine learning techniques; Multimodal behavior analysis; Non-verbal behaviours; Psychotherapy; Qualitative analysis; Semi-supervised active learning; Subjective rating; Learning systems"
"Baur T., Heimerl A., Lingenfelser F., Andre E.","I see what you did there: Understanding when to trust a ML model with NOVA","10.1109/ACIIW.2019.8925214","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077815070&doi=10.1109%2fACIIW.2019.8925214&partnerID=40&md5=231708b8b56cff5b0deb6ac4a0c497d5","In this demo paper we present NOVA, a machine learning and explanation interface that focuses on the automated analysis of social interactions. NOVA combines Cooperative Machine Learning (CML) and explainable AI (XAI) methods to reduce manual labelling efforts while simultaneously generating an intuitive understanding of the learning process of a classification system. Therefore, NOVA features a semi-automated labelling process in which users are provided with immediate visual feedback on the predictions, which gives insights into the strengths and weaknesses of the underlying classification system. Following an interactive and exploratory workflow, the performance of the model can be improved by manual revision of the predictions. © 2019 IEEE.","annotation tools; cooperative machine learning; explainable AI","Automation; Machine learning; Visual communication; Annotation tool; Automated analysis; Classification system; Explanation interfaces; Intuitive understanding; Learning process; Social interactions; Visual feedback; Intelligent computing"
"Bautista-Montesano R., Bustamante-Bello R., Ramirez-Mendoza R.A.","Explainable navigation system using fuzzy reinforcement learning","10.1007/s12008-020-00717-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092206247&doi=10.1007%2fs12008-020-00717-1&partnerID=40&md5=e0bd985184e5d01c59be1fbd75ec6a1a","Abstract: Explainable outcomes in autonomous navigation have become crucial for drivers, other vehicles, as well as for pedestrians. Creating trustworthy strategies is mandatory for the integration of self-driving cars into quotidian environments. This paper presents the successful implementation of an explainable Fuzzy Deep Reinforcement Learning approach for autonomous vehicles based on the AWS DeepRacerTM platform. A model of the environment is created by transforming crisp values into linguistic variables. A fuzzy inference system is used to define the reward of the vehicle depending on its current state. Guidelines to define the actions and to improve performance of the reinforcement learning agent are given based on the characteristics of the existing hardware. The performance of the models is tested on tracks with distinctive properties using agents with different policies and action spaces, and shows explainable and successful navigation of the agent on diverse scenarios. Graphic Abstract: [Figure not available: see fulltext.]. © 2020, Springer-Verlag France SAS, part of Springer Nature.","Autonomous vehicle; Deep learning; Explainable artificial intelligence; Fuzzy inference system; Reinforcement learning","Autonomous vehicles; Deep learning; Fuzzy inference; Navigation systems; Action spaces; Autonomous navigation; Fuzzy inference systems; Fuzzy reinforcement learning; Improve performance; Linguistic variable; Reinforcement learning agent; Reinforcement learning approach; Reinforcement learning"
"Bay D., Bisot C.","Convolutional Neural Network Architecture for Classification of Aircraft Engines Flight Time Series","10.14428/esann/2021.ES2021-91","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129326955&doi=10.14428%2fesann%2f2021.ES2021-91&partnerID=40&md5=5fd42936fc1d10413e73e864bf24a2de","During each flight, an aircraft engine sends data to a ground system. This data corresponds to different sensors measurements (temperatures, pressures, vibrations...) collected at key moments of the flight. It constitutes rich multivariate time series used to monitor the engine's health. In this article, we used flight data to predict the main removal cause of the engine. The problem falls within the framework of time series classification. This article proposes an interpretable neural network architecture which fits with the physical understanding of the modeled phenomenon in order to address the problem on a real-world, industrial dataset. © 2021 ESANN Intelligence and Machine Learning. All rights reserved.",,"Aircraft engines; Convolutional neural networks; Engines; Machine learning; Time series; Convolutional neural network; Flight data; Flight time; Ground systems; Measurement temperatures; Multivariate time series; Neural network architecture; Sensor measurements; Time series classifications; Times series; Network architecture"
"Bay S., Kumaraswamy K., Anderle M.G., Kumar R., Steier D.M.","Large scale detection of irregularities in accounting data","10.1109/ICDM.2006.93","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34748843986&doi=10.1109%2fICDM.2006.93&partnerID=40&md5=38b59838ec0f03cd434e0f4195a7fd51","In recent years, there have been several large accounting frauds where a company's financial results have been intentionally misrepresented by billions of dollars. In response, regulatory bodies have mandated that auditors perform analytics on detailed financial data with the intent of discovering such misstatements. For a large auditing firm, this may mean analyzing millions of records from thousands of clients. This paper proposes techniques for automatic analysis of company general ledgers on such a large scale, identifying irregularities -which may indicate fraud or just honest errors - for additional review by auditors. These techniques have been implemented in a prototype system, called Sherlock, which combines aspects of both outlier detection and classification. In developing Sherlock, we faced three major challenges: developing an efficient process for obtaining data from many heterogeneous sources, training classifiers with only positive and unlabeled examples, and presenting information to auditors in an easily interpretable manner. In this paper, we describe how we addressed these challenges over the past two years and report on experiments evaluating Sherlock. © 2006 IEEE.",,"Accounting data; Auditing firm; Automatic analysis; Large scale detection; Sherlock - prototype system; Data privacy; Demodulation; Errors; Nonbibliographic retrieval systems; Records management; Software prototyping; Data mining"
"Bayat A., Anderson C., Shah P.","Automated end-to-end deep learning framework for classification and tumor localization from native non-stained pathology images","10.1117/12.2582303","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103652857&doi=10.1117%2f12.2582303&partnerID=40&md5=803d21e7332e284718104f56f29fb5c2","Deep learning based quantitative assessment of digital pathology images and understanding the underlying reasons for a specific clinical decision is challenging, and automatic histology pattern classification and tumor localization in whole-slide pathology images are critical for interpretable learning systems. In this study, we propose an end-to-end deep learning framework for automatic detection and localization of tumors directly from non-stained whole slide prostate core biopsy images (WSI). We use a previously described Generative Adversarial Network (GAN)-based model from our laboratory for computational Hematoxylin and Eosin (HE) staining of native non-stained pathology images. A convolutional neural network to detect and classify tumor regions in 1024×1024 virtually stained HE pixel patches, and a concurrent deep weakly supervised (WSL) model that provides localization of predominant histologic patterns used for tumor classification without the need for pixel-level annotations are reported in this study for the first time. The end-to-end system was evaluated on a 17K hold out set of 1024×1024 non-stained patches extracted from 13 whole slide prostate biopsy images. Experimental results yielded 86.37% patch-level classification accuracy with 85.05% precision, and achieved a Dice index of 65.07±1.99 (compared to 70.24±1.86 Dice index in the U-Net reference model for pixel-level segmentation). The end-to-end deep learning framework thus automates digital pathology image workflow from tissue staining to interpretable prostate tumor classification and can be valuable for accurate grading of prostate cancer and generalized to other whole-slide image classification tasks. © 2021 SPIE.","Automated classification; Deep learning; Pathology image analysis; Prostate cancer; Tumor localization","Biopsy; Computer graphics; Convolutional neural networks; Diseases; Grading; Image classification; Learning systems; Medical image processing; Pathology; Pixels; Tumors; Urology; Adversarial networks; Automatic Detection; Classification accuracy; Digital pathologies; Learning frameworks; Quantitative assessments; Tumor classification; Tumor localization; Deep learning"
"Bayat S., Cuggia M., Rossille D., Kessler M., Frimat L.","Comparison of Bayesian network and Decision Tree methods for predicting access to the renal transplant waiting list","10.3233/978-1-60750-044-5-600","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249107821&doi=10.3233%2f978-1-60750-044-5-600&partnerID=40&md5=5fd704c915b2afa95f15ada6dab32d82","The study compares the effectiveness of Bayesian networks versus Decision Trees for predicting access to renal transplant waiting list in a French healthcare network. The data set consisted in 809 patients starting renal replacement therapy. The data were randomly divided into a training set (90%) and a validation set (10%). Bayesian network and CART decision tree were built on the training set. Their predictive performances were compared on the validation set. The age variable was found to be the most important factor in both models. Both models were highly sensitive and specific: sensitivity 90.0% (95%CI: 76.8-100), specificity 96.7% (95%CI: 92.2-100). Moreover, the models were complementary since the Bayesian network provided a global view of the variables' associations while the decision tree was more easily interpretable by physicians. These approaches provide insights on the current care process. This knowledge could be used for optimizing the healthcare process. © 2009 European Federation for Medical Informatics.","Bayesian network; Decision support; Decision tree; Healthcare network; Renal transplant waiting list","Data mining; Decision support systems; Decision trees; Patient treatment; Decision supports; Decision tree method; Healthcare process; Predictive performance; Renal replacement therapies; Renal transplants; Training sets; Waiting lists; Bayesian networks; aged; Bayes theorem; comparative study; conference paper; decision tree; female; health care delivery; hospital admission; human; kidney transplantation; male; Aged; Bayes Theorem; Decision Trees; Female; Health Services Accessibility; Humans; Kidney Transplantation; Male; Waiting Lists"
"Bayati M., Bhaskar S., Montanari A.","A Low-Cost Method for Multiple Disease Prediction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011779274&partnerID=40&md5=55939f5093b1e8aa3b2e69220f2d0057","Recently, in response to the rising costs of healthcare services, employers that are financially responsible for the healthcare costs of their workforce have been investing in health improvement programs for their employees. A main objective of these so called ""wellness programs"" is to reduce the incidence of chronic illnesses such as cardiovascular disease, cancer, diabetes, and obesity, with the goal of reducing future medical costs. The majority of these wellness programs include an annual screening to detect individuals with the highest risk of developing chronic disease. Once these individuals are identified, the company can invest in interventions to reduce the risk of those individuals. However, capturing many biomarkers per employee creates a costly screening procedure. We propose a statistical data-driven method to address this challenge by minimizing the number of biomarkers in the screening procedure while maximizing the predictive power over a broad spectrum of diseases. Our solution uses multi-task learning and group dimensionality reduction from machine learning and statistics. We provide empirical validation of the proposed solution using data from two different electronic medical records systems, with comparisons to a statistical benchmark.",,"biological marker; biological model; chronic disease; computer simulation; economics; electronic health record; health care cost; health promotion; human; predictive value; procedures; risk assessment; Biomarkers; Chronic Disease; Computer Simulation; Electronic Health Records; Health Care Costs; Health Promotion; Humans; Models, Biological; Predictive Value of Tests; Risk Assessment"
"Baydogan C., Alatas B.","Deep-Cov19-Hate: A Textual-Based Novel Approach for Automatic Detection of Hate Speech in Online Social Networks throughout COVID-19 with Shallow and Deep Learning Models","10.17559/TV-20210708143535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122139625&doi=10.17559%2fTV-20210708143535&partnerID=40&md5=f613b024941f75bf8181f28386b30242","The use of various online social media platforms rising day by day caused an increase in the correct or incorrect information shared by users, especially during COVID-19. The introduction of COVID-19 on the world agenda gave rise to an overall bad reaction against East Asia (esp. China) in online social media platforms. The social media users who spread degrading, racist, disrespectful, abusive, discriminatory, critical, abuse, harsh, offensive, etc. posts accused the Asian people of being responsible for the outbreak of COVID-19. For this reason, the development of the Hate Speech Detection (HSD) system was necessary in order to prevent the spread of these posts about COVID-19. In this article, a textual-based study on COVID-19-related hate speech (HS) sharing in online social networks was carried out with Shallow Learning (SL) and Deep Learning (DL) methods. In the first step of this study, typical Natural Language Processing (NLP) pipeline was applied for gathered two different datasets. This NLP pipeline was performed using bag of words, term frequency, document matrix, etc. techniques for features extraction representing datasets. Then, ten different SL and DL models were fine-tuned for HS datasets related to COVID-19. Accuracy, precision, sensitivity, and F-score performance measurement criteria were calculated to compare the performance of the SL and DL algorithms for the problem of HSD. The RNN, one of the models proposed for the first and second dataset in HSD, prevailed with the highest accuracy values of 78.7% and 90.3%, respectively. Due to the promising results of all approaches operated in the HSD, they are forecasted to be chosen in the solution of many other social media and network problems related to COVID-19. © 2022, Strojarski Facultet. All rights reserved.","COVID-19; Hate speech detection; Shallow and deep learning models; Social media analysis; Social network problems","Deep learning; Natural language processing systems; Pipelines; Speech recognition; COVID-19; Hate speech detection; Learning models; Network problems; Online social medias; Shallow and deep learning model; Social media analysis; Social media platforms; Social network problem; Speech detection; Social networking (online)"
"Bayer J., Münch D., Arens M.","A comparison of deep saliency map generators on multispectral data in object detection","10.1117/12.2599742","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118856626&doi=10.1117%2f12.2599742&partnerID=40&md5=1515b9f681c42b1121450d46c039f64e","Deep neural networks, especially convolutional deep neural networks, are state-of-the-art methods to classify, segment or even generate images, movies, or sounds. However, these methods lack of a good semantic understanding of what happens internally. The question, why a COVID-19 detector has classified a stack of lung-ct images as positive, is sometimes more interesting than the overall specificity and sensitivity. Especially when human domain expert knowledge disagrees with the given output. This way, human domain experts could also be advised to reconsider their choice, regarding the information pointed out by the system. In addition, the deep learning model can be controlled, and a present dataset bias can be found. Currently, most explainable AI methods in the computer vision domain are purely used on image classification, where the images are ordinary images in the visible spectrum. As a result, there is no comparison on how the methods behave with multimodal image data, as well as most methods have not been investigated on how they behave when used for object detection. This work tries to close the gaps by investigating three saliency map generator methods on how their maps differ in the different spectra. This is achieved via an accurate and systematic training. Additionally, we examine how they perform when used for object detection. As a practical problem, we chose object detection in the infrared and visual spectrum for autonomous driving. The dataset used in this work, is the Multispectral Object Detection Dataset,1 where each scene is available in the long-wave (FIR), mid-wave (MIR) and short-wave (NIR) infrared as well as the visual (RGB) spectrum. The results show, that there are differences between the infrared and visual activation maps. Further, an advanced training with both, the infrared and visual data not only improves the network’s output, it also leads to more focused spots in the saliency maps. © 2021 SPIE","Multispectral Data; Object Detection; Saliency Map Generators","Computerized tomography; Deep neural networks; Image classification; Image segmentation; Object detection; Semantics; Classifieds; Human domain; Lung CT; Multi-spectral data; Objects detection; Saliency map; Saliency map generator; Semantics understanding; Spectra's; State-of-the-art methods; Object recognition"
"Bayer S., Gimpel H., Markgraf M.","The role of domain expertise in trusting and following explainable AI decision support systems","10.1080/12460125.2021.1958505","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112189285&doi=10.1080%2f12460125.2021.1958505&partnerID=40&md5=772c1e49356a8fd19aad94025603357f","Although the roots of artificial intelligence (AI) stretch back some years, it currently flourishes in research and practice. However, AI deals with trust issues. One possible solution approach is making AI explain itself to its user, but it is still unclear how an AI can accomplish this in decision-making scenarios. This study focuses on how a user’s expertise influences trust in explainable AI (XAI) and how this influences behaviour. To test our theoretical assumptions, we develop an AI-based decision support system (DSS), observe user behaviour in an online experiment, complemented with survey data. The results show that domain-specific expertise negatively affects trust in AI-based DSS. We conclude that the focus on explanations might be overrated for users with low domain-specific expertise, whereas it is vital for users with high expertise. Investigating the influence of expertise on explanations of an AI-based DSS, this study contributes to research on XAI and DSS. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","decision support systems; Explainable artificial intelligence; online experiment; trust; user expertise","Behavioral research; Decision making; Decision support systems; Decision support system (dss); Domain expertise; Domain specific; On-line experiments; Solution approach; Survey data; User behaviour; Artificial intelligence"
"Baykasoǧlu A., Özbakir L.","MEPAR-miner: Multi-expression programming for classification rule mining","10.1016/j.ejor.2006.10.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250010259&doi=10.1016%2fj.ejor.2006.10.015&partnerID=40&md5=23951c0dfbeb69b58b17e85ee1e7929c","Classification and rule induction are two important tasks to extract knowledge from data. In rule induction, the representation of knowledge is defined as IF-THEN rules which are easily understandable and applicable by problem-domain experts. In this paper, a new chromosome representation and solution technique based on Multi-Expression Programming (MEP) which is named as MEPAR-miner (Multi-Expression Programming for Association Rule Mining) for rule induction is proposed. Multi-Expression Programming (MEP) is a relatively new technique in evolutionary programming that is first introduced in 2002 by Oltean and Dumitrescu. MEP uses linear chromosome structure. In MEP, multiple logical expressions which have different sizes are used to represent different logical rules. MEP expressions can be encoded and implemented in a flexible and efficient manner. MEP is generally applied to prediction problems; in this paper a new algorithm is presented which enables MEP to discover classification rules. The performance of the developed algorithm is tested on nine publicly available binary and n-ary classification data sets. Extensive experiments are performed to demonstrate that MEPAR-miner can discover effective classification rules that are as good as (or better than) the ones obtained by the traditional rule induction methods. It is also shown that effective gene encoding structure directly improves the predictive accuracy of logical IF-THEN rules. © 2006 Elsevier B.V. All rights reserved.","Classification rules; Data mining; Evolutionary programming; Multi-expression programming","Classification (of information); Evolutionary algorithms; Gene encoding; Knowledge acquisition; Problem solving; Linear chromosome structure; Multi-expression programming; Rule induction methods; Data mining"
"Bayma L.O., Pereira M.A.","Comparison of machine learning techniques for the estimation of climate missing data in the state of Minas Gerais, Brazil",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060982302&partnerID=40&md5=9ae88c276e3425c7b5669db21c59d2e6","Climate prediction is a relevant activity for humanity and, for the success of the climate forecast, a good historical database is necessary. However, because of several factors, large historical data gaps are found at different meteorological stations, and studies to determine such missing weather values are still scarce. This paper describes a study of a combination of several machine learning techniques to determine missing climatic values. This study produced a computational framework, formed by four different methods: linear regression, neural networks, support vector machines and regression bagged trees. A statistical study is conducted to compare these four methods. The study statistically demonstrated that the regression bagged trees technique was successful in obtaining missing climatic values for the state of Minas Gerais and can be widely used by the responsible agencies to improve their historical databases, consequently, their climate forecasts. © 2017 National Institute for Space Research INPE. All Rights Reserved.",,"Forecasting; Forestry; Learning algorithms; Regression analysis; Climate forecasts; Climate prediction; Computational framework; Historical database; Machine learning techniques; Meteorological station; Minas Gerais , Brazil; Statistical study; Machine learning"
"Bayomie O.S., de Cerqueira R.F.L., Neuendorf L., Kornijez I., Kieling S., Sandermann T.H., Lammers K., Kockmann N.","Detecting flooding state in extraction columns: Convolutional neural networks vs. a white‐box approach for image‐based soft sensor development","10.1016/j.compchemeng.2022.107904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134340135&doi=10.1016%2fj.compchemeng.2022.107904&partnerID=40&md5=fac2bfe89a4ffa6e639b7e57d5579508","Among thermal separation methods, solvent extraction with countercurrent flow is one of the most energy-efficient processes. The separation efficiency is highest close to the flooding point, which characterizes the maximum possible liquid throughput. However, this point is the limiting parameter in the entire extraction process since the flow of at least one phase is stopped. The optical supervision of the equipment enables the efficient operation of the extraction column near the flooding state. This contribution presents an assessment of two approaches in detecting the undesirable flooding state through recorded images of a transparent extraction column. One approach is a feature-based space that is more interpretable in extracting features than a Support Vector Machine classifier. The second is a feature-extraction pipeline followed by a robust Convolutional Neural Network model. The analysis regarding the inference times shows that both models can be used as online classification tools for real-time applications. © 2022","Convolutional neural networks; Extraction columns; Machine learning; Process supervision; Soft sensors","Convolutional neural networks; Energy efficiency; Floods; Image processing; Neural network models; Solvent extraction; Support vector machines; Vector spaces; Convolutional neural network; Extraction columns; Floodings; Image-based; Machine-learning; Process supervision; Sensor development; Soft sensors; Thermal separation; White box; Convolution"
"Bayoudh M., Roux E., Richard G., Nock R.","Structural knowledge learning from maps for supervised land cover/use classification: Application to the monitoring of land cover/use maps in French Guiana","10.1016/j.cageo.2014.08.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918522033&doi=10.1016%2fj.cageo.2014.08.013&partnerID=40&md5=20de798f9fdf4adc8e6ed2b5a134228a","The number of satellites and sensors devoted to Earth observation has become increasingly elevated, delivering extensive data, especially images. At the same time, the access to such data and the tools needed to process them has considerably improved. In the presence of such data flow, we need automatic image interpretation methods, especially when it comes to the monitoring and prediction of environmental and societal changes in highly dynamic socio-environmental contexts. This could be accomplished via artificial intelligence.The concept described here relies on the induction of classification rules that explicitly take into account structural knowledge, using Aleph, an Inductive Logic Programming (ILP) system, combined with a multi-class classification procedure. This methodology was used to monitor changes in land cover/use of the French Guiana coastline. One hundred and fifty-eight classification rules were induced from 3 diachronic land cover/use maps including 38 classes. These rules were expressed in first order logic language, which makes them easily understandable by non-experts. A 10-fold cross-validation gave significant average values of 84.62%, 99.57% and 77.22% for classification accuracy, specificity and sensitivity, respectively. Our methodology could be beneficial to automatically classify new objects and to facilitate object-based classification procedures. Highlights: © 2014 Elsevier Ltd.","Geographic information system; Inductive logic programming (ILP); Land cover map; Machine learning; Supervised classification","Artificial intelligence; Classification (of information); Formal logic; Geographic information systems; Learning systems; 10-fold cross-validation; Classification accuracy; Environmental contexts; First-order logic languages; Land cover; Multi-class classification; Object-based classifications; Supervised classification; Inductive logic programming (ILP); artificial intelligence; GIS; image classification; knowledge; land classification; land cover; land use; learning; map; French Guiana"
"Bayoudhi L., Sassi N., Jaziri W.","How Latest Computer Science Research Copes with COVID-19?","10.1007/978-3-030-96308-8_112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127642428&doi=10.1007%2f978-3-030-96308-8_112&partnerID=40&md5=558048da19d33b434b693574d7155015","Towards the end of 2019, one of the most dangerous viruses in human history emerged. This is the SARS CoV-2 coronavirus, responsible for COVID-19. This virus, originally spread from the Chinese city of Wuhan, continues to propagate all over the world and cause considerable loss of life. Since that time, several investigations have been carried out, not only in medicine, but also in computer science and related fields. This paper aims at reviewing the latest work on the infectious disease COVID-19 in order to identify the latest techniques, datasets, and tools that have been developed. In particular, we examine the latest approaches and tools in three trending fields, namely Deep Learning, Geographic Information Systems and Knowledge-Based Systems. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","COVID-19; Deep learning; Geographic Information Systems; Knowledge graph; Machine learning; Ontology",
"Bayrak C., Kolukisaoglu H., Chiang C.-C.","Di-learn: Distributed knowledge discovery with human interaction","10.1109/ICSMC.2006.384636","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548119148&doi=10.1109%2fICSMC.2006.384636&partnerID=40&md5=01a9f8ba3f4fd9389c10a035740fdef1","Analyzing data is one of the problems that still continue to offer new challenges. The advent of network-based distributed computing environments has made it possible to handle this problem up to some level. Today, mostly centralized algorithms are being used for data modeling and knowledge discovery, which are based on statistical and machine learning algorithms. Many of these algorithms require collection of data from distributed sites. Although, it is possible to store a huge amount of data in some data storages, these data are only useful if the analysts can extract beneficial information in form of patterns or rules. This problem is covered by techniques from the area of Knowledge Discovery in Databases. The KDD process is the non-trivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data [1]. The process includes several steps, which are invoked and parameterized in an interactive and iterative manner, Existing knowledge discovery and machine learning algorithms are based on a rule-set or a rule-base, sometimes this rule information is expanded as the time passes and algorithm processes more data. However, this expansion process is limited because of the variety of the knowledge and initial status of the implementation. In this project, we have implemented a distributed knowledge discovery software called as, Di-Learn, offering a new approach to this problem: human interaction. © 2006 IEEE.","Agent; Data analysis; Data modeling and knowledge discovery; Pattern analysis","Algorithms; Computer software; Data reduction; Data storage equipment; Distributed computer systems; Learning systems; Data modeling; Pattern analysis; Rule information; Data mining"
"Bayrak Ş.H., Takçi H., Eminli M.","Language identification based on n-gram feature extraction method by using classifiers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923384969&partnerID=40&md5=b15bb758b56d89f5734487e6f0278053","The rising opportunities of communication provided us with many documents in many different languages. Language identification has a key role for these documents to be understandable and to study natural language identification procedures. The increasing number of the documents and international communication requirements make new works on language identification obligatory. Until today, there have been a great number of studies on solving language identification problem about document based language identification. In these studies, characters, words and n-gram sequences have been used with machine learning techniques. In this study, sequence of n-gram frequencies will be used and using of the five different classification algorithms' accuracy performances will be analyzed via different sizes of documents belonging to 15 different languages. N-gram based feature method will be used to extract feature vector belonging to languages. The most appropriate method for the problem of language identification will be identified by comparing the performances of the Support Vector Machines, Multilayer Perceptron, Centroid Classifier, k-Means and Fuzzy C Means methods. During the experiments, trainining and testing data will be selected from ECI multilingual corpus.","Document based language identification; ECI corpus; Machine learning algorithms; N-gram feature extraction method","Artificial intelligence; C (programming language); Extraction; Feature extraction; Information retrieval systems; Learning algorithms; Learning systems; Natural language processing systems; Classification algorithm; ECI corpus; Feature extraction methods; Fuzzy C means method; International communication; Language identification; Machine learning techniques; Multi layer perceptron; Computational linguistics"
"Baytar F., Yang Y., Maher M.","The Importance of Deciphering Tacit Knowledge in Apparel Product Development for Explainable AI","10.1145/3549737.3549806","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138379661&doi=10.1145%2f3549737.3549806&partnerID=40&md5=eaa7ff5700cd6c8c8aaa29b378f7ce85","Apparel product development is an iterative problem-solving process that is heuristic in nature and involves turning 2D flat patterns into a 3D garment that would fit human anatomy. The digital transformation trend in the apparel industry will accelerate in the post-pandemic era. Therefore, it is crucial to better understand the dynamics as well as the types of information generated during apparel product development to translate this information into the digital realm to better support the apparel industry. Technical apparel designers acquire important knowledge on garment fit during their years in the workforce. Documenting their knowledge could assure that the company's know-how would be saved and used to train the future workforce. Technical designers' problem-solving strategies can be tracked, coded, and made a part of artificial intelligence (AI) technology for product development sessions. This would help strengthen the competitiveness of both existing and new design companies. However, the knowledge possessed by the technical designers to enable much-improved digitalization would require a thorough analysis of tacit or implicit knowledge, which is acquired through years of experience. Due to the nature of how apparel designers learn by directly manipulating materials with their hands, this knowledge type is challenging to document. Nonetheless, if a successful method can be developed to document this knowledge, it would be very rewarding, especially for organizational knowledge management, and allow companies to digitalize using AI. © 2022 ACM.","AI; Fit session; Product development; Tacit knowledge; Technical apparel design","Hosiery manufacture; Iterative methods; Knowledge management; Personnel; Product design; Technology transfer; 3d garments; Apparel design; Apparel industry; Apparel products; Fit session; Flat pattern; Human anatomy; Problem solving process; Tacit knowledge; Technical apparel design; Product development"
"Baytiyeh H.","Toward the formation of competitive global engineers: The challenges facing engineering education in Lebanon",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005975880&partnerID=40&md5=139e5855eef164c8b9d2ae49fa925362","Globalization has affected the engineering profession and increased the pressure on engineering programs in developing countries to reform curriculum contents and to revolutionize teaching approaches to ensure that graduates can compete in the global engineering market. This article provides a critical overview of the challenges facing Lebanese engineering educational programs in the formation of global engineers and discusses the effects of these challenges on the engineering career. In Lebanon, the curriculum is highly structured which leaves students with limited choice of taking elective or interdisciplinary courses. Moreover, the reliance on a traditional deductive teaching style and the absence of a national accreditation system have been responsible for deficiencies in students' acquisition of essential professional skills and knowledge for a global and competitive career in engineering. This study provides several implications for engineering education in developing countries, namely about the need to shift focus from teaching content to the development of skills and competencies. A reform initiative that aims at revolutionizing the teaching and learning style is recommended to address the pressing challenges. © 2016 TEMPUS Publications.","Accreditation; Active learning; Engineering career; Engineering education; Globalization; Lebanon; Soft skills","Accreditation; Artificial intelligence; Curricula; Developing countries; Education; Facings; Professional aspects; Students; Teaching; Active Learning; Engineering careers; Globalization; Lebanon; Soft skills; Engineering education"
"Baziotis C., Athanasiou N., Chronopoulou A., Kolovou A., Paraskevopoulos G., Ellinas N., Narayanan S., Potamianos A.","NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122294727&partnerID=40&md5=440c2a061a3b1a37484a853c966ae9c4","In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition: “Affect in Tweets”. We participated in all subtasks for English tweets. We propose a Bi-LSTM architecture equipped with a multi-layer self attention mechanism. The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable. Our model utilizes a set of word2vec word embeddings trained on a large collection of 550 million Twitter messages, augmented by a set of word affective features. Due to the limited amount of task-specific training data, we opted for a transfer learning approach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A. The proposed approach ranked 1st in Subtask E “Multi-Label Emotion Classification”, 2nd in Subtask A “Emotion Intensity Regression” and achieved competitive results in other subtasks. © 2018 Association for Computational Linguistics",,"Classification (of information); Semantics; Attention mechanisms; Embeddings; Gain insight; Learning models; Model-making; Modeling performance; Multi-layers; Subtask; Training data; Transfer learning; Long short-term memory"
"Bazoobandi H.R., Beck H., Urbani J.","Expressive stream reasoning with laser","10.1007/978-3-319-68288-4_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032186605&doi=10.1007%2f978-3-319-68288-4_6&partnerID=40&md5=6b9877bda4afa13c504c6bece6b4ffef","An increasing number of use cases require a timely extraction of non-trivial knowledge from semantically annotated data streams, especially on the Web and for the Internet of Things (IoT). Often, this extraction requires expressive reasoning, which is challenging to compute on large streams. We propose Laser, a new reasoner that supports a pragmatic, non-trivial fragment of the logic LARS which extends Answer Set Programming (ASP) for streams. At its core, Laser implements a novel evaluation procedure which annotates formulae to avoid the re-computation of duplicates at multiple time points. This procedure, combined with a judicious implementation of the LARS operators, is responsible for significantly better runtimes than the ones of other state-of-the-art systems like C-SPARQL and CQELS, or an implementation of LARS which runs on the ASP solver Clingo. This enables the application of expressive logic-based reasoning to large streams and opens the door to a wider range of stream reasoning use cases. © Springer International Publishing AG 2017.",,"Computation theory; Computer circuits; Computer programming; Data mining; Extraction; Internet of things; Logic programming; Answer set programming; Based reasonings; Data stream; Internet of thing (IOT); Non-trivial; State-of-the-art system; Stream reasonings; Time points; Semantic Web"
"Bazroun M., Yang Y., Ho Cho I.","Flexible and interpretable generalization of self-evolving computational materials framework","10.1016/j.compstruc.2021.106706","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119175310&doi=10.1016%2fj.compstruc.2021.106706&partnerID=40&md5=0f0274f7740589b01e4c236a83672251","The recent innovations of computational material models by machine learning (ML) methods face formidable challenges. Incorporating internal heterogeneity and diverse boundary conditions (BC's) into existing ML methods remains difficult, and the weak interpretability of ML remains unresolved. To tackle these challenges, this paper generalizes a recently developed self-evolving computational material models framework built upon Bayesian update and evolutionary algorithm. This paper proposes a new material-specific information index (II), which is capable of autonomously quantifying the internal heterogeneity and diverse BC's. Also, this paper introduces highly flexible cubic regression spline (CRS)-based link functions which can offer mathematical expressions of salient material coefficients of the existing computational material models in terms of convolved II. Thereby, this paper suggests a novel means by which ML can directly leverage internal heterogeneity and diverse BC's to autonomously evolve computational material models while keeping interpretability. Validations using a wide spectrum of large-scale reinforced composite structures confirm the favorable performance of the generalization. Example expansions of nonlinear shear of quasi-brittle materials and progressive compressive buckling of reinforcing steel underpin efficiency and accuracy of the generalization. This paper adds a meaningful avenue for accelerating the fusion of computational material models and ML. © 2021 Elsevier Ltd","Computational material model; Cubic regression spline; Evolutionary algorithm; Machine learning for heterogeneity; Machine learning for varying boundary conditions; Nonlinear analysis of reinforced concrete structures","Evolutionary algorithms; Functions; Machine learning; Nonlinear analysis; Reinforced concrete; Computational materials modeling; Cubic regression spline; Generalisation; Interpretability; Machine learning for heterogeneity; Machine learning for varying boundary condition; Machine learning methods; Nonlinear analyse of reinforced concrete structure; Regression splines; Reinforced concrete structures; Boundary conditions"
"Beach M.J.S., Golubeva A., Melko R.G.","Machine learning vortices at the Kosterlitz-Thouless transition","10.1103/PhysRevB.97.045207","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041206784&doi=10.1103%2fPhysRevB.97.045207&partnerID=40&md5=3a15fc7f25aaa31f6135c1efd57a128b","Efficient and automated classification of phases from minimally processed data is one goal of machine learning in condensed-matter and statistical physics. Supervised algorithms trained on raw samples of microstates can successfully detect conventional phase transitions via learning a bulk feature such as an order parameter. In this paper, we investigate whether neural networks can learn to classify phases based on topological defects. We address this question on the two-dimensional classical XY model which exhibits a Kosterlitz-Thouless transition. We find significant feature engineering of the raw spin states is required to convincingly claim that features of the vortex configurations are responsible for learning the transition temperature. We further show a single-layer network does not correctly classify the phases of the XY model, while a convolutional network easily performs classification by learning the global magnetization. Finally, we design a deep network capable of learning vortices without feature engineering. We demonstrate the detection of vortices does not necessarily result in the best classification accuracy, especially for lattices of less than approximately 1000 spins. For larger systems, it remains a difficult task to learn vortices. © 2018 American Physical Society.",,
"Beale M.A., Franklin P.H.","Using Impact Analysis to Drive Process Assessment and Improvement","10.1109/RAM.2018.8462998","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054152778&doi=10.1109%2fRAM.2018.8462998&partnerID=40&md5=c15b91c0f3bbe0d2781ac4483844aecc","Impact Analysis is a method to measure and illustrate the drivers of complex failure and resulting impact on decisions for decision support. Before decision makers can enjoy highly correlated illustrations confirming their decisions the bands of performance based on failure must be confidently defined. Defining failure is the first step of achieving confident decision support for engineers ensuring the resulting visuals will be impacted as designed. The confirmation of a failure is achieved within the final step of Impact Analysis called impact reporting. Impact reporting results in simple statements easily understood and acted upon by decision makers and engineers. As an example, illustrating that 1 % of a given universe is responsible for 15% of failure. Providing the ability to show how it grew over time and the other related elements to the performance. The variance between 15 and 1 % is what we call 'impact', In this example it would be overstated by 14% and ordered higher than the lesser impacted universe. The many attributes of failure can be captured with a single related criterion or leaf level optimized criterion. As an example, a single complex failure criterion could be a combination of low or high performance thresholds being exceeded across many metrics and behaviors over time. As failure is defined business rules and complex logic are mitigated independently by engineers and decision owners once and then collectively resulting in a 'validated' single encapsulated failure. By encapsulating the many thresholds of failure into a single measure allows complex business rules to naturally move efficiently out into new processes without the risk of redefining business rules and overall failure. Once failure is defined its data signature is applied to past data histories with the help of new data shaping technologies that split the data into bands of performance, optimal/prefailure/failurel/critical across the optimal time period increments and data granularity. Impact Analysis compares two or more performance bands over time that are independently measured and plotted horizontally (over time, X-axis to itself per period) and vertically (to other performance bands within the same time period, Y-axis). With the ability of big data and machine learning techniques to better predict and define failure reliability engineering can use Impact Analysis to encapsulate their quality metrics and knowledge and greatly aid in the machine learning process and ultimately prescriptive highly relevant decision support. © 2018 IEEE.","Decision Support; Impact Analysis; Lessons learned","Artificial intelligence; Decision making; Decision support systems; Digital storage; Engineers; Learning systems; Maintainability; Quality control; Reliability analysis; Risk assessment; Complex business rules; Decision supports; Impact analysis; Lessons learned; Machine learning techniques; Process assessments; Reliability engineering; Shaping technologies; Big data"
"Bean D.M., Al-Chalabi A., Dobson R.J.B., Iacoangeli A.","A knowledge-based machine learning approach to gene prioritisation in amyotrophic lateral sclerosis","10.3390/genes11060668","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086957987&doi=10.3390%2fgenes11060668&partnerID=40&md5=1ab6e2e5c1e5f02cadc06620deb57b6a","Amyotrophic lateral sclerosis is a neurodegenerative disease of the upper and lower motor neurons resulting in death from neuromuscular respiratory failure, typically within two to five years of first symptoms. Several rare disruptive gene variants have been associated with ALS and are responsible for about 15% of all cases. Although our knowledge of the genetic landscape of this disease is improving, it remains limited. Machine learning models trained on the available protein–protein interaction and phenotype-genotype association data can use our current knowledge of the disease genetics for the prediction of novel candidate genes. Here, we describe a knowledge-based machine learning method for this purpose. We trained our model on protein–protein interaction data from IntAct, gene function annotation from Gene Ontology, and known disease-gene associations from DisGeNet. Using several sets of known ALS genes from public databases and a manual review as input, we generated a list of new candidate genes for each input set. We investigated the relevance of the predicted genes in ALS by using the available summary statistics from the largest ALS genome-wide association study and by performing functional and phenotype enrichment analysis. The predicted sets were enriched for genes associated with other neurodegenerative diseases known to overlap with ALS genetically and phenotypically, as well as for biological processes associated with the disease. Moreover, using ALS genes from ClinVar and our manual review as input, the predicted sets were enriched for ALS-associated genes (ClinVar p = 0.038 and manual review p = 0.060) when used for gene prioritisation in a genome-wide association study. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Amyotrophic lateral sclerosis; Gene discovery; Gene prioritisation; Knowledge graph; Machine learning; Motor neurone disease","ataxin 3; ciliary neurotrophic factor; cyclic AMP responsive element binding protein binding protein; epidermal growth factor receptor 4; excitatory amino acid transporter 2; glial fibrillary acidic protein; glutathione transferase P1; heat shock transcription factor 1; Janus kinase 3; LAT protein; low density lipoprotein receptor; manganese superoxide dismutase; peripherin; protein deglycase DJ-1; protein fos; protein p53; retinoid X receptor alpha; SHC transforming protein 1; T lymphoma invasion and metastasis inducing protein 1; transcription factor JunD; transient receptor potential channel M7; tumor necrosis factor; UNC13A gene; unclassified drug; vimentin; Wnt7a protein; X linked inhibitor of apoptosis; ALS2 gene; Alzheimer disease; amyotrophic lateral sclerosis; Article; ATXN3 gene; BCL2L1 gene; BSG gene; cardiomyopathy; CASP1 gene; CHMP2B gene; CLU gene; CNTF gene; controlled study; CREBBP gene; cross validation; CST3 gene; CTSD gene; DPP6 gene; dystonia; ERBB4 gene; FOS gene; frontotemporal dementia; GDI1 gene; gene; gene function; gene linkage disequilibrium; gene ontology; genome-wide association study; GFAP gene; GLE1 gene; GSR gene; GSTP1 gene; GSX2 gene; hereditary motor sensory neuropathy; HSF1 gene; human; INA gene; JAK3 gene; JUND gene; KIF3C gene; LAT gene; LDLR gene; PARK7 gene; Parkinson disease; PLA2G4A gene; PPARGC1A gene; prediction; protein protein interaction; PRPH gene; RXRA gene; SCFD1 gene; schizophrenia; SELPLG gene; SHC1 gene; single nucleotide polymorphism; SLC1A2 gene; SNAI1 gene; SOD2 gene; TIAM1 gene; TLE3 gene; TMSB4X gene; TNF gene; TRPM7 gene; tumor suppressor gene; VIM gene; WNT7A gene; XIAP gene; ZFP91 gene; amyotrophic lateral sclerosis; degenerative disease; genetic association study; genetic predisposition; genetics; genotype; knowledge base; machine learning; metabolism; motoneuron; pathology; phenotype; Amyotrophic Lateral Sclerosis; Genetic Association Studies; Genetic Predisposition to Disease; Genome-Wide Association Study; Genotype; Humans; Knowledge Bases; Machine Learning; Motor Neurons; Neurodegenerative Diseases; Phenotype"
"Beasley Z.J., Piegl L.A., Rosen P.","Designing intelligent review forms for peer assessment: A Data-driven Approach",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078782666&partnerID=40&md5=e72f681b77f9aaf41d0da5c3701610ce","This evidence-based practice paper employs a data-driven, explainable, and scalable approach to the development and application of an online peer review system in computer science and engineering courses. Crowd-sourced grading through peer review is an effective evaluation methodology that 1) allows the use of meaningful assignments in large or online classes (e.g. assignments other than true/false, multiple choice, or short answer), 2) fosters learning and critical thinking in a student evaluating another's work, and 3) provides a defendable and non-biased score through the wisdom of the crowd. Although peer review is widely utilized, to the authors' best knowledge, the form and associated grading process have never been subjected to data-driven analysis and design. We present a novel, iterative approach by first gathering the most appropriate review form questions through intelligent data mining of past student reviews. During this process, key words and ideas are gathered for positive and negative sentiment dictionaries, a flag word dictionary, and a negate word dictionary. Next, we revise our grading algorithm using simulations and perturbation to determine robustness (measured by standard deviation within a section). Using the dictionaries, we leverage sentiment gathered from review comments as a quality assurance mechanism to generate a crowd comment “grade”. This grade supplements the weighted average of other review form sections. The result of this semi-automated, innovative process is a peer assessment package (intelligently-designed review form and robust grading algorithm leveraging crowd sentiment) based on actual student work that can be used by an educator to confidently assign and grade meaningful open-ended assignments in any size class. © American Society for Engineering Education, 2019","Automated assessment; Data mining; Opinion mining; Optimal review form; Peer review; Sentiment analysis","Automation; Engineering education; Grading; Iterative methods; Quality assurance; Sentiment analysis; Students; Automated assessment; Computer science and engineerings; Development and applications; Evaluation methodologies; Evidence-based practices; Intelligent data minings; Opinion mining; Peer review; Data mining"
"Beaton B.","Crucial Answers about Humanoid Capital","10.1145/3173386.3173391","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045267279&doi=10.1145%2f3173386.3173391&partnerID=40&md5=55761e4fb50cd0c5bceedb1fd17ba446","Inside AI research and engineering communities, explainable artificial intelligence (XAI) is one of the most provocative and promising lines of AI research and development today. XAI has the potential to make expressible the context and domain-specific benefits of particular AI applications to a diverse and inclusive array of stakeholders and audiences. In addition, XAI has the potential to make AI benefit claims more deeply evidenced. Outside AI research and engineering communities, one of the most provocative and promising lines of research happening today is the work on ""humanoid capital"" at the edges of the social, behavioral, and economic sciences. Humanoid capital theorists renovate older discussions of ""human capital"" as part of trying to make calculable and provable the domain-specific capital value, value-adding potential, or relative worth (i.e., advantages and benefits) of different humanoid models over time. Bringing these two exciting streams of research into direct conversation for the first time is the larger goal of this landmark paper. The primary research contribution of the paper is to detail some of the key requirements for making humanoid robots explainable in capital terms using XAI approaches. In this regard, the paper not only brings two streams of provocative research into much-needed conversation but also advances both streams. © 2018 ACM.","capital; explainable artificial intelligence; humanoid robots; xai","Anthropomorphic robots; Artificial intelligence; Economic and social effects; Human robot interaction; Intelligent robots; Man machine systems; AI applications; capital; Domain specific; Economic science; Engineering community; Human capitals; Humanoid robot; Research and development; Engineering research"
"Beaucé E., Frank W.B., Paul A., Campillo M., van der Hilst R.D.","Systematic Detection of Clustered Seismicity Beneath the Southwestern Alps","10.1029/2019JB018110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074808895&doi=10.1029%2f2019JB018110&partnerID=40&md5=a09c8f7bbed7153f9d23ac29dc932a4e","We present a new automated earthquake detection and location method based on beamforming (or back projection) and template matching and apply it to study the seismicity of the Southwestern Alps. We use beamforming with prior knowledge of the 3-D variations of seismic velocities as a first detection run to search for earthquakes that are used as templates in a subsequent matched-filter search. Template matching allows us to detect low signal-to-noise ratio events and thus to obtain a high spatiotemporal resolution of the seismicity in the Southwestern Alps. We describe how we address the problem of false positives in energy-based earthquake detection with supervised machine learning and how to best leverage template matching to iteratively refine the templates and the detection. We detected 18,754 earthquakes over 1 year (our catalog is available online) and observed temporal clustering of the earthquake occurrence in several regions. This statistical study of the collective behavior of earthquakes provides insights into the mechanisms of earthquake occurrence. Based on our observations, we infer the mechanisms responsible for the seismic activity in three regions of interest: the Ubaye valley, the Briançonnais, and the Dora Maira massif. Our conclusions point to the importance of fault interactions to explain the earthquake occurrence in the Briançonnais and the Dora Maira massif, whereas fluids seem to be the major driving mechanism in the Ubaye valley. ©2019. American Geophysical Union. All Rights Reserved.","earthquake detection; seismicity; template matching; temporal clustering; Western Alps","cluster analysis; detection method; earthquake catalogue; machine learning; seismic velocity; seismicity; spatiotemporal analysis; Alpes de Haute Provence; Alps; France; Provence-Alpes-Cote d'Azur; Ubaye Valley; Western Alps"
"Beaulac C., Rosenthal J.S.","BEST: a decision tree algorithm that handles missing values","10.1007/s00180-020-00987-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083895118&doi=10.1007%2fs00180-020-00987-z&partnerID=40&md5=a497407c392be7dd7a47766d3a3db965","The main contribution of this paper is the development of a new decision tree algorithm. The proposed approach allows users to guide the algorithm through the data partitioning process. We believe this feature has many applications but in this paper we demonstrate how to utilize this algorithm to analyse data sets containing missing values. We tested our algorithm against simulated data sets with various missing data structures and a real data set. The results demonstrate that this new classification procedure efficiently handles missing values and produces results that are slightly more accurate and more interpretable than most common procedures without any imputations or pre-processing. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Applied machine learning; Classification and regression tree; Interpretable models; Missing data; Variable importance analysis",
"Bechini A., Corcuera Barcena J.L., Ducange P., Marcelloni F., Renda A.","Increasing Accuracy and Explainability in Fuzzy Regression Trees: An Experimental Analysis","10.1109/FUZZ-IEEE55066.2022.9882604","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136820047&doi=10.1109%2fFUZZ-IEEE55066.2022.9882604&partnerID=40&md5=5b404a6571480a0dceaf5e5342ba0201","Regression Trees (RTs) have been widely used in the last decades in various domains, also thanks to their inherent explainability. Fuzzy RTs (FRTs) extend RTs by using fuzzy sets and have proven to be particularly suitable for dealing with noisy and/or uncertain environments. The modelling capability of FRTs depends, among other factors, on the model used in the leaves for determining the output, and on the inference strategy. Nevertheless, the impact of such factors on FRTs accuracy and explainability has not been adequately investigated.In this paper, we extend a recently proposed learning scheme for FRTs by employing both linear models in the leaves and the maximum matching inference strategy. The former extension aims to increase accuracy, and the latter to improve explainability. We carried out an extensive experimental analysis by comparing the four FRT versions corresponding to any possible combination of the two extensions introduced in the paper. The results show that the best trade-off between accuracy and explainability is obtained by employing both of them. © 2022 IEEE.","Explainable Artificial Intelligence; Fuzzy Decision Trees; Fuzzy Regression Trees; Regression Models","Decision trees; Forestry; Fuzzy inference; Regression analysis; Experimental analysis; Explainable artificial intelligence; Fuzzy decision trees; Fuzzy regression tree; Fuzzy regressions; Learning schemes; Modelling capabilities; Regression modelling; Regression trees; Uncertain environments; Economic and social effects"
"Beck B., Geppert T.","Industrial applications of in silico ADMET","10.1007/s00894-014-2322-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903256345&doi=10.1007%2fs00894-014-2322-5&partnerID=40&md5=b0d467accd64474d66e769923c274aa3","Quantitative structure activity relationship (QSAR) modeling has been in use for several decades now. One branch of it, in silico ADMET, became more and more important since the late 1990s as studies indicated that poor pharmacokinetics and toxicity were important causes of costly late-stage failures in drug development. In this paper we describe some of the available methods and best practice for the different stages of the in silico model building process. We also describe some more recent developments, like automated model building and the prediction probability. Finally we will discuss the use of in silico ADMET for ""big data"" and the importance and possible further development of interpretable models. © 2014 Springer-Verlag.","Automated model building; Data curation; In silico ADMET; Prediction probability; QSAR","cytochrome P450 2C19; cytochrome P450 2C9; cytochrome P450 2D6; cytochrome P450 3A4; article; artificial neural network; computer model; computer prediction; drug absorption; drug distribution; drug excretion; drug metabolism; machine learning; measurement accuracy; performance measurement system; priority journal; probability; quantitative structure activity relation; quantitative structure property relation; random forest; support vector machine; surface property; workflow"
"Beck F., Fürnkranz J., Huynh V.Q.P.","Structuring Rule Sets Using Binary Decision Diagrams","10.1007/978-3-030-91167-6_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121871939&doi=10.1007%2f978-3-030-91167-6_4&partnerID=40&md5=32c82f3ec56ddaec8a85ef87bb393e4d","Over the years we have seen considerable progress in learning rule-based theories. However, all state-of-the-art rule learners still learn descriptions that directly relate the input features to the target concept and are not able to discover intermediate concepts which might result in a more compact and interpretable theory. An analogous observation can also be made in electronic design automation where the task is to find the minimal representation of a Boolean function: if the representation is not limited to two levels, even smaller circuits can be found. In this paper, we consider binary classification tasks as multi-level logic optimization problems. We take DNF descriptions of the positive class, as obtained by state-of-the-art rule learners, and generate binary decision diagrams with the equivalent expression as the rule set. Finally, a new rule-based theory is extracted from the BDD, which includes new intermediate concepts and is therefore better structured than the original DNF rule set. First experiments on small artificial datasets indicate that intermediate concepts can be reliably detected, and the size of the resulting representations can be compressed, but a first study on a simple real-world dataset showed that the found structures are too complex to be interpretable. © 2021, Springer Nature Switzerland AG.","Binary decision diagrams; Inductive rule learning; Learning in logic; Multi-level logic optimization","Artificial intelligence; Binary decision diagrams; Computer aided design; Computer circuits; Equivalence classes; Learning systems; Inductive rule learning; Intermediate concept; Learning in logic; Logic optimization; Multi-level logic optimization; Multilevels; Rule based; Rule learners; Rule set; State of the art; Boolean functions"
"Beck T., Shorter T., Hu Y., Li Z., Sun S., Popovici C.M., McQuibban N.A.R., Makraduli F., Yeung C.S., Rowlands T., Posma J.M.","Auto-CORPus: A Natural Language Processing Tool for Standardizing and Reusing Biomedical Literature","10.3389/fdgth.2022.788124","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129638895&doi=10.3389%2ffdgth.2022.788124&partnerID=40&md5=6532a8543d50b1bc4861b6dc6334106f","To analyse large corpora using machine learning and other Natural Language Processing (NLP) algorithms, the corpora need to be standardized. The BioC format is a community-driven simple data structure for sharing text and annotations, however there is limited access to biomedical literature in BioC format and a lack of bioinformatics tools to convert online publication HTML formats to BioC. We present Auto-CORPus (Automated pipeline for Consistent Outputs from Research Publications), a novel NLP tool for the standardization and conversion of publication HTML and table image files to three convenient machine-interpretable outputs to support biomedical text analytics. Firstly, Auto-CORPus can be configured to convert HTML from various publication sources to BioC. To standardize the description of heterogenous publication sections, the Information Artifact Ontology is used to annotate each section within the BioC output. Secondly, Auto-CORPus transforms publication tables to a JSON format to store, exchange and annotate table data between text analytics systems. The BioC specification does not include a data structure for representing publication table data, so we present a JSON format for sharing table content and metadata. Inline tables within full-text HTML files and linked tables within separate HTML files are processed and converted to machine-interpretable table JSON format. Finally, Auto-CORPus extracts abbreviations declared within publication text and provides an abbreviations JSON output that relates an abbreviation with the full definition. This abbreviation collection supports text mining tasks such as named entity recognition by including abbreviations unique to individual publications that are not contained within standard bio-ontologies and dictionaries. The Auto-CORPus package is freely available with detailed instructions from GitHub at: https://github.com/omicsNLP/Auto-CORPus. Copyright © 2022 Beck, Shorter, Hu, Li, Sun, Popovici, McQuibban, Makraduli, Yeung, Rowlands and Posma.","biomedical literature; health data; natural language processing; semantics; text mining","article; artifact; biological ontology; health data; human; metadata; mining; natural language processing; pipeline; semantics; standardization"
"Becker A.-K., Ittermann T., Dörr M., Felix S.B., Nauck M., Teumer A., Völker U., Völzke H., Kaderali L., Nath N.","Analysis of epidemiological association patterns of serum thyrotropin by combining random forests and Bayesian networks","10.1371/journal.pone.0271610","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134817728&doi=10.1371%2fjournal.pone.0271610&partnerID=40&md5=a54c94ef5c768682a9e759adc4369c6d","Background Approaching epidemiological data with flexible machine learning algorithms is of great value for understanding disease-specific association patterns. However, it can be difficult to correctly extract and understand those patterns due to the lack of model interpretability. Method We here propose a machine learning workflow that combines random forests with Bayesian network surrogate models to allow for a deeper level of interpretation of complex association patterns. We first evaluate the proposed workflow on synthetic data. We then apply it to data from the large population-based Study of Health in Pomerania (SHIP). Based on this combination, we discover and interpret broad patterns of individual serum TSH concentrations, an important marker of thyroid functionality. Results Evaluations using simulated data show that feature associations can be correctly recovered by combining random forests and Bayesian networks. The presented model achieves predictive accuracy that is similar to state-of-the-art models (root mean square error of 0.66, mean absolute error of 0.55, coefficient of determination of R2 = 0.15). We identify 62 relevant features from the final random forest model, ranging from general health variables over dietary and genetic factors to physiological, hematological and hemostasis parameters. The Bayesian network model is used to put these features into context and make the black-box random forest model more understandable. Conclusion We demonstrate that the combination of random forest and Bayesian network analysis is helpful to reveal and interpret broad association patterns of individual TSH concentrations. The discovered patterns are in line with state-of-the-art literature. They may be useful for future thyroid research and improved dosing of therapeutics. © 2022 Becker et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"thyrotropin; algorithm; Bayes theorem; machine learning; workflow; Algorithms; Bayes Theorem; Machine Learning; Thyrotropin; Workflow"
"Becker C.D., Kotter E.","Communicating with patients in the age of online portals—challenges and opportunities on the horizon for radiologists","10.1186/s13244-022-01222-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129659565&doi=10.1186%2fs13244-022-01222-7&partnerID=40&md5=35ce56e98633d29a4f1d2dd05c2a59b0","The deployment of electronic patient portals increasingly allows patients throughout Europe to consult and share their radiology reports and images securely and timely online. Technical solutions and rules for releasing reports and images on patient portals may differ among institutions, regions and countries, and radiologists should therefore be familiar with the criteria by which reports and images are made available to their patients. Radiologists may also be solicited by patients who wish to discuss complex or critical imaging findings directly with the imaging expert who is responsible for the diagnosis. This emphasises the importance of radiologists’ communication skills as well as appropriate and efficient communication pathways and methods including electronic tools. Radiologists may also have to think about adapting reports as their final product in order to enable both referrers and patients to understand imaging findings. Actionable reports for a medical audience require structured, organ-specific terms and quantitative information, whereas patient-friendly summaries should preferably be based on consumer health language and include explanatory multimedia support or hyperlinks. Owing to the cultural and linguistic diversity in Europe dedicated solutions will require close collaboration between radiologists, patient representatives and software developers; software tools using artificial intelligence and natural language processing could potentially be useful in this context. By engaging actively in the challenges that are associated with increased communication with their patients, radiologists will not only have the opportunity to contribute to patient-centred care, but also to enhance the clinical relevance and the visibility of their profession. © 2022, The Author(s).","Online patient portals; Patient-centred communication; Patient-centred radiology; Patient-friendly radiology report; Professional issues","Article; artificial intelligence; clinical outcome; clinical practice; communication skill; computer assisted tomography; computer language; coronavirus disease 2019; cultural factor; digital imaging and communications in medicine; echography; electronic consultation; fluoroscopy; health care delivery; health care personnel; human; image quality; imaging; language ability; language processing; mammography; nuclear magnetic resonance imaging; online portal; pandemic; patient care; patient satisfaction; perception; personalized medicine; pharmacist; physician; radiography; radiologist; radiology; response evaluation criteria in solid tumors; software; training; validation process"
"Becker F., Skirzyński J., van Opheusden B., Lieder F.","Boosting Human Decision-making with AI-Generated Decision Aids","10.1007/s42113-022-00149-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137456757&doi=10.1007%2fs42113-022-00149-y&partnerID=40&md5=3e06af8fbabffcb6dc8d351bda46033e","Human decision-making is plagued by many systematic errors. Many of these errors can be avoided by providing decision aids that guide decision-makers to attend to the important information and integrate it according to a rational decision strategy. Designing such decision aids used to be a tedious manual process. Advances in cognitive science might make it possible to automate this process in the future. We recently introduced machine learning methods for discovering optimal strategies for human decision-making automatically and an automatic method for explaining those strategies to people. Decision aids constructed by this method were able to improve human decision-making. However, following the descriptions generated by this method is very tedious. We hypothesized that this problem can be overcome by conveying the automatically discovered decision strategy as a series of natural language instructions for how to reach a decision. Experiment 1 showed that people do indeed understand such procedural instructions more easily than the decision aids generated by our previous method. Encouraged by this finding, we developed an algorithm for translating the output of our previous method into procedural instructions. We applied the improved method to automatically generate decision aids for a naturalistic planning task (i.e., planning a road trip) and a naturalistic decision task (i.e., choosing a mortgage). Experiment 2 showed that these automatically generated decision aids significantly improved people’s performance in planning a road trip and choosing a mortgage. These findings suggest that AI-powered boosting might have potential for improving human decision-making in the real world. © 2022, The Author(s).","Automatic strategy discovery; Boosting; Decision aids; Far-sightedness; Improving human decision-making; Interpretable machine learning",
"Becker F., Drichel A., Muller C., Ertl T.","Interpretable Visualizations of Deep Neural Networks for Domain Generation Algorithm Detection","10.1109/VizSec51108.2020.00010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101334529&doi=10.1109%2fVizSec51108.2020.00010&partnerID=40&md5=86978c78de59abda5fc401b46aab1bb4","Due to their success in many application areas, deep learning models have found wide adoption for many problems. However, their black-box nature makes it hard to trust their decisions and to evaluate their line of reasoning. In the field of cybersecurity, this lack of trust and understanding poses a significant challenge for the utilization of deep learning models. Thus, we present a visual analytics system that provides designers of deep learning models for the classification of domain generation algorithms with understandable interpretations of their model. We cluster the activations of the model's nodes and leverage decision trees to explain these clusters. In combination with a 2D projection, the user can explore how the model views the data at different layers. In a preliminary evaluation of our system, we show how it can be employed to better understand misclassifications, identify potential biases and reason about the role different layers in a model may play. © 2020 IEEE.","cybersecurity; DGA detection; Explainable artificial intelligence (XAI); model visualization; visual analytics","Decision trees; Deep neural networks; Learning systems; Neural networks; Security of data; Visualization; 2D projections; Application area; Cyber security; Different layers; Generation algorithm; Learning models; Misclassifications; Visual analytics systems; Deep learning"
"Becker J., Maes F., Wehenkel L.","On the Relevance of Sophisticated Structural Annotations for Disulfide Connectivity Pattern Prediction","10.1371/journal.pone.0056621","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874033484&doi=10.1371%2fjournal.pone.0056621&partnerID=40&md5=52d1da709ed5eddd235b275d869ff8c8","Disulfide bridges strongly constrain the native structure of many proteins and predicting their formation is therefore a key sub-problem of protein structure and function inference. Most recently proposed approaches for this prediction problem adopt the following pipeline: first they enrich the primary sequence with structural annotations, second they apply a binary classifier to each candidate pair of cysteines to predict disulfide bonding probabilities and finally, they use a maximum weight graph matching algorithm to derive the predicted disulfide connectivity pattern of a protein. In this paper, we adopt this three step pipeline and propose an extensive study of the relevance of various structural annotations and feature encodings. In particular, we consider five kinds of structural annotations, among which three are novel in the context of disulfide bridge prediction. So as to be usable by machine learning algorithms, these annotations must be encoded into features. For this purpose, we propose four different feature encodings based on local windows and on different kinds of histograms. The combination of structural annotations with these possible encodings leads to a large number of possible feature functions. In order to identify a minimal subset of relevant feature functions among those, we propose an efficient and interpretable feature function selection scheme, designed so as to avoid any form of overfitting. We apply this scheme on top of three supervised learning algorithms: k-nearest neighbors, support vector machines and extremely randomized trees. Our results indicate that the use of only the PSSM (position-specific scoring matrix) together with the CSP (cysteine separation profile) are sufficient to construct a high performance disulfide pattern predictor and that extremely randomized trees reach a disulfide pattern prediction accuracy of 58.2% on the benchmark dataset SPX+, which corresponds to +3.2% improvement over the state of the art. A web-application is available at http://m24.giga.ulg.ac.be:81/x3CysBridges. © 2013 Becker et al.",,"cysteine; accuracy; algorithm; article; calculation; controlled study; disulfide bond; extremely randomized tree; k nearest neighbor; machine learning; prediction; scoring system; support vector machine; Algorithms; Artificial Intelligence; Disulfides; Models, Chemical; Protein Conformation; Proteins"
"Beckerleg M., Thompson A.","A divide-and-conquer algorithm for binary matrix completion","10.1016/j.laa.2020.04.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084185746&doi=10.1016%2fj.laa.2020.04.017&partnerID=40&md5=1b8f2bcd9d04af406572d956ecfa035e","We propose a practical algorithm for low rank matrix completion for matrices with binary entries which obtains explicit binary factors and show it performs well at the recommender task on real world datasets. The algorithm, which we call TBMC (Tiling for Binary Matrix Completion), gives interpretable output in the form of binary factors which represent a decomposition of the matrix into tiles. Our approach extends a popular algorithm from the data mining community, PROXIMUS, to missing data, applying the same recursive partitioning approach. The algorithm relies upon rank-one approximations of incomplete binary matrices, and we propose a linear programming (LP) approach for solving this subproblem. We also prove a 2-approximation result for the LP approach which holds for any subsampling pattern, and show that TBMC exactly solves the rank-k prediction task for a underlying block-diagonal tiling structure with geometrically decreasing tile sizes, providing the ratio between successive tiles is less than 1/2. Numerical experiments show that TBMC outperforms existing methods on real datasets. © 2020 Elsevier Inc.","Binary matrix completion; Linear programming; Recommender systems","Approximation algorithms; Data mining; Linear programming; Numerical methods; Regression analysis; Data mining community; Divide-and-conquer algorithm; Low-rank matrix completions; Numerical experiments; Prediction tasks; Real-world datasets; Recursive Partitioning; Tiling structures; Matrix algebra"
"Becking D., Dreyer M., Samek W., Müller K., Lapuschkin S.","ECQx : Explainability-Driven Quantization for Low-Bit and Sparse DNNs","10.1007/978-3-031-04083-2_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128884924&doi=10.1007%2f978-3-031-04083-2_14&partnerID=40&md5=9957c9ee4dfb23232b46ea74026a6a30","The remarkable success of deep neural networks (DNNs) in various applications is accompanied by a significant increase in network parameters and arithmetic operations. Such increases in memory and computational demands make deep learning prohibitive for resource-constrained hardware platforms such as mobile devices. Recent efforts aim to reduce these overheads, while preserving model performance as much as possible, and include parameter reduction techniques, parameter quantization, and lossless compression techniques. In this chapter, we develop and describe a novel quantization paradigm for DNNs: Our method leverages concepts of explainable AI (XAI) and concepts of information theory: Instead of assigning weight values based on their distances to the quantization clusters, the assignment function additionally considers weight relevances obtained from Layer-wise Relevance Propagation (LRP) and the information content of the clusters (entropy optimization). The ultimate goal is to preserve the most relevant weights in quantization clusters of highest information content. Experimental results show that this novel Entropy-Constrained and XAI-adjusted Quantization (ECQx ) method generates ultra low-precision (2–5 bit) and simultaneously sparse neural networks while maintaining or even improving model performance. Due to reduced parameter precision and high number of zero-elements, the rendered networks are highly compressible in terms of file size, up to 103 × compared to the full-precision unquantized DNN model. Our approach was evaluated on different types of models and datasets (including Google Speech Commands, CIFAR-10 and Pascal VOC) and compared with previous work. © 2022, The Author(s).","Efficient Deep Learning; Explainable AI (XAI); Layer-wise Relevance Propagation (LRP); Neural Network Compression; Neural Network Quantization","Backpropagation; Computation theory; Deep neural networks; Information theory; Multilayer neural networks; Efficient deep learning; Explainable AI (XAI); Layer-wise; Layer-wise relevance propagation; Network compression; Neural network compression; Neural network quantization; Neural-networks; Quantisation; Entropy"
"Beckley L., McMasters S., Cohen M., Cordano D., Rauch S., McHugh T.","The California GeoTracker Database: A Unique Public Resource for Understanding Contaminated Sites","10.1111/gwmr.12520","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129219705&doi=10.1111%2fgwmr.12520&partnerID=40&md5=6d916510efc42eaf28d3847bfca60055","Data mining as a research tool requires access to high quality datasets. Investigation and cleanup of contaminated sites yield large amounts of monitoring data; however, historically, these data have not been available in large, consolidated datasets. The California GeoTracker web site and database is a public repository for a wide variety of information related to investigation and remediation of cleanup sites in California. Under California regulations, responsible parties must submit laboratory analytical results for environmental samples in electronic form along with reports and other information. The GeoTracker website also supports public access to the entire database of laboratory analytical results, which, for some sites, date back to 2001. This database includes approximately 285,000,000 analytical records for more than 50,000 contaminated and formerly contaminated sites in California. Because of the large volume of publicly-available data, GeoTracker has been used as the primary data source for a number of data mining studies in the last 10 years. This article describes the GeoTracker origin story and how it has evolved to account for changes in regulatory priorities such as understanding vapor intrusion mechanisms and distribution of per- and polyfluoroalkyl substances (PFAS) in the environment, while maintaining database continuity. Finally, we review data mining projects that have utilized GeoTracker to better understand various aspects of contaminated site management. This review illustrates how long-term commitment to collection and sharing of environmental data can support the general public and the regulatory and research communities. © 2022, National Ground Water Association.",,"Contamination; Data mining; Environmental regulations; Large dataset; Websites; Analytical results; California; Contaminated sites; High quality; Large amounts; Public repositories; Public resources; Research tools; Web database; Web-sites; Database systems; cleanup; contaminated land; data mining; database; laboratory method; long-term change; public access; repository; World Wide Web; California; United States"
"Beckmann J.S., Lew D.","Reconciling evidence-based medicine and precision medicine in the era of big data: Challenges and opportunities","10.1186/s13073-016-0388-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006507586&doi=10.1186%2fs13073-016-0388-7&partnerID=40&md5=0608005971dc7b8f384cd2d9424b4896","This era of groundbreaking scientific developments in high-resolution, high-throughput technologies is allowing the cost-effective collection and analysis of huge, disparate datasets on individual health. Proper data mining and translation of the vast datasets into clinically actionable knowledge will require the application of clinical bioinformatics. These developments have triggered multiple national initiatives in precision medicine-a data-driven approach centering on the individual. However, clinical implementation of precision medicine poses numerous challenges. Foremost, precision medicine needs to be contrasted with the powerful and widely used practice of evidence-based medicine, which is informed by meta-analyses or group-centered studies from which mean recommendations are derived. This ""one size fits all"" approach can provide inadequate solutions for outliers. Such outliers, which are far from an oddity as all of us fall into this category for some traits, can be better managed using precision medicine. Here, we argue that it is necessary and possible to bridge between precision medicine and evidence-based medicine. This will require worldwide and responsible data sharing, as well as regularly updated training programs. We also discuss the challenges and opportunities for achieving clinical utility in precision medicine. We project that, through collection, analyses and sharing of standardized medically relevant data globally, evidence-based precision medicine will shift progressively from therapy to prevention, thus leading eventually to improved, clinician-to-patient communication, citizen-centered healthcare and sustained well-being. © 2016 The Author(s).",,"Article; bioinformatics; DNA sequence; doctor patient relation; economics; electronic health record; evidence based medicine; human; medical education; next generation sequencing; personalized medicine; priority journal; evidence based medicine; factual database; personalized medicine; procedures; trends; Databases, Factual; Evidence-Based Medicine; Humans; Precision Medicine"
"Beckstein C., Sack H., Peter H.","Tags and dependencies: An integrated view of document annotation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884645073&partnerID=40&md5=b165c58fbb447078be15db250dba5331","Metadata provided by annotations are an essential prerequisite for the realization of the semantic web. The manifold of metadata uses on the other hand also implies an abundance of different annotation types and formats, each requiring a different semantic treatment of the data. For semantically rich documents this results in a hybrid mixture of metadata about one and the same document. Further metadata diversity arises, if more than one author contributes to the annotation as it is typical for the now popular social tagging systems. Documents however, despite this heterogeneity show common structural properties that can be classified as logical, conceptual, and referential. There are intrinsic dependencies within and across those structures that have to be made explicit. We argue that the dependency structure as an explicit annotation is essential for any semantically rich document in order to be better understandable not only for man but also for machine.",,"Dependency structures; Document annotation; Hybrid mixtures; Social tagging systems; Data mining; Social networking (online); World Wide Web; Metadata"
"Bedane T.T., Assefa B.G., Mohapatra S.K.","Preventing Traffic Accidents Through Machine Learning Predictive Models","10.1109/ICT4DA53266.2021.9672249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125298203&doi=10.1109%2fICT4DA53266.2021.9672249&partnerID=40&md5=ec3e76ece4b3e8824165bbadae16c2e1","Road Traffic Accidents (RTA) are a serious issue of societies resulting in huge losses at the economic and social levels and responsible for millions of deaths and injuries every year in the world. For instance, in Ethiopia, the number of deaths due to traffic accidents is increasing from one year to another. Addis Ababa is one of the popular and known cities that encounter a high number of RTAs due to the increasing number of vehicles and population. The main objective of this paper is to apply machine learning algorithms to predict the accident severity and identify the major causes of accidents in crowded cities (application of Addis Ababa city). The required data are collected from Addis Ababa city police departments and 12316 records of the accident are used for data analysis. We applied seven machine learning classification algorithms (Logistic Regression, Naive Bayes, Decision Tree, Support Vector Machine, K Nearest Neighbor, Random Forest, and AdaBoost) for predicting accident severity and compared the performance to choose the best model. We applied random undersampling and SMOTE oversampling techniques to handle the class imbalance nature of the dependent features and Principal Component Analysis (PCA) for dimension reduction. The experimental result shows that Random Forest achieved a 93.76% F1 score with SMOTE over-sampled data set and about 18% feature size reduction. Moreover, light condition, driving experience, age band of the driver, type of road lane, and types of junctions are identified as major determinant factors of the accident. According to this study, these are major factors to RTA and need to be considered in the design of infrastructure, regulations and policies to reduce accidents. © 2021 IEEE.","Data Analysis; Imbalance data; Machine Learning Classification algorithms; Multi-class classification; Road Traffic Accident","Adaptive boosting; Classification (of information); Data handling; Highway accidents; Logistic regression; Nearest neighbor search; Principal component analysis; Random forests; Roads and streets; Support vector regression; Accident severity; Addis ababa; Classification algorithm; Data analyse; Imbalance datum; Machine learning classification; Machine learning classification algorithm; Machine-learning; Random forests; Road traffic accidents; Decision trees"
"Beddiar D., Oussalah M., Tapio S.","Explainability for Medical Image Captioning","10.1109/IPTA54936.2022.9784146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133129969&doi=10.1109%2fIPTA54936.2022.9784146&partnerID=40&md5=41e02b10c0bbe8a40a7e652a77e89040","Medical image captioning is the process of generating clinically significant descriptions to medical images, which has many applications among which medical report generation is the most frequent one. In general, automatic captioning of medical images is of great interest for medical experts since it offers assistance in diagnosis, disease treatment and automating the workflow of the health practitioners. Recently, many efforts have been put forward to obtain accurate descriptions but medical image captioning still provides weak and incorrect descriptions. To alleviate this issue, it is important to explain why the model produced a particular caption based on some specific features. This is performed through Artificial Intelligence Explainability (XAI), which aims to unfold the 'black-box' feature of deep-learning based models. We present in this paper an explainable module for medical image captioning that provides a sound interpretation of our attention-based encoder-decoder model by explaining the correspondence between visual features and semantic features. We exploit for that, self-attention to compute word importance of semantic features and visual attention to compute relevant regions of the image that correspond to each generated word of the caption in addition to visualization of visual features extracted at each layer of the Convolutional Neural Network (CNN) encoder. We finally evaluate our model using the ImageCLEF medical captioning dataset. © 2022 IEEE.","Artificial Intelligence Explainability; Attention-maps; Encoder-decoder; Image Captioning; Medical images","Behavioral research; Convolutional neural networks; Decoding; Deep learning; Medical imaging; Multilayer neural networks; Semantics; Signal encoding; Artificial intelligence explainability; Attention-map; Diagnose disease; Encoder-decoder; Image captioning; Medical experts; Medical image; Report generation; Semantic features; Visual feature; Diagnosis"
"Beduschi A.","Harnessing the potential of artificial intelligence for humanitarian action: Opportunities and risks","10.1017/S1816383122000261","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129688523&doi=10.1017%2fS1816383122000261&partnerID=40&md5=83f346165561a238f7a8e8714b238efd","Data-driven artificial intelligence (AI) technologies are progressively transforming the humanitarian field, but these technologies bring about significant risks for the protection of vulnerable individuals and populations in situations of conflict and crisis. This article investigates the opportunities and risks of using AI in humanitarian action. It examines whether and under what circumstances AI can be safely deployed to support the work of humanitarian actors in the field. The article argues that AI has the potential to support humanitarian actors as they implement a paradigm shift from reactive to anticipatory approaches to humanitarian action. However, it recommends that the existing risks, including those relating to algorithmic bias and data privacy concerns, must be addressed as a priority if AI is to be put at the service of humanitarian action and not to be deployed at the expense of humanitarianism. In doing so, the article contributes to the current debates on whether it is possible to harness the potential of AI for responsible use in humanitarian action. © 2022 The Author(s). Published by Cambridge University Press on behalf of the ICRC.","artificial intelligence; do no harm; human rights; humanitarian action; humanitarian principles",
"Beebe-Wang N., Okeson A., Althoff T., Lee S.-I.","Efficient and Explainable Risk Assessments for Imminent Dementia in an Aging Cohort Study","10.1109/JBHI.2021.3059563","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101267379&doi=10.1109%2fJBHI.2021.3059563&partnerID=40&md5=9e03d6dbe325a0d1168ca8799fd1a0ad","As the aging US population grows, scalable approaches are needed to identify individuals at risk for dementia. Common prediction tools have limited predictive value, involve expensive neuroimaging, or require extensive and repeated cognitive testing. None of these approaches scale to the sizable aging population who do not receive routine clinical assessments. Our study seeks a tractable and widely administrable set of metrics that can accurately predict imminent (i.e., within three years) dementia onset. To this end, we develop and apply a machine learning (ML) model to an aging cohort study with an extensive set of longitudinal clinical variables to highlight at-risk individuals with better accuracy than standard rudimentary approaches. Next, we reduce the burden needed to achieve accurate risk assessments for those deemed at risk by (1) predicting when consecutive clinical visits may be unnecessary, and (2) selecting a subset of highly predictive cognitive tests. Finally, we demonstrate that our method successfully provides individualized prediction explanations that retain non-linear feature effects present in the data. Our final model, which uses only four cognitive tests (less than 20 minutes to administer) collected in a single visit, affords predictive performance comparable to a standard 100-minute neuropsychological battery and personalized risk explanations. Our approach shows the potential for an efficient tool for screening and explaining dementia risk in the general aging population. © 2013 IEEE.","Dementia; feature selection; geriatrics; interpretability; personalized medicine","Forecasting; Neuroimaging; Risk assessment; Turing machines; Aging population; Clinical assessments; Neuropsychological; Nonlinear features; Prediction tools; Predictive performance; Predictive values; Scalable approach; Neurodegenerative diseases; apolipoprotein E; aged; aging; area under the curve; Article; clinical assessment; cognition; cohort analysis; computer model; controlled study; cross validation; dementia; dementia assessment; diagnostic accuracy; diagnostic test accuracy study; disease risk assessment; education; experimental cognitive test; false negative result; false positive result; feature learning (machine learning); female; genotype; human; imminent dementia; lifestyle; machine learning; major clinical study; male; mild cognitive impairment; Mini Mental State Examination; neuroimaging; nuclear magnetic resonance imaging; positron emission tomography; prediction; receiver operating characteristic; risk prediction; very elderly; dementia; neuropsychological test; risk assessment; Aged; Aging; Cohort Studies; Dementia; Humans; Neuropsychological Tests; Risk Assessment"
"Beedasy J., Samur Zúñiga A.F., Chandler T., Slack T.","Online community discourse during the Deepwater Horizon oil spill: an analysis of Twitter interactions","10.1016/j.ijdrr.2020.101870","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092287237&doi=10.1016%2fj.ijdrr.2020.101870&partnerID=40&md5=f388e786d92ec12da48ffc01018cc1f4","Following the Deepwater Horizon oil spill on April 20, 2010, information was widely broadcast through social media platforms such as Twitter. This study aimed to gain insights into the content and flow of the tweets that had shaped the conversation related to the oil spill within the first 4 months of the rig explosion and to understand how these online interactions may have contributed to disaster response and resilience. A set of tweets (n = 876,298) was identified and processed to yield 736,324 clean tweets. The study used a mixed-methods approach, including human coding, machine learning and social network analysis to examine the online discourse. Visualizations were used to present the patterns and connections within the dataset into a more readable and interpretable format. Our analysis revealed that popular themes such as environmental and economic concerns, clean-up and volunteering, health impacts and frustration towards BP had contributed to the shaping of the discourse. The Twitter users included seekers and providers of information and resources. Another group of users, local volunteers and digital activists, functioned as boundary spanners by facilitating information flow between the seekers and the providers. Additionally, Twitter had been used for risk communication and the dissemination of factual health information. The online communication and coordination efforts may have contributed to the mitigation of the impacts of the oil spill. Our findings suggest that social media empowered community-based users, affording access to power brokers, reliable information, the formation of online networks and social capital and potentially contributed to their resilience. © 2020 Elsevier Ltd","Deepwater Horizon oil spill; Disaster response; Resilience; Social capital; Social media; Twitter",
"Beer R.D.","The dynamics of adaptive behavior: A research program","10.1016/S0921-8890(96)00063-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031153826&doi=10.1016%2fS0921-8890%2896%2900063-2&partnerID=40&md5=22d4a7239af889797831a428820fa5bc","This paper provides a comprehensive overview of the motivations, methodology and current status of an ongoing research program whose long-term goal is to elucidate the essential principles of a theory of adaptive behavior. The thoroughly dynamical nature of both adaptive behavior itself and the causal mechanisms that support it is emphasized throughout. An initial mapping of the basic concepts of adaptive behavior into the language of dynamical systems theory is proposed, and some of the general consequences of this preliminary theoretical framework are discussed. The two key ideas of this framework are (1) that an agent and its environment should be understood as two coupled dynamical systems whose mutual interaction is jointly responsible for the agent's behavior, and (2) that an agent's need to maintain its existence in its environment defines a viability constraint on its behavioral dynamics. A constructive research methodology involving the use of evolutionary algorithms to evolve continuous-time recurrent neural networks for controlling the behavior of model agents is described, and several examples of this methodology are presented, including models of chemotaxis, walking, sequential decision-making and learning. Finally, a detailed dynamical analysis of one evolved walking circuit is presented. This analysis illustrates the kinds of insights that can be obtained by treating agents as dynamical systems and applying the tools of dynamical systems theory to their behavior.","Adaptive behavior; Autonomous agents; Dynamical neural networks; Dynamical systems theory; Evolutionary algorithms","Adaptive systems; Artificial intelligence; Learning algorithms; Neural networks; System theory; Adaptive behavior; Autonomous agents; Dynamical systems theory; Evolutionary algorithms; Robot learning"
"Beer R.D.","A dynamical systems perspective on agent-environment interaction","10.1016/0004-3702(94)00005-L","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029209779&doi=10.1016%2f0004-3702%2894%2900005-L&partnerID=40&md5=9c11900bf818fb28c1a1ca21305f927c","Using the language of dynamical systems theory, a general theoretical framework for the synthesis and analysis of autonomous agents is sketched. In this framework, an agent and its environment are modeled as two coupled dynamical systems whose mutual interaction is in general jointly responsible for the agent's behavior. In addition, the adaptive fit between an agent and its environment is characterized in terms of the satisfaction of a given constraint on the trajectories of the coupled agent-environment system. The utility of this framework is demonstrated by using it to first synthesize and then analyze a walking behavior for a legged agent. © 1995 Elsevier Science B.V. All rights reserved.",,"Constraint theory; Dynamics; Environmental engineering; Mobile robots; Agent-environment interactions; Autonomous agents; Legged agents; Artificial intelligence"
"Beerenwinkel N., Schmidt B., Walter H., Kaiser R., Lengauer T., Hoffmann D., Korn K., Selbig J.","Diversity and complexity of HIV-1 drug resistance: A bioinformatics approach to predicting phenotype from genotype","10.1073/pnas.112177799","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037062513&doi=10.1073%2fpnas.112177799&partnerID=40&md5=eff4ff583d4c9e74ad0d00e29636077c","Drug resistance testing has been shown to be beneficial for clinical management of HIV type 1 infected patients. Whereas phenotypic assays directly measure drug resistance, the commonly used genotypic assays provide only indirect evidence of drug resistance, the major challenge being the interpretation of the sequence information. We analyzed the significance of sequence variations in the protease and reverse transcriptase genes for drug resistance and derived models that predict phenotypic resistance from genotypes. For 14 antiretroviral drugs, both genotypic and phenotypic resistance data from 471 clinical isolates were analyzed with a machine learning approach. Information profiles were obtained that quantify the statistical significance of each sequence position for drug resistance. For the different drugs, patterns of varying complexity were observed, including between one and nine sequence positions with substantial information content. Based on these information profiles, decision tree classifiers were generated to identify genotypic patterns characteristic of resistance or susceptibility to the different drugs. We obtained concise and easily interpretable models to predict drug resistance from sequence information. The prediction quality of the models was assessed in leave-one-out experiments in terms of the prediction error. We found prediction errors of 9.6-15.5% for all drugs except for zalcitabine, didanosine, and stavudine, with prediction errors between 25.4% and 32.0% A prediction service is freely available at http://cartan.gmd.de/geno2pheno.html.",,"abacavir; amprenavir; delavirdine; didanosine; efavirenz; indinavir; lamivudine; nelfinavir; nevirapine; proteinase inhibitor; ritonavir; RNA directed DNA polymerase inhibitor; saquinavir; stavudine; zalcitabine; zidovudine; article; bioinformatics; controlled study; diagnostic error; drug sensitivity; genetic resistance; genotype; human; human cell; Human immunodeficiency virus 1; Human immunodeficiency virus infection; Internet; major clinical study; nucleotide sequence; phenotype; prediction; priority journal; quantitative analysis; sequence homology; statistical analysis; unindexed sequence; virus resistance; Computational Biology; Decision Trees; Disease Susceptibility; Drug Resistance, Viral; Genotype; HIV-1; Humans; Immunity, Natural; Molecular Sequence Data; Phenotype; Variation (Genetics); Human immunodeficiency virus; Human immunodeficiency virus 1; RNA viruses"
"Beetz M., Banerjee A., Grau V.","Multi-Domain Variational Autoencoders for Combined Modeling of MRI-Based Biventricular Anatomy and ECG-Based Cardiac Electrophysiology","10.3389/fphys.2022.886723","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133349803&doi=10.3389%2ffphys.2022.886723&partnerID=40&md5=ae16f92d78f73e2cf68645a8d54df8e6","Human cardiac function is characterized by a complex interplay of mechanical deformation and electrophysiological conduction. Similar to the underlying cardiac anatomy, these interconnected physiological patterns vary considerably across the human population with important implications for the effectiveness of clinical decision-making and the accuracy of computerized heart models. While many previous works have investigated this variability separately for either cardiac anatomy or physiology, this work aims to combine both aspects in a single data-driven approach and capture their intricate interdependencies in a multi-domain setting. To this end, we propose a novel multi-domain Variational Autoencoder (VAE) network to capture combined Electrocardiogram (ECG) and Magnetic Resonance Imaging (MRI)-based 3D anatomy information in a single model. Each VAE branch is specifically designed to address the particular challenges of the respective input domain, enabling efficient encoding, reconstruction, and synthesis of multi-domain cardiac signals. Our method achieves high reconstruction accuracy on a United Kingdom Biobank dataset, with Chamfer Distances between reconstructed and input anatomies below the underlying image resolution and ECG reconstructions outperforming multiple single-domain benchmarks by a considerable margin. The proposed VAE is capable of generating realistic virtual populations of arbitrary size with good alignment in clinical metrics between the synthesized and gold standard anatomies and Maximum Mean Discrepancy (MMD) scores of generated ECGs below those of comparable single-domain approaches. Furthermore, we observe the latent space of our VAE to be highly interpretable with separate components encoding different aspects of anatomical and ECG variability. Finally, we demonstrate that the combined anatomy and ECG representation improves the performance in a cardiac disease classification task by 3.9% in terms of Area Under the Receiver Operating Characteristic (AUROC) curve over the best corresponding single-domain modeling approach. Copyright © 2022 Beetz, Banerjee and Grau.","cardiac disease classification; cardiac electrophysiology; cine magnetic resonance imaging; combined electrocardiogram and cardiac anatomy generation; geometric deep learning; multi-domain variational autoencoder; point clouds; time series analysis",
"Beetz M., Banerjee A., Grau V.","Generating Subpopulation-Specific Biventricular Anatomy Models Using Conditional Point Cloud Variational Autoencoders","10.1007/978-3-030-93722-5_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123978037&doi=10.1007%2f978-3-030-93722-5_9&partnerID=40&md5=09bfc1b321bbee1405c1a40ef4dd42dd","Generative statistical models have a wide variety of applications in modelling of cardiac anatomy and function, including disease diagnosis and prediction, personalized shape analysis, and generation of population cohorts for electrophysiological and mechanical computer simulations. In this work, we propose a novel geometric deep learning method based on the variational autoencoder (VAE) framework capable of accurately encoding, reconstructing, and synthesizing 3D surface models of the biventricular anatomy. Our non-linear approach works directly with memory-efficient point clouds and is able to process multiple substructures of the cardiac anatomy at the same time in a multi-class setting. Furthermore, we introduce subpopulation-specific characteristics as additional conditional inputs to allow the generation of new personalized anatomies. Our method achieves high reconstruction quality on a dataset derived from the UK Biobank study with average Chamfer distances between reconstructed and gold standard point clouds below the underlying image pixel resolution, for all anatomical substructures and combinations of conditional inputs. We investigate our method’s generative capabilities and show that it is able to synthesize virtual populations of realistic hearts with volumetric measurements in line with established clinical precedent. We also analyse the effects of variations in the latent space of the autoencoder on the generated anatomies and find interpretable changes in cardiac shapes and sizes. © 2022, Springer Nature Switzerland AG.","Beta-VAE; Cardiac anatomy reconstruction; Cardiac anatomy synthesis; Cardiac MRI; Conditional generative models; Geometric deep learning; Point cloud generation","Deep learning; Diagnosis; Electrophysiology; Image reconstruction; Medical computing; Medical imaging; Population statistics; Three dimensional computer graphics; Auto encoders; Beta-variational autoencoder; Cardiac anatomy; Cardiac anatomy reconstruction; Cardiac anatomy synthesis; Cardiac MRI; Conditional generative model; Generative model; Geometric deep learning; Point cloud generation; Point-clouds; Heart"
"Beguš G., Zhou A.","INTERPRETING INTERMEDIATE CONVOLUTIONAL LAYERS IN UNSUPERVISED ACOUSTIC WORD CLASSIFICATION","10.1109/ICASSP43922.2022.9746849","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128441366&doi=10.1109%2fICASSP43922.2022.9746849&partnerID=40&md5=c8450c1771d2e550ab62c007ecdb7f34","Understanding how deep convolutional neural networks classify data has been subject to extensive research. This paper proposes a technique to visualize and interpret intermediate layers of unsupervised deep convolutional networks by averaging over individual feature maps in each convolutional layer and inferring underlying distributions of words with non-linear regression techniques. A GAN-based architecture (ciwGAN [1]) that includes a Generator, a Discriminator, and a classifier was trained on unlabeled sliced lexical items from TIMIT. The training process results in a deep convolutional network that learns to classify words into discrete classes only from the requirement of the Generator to output informative data. This classifier network has no access to the training data - only to the generated data. We propose a technique to visualize individual convolutional layers in the classifier that yields highly informative time-series data for each convolutional layer and apply it to unobserved test data. Using non-linear regression, we infer underlying distributions for each word which allows us to analyze both absolute values and shapes of individual words at different convolutional layers, as well as perform hypothesis testing on their acoustic properties. The technique also allows us to test individual phone contrasts and how they are represented at each layer. © 2022 IEEE","ASR; generalized additive mixed models; generative models; interpretable deep learning; unsupervised acoustic word embedding","Acoustic properties; Classification (of information); Convolutional neural networks; Deep neural networks; Well testing; ASR; Convolutional networks; Embeddings; Generalized additive mixed model; Generalized additives; Generative model; Interpretable deep learning; Mixed modeling; Underlying distribution; Unsupervised acoustic word embedding; Convolution"
"Beguš G.","CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks","10.1016/j.neunet.2021.03.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104310195&doi=10.1016%2fj.neunet.2021.03.017&partnerID=40&md5=462bcb890949540068fde1d56a208936","How can deep neural networks encode information that corresponds to words in human speech into raw acoustic data? This paper proposes two neural network architectures for modeling unsupervised lexical learning from raw acoustic inputs: ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN). These combine Deep Convolutional GAN architecture for audio data (WaveGAN; Donahue et al., 2019) with the information theoretic extension of GAN – InfoGAN (Chen et al., 2016) – and propose a new latent space structure that can model featural learning simultaneously with a higher level classification and allows for a very low-dimension vector representation of lexical items. In addition to the Generator and Discriminator networks, the architectures introduce a network that learns to retrieve latent codes from generated audio outputs. Lexical learning is thus modeled as emergent from an architecture that forces a deep neural network to output data such that unique information is retrievable from its acoustic outputs. The networks trained on lexical items from the TIMIT corpus learn to encode unique information corresponding to lexical items in the form of categorical variables in their latent space. By manipulating these variables, the network outputs specific lexical items. The network occasionally outputs innovative lexical items that violate training data, but are linguistically interpretable and highly informative for cognitive modeling and neural network interpretability. Innovative outputs suggest that phonetic and phonological representations learned by the network can be productively recombined and directly paralleled to productivity in human speech: a fiwGAN network trained on suit and dark outputs innovative start, even though it never saw start or even a [st] sequence in the training data. We also argue that setting latent featural codes to values well beyond training range results in almost categorical generation of prototypical lexical items and reveals underlying values of each latent code. Probing deep neural networks trained on well understood dependencies in speech bears implications for latent space interpretability and understanding how deep neural networks learn meaningful representations, as well as potential for unsupervised text-to-speech generation in the GAN framework. © 2021 The Author","Acoustic word embedding; Artificial intelligence; Generative adversarial networks; Lexical learning; Neural network interpretability; Speech","Classification (of information); Encoding (symbols); Linguistics; Network architecture; Network coding; Speech; Acoustic word embedding; Generative adversarial network; Human speech; Interpretability; Learn+; Lexical items; Lexical learning; Neural network interpretability; Neural-networks; Speech; Deep neural networks; article; artificial intelligence; bear; deep neural network; embedding; human; human experiment; learning; nonhuman; productivity; speech; theoretical study; acoustics; automatic speech recognition; machine learning; natural language processing; Acoustics; Machine Learning; Natural Language Processing; Speech Recognition Software"
"Behal V., Singh R.","An Ensemble Approach of Multi-objective Differential Evolution Based Benzene Detection","10.1007/978-981-16-1480-4_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107353729&doi=10.1007%2f978-981-16-1480-4_23&partnerID=40&md5=bfc66b124be166779f13022a40c2d511","Benzene is among the most common and menacing contaminant in the air that accelerate the rate of severe health issues among people. Presently, environmental sensor-based networks are utilized to monitor the quality of the air. The cost including numerous sensors with dynamic network sizes limit the operational and monitoring efficiency. In the proposed study, the advanced non-linear problem-solving principles of Adaptive Neuro-Fuzzy Inference System (ANFIS) and Differential Evolution (DE) algorithm is utilized to monitor the quality of the air and to predict the scale of C6H6 in the surrounding environment of the individual without installing or creating any sensor-based network. The concentration of C6H6 in the air is predicted by utilizing ANFIS through which evaluation of the relationship between several atmospheric gases is accomplished and DE is responsible to optimize the parameters of the ANFIS model for effective prediction accuracy. The prediction performance of the system is evaluated by calculating Accuracy, Coefficient of Determination (r2 ), and Root Mean Squared Error (RMSE) on five publicly available datasets. To validate the experimental results of the proposed system, the calculated results are compared with several base-line and hybrid methods of machine learning. The calculated outcomes justify the suitability of building self-reliable cost-effective and time-sensitive air monitoring system for predicting the concentration of benzene in the air. © 2021, Springer Nature Singapore Pte Ltd.","Air quality; An adaptive neuro-fuzzy inference system; Benzene prediction; Differential evolution; Machine learning","Benzene; Cost effectiveness; Evolutionary algorithms; Forecasting; Fuzzy inference; Fuzzy systems; Inference engines; Mean square error; Optimization; Problem solving; Adaptive neuro-fuzzy inference system; Air monitoring system; Coefficient of determination; Differential evolution algorithms; Multi-objective differential evolutions; Prediction performance; Root mean squared errors; Surrounding environment; Fuzzy neural networks"
"Behandish M., Maxwell J.T., III, de Kleer J.","AI research associate for early-stage scientific discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116710649&partnerID=40&md5=2b89f69a9e3d227659206c154a28bcaa","Artificial intelligence (AI) has been increasingly applied in scientific activities for decades; however, it is still far from an insightful and trustworthy collaborator in the scientific process. Most existing AI methods are either too simplistic to be useful in real problems faced by scientists or too domain-specialized (even dogmatized), stifling transformative discoveries or paradigm shifts. We present an AI research associate for early-stage scientific discovery based on (a) a novel minimally-biased ontology for physics-based modeling that is context-aware, interpretable, and generalizable across classical and relativistic physics; (b) automatic search for viable and parsimonious hypotheses, represented at a high-level (via domain-agnostic constructs) with built-in invariants, e.g., postulated forms of conservation principles implied by a presupposed spacetime topology; and (c) automatic compilation of the enumerated hypotheses to domain-specific, interpretable, and trainable/testable tensor-based computation graphs to learn phenomenological relations, e.g., constitutive or material laws, from sparse (and possibly noisy) data sets. Copyright © 2021for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)",,"Artificial intelligence; C (programming language); Relativity; Artificial intelligence methods; Artificial intelligence research; Context-Aware; Creative Commons; Ontology's; Paradigm shifts; Physics-based models; Real problems; Scientific activity; Scientific discovery; Topology"
"Behera A., Ashraf R., Srivastava A.K., Kumar S.","Bioinformatics analysis and verification of molecular targets in ovarian cancer stem-like cells","10.1016/j.heliyon.2020.e04820","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090580633&doi=10.1016%2fj.heliyon.2020.e04820&partnerID=40&md5=52400b045b748250fcdbca512d36d370","Background: Epithelial ovarian cancer (EOC) is a lethal and aggressive gynecological malignancy. Despite recent advances, existing therapies are challenged by a high relapse rate, eventually resulting in disease recurrence and chemoresistance. Emerging evidence indicates that a subpopulation of cells known as cancer stem-like cells (CSLCs) exists with non-tumorigenic cancer cells (non-CSCs) within a bulk tumor and is thought to be responsible for tumor recurrence and drug-resistance. Therefore, identifying the molecular drivers for cancer stem cells (CSCs) is critical for the development of novel therapeutic strategies for the treatment of EOC. Methods: Two gene datasets were downloaded from the Gene Expression Omnibus (GEO) database based on our search criteria. Differentially expressed genes (DEGs) in both datasets were obtained by the GEO2R web tool. Based on log2 (fold change) &gt;2, the top thirteen up-regulated genes and log2 (fold change) &lt; -1.5 top thirteen down-regulated genes were selected, and the association between their expressions and overall survival was analyzed by OncoLnc web tool. Gene Ontology (GO) analysis, Kyoto Encyclopedia of Genes and Genomes (KEGG) and Reactome pathways analysis, and protein-protein interaction (PPI) networks were performed for all the common DEGs found in both datasets. SK-OV-3 cells were cultured in an adherent culture medium and spheroids were generated in suspension culture with CSCs specific medium. RNA from both cell population was extracted to validate the selected DEGs expression by q-PCR. Growth inhibition assay was performed in SK-OV-3 cells after carboplatin treatment. Results: A total of 200 DEGs, 117 up-regulated and 83 down-regulated genes were commonly identified in both datasets. Analysis of pathways and enrichment tests indicated that the extracellular matrix part, cell proliferation, tissue development, and molecular function regulation were enriched in CSCs. Biological pathways such as interferon-alpha/beta signaling, molecules associated with elastic fibers, and synthesis of bile acids and bile salts were significantly enriched in CSCs. Among the top 13 up-regulated and down-regulated genes, MMP1 and PPFIBP1 expression were associated with overall survival. Higher expression of ADM, CXCR4, LGR5, and PTGS2 in carboplatin treated SK-OV-3 cells indicate a potential role in drug resistance. Conclusions: The molecular signature and signaling pathways enriched in ovarian CSCs were identified by bioinformatics analysis. This analysis could provide further research ideas to find the new mechanism and novel potential therapeutic targets for ovarian CSCs. © 2020 The AuthorsCell biology; Biochemistry; Cancer research; Data mining; Cancer stem-like cells; Differential gene expression; Carboplatin; Ovarian cancer; Interferon-alpha/beta signaling © 2020 The Authors","Biochemistry; Cancer research; Cancer stem-like cells; Carboplatin; Cell biology; Data mining; Differential gene expression; Interferon-alpha/beta signaling; Ovarian cancer",
"Behl S., Rao A., Aggarwal S., Chadha S., Pannu H.S.","Twitter for disaster relief through sentiment analysis for COVID-19 and natural hazard crises","10.1016/j.ijdrr.2021.102101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100664418&doi=10.1016%2fj.ijdrr.2021.102101&partnerID=40&md5=0956af973eac0810c7a06ed8fdbf0ee1","In emergencies and disasters, large numbers of people require basic needs and medical attention. In such situations, online social media comes as a possible solution to aid the current disaster management methods. In this paper, supervised learning approaches are compared for the multi-class classification of Twitter data. A careful setting of Multilayer Perceptron (MLP) network layers and the optimizer has shown promising results for classification of tweets into three categories i.e. ‘resource needs’, ‘resource availability’, and ‘others’ being neutral and of no useful information. Public data of Nepal Earthquake (2015) and Italy Earthquake (2016) have been used for training and validation of the models, and original COVID-19 data is acquired, annotated, and used for testing. Detailed data analysis of tweets collected during different disasters has also been incorporated in the paper. The proposed model has been able to achieve 83% classification accuracy on the original COVID-19 dataset. Local Interpretable Model-Agnostic Explanations (LIME) is used to explain the behavior and shortcomings model on COVID-19 data. This paper provides a simple choice for real-world applications and a good starting point for future research. © 2021 Elsevier Ltd","COVID-19 preparedness; Deep learning; Disaster management; Sentiment analysis",
"Behnisch M., Ultsch A.","Knowledge discovery in spatial planning data: A concept for cluster understanding","10.1007/978-3-319-11469-9_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942089744&doi=10.1007%2f978-3-319-11469-9_3&partnerID=40&md5=53df6a7d82826145daa59c91263d4059","The objective of this paper is to present a methodology for discovering comprehensible, valid, potentially innovative, and useful patterns, i.e., new knowledge, in multidimensional spatial data. Techniques from statistics, machine learning, and data mining are applied in consecutive logical steps to allow the visualization of results and the application of validation procedures at each stage. However, the approach does not end with a data cluster; rather, if such a valid cluster has been achieved, then the question is posed: “What do the clusters mean?”. Symbolic machine learning methods are employed to produce an explanation of the clusters in terms of rules employing an understandable subset of the high-dimensional data variables. This combined with canonical representatives of a cluster and consideration of the spatial distribution of the clusters lead to hypothesis on emergent data structures, that is, potential new knowledge. The approach is demonstrated on an exemplary data set of German urban districts featuring seven dimensions of land use. © Springer International Publishing Switzerland 2015.","Cluster; Data mining; Knowledge discovery; Spatial planning","Artificial intelligence; Clustering algorithms; Data mining; Land use; Learning systems; Canonical representatives; Cluster; Data clusters; High dimensional data; Spatial data; Spatial planning; Symbolic machine learning; Useful patterns; Data visualization"
"Behnke G., Schiller M., Kraus M., Bercher P., Schmautz M., Dorna M., Dambier M., Minker W., Glimm B., Biundo S.","Alice in DIY wonderland or: Instructing novice users on how to use tools in DIY projects","10.3233/AIC-180604","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063111748&doi=10.3233%2fAIC-180604&partnerID=40&md5=bce2f08ad72f98c0ab71cb9b15a4769c","We present the interactive assistant Robert that provides situation-adaptive support in the realisation of do-it-yourself (DIY) home improvement projects. Robert assists its users by providing comprehensive step-by-step instructions for completing the DIY project. Each instruction is illustrated with detailed graphics, written and spoken text, as well as with videos. They explain how the steps of the project have to be prepared and assembled and give precise instructions on how to operate the required electric devices. The step-by-step instructions are generated by a hierarchical planner, which enables Robert to adapt to a multitude of environments easily. Parts of the underlying model are derived from an ontology storing information about the available devices and resources. A dialogue manager capable of natural language interaction is responsible for hands-free interaction. We explain the required background technology and present preliminary results of an empirical evaluation. © 2019-IOS Press and the authors. All rights reserved.","Companion-technology; digital assistant; knowledge representation; planning-based assistance","Artificial intelligence; Communication; Dialogue manager; Digital assistants; Electric devices; Empirical evaluations; Hands-free interactions; Hierarchical planners; Natural language interaction; Step-by-step instructions; Knowledge representation"
"Behnood A., Daneshvar D.","A machine learning study of the dynamic modulus of asphalt concretes: An application of M5P model tree algorithm","10.1016/j.conbuildmat.2020.120544","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090017485&doi=10.1016%2fj.conbuildmat.2020.120544&partnerID=40&md5=6ca3f3e7599fa8f9df4300a82c7270a6","Dynamic modulus of asphalt concrete, which is a key parameter characterizing its performance, can be either measured in the laboratory through time-taking and expensive experiments or estimated through predictive models. In this study, the M5P model tree algorithm is used to develop the predictive models of the dynamic modulus of asphalt concretes. Compared to other machine learning-based techniques, the M5P algorithm is easy-to-use and provides more understandable linear mathematical expressions between the input and output variables. To develop the predictive models, a dataset containing information on the binder properties, gradation characteristics, volumetric properties, and test conditions was collected from the literature. Due to the highly skewed distribution of the output variable (i.e., dynamic modulus), separate predictive models were developed using the real and logarithmic values of dynamic modulus. The performance of the developed models was evaluated and compared with that of the Witczak, Hirsch, Al-Khateeb et al., and ANN models as the most commonly accepted predictive models of dynamic modulus. The findings of this study show that the models developed using the M5P algorithm outperform the previously developed models. Moreover, a logarithmic transformation of the dynamic modulus values significantly improved the performance of the model. © 2020 Elsevier Ltd","Asphalt concrete; Dynamic modulus; Logarithmic transformation; M5P model; Witczak model","Asphalt concrete; Concretes; Forestry; Learning algorithms; Machine learning; Statistical tests; Trees (mathematics); Turing machines; Volumetric analysis; Binder properties; Input and outputs; Logarithmic transformations; Mathematical expressions; Parameter characterizing; Predictive models; Skewed distribution; Volumetric properties; Predictive analytics"
"Behr M., Wang Y., Li X., Yu B.","Provable Boolean interaction recovery from tree ensemble obtained via random forests","10.1073/pnas.2118636119","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130667981&doi=10.1073%2fpnas.2118636119&partnerID=40&md5=921d44b70fa4e663f125850ca51f164f","Random Forests (RFs) are at the cutting edge of supervised machine learning in terms of prediction performance, especially in genomics. Iterative RFs (iRFs) use a tree ensemble from iteratively modified RFs to obtain predictive and stable nonlinear or Boolean interactions of features. They have shown great promise for Boolean biological interaction discovery that is central to advancing functional genomics and precision medicine. However, theoretical studies into how tree-based methods discover Boolean feature interactions are missing. Inspired by the thresholding behavior in many biological processes, we first introduce a discontinuous nonlinear regression model, called the ""Locally Spiky Sparse"" (LSS) model. Specifically, the LSS model assumes that the regression function is a linear combination of piecewise constant Boolean interaction terms. Given an RF tree ensemble, we define a quantity called ""Depth-Weighted Prevalence"" (DWP) for a set of signed features S±. Intuitively speaking, DWP(S±) measures how frequently features in S± appear together in an RF tree ensemble.We prove that,with high probability,DWP(S±) attains a universal upper bound that does not involve any model coefficients, if and only if S± corresponds to a union of Boolean interactions under the LSS model. Consequentially, we show that a theoretically tractable version of the iRF procedure, called LSSFind, yields consistent interaction discovery under the LSS model as the sample size goes to infinity. Finally, simulation results show that LSSFind recovers the interactions under the LSS model, even when some assumptions are violated. © 2022 National Academy of Sciences. All rights reserved.","consistency; decision trees; ensemble methods; interaction selection; interpretable machine learning","article; decision tree; machine learning; nonlinear regression analysis; prevalence; probability; random forest; sample size; simulation; speech; theoretical study; algorithm; Algorithms; Machine Learning"
"Behravan A., Abboush M., Obermaisser R.","Deep Learning Application in Mechatronics Systems' Fault Diagnosis, a Case Study of the Demand-Controlled Ventilation and Heating System","10.1109/ICASET.2019.8714453","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067025834&doi=10.1109%2fICASET.2019.8714453&partnerID=40&md5=091c9fb5d2e5bb22ead599485cff1c61","Mechatronics systems include a vast range of interdisciplinary area of electrical and mechanical systems e.g. heating, ventilation, and air-conditioning systems in building automation systems that are responsible to provide occupants a comfortable and productive environment. The demand-controlled ventilation system as an advanced control approach in smart buildings is used for the main goal of energy saving. But, these kinds of systems because of their numerous components such as sensors and actuators are very prone to the faults. Arise of the faults, if they are not detected and diagnosed early, can lead to the system's performance degradation or extra maintenance cost and effort. Nowadays, introducing a suitable generic technique for fault detection and diagnosis is an utmost challenge. The contribution of this paper is to present a novel fault detection and diagnosis framework based on deep learning method for a case study of mechatronics systems, the demand-controlled ventilation and heating system. This paper presents all the steps including data acquisition, data preprocessing, network model design, model optimization, and network model evaluation. Ten types of faults in different classes as well as the healthy data are used to train and evaluate the performance of the designed network model. The results describe a high accuracy (97.4% in confusion matrix) via the designed deep neural network. Also, this study describes the methodology of selecting the optimum parameters of the training process by analyzing the effect of each parameter on the training accuracy. © 2019 IEEE.","Deep Learning; Deep Neural Network; Fault Detection and Diagnosis; HVAC","Air conditioning; Automation; Computer aided instruction; Data acquisition; Deep learning; Deep neural networks; Energy conservation; Heating equipment; HVAC; Intelligent buildings; Ventilation; Data preprocessing; Demand-controlled ventilation; Fault detection and diagnosis; Mechatronics systems; Model optimization; Optimum parameters; Sensors and actuators; System's performance; Fault detection"
"Behrendt F., Bhattacharya D., Krüger J., Opfer R., Schlaefer A.","Data-Efficient Vision Transformers for Multi-Label Disease Classification on Chest Radiographs","10.1515/cdbme-2022-0009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135571480&doi=10.1515%2fcdbme-2022-0009&partnerID=40&md5=76affd5a16794cb580b01d8b52304412","Radiographs are a versatile diagnostic tool for the detection and assessment of pathologies, for treatment planning or for navigation and localization purposes in clinical interventions. However, their interpretation and assessment by radiologists can be tedious and error-prone. Thus, a wide variety of deep learning methods have been proposed to support radiologists interpreting radiographs. Mostly, these approaches rely on convolutional neural networks (CNN) to extract features from images. Especially for the multi-label classification of pathologies on chest radiographs (Chest X-Rays, CXR), CNNs have proven to be well suited. On the Contrary, Vision Transformers (ViTs) have not been applied to this task despite their high classification performance on generic images and interpretable local saliency maps which could add value to clinical interventions. ViTs do not rely on convolutions but on patch-based self-attention and in contrast to CNNs, no prior knowledge of local connectivity is present. While this leads to increased capacity, ViTs typically require an excessive amount of training data which represents a hurdle in the medical domain as high costs are associated with collecting large medical data sets. In this work, we systematically compare the classification performance of ViTs and CNNs for different data set sizes and evaluate more data-efficient ViT variants (DeiT). Our results show that while the performance between ViTs and CNNs is on par with a small benefit for ViTs, DeiTs outperform the former if a reasonably large data set is available for training. © 2022 by Walter de Gruyter Berlin/Boston.","Chest Radiograph; CheXpert; Convolutional Neural Network; Deep Learning; Vision Transformer","Classification (of information); Convolution; Deep learning; Diagnosis; Image segmentation; Learning systems; Pathology; Radiography; Chest radiographs; Chexpert; Classification performance; Clinical interventions; Convolutional neural network; Deep learning; Diagnostics tools; Disease classification; Multi-labels; Vision transformer; Convolutional neural networks"
"Behrendt W.","Courteous sensors - rules and methodology","10.1007/978-3-319-31456-3_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961223810&doi=10.1007%2f978-3-319-31456-3_5&partnerID=40&md5=bd3a2fe6f171c06260a92acd64fe01db","We posit that sensors of the emerging Internet of Things (IoT) should behave courteously. This means that the sensor makes itself known to a subject, that it stops sensing upon request, that it respects the subject as originator of the data and that it negotiates further use of the subject’s data before activating such further use. We state and justify the four fundamental rules for sensor behaviour and outline a methodology for responsible design of sensor-based information systems. © Springer International Publishing Switzerland 2016.",,"Computer science; Computers; Internet of Things (IOT); Artificial intelligence"
"Behrens G., Beucler T., Gentine P., Iglesias-Suarez F., Pritchard M., Eyring V.","Non-Linear Dimensionality Reduction With a Variational Encoder Decoder to Understand Convective Processes in Climate Models","10.1029/2022MS003130","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136995104&doi=10.1029%2f2022MS003130&partnerID=40&md5=af546863b11a3489b035d9a6bdf0bc2e","Deep learning can accurately represent sub-grid-scale convective processes in climate models, learning from high resolution simulations. However, deep learning methods usually lack interpretability due to large internal dimensionality, resulting in reduced trustworthiness in these methods. Here, we use Variational Encoder Decoder structures (VED), a non-linear dimensionality reduction technique, to learn and understand convective processes in an aquaplanet superparameterized climate model simulation, where deep convective processes are simulated explicitly. We show that similar to previous deep learning studies based on feed-forward neural nets, the VED is capable of learning and accurately reproducing convective processes. In contrast to past work, we show this can be achieved by compressing the original information into only five latent nodes. As a result, the VED can be used to understand convective processes and delineate modes of convection through the exploration of its latent dimensions. A close investigation of the latent space enables the identification of different convective regimes: (a) stable conditions are clearly distinguished from deep convection with low outgoing longwave radiation and strong precipitation; (b) high optically thin cirrus-like clouds are separated from low optically thick cumulus clouds; and (c) shallow convective processes are associated with large-scale moisture content and surface diabatic heating. Our results demonstrate that VEDs can accurately represent convective processes in climate models, while enabling interpretability and better understanding of sub-grid-scale physical processes, paving the way to increasingly interpretable machine learning parameterizations with promising generative properties. © 2022 The Authors. Journal of Advances in Modeling Earth Systems published by Wiley Periodicals LLC on behalf of American Geophysical Union.","convection; dimensionality reduction; explainable artificial intelligence; generative deep learning; machine learning; parameterization","Climate models; Clouds; Deep learning; Feedforward neural networks; Learning systems; Signal encoding; Convection; Decoder structures; Dimensionality reduction; Encoder-decoder; Explainable artificial intelligence; Generative deep learning; Interpretability; Machine-learning; Nonlinear dimensionality reduction; Subgrid scale; Decoding; climate modeling; convection; cumulus; diabatic process; longwave radiation; machine learning; simulation"
"Behrens T., Viscarra Rossel R.A., Ramirez-Lopez L., Baumann P.","Soil spectroscopy with the Gaussian pyramid scale space","10.1016/j.geoderma.2022.116095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135951645&doi=10.1016%2fj.geoderma.2022.116095&partnerID=40&md5=db59e33951cbb43332dd9b25e7c8e87b","Soil visible-near infrared (vis–NIR) spectra are complex and modeling soil properties can be challenging. They can suffer from additive and multiplicative noise, they are hyper-dimensional and highly collinear, making their analyses and interpretation sometimes difficult. Here, we introduce the Gaussian pyramid scale space as a multi-resolution approach for denoising spectra, reducing dimensionality, and improving the interpretability and accuracy of spectroscopic machine learning. We also used the approach to analyse contextual interactions between different resolutions and the stability of feature importance across different resolutions. Using an Australian data set and the German data in the LUCAS spectral database we found that with a single Gaussian scale that represents a relatively coarse spectral resolution, we could estimate organic carbon, clay and pH as accurately as with multiple Gaussian scales, or using all resolutions. This indicates that, in the vis–NIR range, there are no relevant interactions between resolutions, which simplifies interpretations. We conclude that the Gaussian pyramid scale space can help to model soil properties with spectral machine learning, improving both accuracy and interpretability. Because the Gaussian pyramid approach is computationally efficient, it can also be used for preprocessing and knowledge discovery before more elaborate modeling is applied. © 2022 The Authors","Feature engineering; Feature importance; Gaussian pyramid scale space; Interpretable machine learning; Soil spectroscopy","Gaussian distribution; Infrared devices; Machine learning; Soils; Different resolutions; Feature engineerings; Feature importance; Gaussian pyramid scale space; Gaussian pyramids; Interpretable machine learning; Machine-learning; Scale spaces; Soil property; Soil spectroscopies; Organic carbon; additive; data set; Gaussian method; knowledge; machine learning; soil analysis; spectroscopy"
"Behrens T., Viscarra Rossel R.A.","On the interpretability of predictors in spatial data science: the information horizon","10.1038/s41598-020-73773-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092291645&doi=10.1038%2fs41598-020-73773-y&partnerID=40&md5=1785d58c554eca6f6ab9cdc48efc42e8","Two important theories in spatial modelling relate to structural and spatial dependence. Structural dependence refers to environmental state-factor models, where an environmental property is modelled as a function of the states and interactions of environmental predictors, such as climate, parent material or relief. Commonly, the functions are regression or supervised classification algorithms. Spatial dependence is present in most environmental properties and forms the basis for spatial interpolation and geostatistics. In machine learning, modelling with geographic coordinates or Euclidean distance fields, which resemble linear variograms with infinite ranges, can produce similar interpolations. Interpolations do not lend themselves to causal interpretations. Conversely, with structural modeling, one can, potentially, extract knowledge from the modelling. Two important characteristics of such interpretable environmental modelling are scale and information content. Scale is relevant because very coarse scale predictors can show nearly infinite ranges, falling out of what we call the information horizon, i.e. interpretation using domain knowledge isn’t possible. Regarding information content, recent studies have shown that meaningless predictors, such as paintings or photographs of faces, can be used for spatial environmental modelling of ecological and soil properties, with accurate evaluation statistics. Here, we examine under which conditions modelling with such predictors can lead to accurate statistics and whether an information horizon can be derived for scale and information content. © 2020, The Author(s).",,"article; classification algorithm; climate; data science; geostatistical analysis; painting; photography; soil property"
"Behzadi S., Müller N.S., Plant C., Böhm C.","Clustering of mixed-type data considering concept hierarchies: problem specification and algorithm","10.1007/s41060-020-00216-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087966078&doi=10.1007%2fs41060-020-00216-2&partnerID=40&md5=66a20cb80f6fb934ceaec35bceecc405","Most clustering algorithms have been designed only for pure numerical or pure categorical data sets, while nowadays many applications generate mixed data. It raises the question how to integrate various types of attributes so that one could efficiently group objects without loss of information. It is already well understood that a simple conversion of categorical attributes into a numerical domain is not sufficient since relationships between values such as a certain order are artificially introduced. Leveraging the natural conceptual hierarchy among categorical information, concept trees summarize the categorical attributes. In this paper, we introduce the algorithm ClicoT (CLustering mixed-type data Including COncept Trees) as reported by Behzadi et al. (Advances in Knowledge Discovery and Data Mining, Springer, Cham, 2019) which is based on the minimum description length principle. Profiting of the conceptual hierarchies, ClicoT integrates categorical and numerical attributes by means of a MDL-based objective function. The result of ClicoT is well interpretable since concept trees provide insights into categorical data. Extensive experiments on synthetic and real data sets illustrate that ClicoT is noise-robust and yields well-interpretable results in a short runtime. Moreover, we investigate the impact of concept hierarchies as well as various data characteristics in this paper. © 2020, The Author(s).","Information-theoretic clustering; Mixed-type data","Data mining; Trees (mathematics); Categorical attributes; Conceptual hierarchy; Data characteristics; Knowledge discovery and data minings; Minimum description length principle; Numerical attributes; Problem specification; Synthetic and real data; Clustering algorithms"
"Behzadi S., Müller N.S., Plant C., Böhm C.","Clustering of mixed-type data considering concept hierarchies","10.1007/978-3-030-16148-4_43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064904072&doi=10.1007%2f978-3-030-16148-4_43&partnerID=40&md5=57f7eae35d647a7c6ef20731e45f8d1c","Most clustering algorithms have been designed only for pure numerical or pure categorical data sets while nowadays many applications generate mixed data. It arises the question how to integrate various types of attributes so that one could efficiently group objects without loss of information. It is already well understood that a simple conversion of categorical attributes into a numerical domain is not sufficient since relationships between values such as a certain order are artificially introduced. Leveraging the natural conceptual hierarchy among categorical information, concept trees summarize the categorical attributes. In this paper we propose the algorithm ClicoT (CLustering mixed-type data Including COncept Trees) which is based on the Minimum Description Length (MDL) principle. Profiting of the conceptual hierarchies, ClicoT integrates categorical and numerical attributes by means of a MDL based objective function. The result of ClicoT is well interpretable since concept trees provide insights of categorical data. Extensive experiments on synthetic and real data set illustrate that ClicoT is noise-robust and yields well interpretable results in a short runtime. © Springer Nature Switzerland AG 2019.",,"Data mining; Trees (mathematics); Categorical attributes; Concept hierarchies; Conceptual hierarchy; Minimum description length principle; Numerical attributes; Numerical domains; Objective functions; Synthetic and real data; Clustering algorithms"
"Beikmohammadi A., Faez K., Motallebi A.","SWP-LeafNET: A novel multistage approach for plant leaf identification based on deep CNN","10.1016/j.eswa.2022.117470","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129864654&doi=10.1016%2fj.eswa.2022.117470&partnerID=40&md5=0e4f5fa2833e91629d56c393e828cb7e","Modern scientific and technological advances allow botanists to use computer vision-based approaches for plant identification tasks. These approaches have their own challenges. Leaf classification is a computer-vision task performed for the automated identification of plant species, a serious challenge due to variations in leaf morphology, including its size, texture, shape, and venation. Researchers have recently become more inclined toward deep learning-based methods rather than conventional feature-based methods due to the popularity and successful implementation of deep learning methods in image analysis, object recognition, and speech recognition. In this paper, to have an interpretable and reliable system, a botanist's behavior is modeled in leaf identification by proposing a highly-efficient method of maximum behavioral resemblance developed through three deep learning-based models. Different layers of the three models are visualized to ensure that the botanist's behavior is modeled accurately. The first and second models are designed from scratch. Regarding the third model, the pre-trained architecture MobileNetV2 is employed along with the transfer-learning technique. The proposed method is evaluated on two well-known datasets: Flavia and MalayaKew. According to a comparative analysis, the suggested approach is more accurate than hand-crafted feature extraction methods and other deep learning techniques in terms of 99.67% and 99.81% accuracy. Unlike conventional techniques that have their own specific complexities and depend on datasets, the proposed method requires no hand-crafted feature extraction. Also, it increases accuracy as compared with other deep learning techniques. Moreover, SWP-LeafNET is distributable and considerably faster than other methods because of using shallower models with fewer parameters asynchronously. © 2022 The Author(s)","Convolutional neural network; Deep learning; Plant leaf recognition; SWP-LeafNET","Convolutional neural networks; Deep learning; Extraction; Feature extraction; Object recognition; Plants (botany); Speech recognition; Textures; Convolutional neural network; Deep learning; Leaf identification; Leaf recognition; Learning techniques; Multistage approach; Plant leaf recognition; Plant leaves; Scientific advances; SWP-LeafNET; Computer vision"
"Beil F., Ester M., Xu X.","Frequent term-based text clustering","10.1145/775047.775110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242540441&doi=10.1145%2f775047.775110&partnerID=40&md5=003c1586bca68047caba81ed574bd796","Text clustering methods can be used to structure large sets of text or hypertext documents. The well-known methods of text clustering, however, do not really address the special problems of text clustering: very high dimensionality of the data, very large size of the databases and understandability of the cluster description. In this paper, we introduce a novel approach which uses frequent item (term) sets for text clustering. Such frequent sets can be efficiently discovered using algorithms for association rule mining. To cluster based on frequent term sets, we measure the mutual overlap of frequent sets with respect to the sets of supporting documents. We present two algorithms for frequent term-based text clustering, FTC which creates flat clusterings and HFTC for hierarchical clustering. An experimental evaluation on classical text documents as well as on web documents demonstrates that the proposed algorithms obtain clusterings of comparable quality significantly more efficiently than state-of-the-art text clustering algorithms. Furthermore, our methods provide an understandable description of the discovered clusters by their frequent term sets.","Clustering; Frequent item sets; Text documents","Algorithms; Data mining; Data structures; Database systems; Hierarchical systems; Set theory; Frequent item sets; Hierarchical clustering; Hypertext documents; Rule mining; Text clustering; Hypertext systems"
"Beisbart C., Räz T.","Philosophy of science at sea: Clarifying the interpretability of machine learning","10.1111/phc3.12830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128404652&doi=10.1111%2fphc3.12830&partnerID=40&md5=8c64209ea29bafa1711a142375b637ed","In computer science, there are efforts to make machine learning more interpretable or explainable, and thus to better understand the underlying models, algorithms, and their behavior. But what exactly is interpretability, and how can it be achieved? Such questions lead into philosophical waters because their answers depend on what explanation and understanding are—and thus on issues that have been central to the philosophy of science. In this paper, we review the recent philosophical literature on interpretability. We propose a systematization in terms of four tasks for philosophers: (i) clarify the notion of interpretability, (ii) explain the value of interpretability, (iii) provide frameworks to think about interpretability, and (iv) explore important features of it to adjust our expectations about it. © 2022 The Authors. Philosophy Compass published by John Wiley & Sons Ltd.",,
"Bej S., Sarkar J., Biswas S., Mitra P., Chakrabarti P., Wolkenhauer O.","Identification and epidemiological characterization of Type-2 diabetes sub-population using an unsupervised machine learning approach","10.1038/s41387-022-00206-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130844423&doi=10.1038%2fs41387-022-00206-2&partnerID=40&md5=530d610fa98550d150c6e7697c8c198d","Background: Studies on Type-2 Diabetes Mellitus (T2DM) have revealed heterogeneous sub-populations in terms of underlying pathologies. However, the identification of sub-populations in epidemiological datasets remains unexplored. We here focus on the detection of T2DM clusters in epidemiological data, specifically analysing the National Family Health Survey-4 (NFHS-4) dataset from India containing a wide spectrum of features, including medical history, dietary and addiction habits, socio-economic and lifestyle patterns of 10,125 T2DM patients. Methods: Epidemiological data provide challenges for analysis due to the diverse types of features in it. In this case, applying the state-of-the-art dimension reduction tool UMAP conventionally was found to be ineffective for the NFHS-4 dataset, which contains diverse feature types. We implemented a distributed clustering workflow combining different similarity measure settings of UMAP, for clustering continuous, ordinal and nominal features separately. We integrated the reduced dimensions from each feature-type-distributed clustering to obtain interpretable and unbiased clustering of the data. Results: Our analysis reveals four significant clusters, with two of them comprising mainly of non-obese T2DM patients. These non-obese clusters have lower mean age and majorly comprises of rural residents. Surprisingly, one of the obese clusters had 90% of the T2DM patients practising a non-vegetarian diet though they did not show an increased intake of plant-based protein-rich foods. Conclusions: From a methodological perspective, we show that for diverse data types, frequent in epidemiological datasets, feature-type-distributed clustering using UMAP is effective as opposed to the conventional use of the UMAP algorithm. The application of UMAP-based clustering workflow for this type of dataset is novel in itself. Our findings demonstrate the presence of heterogeneity among Indian T2DM patients with regard to socio-demography and dietary patterns. From our analysis, we conclude that the existence of significant non-obese T2DM sub-populations characterized by younger age groups and economic disadvantage raises the need for different screening criteria for T2DM among rural Indian residents. © 2022, The Author(s).",,"addiction; adult; age; Article; clustering algorithm; dbscan algorithm; diabetic patient; dietary pattern; dimensionality reduction; epidemiological data; epidemiological monitoring; feature detection; feature extraction; female; health survey; human; India; lifestyle and related phenomena; major clinical study; male; medical history; non insulin dependent diabetes mellitus; protein diet; quality of life; rural population; socioeconomics; uniform manifold approximation; unsupervised machine learning; vegetarian diet; diet; non insulin dependent diabetes mellitus; obesity; Diabetes Mellitus, Type 2; Diet; Humans; India; Obesity; Unsupervised Machine Learning"
"Bejarano G., Kulkarni A., Raushan R., Seetharam A., Ramesh A.","SWaP: Probabilistic graphical and deep learning models for water consumption prediction","10.1145/3360322.3360846","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077287212&doi=10.1145%2f3360322.3360846&partnerID=40&md5=234656d706d64c0916509a304bc6c6fd","Accurately predicting water consumption in residential and commercial buildings is essential for identifying possible leaks, minimizing water wastage, and for paving the way for a sustainable future. In this paper, we present SWaP, a Smart Water Prediction system that predicts future hourly water consumption based on historical data. To perform this prediction task, in SWaP, we design discriminative probabilistic graphical and deep learning models, in particular, sparse Gaussian Conditional Random Fields (GCRFs) and Long Short Term Memory (LSTM) based deep Recurrent Neural Network (RNN) models, to successfully encode dependencies in the water consumption data. We evaluate our system on water consumption data collected from multiple buildings in a university campus and demonstrate that both the GCRF and LSTM based deep models are able to accurately predict future hourly water consumption in advance using just the last 24 hours of data at test time. SWaP achieves superior prediction performance for all buildings in comparison to the linear regression and ARIMA baselines in terms of Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE), with the GCRF and LSTM models providing 50% and 44% improvements on average, respectively. We also demonstrate that augmenting our models with temporal features such as time of the day and day of the week can improve the overall average prediction performance. Additionally, based on our evaluation, we observe that the GCRF model outperforms the LSTM based deep learning model, while simultaneously being faster to train and execute at test time. The computationally efficient and interpretable nature of GCRF models in SWaP make them an ideal choice for practical deployment. © 2019 ACM.","RNN; Sparse GCRF; Time series modeling","Energy efficiency; Forecasting; Intelligent buildings; Long short-term memory; Mean square error; Office buildings; Computationally efficient; Conditional random field; Prediction performance; Recurrent neural network (RNN); Root mean squared errors; Sparse GCRF; Time series modeling; Water consumption predictions; Deep learning"
"Bejder H., Jensen B.H., Thomsen E., Andresen L.l., Borchersen S.","Commercial perspective when applying ai techniques and traditional IT skills","10.1016/0066-4138(94)90091-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-43949161619&doi=10.1016%2f0066-4138%2894%2990091-4&partnerID=40&md5=81f7e51a6df6e72f762b83745056fb75","RH&H's role in the HINT project was to focus on integration, finishing, utilisation, and marketability of the overall technological result. The other HINT partners were responsible for separate AI techniques and system units, which RH&H integrated to create a commercial product. To identify the industrial requirements of a commercial product, RH&H carried out a survey based on qualitative interviews with experienced, technical managers. We realised that to turn a R&D Project into a commercial product, it is crucial first of all to understand the industrial functional needs and the industry's way to conceptualise the world. © 1995.","application oriented; artificial intelligence; blackboard; Commercial aspects; industrial control; industrial requirements; integrated plant control; integration; portability",
"Bekdaş G., Cakiroglu C., Kim S., Geem Z.W.","Optimal Dimensioning of Retaining Walls Using Explainable Ensemble Learning Algorithms","10.3390/ma15144993","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137273474&doi=10.3390%2fma15144993&partnerID=40&md5=3e26aaa2c1d3ad6dcf98c01637b7e392","This paper develops predictive models for optimal dimensions that minimize the construction cost associated with reinforced concrete retaining walls. Random Forest, Extreme Gradient Boosting (XGBoost), Categorical Gradient Boosting (CatBoost), and Light Gradient Boosting Machine (LightGBM) algorithms were applied to obtain the predictive models. Predictive models were trained using a comprehensive dataset, which was generated using the Harmony Search (HS) algorithm. Each data sample in this database consists of a unique combination of the soil density, friction angle, ultimate bearing pressure, surcharge, the unit cost of concrete, and six different dimensions that describe an optimal retaining wall geometry. The influence of these design features on the optimal dimensioning and their interdependence are explained and visualized using the SHapley Additive exPlanations (SHAP) algorithm. The prediction accuracy of the used ensemble learning methods is evaluated with different metrics of accuracy such as the coefficient of determination, root mean square error, and mean absolute error. Comparing predicted and actual optimal dimensions on a test set showed that an R2 score of 0.99 could be achieved. In terms of computational speed, the LightGBM algorithm was found to be the fastest, with an average execution speed of 6.17 s for the training and testing of the model. On the other hand, the highest accuracy could be achieved by the CatBoost algorithm. The availability of open-source machine learning algorithms and high-quality datasets makes it possible for designers to supplement traditional design procedures with newly developed machine learning techniques. The novel methodology proposed in this paper aims at producing larger datasets, thereby increasing the applicability and accuracy of machine learning algorithms in relation to optimal dimensioning of structures. © 2022 by the authors.","machine learning; optimization; structural design","Adaptive boosting; Availability; Decision trees; Learning systems; Machine learning; Mean square error; Reinforced concrete; Structural design; Structural optimization; Construction costs; Ensemble learning algorithm; Gradient boosting; Light gradients; Machine algorithm; Machine learning algorithms; Machine-learning; Optimisations; Predictive models; Reinforced concrete retaining walls; Retaining walls"
"Beker T., Ansari H., Montazeri S., Song Q., Zhu X.X.","Explainability Analysis of CNN in Detection of Volcanic Deformation Signal","10.1109/IGARSS46834.2022.9883340","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140393386&doi=10.1109%2fIGARSS46834.2022.9883340&partnerID=40&md5=099a9ad1cfa134535433c1bc9a6216ae","With improvement in the processing of synthetic aperture radar interferometry (InSAR) data, the detection of long-term volcanic de-formations becomes possible. While deep learning (DL) models are considered black-box models, challenging to debug, the advances in explainable AI (XAI) help understand the model and how it makes decisions. In this paper, the model is trained on synthetic InSAR velocity maps to detect slow, sustained deformations. XAI tools, including Grad-CAM and t-SNE, are utilized for understanding and improving the trained model. Grad-CAM helps identify the slopeinduced signal and salt lake patterns responsible for the model's misclassifications. T-SNE feature representation visualizations are used to estimate data sets and model class separation ability. Additionally, a sensitivity analysis shows the model performance with different intensity deformation data and uncovers the minimal detectable deformations of 1 cm cumulative deformation over five years. © 2022 IEEE.","Explainable AI; Grad-CAM; InSAR; Sensitivity Analysis; Volcano Detection",
"Bektaş A.B., Ak Ç., Gönen M.","Fast and interpretable genomic data analysis using multiple approximate kernel learning","10.1093/bioinformatics/btac241","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133882826&doi=10.1093%2fbioinformatics%2fbtac241&partnerID=40&md5=b50ef991cdea6474ae1bdc1e71f93ace","Motivation: Dataset sizes in computational biology have been increased drastically with the help of improved data collection tools and increasing size of patient cohorts. Previous kernel-based machine learning algorithms proposed for increased interpretability started to fail with large sample sizes, owing to their lack of scalability. To overcome this problem, we proposed a fast and efficient multiple kernel learning (MKL) algorithm to be particularly used with large-scale data that integrates kernel approximation and group Lasso formulations into a conjoint model. Our method extracts significant and meaningful information from the genomic data while conjointly learning a model for out-of-sample prediction. It is scalable with increasing sample size by approximating instead of calculating distinct kernel matrices. Results: To test our computational framework, namely, Multiple Approximate Kernel Learning (MAKL), we demonstrated our experiments on three cancer datasets and showed that MAKL is capable to outperform the baseline algorithm while using only a small fraction of the input features. We also reported selection frequencies of approximated kernel matrices associated with feature subsets (i.e. gene sets/pathways), which helps to see their relevance for the given classification task. Our fast and interpretable MKL algorithm producing sparse solutions is promising for computational biology applications considering its scalability and highly correlated structure of genomic datasets, and it can be used to discover new biomarkers and new therapeutic guidelines. © 2022 The Author(s) 2022. Published by Oxford University Press.",,
"Bektemyssova G., Kuandykov A., Cho Young I., Muhamediyev R., Iskakov S., Khamitov A.","Construction of recognition system at the uranium production process","10.1109/ICCAS.2014.6987791","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920192931&doi=10.1109%2fICCAS.2014.6987791&partnerID=40&md5=dc7099815523312e40808d6992123428","Taught systems such as artificial neural network (ANN) can be used for data interpretation of electric logging. Using only the ANN algorithm gives the result of coincidence between interpretable data and experimental results in certain samplings from 66% to 73%. But using additional algorithms of recognition and comparing the results of recognition could enhance quality of recognition by 1-3%. The problem of building recognition system by ensemble of classification algorithms was formulated. The paper describes the recognition algorithms which used in research, results of recognition and the process of construction of Recognition System. © 2014 Institute of Control, Robotics and Systems (ICROS).","ensemble of algorithms; Lithology; machine learning; uranium deposit","Learning systems; Lithology; Uranium; Uranium deposits; ANN algorithm; Building recognition; Classification algorithm; Data interpretation; Recognition algorithm; Recognition systems; Uranium production; Machine learning"
"Belanger D., McCallum A.","Structured prediction energy networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998953764&partnerID=40&md5=9c4c3350b05bc01750940892b435133b","We introduce structured prediction energy networks (SPENs), a flexible framework for structured prediction. A deep architecture is used to define an energy function of candidate labels, and then predictions are produced by using backpropagation to iteratively optimize the energy with respect to the labels. This deep architecture captures dependencies between labels that would lead to intractable graphical models, and performs structure learning by automatically learning discriminative features of the structured output. One natural application of our technique is multi-label classification, which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction. We are able to apply SPENs to multi-label problems with substantially larger label sets than previous applications of structured prediction, while modeling high-order interactions using minimal structural assumptions. Overall, deep learning provides remarkable tools for learning features of the inputs to a prediction problem, and this work extends these techniques to learning features of structured outputs. Our experiments provide impressive performance on a variety of benchmark multi-label classification tasks, demonstrate that our technique can be used to provide interpretable structure learning, and illuminate fundamental trade-offs between feedforward and iterative structured prediction.",,"Artificial intelligence; Benchmarking; Classification (of information); Economic and social effects; Iterative methods; Learning systems; Network architecture; Deep architectures; Discriminative features; Flexible framework; Multi label classification; Multi-label problems; Structural assumption; Structure-learning; Structured prediction; Forecasting"
"Belavin V., Arzymatov K., Karpov M., Nevolin A., Sapronov A., Ustyuzhanin A.","Tuning hybrid distributed storage system digital twins by reinforcement learning","10.25728/assa.2018.18.4.660","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072020455&doi=10.25728%2fassa.2018.18.4.660&partnerID=40&md5=bcc33dd3fcefc9ccf3f078e9197cbd61","In this paper we develop a concept of data-driven control of distributed storage systems digital twin. The design of the digital twin is supported by an optimal control strategy created by Reinforcement Learning technique. In the proposed approach we consider a combination of a trained neural network that tunes the parameters of a discrete event-driven storage system simulator to gain the best resemblance to the actual system. The proposed method has several benefits compared to conventional approaches providing trade-offs between the simplicity of customization, the ability to infer nontrivial patterns from the real system represented by the simulation model and interpretable behavior. We consider different optimization metrics and demonstrate the viability of the approach using a toy training system until physical system becomes available. © 2018 ASSA.","Digital twin; Machine learning; Reinforcement learning; Simulation; Storage area network",
"Belay Gebremeskel G., Hailu B., Biazen B.","Architecture and optimization of data mining modeling for visualization of knowledge extraction: Patient safety care","10.1016/j.jksuci.2019.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077170654&doi=10.1016%2fj.jksuci.2019.12.001&partnerID=40&md5=a4d86f98a819aece1ba2e7a95148fcb9","Visualization of the knowledge extraction process is a front line to reveal the detail process and data structure, which is an advanced technique for the presentation of data modeling. However, the mechanisms for healthcare are challenging and dynamic processes to gain a clear insight or understanding of patient care. In this paper, we proposed a new approach of architecture and optimization of data mining modeling for visualization of knowledge extraction by analyzing clinical data sets to define the determinant attributes through modeling techniques. Therefore, architecture for the visualization of the knowledge extraction process is a systematic approach to support users to the best of their knowledge of the issues over the challenge of visualizing techniques. The proposed approach is capable and dynamic to handle and analyze large-scale data in its dimension and context. Such a variable is defined using various techniques to characterize them towards the detection of determinant variables as its influential circumstance. We focused on modeling based visualization as model representation, factor's interaction and integration. The detection process experimented in a different approach and justification as discussed in section five. The finding showed a deep understandability for an advanced and dynamic data mining modeling techniques to integrate applications with domain contexts for the optimal and understandable decision process. The strength of this approach is the depth for visualization towards the knowledge extraction process and its understandability for users as per their background and circumstances. It is also essential to inference for architecture based modeling and visualization for large scale data. Researchers, physicians, experts, and other users are the potentials to refer to these novel ideas and findings. © 2019 The Authors","Architectures; Clinical datasets; Data mining; Data visualization; Decision tree; Knowledge extraction; Pattern analysis",
"Belazreg L., Mahmood S.M., Aulia A.","Fast and cost-effective mathematical models for hydrocarbon-immiscible water alternating gas incremental recovery factor prediction","10.1021/acsomega.1c01901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110991051&doi=10.1021%2facsomega.1c01901&partnerID=40&md5=c04d1c53471883d1deed2be74dfc343a","Predicting the incremental recovery factor with an enhanced oil recovery (EOR) technique is a very crucial task. It requires a significant investment and expert knowledge to evaluate the EOR incremental recovery factor, design a pilot, and upscale pilot result. Water-alternating-gas (WAG) injection is one of the proven EOR technologies, with an incremental recovery factor typically ranging from 5 to 10%. The current approach of evaluating the WAG process, using reservoir modeling, is a very time-consuming and costly task. The objective of this research is to develop a fast and cost-effective mathematical model for evaluating hydrocarbon-immiscible WAG (HC-IWAG) incremental recovery factor for medium-to-light oil in undersaturated reservoirs, designing WAG pilots, and upscaling pilot results. This integrated research involved WAG literature review, WAG modeling, and selected machine learning techniques. The selected machine learning techniques are stepwise regression and group method of data handling. First, the important parameters for the prediction of the WAG incremental recovery factor were selected. This includes reservoir properties, rock and fluid properties, and WAG injection scheme. Second, an extensive WAG and waterflood modeling was carried out involving more than a thousand reservoir models. Third, WAG incremental recovery factor mathematical predictive models were developed and tested, using the group method of data handling and stepwise regression techniques. HC-IWAG incremental recovery factor mathematical models were developed with a coefficient of determination of about 0.75, using 13 predictors. The developed WAG predictive models are interpretable and user-friendly mathematical formulas. These developed models will help the subsurface teams in a variety of ways. They can be used to identify the best candidates for WAG injection, evaluate and optimize the WAG process, help design successful WAG pilots, and facilitate the upscaling of WAG pilot results to full-field scale. All this can be accomplished in a short time at a low cost and with reasonable accuracy. © 2021 The Authors. American Chemical Society.",,
"Belciug S., Gorunescu F.","A hybrid genetic algorithm-queuing multi-compartment model for optimizing inpatient bed occupancy and associated costs","10.1016/j.artmed.2016.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979461243&doi=10.1016%2fj.artmed.2016.03.001&partnerID=40&md5=916e8e5ad19bd708cc7f512ae9d93fd2","Purpose: Explore how efficient intelligent decision support systems, both easily understandable and straightforwardly implemented, can help modern hospital managers to optimize both bed occupancy and utilization costs. Methods and materials: This paper proposes a hybrid genetic algorithm-queuing multi-compartment model for the patient flow in hospitals. A finite capacity queuing model with phase-type service distribution is combined with a compartmental model, and an associated cost model is set up. An evolutionary-based approach is used for enhancing the ability to optimize both bed management and associated costs. In addition, a ""What-if analysis"" shows how changing the model parameters could improve performance while controlling costs. The study uses bed-occupancy data collected at the Department of Geriatric Medicine - St. George's Hospital, London, period 1969-1984, and January 2000. Results: The hybrid model revealed that a bed-occupancy exceeding 91%, implying a patient rejection rate around 1.1%, can be carried out with 159 beds plus 8 unstaffed beds. The same holding and penalty costs, but significantly different bed allocations (156 vs. 184 staffed beds, and 8 vs. 9 unstaffed beds, respectively) will result in significantly different costs (£755 vs. £1172). Moreover, once the arrival rate exceeds 7 patient/day, the costs associated to the finite capacity system become significantly smaller than those associated to an Erlang B queuing model (£134 vs. £947). Conclusion: Encoding the whole information provided by both the queuing system and the cost model through chromosomes, the genetic algorithm represents an efficient tool in optimizing the bed allocation and associated costs. The methodology can be extended to different medical departments with minor modifications in structure and parameterization. © 2016 Elsevier B.V..","Bed occupancy; Compartmental model; Cost model; Genetic algorithm; Queuing model","Algorithms; Artificial intelligence; Costs; Decision support systems; Genetic algorithms; Hospitals; Hybrid materials; Queueing networks; Queueing theory; Bed-occupancy; Compartmental model; Cost modeling; Hybrid genetic algorithms; Intelligent decision support systems; Methods and materials; Multi-compartment models; Queuing models; Cost benefit analysis; Article; compartment model; decision support system; genetic algorithm; hospital bed capacity; hospital bed utilization; hospital cost; hospitalization; priority journal; algorithm; health care organization; hospital patient; theoretical model; Algorithms; Bed Occupancy; Health Care Rationing; Inpatients; Models, Theoretical"
"Beldiceanu N., Dolgui A., Gonnermann C., Gonzalez-Castañé G., Kousi N., Meyers B., Prud'homme J., Thevenin S., Vyhmeister E., Östberg P.-O.","Assistant: Learning and robust decision support system for agile manufacturing environments","10.1016/j.ifacol.2021.08.074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119693666&doi=10.1016%2fj.ifacol.2021.08.074&partnerID=40&md5=a2cb2bea6865e9eb560dc03d04f414f2","The European project ASSISTANT will provide a set of AI-based digital twins that helps process engineers and production planners to operate collaborative mixed-model assembly lines based on the data collected from IoT devices and external data sources. Such a tool will help planners to design the assembly line, plan the production, operate the line, and improve process tuning. In addition, the system monitors the line in real-time, ensures that all required resources are available, and allows fast re-planning when necessary. ASSISTANT aims to make cost-effective decisions while ensuring product quality, safety and wellbeing of the workers, and managing the various sources of uncertainties. The resulting digital twin systems will be data-driven, agile, autonomous, collaborative and explainable, safe but reactive. © 2021 The Authors. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0)","Artificial intelligence; Data analytics; Decision aid; Digital twins; Process and production planning; Real-time control; Reconfigurable manufacturing systems; Scheduling","Agile manufacturing systems; Artificial intelligence; Assembly; Assembly machines; Cost effectiveness; Data Analytics; Digital devices; Learning systems; Planning; Production control; Real time control; Agile manufacturing; Data analytics; Decision aids; European program; Manufacturing environments; Mixed-model assembly lines; Process and production planning; Real-time control; Reconfigurable manufacturing system; Robust decisions; Decision support systems"
"Belharbi S., Rony J., Dolz J., Ayed I.B., McCaffrey L., Granger E.","Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images via Max-Min Uncertainty","10.1109/TMI.2021.3123461","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118535636&doi=10.1109%2fTMI.2021.3123461&partnerID=40&md5=65c8d516d78d2544f4cbec6ace0b871b","Weakly-supervised learning (WSL) has recently triggered substantial interest as it mitigates the lack of pixel-wise annotations. Given global image labels, WSL methods yield pixel-level predictions (segmentations), which enable to interpret class predictions. Despite their recent success, mostly with natural images, such methods can face important challenges when the foreground and background regions have similar visual cues, yielding high false-positive rates in segmentations, as is the case in challenging histology images. WSL training is commonly driven by standard classification losses, which implicitly maximize model confidence, and locate the discriminative regions linked to classification decisions. Therefore, they lack mechanisms for modeling explicitly non-discriminative regions and reducing false-positive rates. We propose novel regularization terms, which enable the model to seek both non-discriminative and discriminative regions, while discouraging unbalanced segmentations. We introduce high uncertainty as a criterion to localize non-discriminative regions that do not affect classifier decision, and describe it with original Kullback-Leibler (KL) divergence losses evaluating the deviation of posterior predictions from the uniform distribution. Our KL terms encourage high uncertainty of the model when the latter inputs the latent non-discriminative regions. Our loss integrates: (i) a cross-entropy seeking a foreground, where model confidence about class prediction is high; (ii) a KL regularizer seeking a background, where model uncertainty is high; and (iii) log-barrier terms discouraging unbalanced segmentations. Comprehensive experiments and ablation studies over the public GlaS colon cancer data and a Camelyon16 patch-based benchmark for breast cancer show substantial improvements over state-of-the-art WSL methods, and confirm the effect of our new regularizers (our code is publicly available at https://github.com/ sbelharbi/deep-wsl-histo-min-max-uncertainty). 1558-254X © 2021 IEEE.","Deep weakly-supervised learning; histology images; image classification; interpretability; semantic segmentation","Deep learning; Diseases; Forecasting; Histology; Image classification; Image segmentation; Pixels; Semantics; Supervised learning; Uncertainty analysis; Deep weakly-supervised learning; Histology images; Histopathology; Images classification; Images segmentations; Interpretability; Predictive models; Semantic segmentation; Solid modelling; Uncertainty; Weakly supervised learning; Semantic Segmentation; breast tumor; diagnostic imaging; entropy; female; histology; human; pathology; uncertainty; Breast Neoplasms; Entropy; Female; Histological Techniques; Humans; Uncertainty"
"Belhekar V., Paranjpye P., Bhatkhande A., Chavan R.","Guarding the guardians: understanding the psychological well-being of forest guards in Indian tiger reserves","10.1080/14888386.2020.1809521","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091096284&doi=10.1080%2f14888386.2020.1809521&partnerID=40&md5=52f542a9ceb33cd808d9fc5c77f602c2","Working on the interface between wildlife and communities, forests guards in Indian tiger reserves are responsible not only for the protection of forests, but also for implementing development initiatives in forest villages. Their job profile entails living in isolated environments with multiple threats to their safety, and relatively little data is available about their psychological well-being. This paper presents findings from a study of forest guards’ experience with their work environment, work satisfaction and psychological well-being. A randomized sample of 242 forest guards from six tiger reserves in Madhya Pradesh, India, responded to questionnaires measuring employee satisfaction and motivation, and completed implicit association tests. The machine learning algorithms and mean comparisons that were carried out suggest that workers from Kanha, Bandhavgarh and Panna Tiger Reserves form a group who have high association with their work indicative of high performance but low work satisfaction and well-being. Whereas workers from Pench, Satpura and Sanjay-Dubri Reserves form another group whose members have high employee engagement along with high work satisfaction and well-being but low association with work. Machine learning-based classification and regression tree (CART) analysis suggested that feedback, task identity, skill variety and organizational commitment are some of the important factors affecting work motivation. © 2020 Biodiversity Conservancy International.","conservation psychology; forest guards; implicit association test; India; tiger reserves",
"Beliard C., Finamore A., Rossi D.","Opening the deep pandora box: Explainable traffic classification","10.1109/INFOCOMWKSHPS50562.2020.9162704","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091480703&doi=10.1109%2fINFOCOMWKSHPS50562.2020.9162704&partnerID=40&md5=cf72afa05a9fcea8f03461ba0c590ac2","Fostered by the tremendous success in the image recognition field, recently there has been a strong push for the adoption of Convolutional Neural Networks (CNN) in networks, especially at the edge, assisted by low-power hardware equipment (known as 'tensor processing units') for the acceleration of CNN-related computations. The availability of such hardware has reignited the interest for traffic classification approaches that are based on Deep Learning. However, unlike tree-based approaches that are easy to interpret, CNNs are in essence represented by a large number of weights, whose interpretation is particularly obscure for the human operators. Since human operators will need to deal, troubleshoot, and maintain these automatically learned models, that will replace the more easily human-readable heuristic rules of DPI classification engine, there is a clear need to open the 'deep pandora box', and make it easily accessible for network domain experts. In this demonstration, we shed light in the inference process of a commercial-grade classification engine dealing with hundreds of classes, enriching the classification workflow with tools to enable better understanding of the inner mechanics of both the traffic and the models. © 2020 IEEE.",,"Computer hardware; Deep learning; Engines; Image recognition; Knowledge acquisition; Low power electronics; Commercial grade; Heuristic rules; Inference process; Lowpower hardware; Network domains; Processing units; Traffic classification; Tree-based approach; Convolutional neural networks"
"Belikov S.B., Boguslayev V.A.","Enhancing the Management Component of Engineers' Knowledge: Collaboration between a Technical University and Industry in the Ukraine","10.5367/000000001101295560","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034107090&doi=10.5367%2f000000001101295560&partnerID=40&md5=d9e7d36b74987c4477c3869262c6947e","Science and investigation are pursued in fulfilment of a natural desire to enrich knowledge, and they create the potential for personal and practical achievement in the industrial environment. This has been a key motivating factor for the continuous development and improvement of education systems, especially at the end of the last millennium. The process of transition from an industrial to an information society has seen the introduction and development of computerization, automatic control, information technologies, and artificial intelligence. In this paper, the authors discuss progress in education in one of the newest independent countries, the Ukraine – a country which has to combine rapid development with the complexities of its transition to a market economy. In this context, the paper discusses the importance of education and training, stressing the importance of cooperation between higher education institutions responsible for technical education and industrial enterprises that need engineers with in-depth technical and economic knowledge. They address in particular collaboration between Zaporozhya State Technical University (ZSTU) and the joint stock company Motor Sich, a manufacturer of gas turbine engines. They also discuss ZSTU's involvement in a major regional effort to prepare secondary school pupils more effectively for higher education and training. © 2001, © 2001 SAGE Publications.",,
"Belinsky A., Afanasev V.","Optimal Control of Energy Pipeline Systems Based on Deep Reinforcement Learning","10.1007/978-3-030-59126-7_148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097936601&doi=10.1007%2f978-3-030-59126-7_148&partnerID=40&md5=8ee84e99e740475e40d4206d9c89e085","Modern pipeline energy systems are structurally complex and geographically distributed engineering installations with hundreds of thousands of interconnected process facilities. The optimal control of such systems is a highly responsible task that cannot be solved without applied decision support software and automated control systems. This article addresses the promising area for the development of intelligent systems of the pipeline energy infrastructure control based on deep reinforcement learning technologies. The authors suggested approaches to the learning of digital models and proved them using the examples of operating gas industry systems. Also, they considered the peculiarities of introducing new technologies and outlined further research prospects. © 2020, Springer Nature Switzerland AG.","Artificial intelligence; Digital twin; Energy industry; Machine learning; Optimal control; Pipeline systems; Reinforcement learning",
"Bélisle-Pipon J.-C., Monteferrante E., Roy M.-C., Couture V.","Artificial intelligence ethics has a black box problem","10.1007/s00146-021-01380-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122322223&doi=10.1007%2fs00146-021-01380-0&partnerID=40&md5=5cc19cca9544dd814a2f966a6032338a","It has become a truism that the ethics of artificial intelligence (AI) is necessary and must help guide technological developments. Numerous ethical guidelines have emerged from academia, industry, government and civil society in recent years. While they provide a basis for discussion on appropriate regulation of AI, it is not always clear how these ethical guidelines were developed, and by whom. Using content analysis, we surveyed a sample of the major documents (n = 47) and analyzed the accessible information regarding their methodology and stakeholder engagement. Surprisingly, only 38% report some form of stakeholder engagement (with 9% involving citizens) and most do not report their methodology for developing normative insights (15%). Our results show that documents with stakeholder engagement develop more comprehensive ethical guidance with greater applicability, and that the private sector is least likely to engage stakeholders. We argue that the current trend for enunciating AI ethical guidance not only poses widely discussed challenges of applicability in practice, but also of transparent development (as it rather behaves as a black box) and of active engagement of diversified, independent and trustworthy stakeholders. While most of these documents consider people and the common good as central to their telos, engagement with the general public is significantly lacking. As AI ethics moves from the initial race for enunciating general principles to more sustainable, inclusive and practical guidance, stakeholder engagement and citizen involvement will need to be embedded into the framing of ethical and societal expectations towards this technology. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","AI ethics principles; Artificial intelligence; Citizen involvement; Ethical guidance; Responsible AI; Stakeholder engagement","Ethical technology; Artificial intelligence ethic principle; Black boxes; Citizen involvement; Civil society; Content analysis; Ethical guidance; Industry government; Responsible artificial intelligence; Stakeholder engagement; Technological development; Artificial intelligence"
"Belknap Robert, Riseman Edward, Hanson Allen","INFORMATION FUSION PROBLEM AND RULE-BASED HYPOTHESES APPLIED TO COMPLEX AGGREGATIONS OF IMAGE EVENTS.",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022583676&partnerID=40&md5=4b512f56e315e496d97f8ace397ef889","A rule-based system for combining information from multiple sources of sensory data is described. Relational rules, integrating data from the output of multiple low level processes, are responsible for creating complex aggregations of the data in order to obtain object hypotheses with an associated confidence. Relational rules are defined between primitive elements of the data abstractions so that sets of elements across representations can be selected and grouped on the basis of relational scalar measures. The system is demonstrated using region and line data and a set of relational measures defined over the two pixel-based representations. The techniques presented are extensions of an earlier rule-based system operating on single types of data abstractions and are easily extended to include motion, stereo, and range data.",,"ARTIFICIAL INTELLIGENCE - Expert Systems; INFORMATION FUSION; RELATIONAL RULES; RULE-BASED SYSTEMS; SENSORY DATA FUSION; IMAGE PROCESSING"
"Bell A., Solano-Kamaiko I., Nov O., Stoyanovich J.","It's Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy","10.1145/3531146.3533090","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133005363&doi=10.1145%2f3531146.3533090&partnerID=40&md5=3d2768d325d769b5a5461f63cf21fc7d","To achieve high accuracy in machine learning (ML) systems, practitioners often use complex ""black-box""models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature - and even the existence - of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque. Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human's ability to anticipate a model's output or identify the most important feature of a model, and subjective measures, such as a human's perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It's just not that simple! © 2022 ACM.","explainability; machine learning; public policy; responsible AI","Decision trees; Economic and social effects; Machine learning; Random forests; Black box modelling; Empirical studies; Explainability; High-accuracy; Human-in-the-loop; Machine-learning; Modeling accuracy; Responsible AI; Simple++; Trade off; Public policy"
"Bell A., Rich A., Teng M., Orešković T., Bras N.B., Mestrinho L., Golubovic S., Pristas I., Zejnilovic L.","Proactive advising: A machine learning driven approach to vaccine hesitancy","10.1109/ICHI.2019.8904616","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075927508&doi=10.1109%2fICHI.2019.8904616&partnerID=40&md5=2f5242b65905e9316ab7f015855f9ad4","Despite once being nearly eradicated, Measles cases in Europe have surged to a 20-year high with more than 60,000 cases in 2018, due to a dramatic decrease in vaccination rates. The decrease in Measles, Mumps, and Rubella (MMR) vaccination rates can be attributed to an increase in 'vaccine hesitancy', or the delay in acceptance or refusal of vaccines despite their availability. Vaccine hesitancy is a relatively new global problem for which effective interventions are not yet established. In this paper, a novel machine learning approach to identify children at risk of not being vaccinated against MMR is proposed, with the objective of facilitating proactive action by healthcare workers and policymakers. A use case of the approach is the provision of individualized informative guidance to families that may otherwise become or are already vaccine hesitant. Using a LASSO logistic regression model trained on 44,000 child Electronic Health Records (EHRs), vaccine hesitant families can be identified with a higher precision (0.72) than predicting vaccine uptake based on a child's infant vaccination record alone (0.63). The model uses a low number of attributes of the child and his or her family and community to produce a prediction, making it readily interpretable by healthcare professionals. The implementation of the machine learning model into an open source dashboard for use by healthcare providers and policymakers as an Early Warning and Monitoring System (EWS) against vaccine hesitancy is proposed. The EWS would facilitate a wide variety of proactive, anticipatory and therefore potentially more effective public health interventions, compared to reactive interventions taken after vaccine rejections. © 2019 IEEE.",,"Air navigation; Health care; Information services; Machine learning; Open systems; Regression analysis; Early warning and monitoring system; Electronic health record (EHRs); Health care professionals; Health care providers; Health interventions; Logistic Regression modeling; Machine learning approaches; Machine learning models; Vaccines"
"Bell K., Hennessy M., Henry M., Malik A.","Predicting Liver Utilization Rate and Post-Transplant Outcomes from Donor Text Narratives with Natural Language Processing","10.1109/SIEDS55548.2022.9799424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134349997&doi=10.1109%2fSIEDS55548.2022.9799424&partnerID=40&md5=5aecca7f586e42c87095dd610b148651","Liver transplantation is a critical, life-saving treatment option for patients with terminal liver disease. Despite an organ shortage, many donated livers are discarded for reasons such as poor organ condition and physical incompatibility with a recipient. Current clinical models for liver risk assessment only utilize tabular data and result in poor precision and recall. Critical information relevant to this decision-making is likely included in the free-Text clinical notes from donor evaluations that contain pertinent medical and social history of the donor that is currently unavailable in tabular data sources. This article describes the development of a model using these free-Text clinical notes using a variety of Natural Language Processing (NLP) and machine learning (ML) techniques to predict the outcomes of three key metrics: 1) liver utilization rate, 2) 30-day mortality rate, and 3) 1-year mortality rate. The free-Text narratives were useful for predicting liver utilization, with an associated area under the curve (AUC) score of 0.81, but were not useful for predicting both mortality outcomes, with associated AUC scores of 0.53 and 0.52, for 30-day and 1-year mortality, respectively. Using a locally interpretable model-Agnostic explanations (LIME) algorithm, key phrases, like 'dcd' and 'alcohol' were found to be associated with unutilized livers, while 'brain' and 'heroin' were associated with utilized livers. Based on these findings, modeling donor text narratives may substantially contribute to improved decision-making and outcomes of liver transplantation. © 2022 IEEE.","Electronic Health Records (EHR); liver transplantation; Natural Language Processing (NLP); text analytics","Decision making; Forecasting; Learning algorithms; Lime; Natural language processing systems; Patient treatment; Population statistics; Electronic health; Electronic health record; Free texts; Health records; Language processing; Liver transplantation; Natural language processing; Natural languages; Text analytics; Utilization rates; Risk assessment"
"Bella, Hendryli J., Herwindiati D.E.","Voice Authentication Model for One-time Password Using Deep Learning Models","10.1145/3378904.3378908","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083312429&doi=10.1145%2f3378904.3378908&partnerID=40&md5=8b8db71f57b6768eeed75ebbe6789a24","This paper explores the possibility of implementing a voice authentication system consisting of speech recognition and speaker verication model for the one-time password (OTP) system. The speech recognition model is responsible for classifying user utterances of random OTP digits in Bahasa Indonesia and the speaker verification model is used to verify the identity of the speaker. The long short-term memory network and siamese network with convolutional neural networks are employed as the model, where they aim to recognize and verify human voices represented by MFCC feature vectors. From the experiments, it is found that the validation accuracy of the speech recognition model is reliable, yet the speaker verication model cannot achieve satisfactory result. © 2020 ACM.","deep learning; one-time password; speaker verication; Speech recognition","Authentication; Big data; Convolutional neural networks; Deep learning; Feature vectors; Human voice; Indonesia; Learning models; One time passwords; Short term memory; Speaker verification; Voice authentication; Speech recognition"
"Bellaby R.W.","Can AI Weapons Make Ethical Decisions?","10.1080/0731129X.2021.1951459","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111098162&doi=10.1080%2f0731129X.2021.1951459&partnerID=40&md5=2fce1732691f1e8ae55302c18109c98a","The ability of machines to make truly independent and autonomous decisions is a goal of many, not least of military leaders who wish to take the human out of the loop as much as possible, claiming that autonomous military weaponry—most notably drones—can make decisions more quickly and with greater accuracy. However, there is no clear understanding of how autonomous weapons should be conceptualized and of the implications that their “autonomous” nature has on them as ethical agents. It will be argued that autonomous weapons are not full ethical agents due to the restrictions of their coding. However, the highly complex machine-learning nature gives the impression that they are making their own decisions and creates the illusion that their human operators are protected from the responsibility of the harm they cause. Therefore, it is important to distinguish between autonomous AI weapons and an AI with autonomy, a distinction that creates two different ethical problems for their use. For autonomous weapons, their limited agency combined with machine-learning means their human counterparts are still responsible for their actions while having no ability to control or intercede in the actual decisions made. If, on the other hand, an AI could reach the point of autonomy, the level of critical reflection would make its decisions unpredictable and dangerous in a weapon. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group on behalf of John Jay College of Criminal Justice of The City University of New York.","artificial intelligence; autonomous; autonomy; ethics; weapons",
"Bellagarda J.S., Abu-Mahfouz A.M.","An Updated Survey on the Convergence of Distributed Ledger Technology and Artificial Intelligence: Current State, Major Challenges and Future Direction","10.1109/ACCESS.2022.3173297","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130859580&doi=10.1109%2fACCESS.2022.3173297&partnerID=40&md5=127b161f434a75c71a6e975cb265a4b8","In recent times, Artificial Intelligence (AI) and Distributed Ledger Technology (DLT) have become two of the most discussed sectors in Information Technology, with each having made a major impact. This has generated space for further innovation to occur in the convergence of the two technologies. In this paper, we gather, analyse, and present a detailed review of the convergence of AI and DLT in a vice versa manner. We review how AI is impacts DLT by focusing on AI-based consensus algorithms, smart contract security, selfish mining, decentralized coordination, DLT fairness, non-fungible tokens, decentralized finance, decentralized exchanges, decentralized autonomous organizations, and blockchain oracles. In terms of the impact DLT has on AI, the areas covered include AI data privacy, explainable AI, smart contract-based AIs, parachains, decentralized neural networks, Internet of Things, 5G technology and data markets, and sharing. Furthermore, we identify research gaps and discuss open research challenges in developing future directions. © 2013 IEEE.","Artificial intelligence; Blockchain technology; Distributed ledger technology; Machine learning","5G mobile communication systems; Data privacy; Distributed ledger; Engineering education; Internet of things; Machine learning; 'current; Block-chain; Blockchain technology; Consensus algorithms; Convergence; Decentralised; Decentralized coordination; Decentralized exchange; Distributed ledg technology; Blockchain"
"Bellandi V., Ceravolo P., Damiani E., Maghool S.","Agent-Based Vector-Label Propagation for Explaining Social Network Structures","10.1007/978-3-031-07920-7_24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135044231&doi=10.1007%2f978-3-031-07920-7_24&partnerID=40&md5=48579bae9a2de252adc45aade5ce223e","Even though Social Network Analysis is quite helpful in studying the structural properties of interconnected systems, real-world networks reveal much more hidden characteristics from interacting domain-specific features. In this study, we designed an Agent-based Vector-label PRopagation Algorithm (AVPRA), which captures both structural properties and domain-specific features of a given network by assigning vectors of features to constituting agents. Experimental analysis proves that our algorithm is accurate in revealing the structural properties of a network in an explainable fashion. Furthermore, the resulting vector-labels are suitable for downstream machine learning tasks. © 2022, Springer Nature Switzerland AG.","Explainability; Social network analysis; Vector-label propagation","Structural properties; Agent based; Domain specific; Explainability; Label propagation; Propagation algorithm; Real-world networks; Social Network Analysis; Social network structures; Structural domains; Vector-label propagation; Vectors"
"Bellatreche L., Garcia F., Pham D.N., Jiménez P.Q.","Sonder: A data-driven methodology for designing net-zero energy public buildings","10.1007/978-3-030-59065-9_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091508896&doi=10.1007%2f978-3-030-59065-9_5&partnerID=40&md5=4bf014131d36290464aa6b760bb149bb","The reduction of carbon emissions into the atmosphere has become an urgent health issue. The energy in buildings and their construction represents more than 1/3 of final global energy consumption and contributes to nearly 1/4 of greenhouse gas emissions worldwide. Heating, Ventilation, and Air-Conditioning (HVAC) systems are major energy consumers and responsible for about 18% of all building energy use. To reduce this huge amount of energy, the Net-Zero Energy Building (nZEB) concept has been imposed by energy authorities. They recommend a massive use of renewable energy technology. With the popularization of Smart Grid, Internet of Things devices, and the Machine Learning (ML), a couple of data-driven approaches emerged to reach this crucial objective. By analysing these approaches, we figure out that they lack a comprehensive methodology with a well-identified life cycle that favours collaboration between nZEB actors. In this paper, we share our vision for developing Energy Management Systems for nZEB as part of IMPROVEMENT EU Interreg Sudoe programm. First, we propose a comprehensive methodology (SONDER), associated with a well-identified life cycle for developing data-driven solutions. Secondly, an instantiation of this methodology is given by considering a case study for predicting the energy consumption of the domestic hot water system in the Regional Hospital of La Axarquia, Spain that includes gas and electricity sections. This prediction is conducted using four ML techniques: multivariate regression, XGBoost, Random Forest and ANN. Our obtained results show the effectiveness of SONDER by offering a fluid collaboration among project actors and the prediction efficiency of ANN. © Springer Nature Switzerland AG 2020.","HVAC; Methodology; Microgrids; ML; nZEB; Prediction","Advanced Analytics; Air conditioning; Big data; Decision trees; Energy management systems; Energy utilization; Forecasting; Gas emissions; Greenhouse gases; Hot water distribution systems; Life cycle; Regression analysis; Building energy use; Data-driven approach; Domestic hot water; Energy in buildings; Multivariate regression; Net-zero energy buildings; Public buildings; Use of renewable energies; Zero energy buildings"
"Belle V., Papantonis I.","Principles and Practice of Explainable Machine Learning","10.3389/fdata.2021.688969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110354249&doi=10.3389%2ffdata.2021.688969&partnerID=40&md5=8491c3bcc14c78ee4ff21a03530f53f2","Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions. © Copyright © 2021 Belle and Papantonis.","black-box models; explainable AI; machine learning; survey; transparent models",
"Belle V.","The quest for interpretable and responsible artificial intelligence","10.1042/bio04105016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075150425&doi=10.1042%2fbio04105016&partnerID=40&md5=0993390260d1c07550dfeebadc4d1742","Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in computational biology, finance, law and robotics. However, such a highly positive impact is coupled with significant challenges: how do we understand the decisions suggested by these systems in order that we can trust them? How can they be held accountable for those decisions?. © Biochemical Society 2019.",,
"Bellini P., Cenni D., Palesi L.A.I., Nesi P., Pantaleo G.","A Deep Learning Approach for Short Term Prediction of Industrial Plant Working Status","10.1109/BigDataService52369.2021.00007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126194328&doi=10.1109%2fBigDataService52369.2021.00007&partnerID=40&md5=4a72c9a407dc3898b7fa35300d122985","Predictive Maintenance has gained more and more research and commercial interests, being a pivotal topic for improving the efficiency of many production industrial plants to minimize downtimes, as well as to reduce operational costs for interventions. Solutions reviewed in literature are increasingly based on machine learning and deep learning methods for prediction of fault proneness with respect to normal working conditions. Many state-of-the art solutions are not actually applied in real scenarios, and have restrictions to be executed in real-time in the production environment. In this paper, a framework for predictive maintenance is presented. It has been built upon a deep learning model based on Long-Short Term Memory Neural Networks, LSTM and Convolutional LSTM. The proposed model provides a one-hour prediction of the plant status and indications on the areas in which the intervention should be performed by using explainable LSTM technique. The solution has been validated against real data of ALTAIR chemical plant, demonstrating an high accuracy with the capability of being executed in real-time in a production operative scenario. The paper also introduced business intelligence tools on maintenance data and the architectural infrastructure for the integration of predictive maintenance approach. © 2021 IEEE.","CNN; Convolutional Neural Networks; Deep Learning; Industry 4.0; Long-Short Term Memory Networks; LSTM; Predictive Maintenance","Brain; Chemical plants; Convolution; Convolutional neural networks; Forecasting; Industrial research; Industry 4.0; Maintenance; CNN; Convolutional neural network; Deep learning; Learning approach; Long-short term memory network; LSTM; Memory network; Predictive maintenance; Real- time; Short term prediction; Long short-term memory"
"Bellini V., Di Noia T., Sciascio E.D., Schiavone A.","Semantics-Aware Autoencoder","10.1109/ACCESS.2019.2953308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077745343&doi=10.1109%2fACCESS.2019.2953308&partnerID=40&md5=87f0b30b149f45742b5802e163069030","Recommender Systems are widely adopted in nowadays services such as e-commerce websites, multimedia streaming platforms, and many others. They help users to find what they are looking for by suggesting relevant items leveraging their past preferences. Deep Learning models are very effective in solving the recommendation problem; as a matter of fact, many deep learning architectures have been proposed over the years. Even if deep learning models outperform many state-of-the-art algorithms, the worst disadvantage is about their interpretability: explaining the reason a specific item has been recommended to a user is quite a difficult task since the model is not interpretable. Accuracy in the recommendation is no more enough since users are also expecting a useful explanation for the suggested items. Users, on the other hand, want to know why. In this paper, we present SemAuto, a novel approach based on an Autoencoder Neural Network that makes it possible to semantically label neurons in hidden layers, thus paving the way to the model's interpretability and consequently to the explanation of a recommendation. We tested our semantics-aware approach with respect to other state-of-the-art algorithms to prove the recommendation's accuracy. Furthermore, we performed an extensive A/B test with real users to evaluate the explanation we generate. © 2013 IEEE.","Autoencoder neural network; cold start problem; deep learning; explanation; knowledge graph; recommender system","Deep learning; Deep neural networks; Electronic commerce; Multimedia services; Recommender systems; Semantics; Auto encoders; Cold start problems; E-commerce websites; explanation; Knowledge graphs; Learning architectures; Multimedia streaming; State-of-the-art algorithms; Multilayer neural networks"
"Bellini V., Schiavone A., Di Noia T., Ragone A., Di Sciascio E.","Knowledge-aware Autoencoders for Explainable Recommender Systems","10.1145/3270323.3270327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056621171&doi=10.1145%2f3270323.3270327&partnerID=40&md5=95e645b8857e4bd6918b914b1826a820","Recommender Systems have been widely used to help users in finding what they are looking for thus tackling the information overload problem. After several years of research and industrial findings looking after better algorithms to improve accuracy and diversity metrics, explanation services for recommendation are gaining momentum as a tool to provide a human-understandable feedback to results computed, in most of the cases, by black-box machine learning techniques. As a matter of fact, explanations may guarantee users satisfaction, trust, and loyalty in a system. In this paper, we evaluate how different information encoded in a Knowledge Graph are perceived by users when they are adopted to show them an explanation. More precisely, we compare how the use of categorical information, factual one or a mixture of them both in building explanations, affect explanatory criteria for a recommender system. Experimental results are validated through an A/B testing platform which uses a recommendation engine based on a Semantics-Aware Autoencoder to build users profiles which are in turn exploited to compute recommendation lists and to provide an explanation. © 2018 Association for Computing Machinery.","Autoencoder neural networks; Deep learning; Explainable models; Explanation; Recommender systems","Deep learning; Industrial research; Semantics; Accuracy and diversities; Auto encoders; Explanation; Gaining momentum; Information overloads; Knowledge graphs; Machine learning techniques; Users profiles; Recommender systems"
"Bellino A., Baldantoni D., Picariello E., Morelli R., Alfani A., De Nicola F.","Role of different microorganisms in remediating PAH-contaminated soils treated with compost or fungi","10.1016/j.jenvman.2019.109675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073099798&doi=10.1016%2fj.jenvman.2019.109675&partnerID=40&md5=4de9ab13a8f4e8bd66bf08b5e85bcb93","Microbial degradation is the main responsible for polycyclic aromatic hydrocarbons (PAHs) removal from contaminated soils, and the understanding of this process is pivotal to define effective bioremediation approaches. To evaluate the contribution of several microbial groups in soil anthracene and benzo[a]pyrene degradation, the analysis of phospholipid fatty acid (PLFA) profiles and machine learning techniques were employed. To this end, PLFAs and PAH concentrations were analysed, along 274 days of incubation in mesocosms, in soils artificially contaminated with anthracene and benzo[a]pyrene, subjected to different treatments: untreated soil and soils treated with biowaste compost or fungal consortium. Random forest models, figuring anthracene or benzo[a]pyrene concentrations as dependent variables and PLFAs as predictors, were then built to evaluate the contribution of each variable in PAH degradation. PLFA profiles varied substantially among soil treatments and along time, with the increase of Actinomycetes in soils added with fungi and other Gram+ bacteria in compost amended soils. The former, together with fungi, are primarily responsible for anthracene and benzo[a]pyrene degradation in both treated soils, a process in which also metanotrophs and other Gram+ and Gram- bacteria participate. In untreated soil, the cooperation of a multitude of different microorganisms was, instead, responsible for PAH removal, a process with lower efficiency in respect to treated soils. © 2019 Elsevier Ltd","Anthracene; benzo[a]pyrene; Biowaste compost; Fungal consortium; PLFA; Random forest","anthracene; benzo[a]pyrene; fatty acid; polycyclic aromatic hydrocarbon; algorithm; bioremediation; compost; concentration (composition); contaminated land; degradation; fatty acid; fungus; microorganism; PAH; phospholipid; pollutant removal; pyrene; soil pollution; soil remediation; Actinobacteria; Article; bioremediation; compost; concentration (parameter); controlled study; fungus; Gram negative bacterium; Gram positive bacterium; machine learning; microbial consortium; microbial degradation; nonhuman; random forest; soil amendment; soil microflora; soil pollution; bioremediation; composting; fungus; microbiology; soil; soil pollutant; Actinobacteria (class); Bacteria (microorganisms); Fungi; Biodegradation, Environmental; Composting; Fungi; Polycyclic Aromatic Hydrocarbons; Soil; Soil Microbiology; Soil Pollutants"
"Bellizio F., Cremer J.L., Sun M., Strbac G.","A causality based feature selection approach for data-driven dynamic security assessment","10.1016/j.epsr.2021.107537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115421654&doi=10.1016%2fj.epsr.2021.107537&partnerID=40&md5=f8a957802e0ed17616fecfa4c02aae48","The integration of renewable energy sources increases the operational uncertainty of electric power systems and can lead to more frequent dynamic phenomena. The use of classifiers from machine learning is promising to include dynamics in the security assessment of the power system. The training of these classifiers is typically performed offline on synthetically generated operating conditions (OCs) that are similar to real-time operation. However, the uncertainty in the generated OCs and the classifier's inaccuracy is larger the longer the time between offline and real-time operation. Moving the classifier training closer to real-time operation is an important step forward to reduce inaccurate predictions and improve reliability. In this paper, a novel causality-based feature selection approach for an online dynamic security assessment (DSA) framework is proposed. The key novelty is to use the system's physics to learn the causal structure between the features and then select the features based on this causal structure. The proposed approach results in faster computations, is more robust and more interpretable. Moreover, classifiers can be trained closer to real-time operation which enhances the predictive performance. Through a case study using transient stability on the IEEE 68-bus system, the proposed method reduces computational time by 75% in comparison to state of the art feature selection techniques. The proposed workflow showed superior performance in accuracy and robustness against uncertainty compared to conventional machine learning approaches for DSA. The computational benefit was also projected to a dataset of the French transmission system where the approach has the potential to achieve computational savings of up-to two orders of magnitudes. © 2021 The Authors","Decision trees; Dynamic security assessment; Feature selection; Markov blanket; Power systems operation","Decision trees; Electric power system security; Electric power transmission; Machine learning; Real time systems; Renewable energy resources; Data driven; Dynamic security assessment; Features selection; Integration of renewable energies; Markov Blankets; Offline; Operating condition; Power system operations; Real-time operation; Uncertainty; Feature extraction"
"Bello B.S., Heckel R., Minku L.","Reverse Engineering the Behaviour of Twitter Bots","10.1109/SNAMS.2018.8554675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060011780&doi=10.1109%2fSNAMS.2018.8554675&partnerID=40&md5=290574083dfd24b226bdb9ca9f84fb30","Recent research has shown significant success in the detection of social bots. While there are tools to distinguish automated bots from regular user accounts, information about their strategies, biases and influence on their target audience remains harder to obtain. To uncover such details, e.g., to understand the role of bots in political campaigns, we address three questions: Can we describe the behaviour of a bot (when and how a bot takes actions) by a set of understandable rules? How can we express bias and influence? Can we extract such information automatically, from observations of a bot? In this paper, we present an approach to reverse engineering the behaviour of Twitter bots to create a visual model explaining their actions. We use machine learning to infer a set of simple and general rules governing the behaviour of a bot. We propose the notion of differential sentiment analysis to provide means of understanding the behaviour with respect to the topics on its network in relation to both its sources of information (friends) and its target audience (followers). Respectively, this provides insights into their bias and the influence aimed at their target audience. We evaluate our approach using prototype bots we created and selected real Twitter bots. The results show that we are successful in correctly describing the behaviour of the bots and potentially useful in understanding their impact. © 2018 IEEE.","behavioral analysis; differential sentiment analysis; reverse engineering; social network analysis; Twitter bots","Data mining; Learning systems; Reverse engineering; Sentiment analysis; Social networking (online); Behavioral analysis; Political campaign; Recent researches; Social bots; Sources of informations; Target audience; Twitter bots; Visual model; Botnet"
"Bello H.J., Palomar-Ciria N., Baca-García E., Lozano C.","Suicide Classification for News Media Using Convolutional Neural Networks","10.1080/10410236.2022.2058686","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136348066&doi=10.1080%2f10410236.2022.2058686&partnerID=40&md5=b1093c3a4429167c104235ed829f889e","Currently, the process of evaluating suicide is highly subjective, which limits the efficacy and accuracy of prevention efforts. Artificial intelligence (AI) has emerged as a mean of investigating large datasets to identify patterns within ‘big data’ that can determine the factors on suicide outcomes. Here, we used AI tools to extract the topic from (press and social) media texts. However, news media articles lack of suicide tags. Using tweets with hashtags related to suicide, we trained a neuronal model that identifies if a given text has a suicide-related topic. Our results suggest a high level of impact of suicide cases in the media, and an intrinsic thematic relationship of suicide news. These results pave the way to build more interpretable suicide data from the media, which may help to better track, understand its origin, and improve prevention strategies. © 2022 Taylor & Francis Group, LLC.",,
"Bello P.F., Bridewell W.","There is no agency without attention","10.1609/aimag.v38i4.2742","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040834071&doi=10.1609%2faimag.v38i4.2742&partnerID=40&md5=42326b21daed01466fb277013b0ac4b9","Over the decades, the view of agency in artificial intelligence (AI) has narrowed to one that emphasizes acting in a way that maximizes reward. This perspective fails to make contact with the broader academic and legal communities where agency is bound up with personal accountability. To explore this gap in meaning, we introduce a spectrum of control that characterizes standard approaches to constructing agents and points the way toward agents that can be held responsible. The linchpin that enables agents to control their actions in the ""right way"" is attention. Broadly construed, attention lets an agent that is responsive to its environment consider the relationships among its actions, goals, and norms while also avoiding distraction. This ability enables strategic norm violations and opens the door to artificial, human-level agency. © Copyright 2017, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Human levels; Legal community; Norm violation; Artificial intelligence"
"Belloir N., Ouerdane W., Pastor O., Frugier É., de Barmon L.-A.","A Conceptual Characterization of Fake News: A Positioning Paper","10.1007/978-3-031-05760-1_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130977822&doi=10.1007%2f978-3-031-05760-1_41&partnerID=40&md5=ce91a009027328ce9ba26374dc341e95","Fake News have become a global phenomenon due to its explosive growth, particularly on social media. How to identify fake news is becoming an extremely attractive working domain. The lack of a sound, well-grounded conceptual characterization of what exactly a Fake news is and what are its main features, makes difficult to manage Fake News understanding, identification and creation. In this research we propose that conceptual modeling must play a crucial role to characterize Fake News content in a precise way. Only clearly delimiting what a Fake News is, it will be possible to understand and managing their different perspectives and dimensions, with the final purpose of developing any reliable framework for online Fake News detection as much automated as possible. This paper discusses the effort that should be made towards a precise conceptual model of Fake News and its relation with an XAI approach. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conceptual modeling; Explainable artificial intelligence; Fake news","Computers; Fake detection; Conceptual model; Explainable artificial intelligence; Explosive growth; Fake news; News content; Reliable frameworks; Social media; Artificial intelligence"
"Bellomarini L., Blasi L., Laurendi R., Sallinger E.","A Reasoning Approach to Financial Data Exchange with Statistical Confidentiality","10.1007/978-3-030-93733-1_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126194488&doi=10.1007%2f978-3-030-93733-1_16&partnerID=40&md5=d112ca7a85d0ab05c1cea106e9172817","Motivated by our experience with the Bank of Italy, in this work we present Vada-SA, a reasoning framework for financial data exchange with statistical confidentiality. By reasoning on the interplay of the features that may lead to identity disclosure, the framework is able to guarantee explainable, declarative, and context-aware confidentiality. © 2021, Springer Nature Switzerland AG.",,"Artificial intelligence; Context-Aware; Financial data; Reasoning approach; Reasoning framework; Statistical confidentiality; Electronic data interchange"
"Bellomarini L., Laurenza E., Sallinger E.","Rule-based Anti-Money Laundering in Financial Intelligence Units: Experience and Vision∗",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092291880&partnerID=40&md5=63df7d39f261bb0192c53f4d46198dd2","Money laundering is a major threat to the good functioning of financial systems. Despite huge technological investments, with machine learning at the heart of the Fintech revolution, we are still lacking explainable solutions in fighting money laundering, especially for Financial Intelligence Units (FIUs). This paper is based on the joint committment of the Fintech community and academia in applying state-of-the-art rule-based reasoning to counteract money laundering. We report a visionary position about the application of logic-based Knowledge Graphs and reasoning with languages in the Datalog+/- family in the anti-money laundering (AML) domain. After motivating the impact and the importance of an explainable rule-based solution, we pin down the core AML problems in the form of high-level decision tasks. We envision that the FIU knowledge is modeled as the ground truth of a KG, so that AML tasks are formulated and carried out as reasoning tasks, addressing specific quality desiderata. We provide technical zoom and concrete exemplification of the approach with a real money laundering case. We discuss relevant research and technological challenges. © 2020 CEUR-WS. All rights reserved.","Anti-money laundering; Fintech; Knowledge Graphs; Reasoning","Computer programming languages; Fintech; Knowledge representation; Anti-money laundering; Financial system; Knowledge graphs; Money laundering; Reasoning tasks; Rule based reasoning; State of the art; Technological challenges; Laundering"
"Bellos C.C., Papadopoulos A., Rosso R., Fotiadis D.I.","Extraction and analysis of features acquired by wearable sensors network","10.1109/ITAB.2010.5687761","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951640962&doi=10.1109%2fITAB.2010.5687761&partnerID=40&md5=1c74a2e75c40195674908946a6b386de","The developed system aims to collect and analyze information valuable for estimating the severity of a health episode regarding patients suffering from chronic diseases. The system addresses a wearable platform, based on multi-parametric sensor data, a device which collects the data and a Smart Assistant Device, responsible for analyzing the collected multi-parametric data and facilitating the monitor of people suffering from chronic diseases in long-stay setting. The wearable platform integrates several sensors collecting information regarding motion, ECG, respiration, oxygen saturation (SpO 2), temperature and various other parameters. The Smart Assistant Device, after applying pre-processing and feature extraction techniques, fuses all sensor acquired data with information inserted by the patient. An Intelligent System is developed to analyze sensors' acquired and patient inputted data, facilitating data mining techniques, and decides upon the severity of a pathological episode that was appeared to the patient. © 2010 IEEE.",,"Chronic disease; Data mining techniques; Feature extraction techniques; Oxygen saturation; Pre-processing; Sensor data; Wearable sensors; Data mining; Diseases; Feature extraction; Information technology; Intelligent systems; Oxygen; Sensors"
"Bellot A., van der Schaar M.","Tree-based Bayesian mixture model for competing risks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059875044&partnerID=40&md5=af24a2237205725ca60a3fa96519e13e","Many chronic diseases possess a shared biology. Therapies designed for patients at risk of multiple diseases need to account for the shared impact they may have on related diseases to ensure maximum overall well-being. Learning from data in this setting differs from classical survival analysis methods since the incidence of an event of interest may be obscured by other related competing events. We develop a semi-parametric Bayesian regression model for survival analysis with competing risks, which can be used for jointly assessing a patient’s risk of multiple (competing) adverse outcomes. We construct a Hierarchical Bayesian Mixture (HBM) model to describe survival paths in which a patient’s covariates influence both the estimation of the type of adverse event and the subsequent survival trajectory through Multivariate Random Forests. In addition variable importance measures, which are essential for clinical interpretability are induced naturally by our model. We aim with this setting to provide accurate individual estimates but also interpretable conclusions for use as a clinical decision support tool. We compare our method with various state-of-the-art benchmarks on both synthetic and clinical data. Copyright 2018 by the author(s).",,"Artificial intelligence; Bioinformatics; Decision support systems; Decision trees; Disease control; Mixtures; Regression analysis; Bayesian mixture model; Bayesian regression; Clinical decision support; Hierarchical bayesian; Interpretability; State of the art; Survival analysis; Variable importances; Risk assessment"
"Bellucci M., Delestre N., Malandain N., Zanni-Merk C.","Towards a terminology for a fully contextualized XAI","10.1016/j.procs.2021.08.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116853963&doi=10.1016%2fj.procs.2021.08.025&partnerID=40&md5=d7555c118ca3396547b82ac30efe0fd3","Explainable Artificial Intelligence (XAI) has seen a surge in popularity in the past few years, thanks to new legislations that promote the “right to explanation”. Many popular methods have been developed recently to help understand black-box models, but it is not clear yet how an explanation is defined. Furthermore, the community agrees to say that many important terms do not have commonly accepted definitions. In this paper, we review the literature and show that there is a major issue concerning the definitions of terms such as explainability or interpretability. There is a lack of consensus that slows the development of this field. To address this problem, we propose a terminology that takes into account the context of an AI system, i.e., its users, purposes or design. This terminology is compatible with the majority of the definitions encountered in the literature so that it can be a foundation for future works. © 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International.","Explainability; Interpretability; Terminology","Industry 4.0; AI systems; Black box modelling; Explainability; Interpretability; Terminology"
"Belo O.","When agents make suggestions about readings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938582447&partnerID=40&md5=838ab28f3dafe3abd192d4b550eddb4a","Significant efforts have been made during the last few years in the design and implementation of pedagogical agents for a wide range of application domains. One of the most common target area is the assistance to students in cases of regular subject studying, promoting means that help them to improve their performance and expertise in some specific subject areas. Frequently students ask their teachers about the ""best"" and more effective bibliographic resources that they could use to study and validate knowledge for some working topics. In this paper we will discuss the basic characteristics of pedagogical agents, approaching their typical functional architecture, and services, reinforcing the discussion on a specific class of pedagogical agents that are responsible to support students during their studying sessions, helping them in the validation of their knowledge, suggesting bibliographic resources information whenever requested.","Agent based computing; Artificial intelligent tutors; Bibliographic resources suggestion; eLearning platforms; Intelligent tutoring systems; Software agents","Artificial intelligence; Bibliographies; Computer aided instruction; E-learning; Education; Students; Teaching; Agent based computing; Artificial intelligent; Bibliographic resources suggestion; E-learning platforms; Intelligent tutoring system; Software agents"
"Belotti J., Mendes J.J., Leme M., Trojan F., Stevan S.L., Siqueira H.","Comparative study of forecasting approaches in monthly streamflow series from Brazilian hydroelectric plants using Extreme Learning Machines and Box & Jenkins models","10.2478/johh-2021-0001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106985414&doi=10.2478%2fjohh-2021-0001&partnerID=40&md5=e4d4bea3478b660f695f826d75ac17a2","Several activities regarding water resources management are dependent on accurate monthly streamflow forecasting, such as flood control, reservoir operation, water supply planning, hydropower generation, energy matrix planning, among others. Most of the literature is focused on propose, compare, and evaluate the forecasting models. However, the decision on forecasting approaches plays a significant role in such models' performance. In this paper, we are focused on investigating and confront the following forecasting approaches: i) use of a single model for the whole series (annual approach) versus using 12 models, each one responsible for predicting each month (monthly approach); ii) for multistep forecasting, the use of direct and recursive methods. The forecasting models addressed are the linear Autoregressive (AR) and Periodic Autoregressive (PAR) models, from the Box & Jenkins family, and the Extreme Learning Machines (ELM), an artificial neural network architecture. The computational analysis involves 20 time series associated with hydroelectric plants indicated that the monthly approach with the direct multistep method achieved the best overall performances, except for the cases in which the coefficient of variation is higher than two. In this case, the recursive approach stood out. Also, the ELM overcame the linear models in most cases. © 2021 Jonatas Belotti et al., published by Sciendo 2021.","Box & Jenkins models; Extreme Learning Machines; Monthly approach; Multistep forecasting; Streamflow series prediction","Flood control; Forecasting; Knowledge acquisition; Machine learning; Network architecture; Neural networks; Reservoir management; Reservoirs (water); Stream flow; Time series analysis; Water resources; Water supply; Coefficient of variation; Computational analysis; Extreme learning machine; Hydro-power generation; Multi-step forecasting; Streamflow forecasting; Water resources management; Water supply planning; Hydroelectric power plants; comparative study; hydroelectric power plant; hydromechanics; machine learning; streamflow"
"Beloufa F., Chikh M.A.","Design of fuzzy classifier for diabetes disease using Modified Artificial Bee Colony algorithm","10.1016/j.cmpb.2013.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883746011&doi=10.1016%2fj.cmpb.2013.07.009&partnerID=40&md5=0dd8a875bfe7251b92989e4d6ae012a4","In this study, diagnosis of diabetes disease, which is one of the most important diseases, is conducted with artificial intelligence techniques. We have proposed a novel Artificial Bee Colony (ABC) algorithm in which a mutation operator is added to an Artificial Bee Colony for improving its performance. When the current best solution cannot be updated, a blended crossover operator (BLX-α) of genetic algorithm is applied, in order to enhance the diversity of ABC, without compromising with the solution quality. This modified version of ABC is used as a new tool to create and optimize automatically the membership functions and rules base directly from data. We take the diabetes dataset used in our work from the UCI machine learning repository. The performances of the proposed method are evaluated through classification rate, sensitivity and specificity values using 10-fold cross-validation method. The obtained classification rate of our method is 84.21% and it is very promising when compared with the previous research in the literature for the same problem. © 2013 Elsevier Ireland Ltd.","Artificial Bee Colony; Diabetes disease; Fuzzy rules; Interpretable classification","10-fold cross-validation; Artificial bee colonies; Artificial bee colony algorithms; Artificial bee colony algorithms (ABC); Artificial intelligence techniques; Classification rates; Sensitivity and specificity; UCI machine learning repository; Artificial intelligence; Evolutionary algorithms; Fuzzy rules; Fuzzy sets; Diagnosis; article; Artificial Bee Colony algorithm; artificial intelligence; diabetes mellitus; fuzzy system; genetic algorithm; machine learning; principal component analysis; sensitivity and specificity; validation study; Artificial Bee Colony; Diabetes disease; Fuzzy rules; Interpretable classification; Algorithms; Artificial Intelligence; Databases, Factual; Diabetes Mellitus; Diagnosis, Computer-Assisted; Fuzzy Logic; Humans; Software Design"
"Beltrami E.J., Brown A.C., Salmon P.J.M., Leffell D.J., Ko J.M., Grant-Kels J.M.","Artificial intelligence in the detection of skin cancer","10.1016/j.jaad.2022.08.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137674809&doi=10.1016%2fj.jaad.2022.08.028&partnerID=40&md5=9d6264a67a3b524521a499d961818f75","Recent advances in artificial intelligence (AI) in dermatology have demonstrated the potential to improve the accuracy of skin cancer detection. These capabilities may augment current diagnostic processes and improve the approach to the management of skin cancer. To explain this technology, we discuss fundamental terminology, potential benefits, and limitations of AI, and commercial applications relevant to dermatologists. A clear understanding of the technology may help to reduce physician concerns about AI and promote its use in the clinical setting. Ultimately, the development and validation of AI technologies, their approval by regulatory agencies, and widespread adoption by dermatologists and other clinicians may enhance patient care. Technology-augmented detection of skin cancer has the potential to improve quality of life, reduce health care costs by reducing unnecessary procedures, and promote greater access to high-quality skin assessment. Dermatologists play a critical role in the responsible development and deployment of AI capabilities applied to skin cancer. © 2022 American Academy of Dermatology, Inc.","artificial intelligence; clinical practice; diagnosis; health care dollars; machine learning; neural networks; skin cancer; technology",
"Beltramin D., Lamas E., Bousquet C.","Ethical Issues in the Utilization of Black Boxes for Artificial Intelligence in Medicine","10.3233/SHTI220709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133228023&doi=10.3233%2fSHTI220709&partnerID=40&md5=17ec14c8beb0c8b4346a8d417d906503","Artificial Intelligence (AI) has made major progress in recent years in many fields. With regard of medicine however, the utilization of AI raises numerous ethical questions, especially since newer and much more accurate algorithms function as black boxes. A trade-off must then be made between having algorithms being very accurate and effective, and algorithms that are explainable but less proficient. In this paper we address the ethical implications of utilizing black box algorithms in medicine. © 2022 The authors and IOS Press.","Artificial intelligence; Black boxes; Decision making; Ethics; Trust","Artificial intelligence; Economic and social effects; Ethical technology; Artificial intelligence in medicine; Black box algorithms; Black boxes; Decisions makings; Ethical implications; Ethical issues; Ethical question; Trade off; Trust; Decision making; algorithm; artificial intelligence; conference paper; decision making; ethics; trust; algorithm; medicine; Algorithms; Artificial Intelligence; Medicine"
"Beltrán S., Castro A., Irizar I., Naveran G., Yeregui I.","Framework for collaborative intelligence in forecasting day-ahead electricity price","10.1016/j.apenergy.2021.118049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118701086&doi=10.1016%2fj.apenergy.2021.118049&partnerID=40&md5=e30a237410f3d5de91592efe2cb9ba96","Electricity price forecasting in wholesale markets is an essential asset for deciding bidding strategies and operational schedules. The decision making process is limited if no understanding is given on how and why such electricity price points have been forecast. The present article proposes a novel framework that promotes human–machine collaboration in forecasting day-ahead electricity price in wholesale markets. The framework is based on a new model architecture that uses a plethora of statistical and machine learning models, a wide range of exogenous features, a combination of several time series decomposition methods and a collection of time series characteristics based on signal processing and time series analysis methods. The model architecture is supported by open-source automated machine learning platforms that provide a baseline reference used for comparison purposes. The objective of the framework is not only to provide forecasts, but to promote a human-in-the-loop approach by providing a data story based on a collection of model-agnostic methods aimed at interpreting the mechanisms and behavior of the new model architecture and its predictions. The framework has been applied to the Spanish wholesale market. The forecasting results show good accuracy on mean absolute error (1.859, 95% HDI [0.575, 3.924] EUR(MWh)−1) and mean absolute scaled error (0.378, 95% HDI [0.091, 0.934]). Moreover, the framework demonstrates its human-centric capabilities by providing graphical and numeric explanations that augments understanding on the model and its electricity price point forecasts. © 2021 The Authors","Augmented analytics; Automated machine learning; Ensemble models; Explainable artificial intelligence; Time series decomposition; Time series hybrid models","Decision making; Machine learning; Power markets; Signal processing; Time series analysis; Augmented analytic; Automated machine learning; Electricity prices; Ensemble models; Explainable artificial intelligence; Hybrid model; Series hybrids; Time series decomposition; Time series hybrid model; Times series; Forecasting; artificial intelligence; decision making; electricity; forecasting method; machine learning; price dynamics; signal processing; strategic approach"
"Beltrán-Camacho L., Eslava-Alcón S., Rojas-Torres M., Sánchez-Morillo D., Martinez-Nicolás M.ªP., Martín-Bermejo V., de la Torre I.G., Berrocoso E., Moreno J.A., Moreno-Luna R., Durán-Ruiz M.ªC.","The serum of COVID-19 asymptomatic patients up-regulates proteins related to endothelial dysfunction and viral response in circulating angiogenic cells ex-vivo","10.1186/s10020-022-00465-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127880723&doi=10.1186%2fs10020-022-00465-w&partnerID=40&md5=0144095f5b9f40199eed686f818104a9","Background: Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has already caused 6 million deaths worldwide. While asymptomatic individuals are responsible of many potential transmissions, the difficulty to identify and isolate them at the high peak of infection constitutes still a real challenge. Moreover, SARS-CoV-2 provokes severe vascular damage and thromboembolic events in critical COVID-19 patients, deriving in many related deaths and long-hauler symptoms. Understanding how these processes are triggered as well as the potential long-term sequelae, even in asymptomatic individuals, becomes essential. Methods: We have evaluated, by application of a proteomics-based quantitative approach, the effect of serum from COVID-19 asymptomatic individuals over circulating angiogenic cells (CACs). Healthy CACs were incubated ex-vivo with the serum of either COVID-19 negative (PCR −/IgG −, n:8) or COVID-19 positive asymptomatic donors, at different infective stages: PCR +/IgG − (n:8) and PCR −/IgG + (n:8). Also, a label free quantitative approach was applied to identify and quantify protein differences between these serums. Finally, machine learning algorithms were applied to validate the differential protein patterns in CACs. Results: Our results confirmed that SARS-CoV-2 promotes changes at the protein level in the serum of infected asymptomatic individuals, mainly correlated with altered coagulation and inflammatory processes (Fibrinogen, Von Willebrand Factor, Thrombospondin-1). At the cellular level, proteins like ICAM-1, TLR2 or Ezrin/Radixin were only up-regulated in CACs treated with the serum of asymptomatic patients at the highest peak of infection (PCR + /IgG −), but not with the serum of PCR −/IgG + individuals. Several proteins stood out as significantly discriminating markers in CACs in response to PCR or IgG + serums. Many of these proteins particiArticle title: Kindly check and confirm the edit made in the articletitle.pate in the initial endothelial response against the virus. Conclusions: The ex vivo incubation of CACs with the serum of asymptomatic COVID-19 donors at different stages of infection promoted protein changes representative of the endothelial dysfunction and inflammatory response after viral infection, together with activation of the coagulation process. The current approach constitutes an optimal model to study the response of vascular cells to SARS-CoV-2 infection, and an alternative platform to test potential inhibitors targeting either the virus entry pathway or the immune responses following SARS-CoV-2 infection. © 2022, The Author(s).","Asymptomatic; Circulating angiogenic cells; COVID-19; Endothelial dysfunction; Endothelial progenitor cells; Proteomics; SARS-CoV-2","ezrin; fibrinogen; intercellular adhesion molecule 1; radixin; thrombospondin 1; toll like receptor 2; von Willebrand factor; immunoglobulin G; adult; Article; asymptomatic carrier; circulating angiogenic cell; clinical article; coronavirus disease 2019; endothelial dysfunction; ex vivo study; female; human; human cell; inflammation; machine learning; male; mass spectrometry; polymerase chain reaction; protein blood level; proteomics; upregulation; vascular cell line; nucleic acid amplification techniques; COVID-19; Humans; Immunoglobulin G; Nucleic Acid Amplification Techniques; SARS-CoV-2"
"Belu R.","A project-based power electronics course with an increased content of renewable energy applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029052244&partnerID=40&md5=8fd69aecbd64d0af50d851d1485dedb8","This paper will described a project- and problem-based learning approach in teaching power electronics for upper-level undergraduate students enrolled in the applied engineering technology program at our university. This course will have an increased content of applications of power electronics in renewable energy conversion systems. Power electronics, still an emerging technology is multidisciplinary in its nature and the design and analysis of power electronics circuits include the applications of circuit theory, electronics, control theory, electromagnetics, semiconductor devices, microprocessors, numerical methods, signal processing, computer simulation, heat transfer, electromagnetic compatibility, and artificial intelligence. However it is also important to teach students the different fields in which technology is used, e.g. adjustable speed drives, switched mode power supplies, or power electronics for renewable energy. Experience has also shown that students have a difficult time with power electronics mostly because of the maturity the subject demands. A natural and efficient way of teaching power electronics is the problem-oriented and project-based learning approach. Students are often unaccustomed to assimilating materials from many areas at one time, thereby making it difficult for them to simultaneously bring together the circuit, signal and system analysis, electromagnetics and control theory topics which are required to fully describe the operation of a power electronic converter. The project-based course and laboratory described in this paper directly addresses these difficulties by helping students to reduce theory to practice. This approach supports the prerequisite lecture material and allows study of some practical issues which are best handled in a laboratory setting. The course format makes the students gradually more responsible for the analysis and design of control circuitry which permits nominal operation of generic power converters. The laboratory experience will culminate in projects where students analyze, design, simulate and demonstrate power electronics related topics. Each project will be carried out by a team of three or four students. The projects and part of the laboratory experiments will be focused on power applications in the fast growing emerging fields of the renewable industry, such as wind and solar energy or fuel cells. We believe that this will be an efficient approach in teaching power electronics because it can give the students some of the necessary skills the industry is asking for. © American Society for Engineering Education, 2009.",,"Circuit theory; Control theory; Curricula; Education; Electric network analysis; Electronics industry; Energy conversion; Engineering education; Fuel cells; Heat transfer; Laboratories; Numerical methods; Power converters; Power electronics; Semiconductor devices; Signal processing; Solar energy; Students; Switched mode power supplies; Teaching; Variable speed drives; Engineering technology program; Power electronic converters; Power electronics circuits; Power electronics course; Problem-based learning approaches; Project-based learning approach; Renewable energy applications; Renewable energy conversion systems; E-learning"
"Belzberg H., Murray J., Shoemaker W.C., Cornwell III E.E., Oder D., Guenon J., Velmahos G., Demetriades D.","Use of large databases for resolving critical care problems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030473453&partnerID=40&md5=9916a05061094fb34d534ecbe4525ff8","Large databases allow for rapid access to large volumes of data. To convert raw data to information, large numbers of data points must be correlated into a descriptive pattern that can be interpreted by the user. Databases must be constructed so as to allow reliable extraction of the raw data into a format that supports analysis of events in a meaningful, objective, and reproducible manner. Databases must be responsive to a variety of users. They must not demand unrealistic amounts of effort on those responsible for data entry. Standard protocols in various stages of development will make databases easier to use and more reliable. Database management tools such as the Internet and the National Library of Medicine will become more integrated into the practice of critical care medicine at all levels, including administration, clinical care, and research. This article provides an overview of the capabilities and difficulties associated with large databases. The major areas of use of large databases in the hospital setting are administration, bibliographic, patient care, research, and education. Each of these areas has different requirements and is supported by different types of databases. The advantages and disadvantages of linear, relational, and object-oriented databases are discussed. Issues relating to methods of data entry and the accuracy and reliability of data are discussed. The challenges involving integration of various sources of data and the interfacing of devices are reviewed.","artificial intelligence; clinical information systems; computer applications; database; information systems; intensive care unit; linear database; object-oriented database; relational database; system integration","artificial intelligence; computer; data base; hospitalization; information processing; information system; intensive care; internet; medical education; medical research; patient care; priority journal; publication; review; Critical Care; Databases, Bibliographic; Hospital Information Systems; Humans; Information Storage and Retrieval; Information Systems"
"Bemthuis R.H., Koot M., Mes M.R.K., Bukhsh F.A., Iacob M.-E., Meratnia N.","An agent-based process mining architecture for emergent behavior analysis","10.1109/EDOCW.2019.00022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076053543&doi=10.1109%2fEDOCW.2019.00022&partnerID=40&md5=5573ce2dcb38d47811eb0c34d2d78a24","Information systems leave a traceable digital footprint whenever an action is executed. Business process modelers capture these digital traces to understand the behavior of a system, and to extract actual run-time models of those business processes. Despite the omnipresence of such traces, most organizations face substantial differences between the process specifications and the actual run-time behavior. Analyzing and implementing the results of systems that model business processes tend, however, to be difficult due to the inherent complexity of the models. Moreover, the observed reality in the form of lower-level real-time events, as recorded in event logs, is seldom solely explainable by higher-level process models. In this paper, we propose an architecture to model system-wide behavior by combining process mining with a multi-agent system. Digital traces, in the form of event logs, are used to iteratively mine process models from which agents can learn. The approach is initially applied to a case study of a simplified job-shop factory in which automated guided vehicles (AGVs) carry out transportation tasks. Numerical experiments show that the workflow of a process mining model can be used to enhance the agent-based system, particularly, in analyzing bottlenecks and improving decision-making. © 2019 IEEE.","Emergent Behavior; Enterprise Architecture; Internet of Things; Job-shop; Multi-agent System; Process Mining; Supply Chain Logistics","Automatic guided vehicles; Data mining; Decision making; Industrial plants; Internet of things; Multi agent systems; Supply chains; Emergent behaviors; Enterprise Architecture; Job shop; Process mining; Supply chain logistics; Computer architecture"
"Ben David D., Resheff Y.S., Tron T.","Explainable AI and Adoption of Financial Algorithmic Advisors: An Experimental Study","10.1145/3461702.3462565","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112475804&doi=10.1145%2f3461702.3462565&partnerID=40&md5=0bc7d9d3505d7e3cd5261c36939e2607","We study whether receiving advice from either a human or algorithmic advisor, accompanied by five types of Local and Global explanation labelings, has an effect on the readiness to adopt, willingness to pay, and trust in a financial AI consultant. We compare the differences over time and in various key situations using a unique experimental framework where participants play a web-based game with real monetary consequences. We observed that accuracy-based explanations of the model in initial phases leads to higher adoption rates. When the performance of the model is immaculate, there is less importance associated with the kind of explanation for adoption. Using more elaborate feature-based or accuracy-based explanations helps substantially in reducing the adoption drop upon model failure. Furthermore, using an autopilot increases adoption significantly. Participants assigned to the AI-labeled advice with explanations were willing to pay more for the advice than the AI-labeled advice with ""No-explanation""alternative. These results add to the literature on the importance of XAI for algorithmic adoption and trust. © 2021 ACM.","algorithm adoption; experiment; explainable ai; financial advice; hci; trust","Economics; Philosophical aspects; Feature-based; Initial phasis; Labelings; Model failure; Web based; Willingness to pay; Artificial intelligence"
"Ben Dkhil M., Chawech N., Wali A., Alimi A.M.","Towards an automatic drowsiness detection system by evaluating the alpha band of EEG signals","10.1109/SAMI.2017.7880336","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017631324&doi=10.1109%2fSAMI.2017.7880336&partnerID=40&md5=f240ef5a898cbe7ecf30ec2829d88500","In this paper, we present the important role of alpha band for the evaluation of the drowsiness degree. By filtering the alpha band and by using the power spectral density of that same band, our data were analyzed using the percentiles as measures of dispersion. A threshold discriminating the two states was found, which helped to highlight the area of the brain responsible for the state of drowsiness for driver. So, in this work, we look to develop a drowsiness monitoring system in the goal to participate in reducing of the big number of road accidents to estimate the drowsiness level by analysis of EEG (electroencephalography) signals records. Finally, the algorithm developed in this work has been tested on twelve samples from the Physionet sleep-EDF database. © 2017 IEEE.","Drowsiness; DSP; EEG; Fuzzy rules; Physionet sleep-EDF database","Artificial intelligence; Electrophysiology; Fuzzy inference; Fuzzy rules; Highway accidents; Power spectral density; Spectral density; Drowsiness; Drowsiness detection; EEG signals; Monitoring system; PhysioNet; Two-state; Electroencephalography"
"Ben Jabeur S., Khalfaoui R., Ben Arfi W.","The effect of green energy, global environmental indexes, and stock markets in predicting oil price crashes: Evidence from explainable machine learning","10.1016/j.jenvman.2021.113511","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112366392&doi=10.1016%2fj.jenvman.2021.113511&partnerID=40&md5=3c2d15da8ecac0a6217e3e3143e32a52","This study aims to predict oil prices during the 2019 novel coronavirus (COVID-19) pandemic by looking into green energy resources, global environmental indexes (ESG), and stock markets. The study employs advanced machine learning, such as the LightGBM, CatBoost, XGBoost, Random Forest (RF), and neural network models. An accurate forecasting framework can effectively capture the trend of the changes in oil prices and reduce the impact of the COVID-19 pandemic on such prices. Additionally, a large dataset with different asset classes was used to investigate the crash period. The research also introduced SHapely Additive exPlanations (SHAP) values for model analysis and interpretability. The empirical results indicate the superiority of the RF and LightGBM over traditional models. Moreover, this new framework provides favorable explanations of the model performance using the efficient SHAP algorithm. It also highlights the core features of predicting oil prices. The study found that high values of GER and ESG lead to lower crude oil prices. Our results are crucial for investors and policymakers in promoting climate change mitigation and sustained economic prosperity through green energy resources. © 2021 Elsevier Ltd","COVID-19; Crash; Crude oil; Machine learning models","oil; petroleum; algorithm; climate change; COVID-19; crude oil; detection method; energy resource; index method; machine learning; policy making; stock market; accuracy; algorithm; Article; artificial neural network; climate change; coronavirus disease 2019; energy resource; forecasting; human; machine learning; pandemic; price; random forest; soybean; stock market; machine learning; pandemic; traffic accident; Coronavirus; Accidents, Traffic; COVID-19; Humans; Machine Learning; Pandemics; SARS-CoV-2"
"Ben Letaifa A.","Real Time ML-Based QoE Adaptive Approach in SDN Context for HTTP Video Services","10.1007/s11277-018-5952-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053721701&doi=10.1007%2fs11277-018-5952-6&partnerID=40&md5=022074756ede5c54a6c9d67b04ec544f","Due to the high dynamism of network conditions, operators and service providers are facing the challenge of providing satisfactory user experience during a real-time video streaming session where clients are often suffering from frequent interruptions and significant visual quality degradation. Video parameters such as playback quality, rate switching amplitude/frequency, occupancy, overflow/underflow buffer are the main key factors responsible for affecting the user experience’s quality. Recently, adaptive streaming protocols over HTTP have become widely adopted for providing continuous video streaming services to users with their different heterogeneous devices under dynamic network conditions. In this paper, we leverage the emerging paradigm of software defined networking SDN. Our contribution consists in developing some scenarios on SDN helping to adapt video streaming to the network state. The current work proposes to experience ML algorithms in order to predict user QoE over SDN networks. We present an approach that collects MOS score from users under varying network parameters as well as objective parameters such as SSIM, VQM and PSNR. The MOS scores are collected by playing videos to actual users in an SDN environment. We design an architecture which could use the measured MOS values under varying network conditions to predict the expected MOS based on machine learning algorithms. This work provides an outlook of experiments done for demonstration, by describing SDN environment deployment, detailing the realized scenarios and finally giving the results and values. We highlight, at the end of this paper, the perspectives of our proposition. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Machine learning (ML); QoE enforcement; Quality of experience (QoE); Quality of service (QoS); Software defined networking (SDN); Video streaming services","Artificial intelligence; HTTP; Learning algorithms; Learning systems; Software defined networking; Video streaming; Heterogeneous devices; Network parameters; Objective parameters; QoE enforcement; Quality of experience (QoE); Real-time videostreaming; Software defined networking (SDN); Video streaming services; Quality of service"
"Ben Miled Z., Haas K., Black C.M., Khandker R.K., Chandrasekaran V., Lipton R., Boustani M.A.","Predicting dementia with routine care EMR data","10.1016/j.artmed.2019.101771","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076191922&doi=10.1016%2fj.artmed.2019.101771&partnerID=40&md5=d4517f933a02d5fef200db5dcddffad7","Our aim is to develop a machine learning (ML) model that can predict dementia in a general patient population from multiple health care institutions one year and three years prior to the onset of the disease without any additional monitoring or screening. The purpose of the model is to automate the cost-effective, non-invasive, digital pre-screening of patients at risk for dementia. Towards this purpose, routine care data, which is widely available through Electronic Medical Record (EMR) systems is used as a data source. These data embody a rich knowledge and make related medical applications easy to deploy at scale in a cost-effective manner. Specifically, the model is trained by using structured and unstructured data from three EMR data sets: diagnosis, prescriptions, and medical notes. Each of these three data sets is used to construct an individual model along with a combined model which is derived by using all three data sets. Human-interpretable data processing and ML techniques are selected in order to facilitate adoption of the proposed model by health care providers from multiple institutions. The results show that the combined model is generalizable across multiple institutions and is able to predict dementia within one year of its onset with an accuracy of nearly 80% despite the fact that it was trained using routine care data. Moreover, the analysis of the models identified important predictors for dementia. Some of these predictors (e.g., age and hypertensive disorders) are already confirmed by the literature while others, especially the ones derived from the unstructured medical notes, require further clinical analysis. © 2019 Elsevier B.V.","Dementia; EMR; Machine learning; Prediction; Random forest","Cost effectiveness; Decision trees; Diagnosis; Forecasting; Health care; Learning systems; Machine learning; Medical applications; Medical computing; Neurodegenerative diseases; Clinical analysis; Dementia; Electronic medical record; Health care providers; Individual modeling; Patient population; Random forests; Unstructured data; Data handling; Article; controlled study; cost effectiveness analysis; data processing; dementia; diagnostic accuracy; electronic medical record; female; human; machine learning; male; priority journal; sensitivity and specificity; age; aged; complication; cost benefit analysis; dementia; economics; electronic health record; hypertension; mass screening; middle aged; neuropsychological test; predictive value; prescription; reproducibility; risk factor; theoretical model; very elderly; Age Factors; Aged; Aged, 80 and over; Cost-Benefit Analysis; Dementia; Drug Prescriptions; Electronic Health Records; Humans; Hypertension; Machine Learning; Mass Screening; Middle Aged; Models, Theoretical; Neuropsychological Tests; Predictive Value of Tests; Reproducibility of Results; Risk Factors"
"Ben Rabah N., Le Grand B., Pinheiro M.K.","IoT Botnet Detection using Black-box Machine Learning Models: The Trade-off between Performance and Interpretability","10.1109/WETICE53228.2021.00030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125180290&doi=10.1109%2fWETICE53228.2021.00030&partnerID=40&md5=80415e5963f6d2a31cad6a48ef97e235","The growth of the Internet of Things (IoT) has led to the increase of new threats, particularly IoT botnets that are a serious cybersecurity concern. This paper proposes a Machine Learning (ML) framework using black-box models for IoT botnet detection. It offers a trade-off between performance - high model accuracy - and interpretability - by providing security experts with explainable results. Our experimentation on real traffic data infected with Mirai and Bashlite malwares, showed that the framework achieves the best accuracy using random forest and extra tree models. In addition, it provides security experts with information on features that are important for a particular instance (i.e., local interpretation) and for the whole dataset (i.e., global interpretation), allowing them to trust the models results and save time and resources. © 2021 IEEE.","black-box models; botnet detection; Internet of things; interpretation; machine learning; performance","Botnet; Cybersecurity; Decision trees; Economic and social effects; Machine learning; Malware; Black box modelling; Black boxes; Botnet detections; Botnets; Interpretability; Interpretation; Machine learning models; Performance; Security experts; Trade off; Internet of things"
"Ben Yahia S., Mephu Nguifo E.","Visualisation of association rules: Towards a meta-cognitive approach [Visualisation des règles associatives: Vers une approche méta-cognitive]",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884297981&partnerID=40&md5=aa6e89fcdc59113ff71cfa413c797538","The extremely large number of association rules that can be drawn from .even reasonably sized. datasets bootstrapped the development of more acute techniques or methods to reduce the size of the reported rule sets. In order to be reliable in a decision making process, such discovered rules have to be both concise and easily understandable for users, and/or as an input to visualization tools. In this paper, we present, GERVIS, a graphical visualization prototype for handling generic bases of association rules. GERVIS's main originality is based on the fact that it uses a back-end meta-knowledge formally expressed by means of fuzzy meta-association rules. This meta-knowledge allows a cooperative exploration of generic bases of association rules. Besides displaying on user demand associated derivable rules, the additional knowledge highlighting connected rules to a user-selected rule provides an improvement of man-machine interaction.","Data mining; Generic association rule; Man machine interaction; Meta-knowledge; Visualization","Additional knowledge; Cooperative exploration; Decision making process; Graphical visualization; Man-machine interaction; Meta-knowledge; Metacognitives; Visualization tools; Data mining; Flow visualization; Human computer interaction; Visualization; Association rules"
"Ben Yahia S., Mephu Nguifo E.","Revisiting generic bases of association rules",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048880717&partnerID=40&md5=70f62b21e73ff8a9bd43a8a40e6d018c","As a side effect of unprecedented amount of digitization of data, classical retrieval tools found themselves unable to go further beyond the tip of the Iceberg. Data Mining in conjunction with the Formal Concept Analysis, is a clear promise to furnish adequate tools to do so and specially to be able to derive concise generic and easy understandable bases of ""hidden"" knowledge, that can be reliable in a decision making process. In this paper, we propose to revisit the notion of association rule redundancy and to present sound inference axioms for deriving all association rules from generic bases of association rules. © Springer-Verlag Berlin Heidelberg 2004.",,"Association rules; Data mining; Data warehouses; Decision making; Sea ice; Decision making process; Generic basis; Retrieval tool; Side effect; Formal concept analysis"
"Ben Yahia S., Nguifo E.M.","Emulating a cooperative behavior in a generic association rule visualization tool",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244421052&partnerID=40&md5=17452154c18e630feed7c11d1335b1bf","Traditional framework for mining association rules has pointed out the derivation of many redundant rules. In order to be reliable in a decision making process, such discovered rules have to be both concise and easily understandable for users, and/or as an input to visualization tools. In this paper, we present a graphical visualization prototype for handling generic bases of association rules. We discuss also the most adequate graphical visualization technique depending on the intrinsic structure of the generic bases of association rules. An interesting feature of the prototype is that it provides a ""contextual"" exploration of such rule set. Such exploration, based on the discovery of fuzzy meta-rules, enhances man-machine interaction by emulating a cooperative behavior. © 2004 IEEE.",,"Algorithms; Computer architecture; Computer graphics; Computer hardware; Database systems; Decision making; Human computer interaction; User interfaces; Visualization; Generic bases; Graphical visualization; Intrinsic structure; Visualization tools; Data mining"
"Ben Yahia S., Nguifo E.M.","Contextual generic association rules visualization using hierarchical fuzzy meta-rules",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-11144326336&partnerID=40&md5=0b7fa3a1aafb750930ee669deb4e0da4","Traditional framework for mining association rules has pointed out the derivation of many redundant rules. In order to be reliable in a decision making process, such discovered rules have to be concise and easily understandable for users or as well as an input to visualization tools. In this paper, we present a 3D Histograms -based visualization prototype for handling generic bases of association rules. An interesting feature of the prototype is that it provides a ""contextual"" exploration of such rule set. Such additional displayed knowledge, based on the construction of fuzzy meta-rules, enhances man-machine interaction by emulating a cooperative behavior.",,"Cognitive systems; Database systems; Decision making; Fuzzy sets; Graphical user interfaces; Hierarchical systems; Human computer interaction; Formal Concept Analysis (FCA); Hierarchical fuzzy meta-rules; Mining association rules; Visualization tools; Data mining"
"Ben Yedder H., Cardoen B., Hamarneh G.","Deep learning for biomedical image reconstruction: a survey","10.1007/s10462-020-09861-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089037737&doi=10.1007%2fs10462-020-09861-2&partnerID=40&md5=d71dfb2d9d2c8d2f2927d16d958ff372","Medical imaging is an invaluable resource in medicine as it enables to peer inside the human body and provides scientists and physicians with a wealth of information indispensable for understanding, modelling, diagnosis, and treatment of diseases. Reconstruction algorithms entail transforming signals collected by acquisition hardware into interpretable images. Reconstruction is a challenging task given the ill-posedness of the problem and the absence of exact analytic inverse transforms in practical cases. While the last decades witnessed impressive advancements in terms of new modalities, improved temporal and spatial resolution, reduced cost, and wider applicability, several improvements can still be envisioned such as reducing acquisition and reconstruction time to reduce patient’s exposure to radiation and discomfort while increasing clinics throughput and reconstruction accuracy. Furthermore, the deployment of biomedical imaging in handheld devices with small power requires a fine balance between accuracy and latency. The design of fast, robust, and accurate reconstruction algorithms is a desirable, yet challenging, research goal. While the classical image reconstruction algorithms approximate the inverse function relying on expert-tuned parameters to ensure reconstruction performance, deep learning (DL) allows automatic feature extraction and real-time inference. Hence, DL presents a promising approach to image reconstruction with artifact reduction and reconstruction speed-up reported in recent works as part of a rapidly growing field. We review state-of-the-art image reconstruction algorithms with a focus on DL-based methods. First, we examine common reconstruction algorithm designs, applied metrics, and datasets used in the literature. Then, key challenges are discussed as potentially promising strategic directions for future research. © 2020, Springer Nature B.V.","Analytical approach; Deep learning; Image reconstruction; Inverse problem; Iterative approach; Limited data representation; Modality","Deep learning; Diagnosis; Inference engines; Inverse problems; Inverse transforms; Medical imaging; Automatic feature extraction; Image reconstruction algorithm; Real-time inference; Reconstruction accuracy; Reconstruction algorithms; Reconstruction speed; Temporal and spatial; Wealth of information; Image reconstruction"
"Benabbes S., Hemam S.M.","An Approach Based on (Tasks-VMs) Classification and MCDA for Dynamic Load Balancing in the CloudIoT","10.1007/978-3-030-37207-1_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077439987&doi=10.1007%2f978-3-030-37207-1_41&partnerID=40&md5=a191bd59b6d503a35cbdab9d7631244a","Cloud IoT is a new paradigm that has emerged as a result of the integration of Cloud Computing and the Internet of Things. It provides a set of intelligent services and applications that can strongly influence on our daily lives. In the CloudIoT, physical sensors are responsible for detecting and transmitting data to the cloud in order to be treated and stored. In this context, the amount of data to be processed through the CloudIoT is in increasing usually and, in this situation, the load balancing mechanism is needed in order to distribute the captured data between the different CloudIoT resources. In this paper, we propose an approach that allows to balance the load between the different virtual machines of a CloudIoT. The proposed approach is composed of four essential components: The Classifier which assigns Tasks to its corresponding class and assigns the Virtual Machines (VM) to its corresponding class too, the Dispatcher which sends the tasks to the VM classes, the Local Balancer whose role is to balance the tasks between different VMs belonging to the same class used the spooling method for duplicate same VM, finally, the Manager component or general balancer that is responsible to balance the load between VMs classes with the test of the maximum cloud allowance bar to release VM. The results obtained through the proposed the case study show that our approach allows effectively the load balancing between VM. © 2020, Springer Nature Switzerland AG.","Cloud computing; CloudIoT; Internet of Things; Load balancing; Machine learning; Worst-Case Execution Time",
"Benabdellah A.C., Zekhnini K., Cherrafi A., Garza-Reyes J.A., Kumar A.","Design for the environment: An ontology-based knowledge management model for green product development","10.1002/bse.2855","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108886130&doi=10.1002%2fbse.2855&partnerID=40&md5=bc5794bfdfcb7fe84d52d15ed0bdcb26","Through appropriate operations and policies, such as green processes and product development process (PDP), companies can respond to environmental sustainability. To remain competitive, one such approach, Design for X (DFX), involves considering different environment and sustainable strategies through different factors Xs. With regard to the availability of different DFX techniques that consider environmental issues, the decision as to which approach needs to be adopted remains absent. This paper aims at presenting an overview from 1980 to 2020 of the developed research, applications, and DFX techniques for assessing green issues. Selected DFX techniques are linked with strategies used in organizations. Following a literature analysis, a collaborative knowledge-based framework that addresses the design concepts needed to assess environmental, safety, and health concerns in the development of green products is proposed. Furthermore, as a pillar for considering the Semantic Web and an evolving approach linked with natural language processing (NLP) and artificial intelligence (AI), an ontology-based knowledge management model for green assessment is developed for the representation, acquisition, organization, and capitalization of knowledge in a computer interpretable manner. The findings are useful for both managers and practitioners as they provide a coherent domain ontology that can help them manage knowledge, improve teamwork, and make decisions in a collaborative green PDP. Besides, an understanding of the essential design considerations that are required to implement environmental, safety, and health issues, as well as competencies used in the PDP is presented. Key barriers, managerial and strategic implications, and mitigation actions are also identified in this paper. © 2021 ERP Environment and John Wiley & Sons Ltd.","business strategy; Design for X; green product design, design for the environment; knowledge management; ontology","artificial intelligence; business development; corporate strategy; decision making; green economy; literature review; strategic approach; sustainability; sustainable development"
"Benabderrahmane S., Berrada G., Cheney J., Valtchev P.","A Rule Mining-Based Advanced Persistent Threats Detection System",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125171506&partnerID=40&md5=7db9e7f0dce757cdbc75fb8a0fb47261","Advanced persistent threats (APT) are stealthy cyber-attacks that are aimed at stealing valuable information from target organizations and tend to extend in time. Blocking all APTs is impossible, security experts caution, hence the importance of research on early detection and damage limitation. Whole-system provenance-tracking and provenance trace mining are considered promising as they can help find causal relationships between activities and flag suspicious event sequences as they occur. We introduce an unsupervised method that exploits OS-independent features reflecting process activity to detect realistic APT-like attacks from provenance traces. Anomalous processes are ranked using both frequent and rare event associations learned from traces. Results are then presented as implications which, since interpretable, help leverage causality in explaining the detected anomalies. When evaluated on Transparent Computing program datasets (DARPA), our method outperformed competing approaches. © 2021 International Joint Conferences on Artificial Intelligence. All rights reserved.",,"Artificial intelligence; Cybersecurity; Network security; Blockings; Causal relationships; Damage limitation; Event sequence; Persistent threat detection systems; Process activities; Rule mining; Security experts; Transparent computing; Unsupervised method; Damage detection"
"Benali Aoudia L., Belkacemi Zebda D.","How to manage and enhance archeology and landscape at the royal mausoleum of mauretania (Tipasa, Algeria)","10.5194/isprs-archives-XLIV-M-1-2020-513-2020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090958158&doi=10.5194%2fisprs-archives-XLIV-M-1-2020-513-2020&partnerID=40&md5=48df9b0b46bb231f8f00d47d0fd3499c","Archaeological site and landscape are two interdependent and sometimes merged notions. The first should be taken into consideration to achieve coherent territorial planning that reveals and preserves the character and identity of a setting, wh ile the second should be respected, protected and promoted in responsible projects of management and enhancement of archaeological sites. Dynamics of landscape transformations should closely consider the archaeological sites thereof and regard them as ingredients to emphasize rather than impediments to surmount. On the other hand, landscape should be conceived as valuable asset and a resource for archaeological heritage development. Based on these views, this paper develops a reflection on how the enhancement of both Archaeology and Landscape can be combined in the case of the Royal Mausoleum of Mauretania set in the city of Tipasa, Algeria. It highlights the Tomb's architectural and archaeological significance as well as the ecological, natural, symbolic and emotional values of its exceptional landscape. It proposes a specific strategy to conceive and develop this vestige in harmony with its landscape. © Authors 2020. CC BY 4.0 License.","Archaeology; Landscape; Management; The Royal Mausoleum of Mauretania","Artificial intelligence; Earth sciences; Remote sensing; Software engineering; Algeria; Archaeological site; Territorial planning; Architecture"
"Benassi R., Bergamaschi S., Fergnani A., Miselli D.","Extending a lexicon ontology for intelligent information integration",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017384844&partnerID=40&md5=72cbe30ea142a9bf07d3a6ae71aafbcd","One of the current research on the Semantic Web area is semantic annotation of information sources. On-line lexical ontologies can be exploited as a-priori common knowledge to provide easily understandable, machine-readable metadata. Nevertheless, the absence of terms related to specific domains causes a loss of semantics. In this paper we present WNEditor, a tool that aims at guiding the annotation designer during the creation of a domain lexicon ontology, extending the pre-existing WordNet ontology. New terms, meanings and relations between terms are virtually added and managed by preserving the WordNet's internal organization.",,"Artificial intelligence; Deep neural networks; Intelligent systems; Common knowledge; Information sources; Intelligent information; Lexical ontology; New terms; Semantic annotations; Wordnet; Ontology"
"Benavente-Peces C., Ibadah N.","Buildings energy efficiency analysis and classification using various machine learning technique classifiers","10.3390/en13133497","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090270514&doi=10.3390%2fen13133497&partnerID=40&md5=4c43b151ea9bc91257be56a7d590c6bf","Energy efficiency is a major concern to achieve sustainability in modern society. Smart cities sustainability depends on the availability of energy-efficient infrastructures and services. Buildings compose most of the city, and they are responsible for most of the energy consumption and emissions to the atmosphere (40%). Smart cities need smart buildings to achieve sustainability goals. Building s thermal modeling is essential to face the energy efficiency race. In this paper, we show how ICT and data science technologies and techniques can be applied to evaluate the energy efficiency of buildings. In concrete, we apply machine learning techniques to classify buildings based on their energy efficiency. Particularly, our focus is on single-family buildings in residential areas. Along this paper, we demonstrate the capabilities of machine learning techniques to classify buildings depending on their energy efficiency. Moreover, we analyze and compare the performance of different classifiers. Furthermore, we introduce new parameters which have some impact on the buildings thermal modeling, especially those concerning the environment where the building is located. We also make an insight on ICT and remark the growing relevance in data acquisition and monitoring of relevant parameters by using wireless sensor networks. It is worthy to remark the need for an appropriate and reliable dataset to achieve the best results. Moreover, we demonstrate that reliable classification is feasible with a few featured parameters. ©2020 by the authors","Buildings energy efficiency; ICT; Machine learning; Smart buildings; Smart cities; Sustainability","Buildings; Data acquisition; Data Science; Energy utilization; Green computing; Machine learning; Smart city; Sustainable development; Thermoanalysis; Wireless sensor networks; Energy efficiency analysis; Energy efficient; Machine learning techniques; New parameters; Residential areas; Science technologies; Thermal model; Energy efficiency"
"Benavente-Peces C., Bartolini D.","Insights in Machine Learning for Cyber-Security Assessment","10.1007/978-3-030-22964-1_33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068342489&doi=10.1007%2f978-3-030-22964-1_33&partnerID=40&md5=212444322b6dd7a9a220e8f0f037d6d7","Information transactions and sharing is a common activity in daily life, at work, private and leisure activities. Currently, companies perform most of information delivery, storing and sharing electronically through internal and external connections. External connections are source of cyber-risk as unauthorized connections could provoke enormous damages. In this framework, cyber-risk assessment is a must for companies as their incomings are in risk, but its implementation is cumbersome. Financial issues produced by breaches in information infrastructures and procedures security are covered by the so called cyber-insurance. Cyber-security management requires a lot of resources both economical and human, involving all the departments and people of the company. Among other activities, alerting of potential risks and damages before any attack is produced, or providing the appropriate means, after a cyber-attack, to prevent a new one and restore, when possible, the damages, are responsibilities of decision-makers. In this scenario, insurability assessment is a must for insurance companies, and risk management engineers are the staff responsible to analyse and report about cyber insurance acceptance test. In order to optimize and automatize the procedure of insurability assessment, artificial intelligence and machine learning techniques constitutes an appropriate tool. In this paper the authors focus the attention on the analysis of the collected data used to take decisions in order to determine dependencies, correlations and take further steps to predict insurability, which in next investigations will be optimized through machine learning techniques. © 2019, Springer Nature Switzerland AG.","Cyber-insurability prediction; Cyber-risk management; Cyber-security; Data protection; Information security","Acceptance tests; Data privacy; Ergonomics; Human resource management; Insurance; Learning algorithms; Machine learning; Risk management; Risk perception; Security of data; Cyber security; Information delivery; Information infrastructures; Information transaction; Insurance companies; Leisure activities; Machine learning techniques; Unauthorized connections; Risk assessment"
"Benazzouz Y., Boudour R.","Using Sequence to Sequence Model to Transform Recognized ASL Words to Understandable Sentences","10.1007/978-3-030-63128-4_57","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096494718&doi=10.1007%2f978-3-030-63128-4_57&partnerID=40&md5=f37a44887f2aa707cf0965819a58b1ce","There was widespread agreement that Communication can be a struggle for some deaf and hearing impaired people. The big breakthrough is the development of sign language (SL) which has enabled this category of people to communicate. SL has enabled them to integrate into modern society, and access to education and work. Although sign language is very well documented and taught it does not allow them to interact with all people. It still hard for them to conduct a real life as the rest of the society, for example, participating in work meetings, reading any books, taking classes at the university, nor to use the available means of communication, such as video chatting and conferencing. With the recent development in artificial intelligence solutions began to appear on the horizon to accentuate the integration of deaf and dumb people. This paper attempts to answer a small corner of these needs. It presents a technique for transforming sign language into understandable sentences that can in turn be synthesized into speech or sent as it is by email or SMS. The aim is to allow the society understand SL without making a learning effort. In order to show the feasibility of this approach, a real-time prototype is developed by integrating an existing sign recognition model. © 2021, Springer Nature Switzerland AG.","ASL language; ASL sign recognition; Sentence reconstruction; Sequence to sequence model","Artificial intelligence; Pattern recognition; Video conferencing; Deaf and hearing impaired; Learning efforts; Real time; Sequence modeling; Sign language; Sign recognition; Audition"
"Bench-Capon T.J.M., Dunne P.E.","Argumentation in AI and Law: Editors' introduction","10.1007/s10506-006-9007-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746924095&doi=10.1007%2fs10506-006-9007-z&partnerID=40&md5=afa53bb99f22685484b691fb04874a33","Different papers, derived from a workshop run in conjunction with the Tenth International Conference on AI and Law, held in Bologna in June 2005, which represent some of the latest work on argumentation in AI and Law are presented. Major topics, which have emerged as important in AI and Law and argumentation include arguing on the basis of precedent cases, using argumentation to resolve rule conflicts and explore defeasibility, and determining whether an attack on an argument is successful. Dialogue Games such as those introduced into AI and Law by Tom Gordon's Pleadings Game, provide a way of modeling the process of the dispute. Different parties in a dispute may be responsible for different arguments, and different standards of proof may apply to different arguments. The final paper presented by Paul Dunne, provides a formal basis, using value based argumentation frameworks for the analysis of social conventions.",,"Codes (standards); Computer simulation; Game theory; Laws and legislation; Technical presentations; Three dimensional computer graphics; Dialogue Games; Social conventions; Value based argumentation; Artificial intelligence"
"Bencke L., Cechinel C., Munoz R.","Automated classification of social network messages into Smart Cities dimensions","10.1016/j.future.2020.03.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082946462&doi=10.1016%2fj.future.2020.03.057&partnerID=40&md5=383d0df57610dc0ba0c2de599bde59cb","A Smart City can be defined as a high-tech city with several public and private services capable to strategically solve (or mitigate) problems normally generated by rapid urbanization. Different models of indicators have been developed to follow cities’ evolution to become a Smart City. An example of such model is the standard 37120 from the International Organization for Standardization (ISO) that proposes a set of dimensions and indicators (e.g. Transportation, Recreation, Solid Waste) for services and quality of life for sustainable cities and communities. It has been common to find official social network profiles of organizations and governmental entities related to the services they provide or are responsible for (water, waste, transportation, cultural events, etc.) and that are used by citizens as a gateway to directly interact and communicate their complains and problems about those services. The present paper proposes to apply machine learning algorithms over the urban data generated by social networks in order to create classifiers to automatically categorize citizens messages according to the different cities services dimensions. For that, two distinct text datasets in Portuguese were collected from two social networks: Twitter (1,950 tweets) and Colab.re (65,066 posts). The texts were mapped according to the different ISO 37120 categories, preprocessed and mined through the use of 8 algorithms implemented in Scikit-Learn. Initial results pointed out the feasibility of the proposal with models achieving average F1-measures around 55% for F1-macro and 78% for F1-micro when using Linear Vector Classification, Logistic Regression, Decision Tree and Complement Naive Bayes. However, as the datasets were highly unbalanced, the performances of the models vary significantly for each ISO category, with the best results occurring for Wastewater, Water & Sanitation, Energy and Transportation. The classifiers generated here can be integrated on a number of different city services and systems such as: governmental support decision systems, customer complain systems, communities dashboards, police offices, transportation's companies, cultural producers, environmental agencies, and recyclers’ companies. © 2020 Elsevier B.V.","ISO 37120; Machine learning; Smart City services; Text Classification; Topic Classification","Classification (of information); Classifiers; Decision trees; Gateways (computer networks); Learning algorithms; Learning systems; Logistic regression; Smart city; Text processing; Automated classification; Energy and transportation; Environmental agency; International organization for standardizations; ISO 37120; Rapid urbanizations; Text classification; Topic Classification; Machine learning"
"Benda Prokeinová R., Paluchová J.","Identification of the patterns behavior consumptions by using chosen tools of data mining - Association rules",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907816681&partnerID=40&md5=c1e9313582b30ac7d4b0919d8d793de3","The research and development in sustainable environment, that is a subject of research goal of many various countries and food producers, now, it has a long tradition. The research aim of this paper allows for an identification of the patterns behaviour consumptions by using of association rules, because of knowledge's importance of segmentation differences between consumers and their opinions on current sustainable tendencies. The research area of sustainability will be in Slovakia still discussed, primarily because of impacts and consumer's influencing to product's buying, that are safety to environment and to nature. We emphasize an importance of sustainability in consumer behaviour and we detailed focused on segmentation differences between respondents. We addressed a sample made by 318 respondents. The article aims identifying sustainable consumer behaviour by using chosen data mining tool - association rules. The area of knowledge-based systems is widely overlaps with the techniques in data mining. Mining in the data is in fact devoted to the process of acquiring knowledge from large amounts of data. Its techniques and approaches are useful only when more focused external systems as well as more general systems to work with knowledge. One of the challenges of knowledge-based systems is to derive new knowledge on the basis of known facts and knowledge. This function in a sense meets methods using association rules. Association rules as a technique in data mining is useful in various applications such as analysis of the shopping cart, discovering hidden dependencies entries or recommendation. After an introduction and explanation of the principle of sustainability in consumption, association rules, follows description of the algorithm for obtaining rules from transaction data. Then will present the practical application of the data obtained by questionnaire survey. Calculations are performed in the free data mining software Tanagra.","Apriori; Association rules; Consumer; Responsible consumption; Sustainability","algorithm; consumption behavior; data mining; knowledge; software; sustainability; Slovakia"
"Bendarkar D., Somase P., Rebari P., Paturkar R., Khan A.","Web Based Recognition and Translation of American Sign Language with CNN and RNN","10.3991/ijoe.v17i01.18585","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100635450&doi=10.3991%2fijoe.v17i01.18585&partnerID=40&md5=4f7a3c1c7200bb93d4cecd598363c552","Individuals with hearing hindrance utilize gesture-based communication to exchange their thoughts. Generally, hand movements are used by them to communicate among themselves. But there are certain limitations when they communicate with other people who cannot understand these hand movements. There is a need to have a mechanism that can act as a translator between these people to communicate. It would be easier for these people to interact if there exists direct infrastructure that is able to convert signs to text and voice messages. As of late, numerous such frameworks for gesture-based communication acknowledgment have been developed. But most of them are made either for static gesture recognition or dynamic gesture recognition. As sentences are generated using combinations of static and dynamic gestures, it would be simpler for hearing debilitated individuals if such computerized frameworks can detect both the static and dynamic motions together. We have proposed a design and architecture of American Sign Language (ASL) recognition with convolutional neural networks (CNN). This paper utilizes a pretrained VGG-16 architecture for static gesture recognition and for dynamic gesture recognition, spatiotemporal features were learnt with the complex architecture, called deep learning. It contains a bidirectional convolutional Long Short Term Memory network (ConvLSTM) and 3D convolutional neural network (3DCNN) and this architecture is responsible to extract 2D spatio temporal features. © 2021, iJOE. All Rights Reserved","3DCNN; ASL; CNN; ConvLSTM; VGG-16",
"Ben-David A.","Mossad head reshuffles senior echelons",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861246090&partnerID=40&md5=c9f833699d78d5588cd98a7453c8573d","Meir Dagan, the head of Israel's foreign intelligence service (Mossad), is for the second time since taking office, reshuffling the senior officers of the agency to revive its human intelligence (HUMINT) capabilities. Dagan announced a series of nominations approved by Prime Minister Ariel Sharon in September 2005, replacing all his leading lieutenants and retiring several heads of department. Dagan initially appointed as his deputy 'T', former commander of the Neviot operational unit, believed to be responsible for surveillance and wiretapping. However, Dagan decided not to carry out the promised rotation and announced the appointment of 'N' head of the Tzomet HUMINT Department to be his deputy.",,"Human intelligence; Meir Dagan; Wiretapping; Artificial intelligence; Societies and institutions"
"Bendjebar S., Lafifi Y., Seridi H.","Modeling and evaluating tutors' function using data mining and fuzzy logic techniques","10.4018/IJWLTT.2016040103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969256582&doi=10.4018%2fIJWLTT.2016040103&partnerID=40&md5=7a285890d366287d7144142afc24a134","In e-learning systems, the tutors play many roles and carry out several tasks that differ from one system to another. The activity of tutoring is influenced by many factors. One factor among them is the assignment of the appropriate profile to the tutor. For this reason, the authors propose a new approach for modeling and evaluating the function of the tutors. This technique facilitates the classification among tutors for adapting tutoring to student's problems. The component of the proposed tutor model is a set of profiles which are responsible for representing the necessary information about each tutor. A fuzzy logic technique is used in order to define tutor's tutoring profile. Furthermore, the K nearest neighbor algorithm is used to offer much information for each new tutor based on the models of other similar tutors. This new approach has been tested by tutors from an Algerian University. The first results were very encouraging and sufficient. They indicate that the use of fuzzy logic technique is very useful and estimate the adaptation of the tutoring process according to tutors' skills. Copyright © 2016, IGI Global.","E-learning systems; Fuzzy logic; K nearest neighbor; Tutor model; Tutoring; Tutors' functions","Computer circuits; E-learning; Function evaluation; Fuzzy logic; Learning systems; Motion compensation; Nearest neighbor search; Pattern recognition; Fuzzy logic techniques; K nearest neighbor algorithm; K-nearest neighbors; New approaches; One-factor; Tutoring; Use of fuzzy logic; Data mining"
"Bendjebar S., Lafifi Y.","Initializing the tutor model using k-means algorithm","10.1007/978-3-319-00560-7_42","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893122637&doi=10.1007%2f978-3-319-00560-7_42&partnerID=40&md5=267ec0f70e546ff14e1a1a819ca40322","This paper proposes an approach for the initialization and the construction of tutor's model in the e-learning systems. This actor has several roles and different tasks from a system to another. His main purpose is tracking and guiding students throughout their learning process. In their first interaction, the system has rather little information about its new tutors. The proposed approach serves to offer much information for each specific tutor based on the models of other similar tutors. The problem of initializing the tutor model can be resolved by assigning the tutor to certain group of tutors. Thus, a data mining algorithm, namely k-means is responsible for creating clusters based on the preentered information on tutors. Then, each new tutor is assigned to his closest cluster center. This model facilitates the assignment of tutors to learners for adapting the monitoring process. © Springer International Publishing Switzerland 2013.","E-learning; Initialization; K-means; Profile; Tutor model; Tutoring",
"Bendjenna H., Zarour N.E., Charrel P.-J.","MAMIE: A methodology to elicit requirements in inter-company co-operative information systems","10.1109/CIMCA.2008.101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449558333&doi=10.1109%2fCIMCA.2008.101&partnerID=40&md5=e64676fa0cf5d0c4776232a592bfc974","In this paper, we propose a methodology, namely MAMIE (from MAcro to MIcro level requirements Elictation) in order to elicit requirements for an Intercompany Co-operative Information Systems development. It is based on UML use cases and sequence diagrams: they represent understandable notations by all stakeholders and they are useful for acquiring, analyzing and modeling requirements. The sequence diagram allows also to model interaction between companies and constraints of co-operation. MAMIE methodology is also based on viewpoints which have advocated as a means of partitioning requirements as a set of partial specifications that are helpful for traceability and consistency management. Preliminary results suggest that MAMIE methodology is of valuable help to requirements engineers during elicitation process. © 2008 IEEE.",,"Co-operative information system; Consistency management; Micro level; Model interaction; Modeling requirements; Partial specifications; Requirements engineers; Sequence diagrams; UML use case; Artificial intelligence; Management information systems; Information systems"
"Bendre N., Desai K., Najafirad P.","Show Why the Answer is Correct Towards Explainable AI using Compositional Temporal Attention","10.1109/SMC52423.2021.9659223","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124306455&doi=10.1109%2fSMC52423.2021.9659223&partnerID=40&md5=fae8cd35d17800937edd25fa37eef456","Visual Question Answering (VQA) models have achieved significant success in recent times. Despite the success of VQA models, they are mostly black-box models providing no reasoning about the predicted answer, thus raising questions for their applicability in safety-critical such as autonomous systems and cyber-security. Current state of the art fail to better complex questions and thus are unable to exploit compositionality. To minimize the black-box effect of these models and also to make them better exploit compositionality, we propose a Dynamic Neural Network (DMN), which can understand a particular question and then dynamically assemble various relatively shallow deep learning modules from a pool of modules to form a network. We incorporate compositional temporal attention to these deep learning based modules to increase compositionality exploitation. This results in achieving better understanding of complex questions and also provides reasoning as to why the module predicts a particular answer. Experimental analysis on the two benchmark datasets, VQA2.0 and CLEVR, depicts that our model outperforms the previous approaches for Visual Question Answering task as well as provides better reasoning, thus making it reliable for mission critical applications like safety and security. © 2021 IEEE.",,"Benchmarking; Complex networks; Computer vision; Deep learning; Neural networks; 'current; Black box modelling; Black boxes; Complex questions; Compositionality; Cyber security; Dynamic neural networks; Question Answering; State of the art; System security; Safety engineering"
"Benferhat Salem, Garcia Laurent","Local approach to reasoning with conditional knowledge bases",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030415703&partnerID=40&md5=f62f6647d8a2e1a826fe0af57efaf29d","This paper investigates a local approach for reasoning with conditional knowledge bases (with default rules of the form `generally, if α then β' and having possibly some exceptions). The idea is that when a conflict appears (due to observing exceptional situations), we first localize the sets of pieces of information which are responsible for conflicts. Next, using specificity principle (subclasses must be preferred to general classes), we attach priorities to default rules inside each conflict. These priorities, implicitly computed from the knowledge base, reflect the hierarchical structure of the knowledge base. Lastly, we rank-order and solve conflicts in a way that only minimal sets of rules are given up from the knowledge base in order to restore its consistency. This local way to deal with conflicts addresses correctly the well known problems of specificity, irrelevance, blocking of inheritance, etc.",,"Artificial intelligence; Formal logic; Problem solving; Conditional knowledge bases; Knowledge based systems"
"Benita F., Bansal G., Virupaksha D., Scandola F., Tunçer B.","Body responses towards a morning walk in a tropical city","10.1080/01426397.2020.1808956","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090442039&doi=10.1080%2f01426397.2020.1808956&partnerID=40&md5=7d62819a76314985585d6ce145bbe1b6","The purpose of this study is to present an exploratory analysis of the relationship between body responses, immediate environmental factors and stress-related events. Using an experimental setup for data collection and information fusion from wearable sensors, this work tests three Machine Learning Algorithms for supervised classification of stress detection. Body skin temperature and electrodermal activity are processed to identify patterns of stress reaction while walking. Immediate environmental features from continuous sensor data are found to be useful in identifying stress-related events. The experiment was carried out in Singapore, a city-state with hot tropical weather where the climate conditions of the city encourage urban planners to meet walkability needs of the residents as well as to ensure short walking trips. © 2020 Landscape Research Group Ltd.","Stress, walkability, tropical weather outdoor comfort, interpretable features, Singapore","climate conditions; environmental factor; skin; urban area; walking; Singapore [Singapore (NTN)]; Singapore [Southeast Asia]"
"Benites-Lazaro L.L., Giatti L., Giarolla A.","Sustainability and governance of sugarcane ethanol companies in Brazil: Topic modeling analysis of CSR reporting","10.1016/j.jclepro.2018.06.212","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049884289&doi=10.1016%2fj.jclepro.2018.06.212&partnerID=40&md5=696e49ba6414767ecb09e47be5751071","The current debate on corporate social responsibility was triggered by the understanding that companies should be responsible for mitigating the impacts of their activities on the society and the environment. In this paper, drawing on the structural power of business, we examine the rule-setting power in which sugarcane ethanol companies engage, and then discuss why these companies develop and use institutions to promote sustainability. We use a machine learning algorithm, latent Dirichlet allocation, to identify companies’ commitment to sustainability and business-led governance by analyzing a large volume of data from public corporate documents. The results reveal 36 main themes that demonstrate the rule-setting power of the sugarcane ethanol industry through voluntary standards, codes of conduct, and corporate social responsibility these companies use as indicators of superior social and environmental performance and to show sustainability is embedded in companies’ priorities. However, the results also show critical issues of unsustainable production practices can affect industry image and stress socio-environmental relationships. These issues can be ameliorated with integrative governance that considers the independence of the sectors involved in ethanol production. © 2018 Elsevier Ltd","Biofuels; Bonsucro; Certification; Code of conduct; Corporate social responsibility (CSR); Ethanol; Governance; Latent Dirichlet allocation (LDA); Network analysis; Sustainability","Biofuels; Electric network analysis; Environmental management; Ethanol; Learning algorithms; Learning systems; Statistics; Bonsucro; Certification; Code of conduct; Corporate social responsibilities (CSR); Governance; Latent dirichlet allocations; Sustainable development"
"Benjamin A.S., Fernandes H.L., Tomlinson T., Ramkumar P., Versteeg C., Chowdhury R.H., Miller L.E., Kording K.P.","Modern machine learning as a benchmark for fitting neural responses","10.3389/fncom.2018.00056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053429760&doi=10.3389%2ffncom.2018.00056&partnerID=40&md5=3951a69f545b8a13e1fbde146bd2aca0","Neuroscience has long focused on finding encoding models that effectively ask “what predicts neural spiking?” and generalized linear models (GLMs) are a typical approach. It is often unknown how much of explainable neural activity is captured, or missed, when fitting a model. Here we compared the predictive performance of simple models to three leading machine learning methods: feedforward neural networks, gradient boosted trees (using XGBoost), and stacked ensembles that combine the predictions of several methods. We predicted spike counts in macaque motor (M1) and somatosensory (S1) cortices from standard representations of reaching kinematics, and in rat hippocampal cells from open field location and orientation. Of these methods, XGBoost and the ensemble consistently produced more accurate spike rate predictions and were less sensitive to the preprocessing of features. These methods can thus be applied quickly to detect if feature sets relate to neural activity in a manner not captured by simpler methods. Encoding models built with a machine learning approach accurately predict spike rates and can offer meaningful benchmarks for simpler models. © 2018 Benjamin, Fernandes, Tomlinson, Ramkumar, VerSteeg,.","Encoding models; Generalized linear model; GLM; Machine learning; Neural coding; Spike prediction; Tuning curves","Brain; Encoding (symbols); Feedforward neural networks; Forecasting; Maximum likelihood; Neurons; Signal encoding; Encoding models; Generalized linear model; Machine learning approaches; Machine learning methods; Neural coding; Predictive performance; Rat hippocampal cells; Tuning curve; Learning systems; animal cell; animal experiment; article; hippocampus; kinematics; Macaca; machine learning; nerve potential; nonhuman; prediction; rat; spike; tuning curve"
"Benjamin J.J., Kinkeldey C., Müller-Birn C., Korjakow T., Herbst E.-M.","Explanation Strategies as an Empirical-Analytical Lens for Socio-Technical Contextualization of Machine Learning Interpretability","10.1145/3492858","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123280136&doi=10.1145%2f3492858&partnerID=40&md5=60aa37e80d64475bea0fd3841c9ab866","During a research project in which we developed a machine learning (ML) driven visualization system for non-ML experts, we reflected on interpretability research in ML, computer-supported collaborative work and human-computer interaction. We found that while there are manifold technical approaches, these often focus on ML experts and are evaluated in decontextualized empirical studies. We hypothesized that participatory design research may support the understanding of stakeholders' situated sense-making in our project, yet, found guidance regarding ML interpretability inexhaustive. Building on philosophy of technology, we formulated explanation strategies as an empirical-analytical lens explicating how technical explanations mediate the contextual preferences concerning people's interpretations. In this paper, we contribute a report of our proof-of-concept use of explanation strategies to analyze a co-design workshop with non-ML experts, methodological implications for participatory design research, design implications for explanations for non-ML experts and suggest further investigation of technological mediation theories in the ML interpretability space. © 2022 Owner/Author.","explainable machine learning; explanation strategies; participatory design; post-phenomenology; subject-matter experts","Machine learning; Contextualization; Design research; Explainable machine learning; Explanation strategy; Interpretability; Machine-learning; Participatory design; Post-phenomenology; Sociotechnical; Subject matter experts; Human computer interaction"
"Benjamin J.J., Müller-Birn C.","Materializing interpretability: Exploring meaning in algorithmic systems","10.1145/3301019.3323900","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069470381&doi=10.1145%2f3301019.3323900&partnerID=40&md5=d8217ba97cc95754ce7d11a04877f7c6","Interpretability has become a key objective in the research, development and implementation of machine learning algorithms. However, existing notions of interpretability may not be conducive to how meaning emerges in algorithmic systems that employ ML algorithms. In this provocation, we suggest that hermeneutic analysis can be used to probe assumptions in interpretability. First, we propose three levels of interpretability that may be analyzed: formality, achievability, and linearity. Second, we discuss how the three levels have surfaced in prior work, in which we conducted an explicitation interview with a developer to understand decision-making in an algorithmic system implementation. Third, we suggest that design practice may be needed to move beyond analytic deconstruction, and showcase two design projects that exemplify possible strategies. In concluding, we suggest how the proposed approach may be taken up in future work and point to research avenues. Copyright held by the owner/author(s).","Design; Explainable AI; Hermeneutics; Interpretability; Materiality","Decision making; Design; Learning algorithms; Achievability; Design practice; Design projects; Hermeneutics; Interpretability; Key objective; Materiality; System implementation; Machine learning"
"Benjamins R.","Towards organizational guidelines for the responsible use of ai","10.3233/FAIA200433","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091765941&doi=10.3233%2fFAIA200433&partnerID=40&md5=13be27cb5e6fee1a70d439be085c75f0","In the past few years, several large companies have published ethical principles of Artificial Intelligence (AI). National governments, the European Commission, and inter-governmental organizations have come up with requirements to ensure the good use of AI. However, individual organizations that want to join this effort, are faced with many unsolved questions. This paper proposes guidelines for organizations committed to the responsible use of AI, but lack the required knowledge and experience. The guidelines consist of two parts: I) helping organizations to decide what principles to adopt, and ii) a methodology for implementing the principles in organizational processes. In case of future AI regulation, organizations following this approach will be well-prepared. © 2020 The authors and IOS Press.",,"Ethical principles; European Commission; Inter-governmental organization; Knowledge and experience; Large companies; National governments; Organizational process; Artificial intelligence"
"Benkert R., Aribido O.J., AlRegib G.","Explainable seismic neural networks using learning statistics","10.1190/segam2021-3583675.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120960383&doi=10.1190%2fsegam2021-3583675.1&partnerID=40&md5=ecbb43abf28bc8ab385b6f9a7ff25294","Even though deep neural networks are incredibly successful in speeding up seismic interpretation, they are frequently met with skepticism. Specifically, the main critique of deep learning remains the lack of explainable predictions and their poor generalization to difficult textures or structures that are not present in the training volume. The objective of this paper is to address this highly relevant issue by explaining deep seismic networks with their behaviour during model training. Our approach is based on analysing how networks”forget” previously learned seismic reflections and on visualizing the forgetting behavior in heat maps. We show that our method connects difficult seismic regions with incoherent neural network behaviour by establishing a link between forgettable seismic reflections and the learned decision boundary of neural networks. This makes our method especially useful for reliability and robustness analysis of deep seismic models. Finally, we group forgettable seismic regions into sub-categories by analyzing when the network forgets specific areas. Each subgroup is characterized by unique features that are related to the learning difficulty of specific subsurfaces and contain valuable insights in the interpretation properties of neural networks. © 2021 Society of Exploration Geophysicists First International Meeting for Applied Geoscience & Energy",,"Deep neural networks; Reliability analysis; Seismic waves; Textures; Generalisation; Heat maps; Learning statistics; Model training; Network behaviors; Neural-networks; Seismic interpretation; Seismic networks; Seismic reflections; Seismic regions; Seismology"
"Benkhaled H.N., Berrabah D.","Data quality management for data warehouse systems: State of the art",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064942813&partnerID=40&md5=cf7c260e6872adf175764c455b1cc660","During the last years, Data Warehouse (DW) systems have been considered as the most effective tool for decision support making. Most of the enterprises are obliged to implement their own Data Warehouse systems in order to use their collected data, make decisive decisions out of it and have a place in the market. However, most of the DW projects are interrupted due to poor Data Quality (DQ) problems like missing values, duplicate values and referential integrity issues. DQ problems can decrease customer satisfaction and increase the cost of the data warehouse projects. At the same time, the arriving of Big Data puts new requirements on the traditional DW systems and specifically on the ETL (Extract, Transform, Load) process, which is responsible for data collecting, cleansing and loading. These requirements can be summarized into the real time analyzing and the need of collecting the most recent data. This paper will include two important points: (1) a survey of the existing approaches in the literature for managing data quality in the traditional data warehouse systems, (2) a survey about the existing approaches for adapting traditional DW systems to the new requirements of Big Data. © 2019 CEUR-WS. All rights reserved.","Big data; Data quality; Data warehouse; ETL","Big data; Customer satisfaction; Data mining; Data reduction; Data warehouses; Decision support systems; Quality management; Surveys; Data collecting; Data quality; Data quality management; Data warehouse systems; Decision supports; Effective tool; Referential integrity; State of the art; Information management"
"Benmalek M., Attia A., Bouziane A., Hassaballah M.","A semi-supervised deep rule-based classifier for robust finger knuckle-print verification","10.1007/s12530-021-09417-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122650499&doi=10.1007%2fs12530-021-09417-x&partnerID=40&md5=a1b73a74922a9f771eb434cc5a9c2477","Today, biometric recognition systems play an important role in various applications of different domains. Despite remarkable progress, their performance remains insufficient for security applications. Recently, semi-supervised deep rule classifier (SSDRB) is clearly explainable and universal classification tool used to solve different problems of classification or prediction. Thus, in this paper, we propose an effective scheme based SSDRB classifier for personal authentication systems, where, finger knuckle print (FKP) has been exploited. The proposed scheme is data driven and completely automatic. In this scheme, the pertinent and relevant features are extracted from the input finger knuckle image by binarized statistical image features descriptor (BSIF), which are then fed into fuzzy rules based multilayer semi-supervised learning approach based on a deep rule-based (DRB) classifier to decide whether the person is genuine or impostor. The experiments were conducted on the publicly available PolyU-FKP database provided by University of Hong Kong. The results are represented in form of rank-1, equal error rate (EER), cumulative match curve (CMC) and receiver operating characteristic (ROC) curves. The obtained results demonstrate that the proposed SSDRB classifier is a promising tool for the FKP biometric identification systems. Experimental results on the PolyU-FKP database show that the proposed SSDRB achieves lower error rates with an EER of 0.00% and a rank-1 of 99.90% on the FKP single modality outperforming several published methods. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Biometric systems; BSIF descriptor; Finger knuckle; Semi-supervised deep rule-based classifiers","Fuzzy inference; Machine learning; Binarized statistical image feature descriptor descriptor; Biometric systems; Descriptors; Feature descriptors; Finger knuckle; Image features; Rule-based classifier; Semi-supervised; Semi-supervised deep rule-based classifier; Statistical images; Biometrics"
"Benmohammed K., Valensi P., Omri N., Al Masry Z., Zerhouni N.","Metabolic syndrome screening in adolescents: New scores AI_METS based on artificial intelligence techniques","10.1016/j.numecd.2022.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139286366&doi=10.1016%2fj.numecd.2022.08.007&partnerID=40&md5=12ea1668265c621990e54e05bd8f15bd","Background and aims: Metabolic syndrome (MetS) definitions in adolescents based on the percentiles of its components are rather complicated to use in clinical practice. The aim of this study was to test the validity of artificial intelligence (AI)-based scores (AI_METS) that do not use these percentiles for MetS screening for adolescents. Methods and results: This study included 1086 adolescents aged 12 to 18. The cohort underwent anthropometric measurements and blood tests. Mean blood pressure (MBP), and triglyceride glucose index (TyG) were calculated. Explainable AI methods are used to extract the learned function. Gini importance techniques were tested and used to build new scores for the screening of MetS. IDF, Cook, De Ferranti, Viner, and Weiss definitions of MetS were used to test the validity of these scores. MetS prevalence was 0.4%–4.7% according to these definitions. AI_METS used age, waist circumference, MBP, and TyG index. They offer area under the curves (AUCs) 0.91, 0.93, 0.89, 0.93, and 0.98; specificity 81%, 75%, 72%, 80%, and 97%; and sensitivity 90%, 100%, 90%, 100%, and 100%, respectively, for the detection of MetS according to these definitions. Considering only MBP offers a better specificity and sensitivity to detect MetS than considering only TyG index. MBP offers slightly lower performance than AI_METS. Conclusion: AI techniques have proven their ability to extract knowledge from data. They allowed us to generate new scores for MetS detection in adolescents without using specific percentiles for each component. Although these scores are less intuitive than the percentile-based definition, their accuracy is rather effective for the detection of MetS. © 2022 The Italian Diabetes Society, the Italian Society for the Study of Atherosclerosis, the Italian Society of Human Nutrition and the Department of Clinical Medicine and Surgery, Federico II University","Adolescent; Artificial intelligence; Cardiometabolic risk factors; Metabolic syndrome",
"Bennati S., Wossnig L., Thiele J.","The role of information in group formation","10.5220/0005751802310235","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969132673&doi=10.5220%2f0005751802310235&partnerID=40&md5=59851b5a766ac451123139b23aa5878a","A vast body of literature studies problems such as cooperation and coordination in groups, but the reasons why groups exist in the first place and hold together are still not clear: in presence of within-group competition, individuals are better off leaving the group. An environment that is advantageous to groups, e.g. better chances of succeeding at or escaping from predation, seems to play a key role for the existence of groups. Another recurrent explanation in the literature is between-group competition. We argue that information constraints can foster sociable behavior, which in turn is responsible for group creation. We compare, by means of an agent-based simulation, navigation strategies that exploit information about the behavior of others. We find that individuals that have sociable behavior have higher fitness than individualistic individuals for certain environmental configuration. Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","Group behavior; Information; Simulation; Swarming","Agent based simulation; Cooperation and coordination; Group behavior; Information; Literature studies; Navigation strategies; Simulation; Swarming; Artificial intelligence"
"Bennetot A., Franchi G., Ser J.D., Chatila R., Díaz-Rodríguez N.","Greybox XAI: A Neural-Symbolic learning framework to produce interpretable predictions for image classification","10.1016/j.knosys.2022.109947","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140443412&doi=10.1016%2fj.knosys.2022.109947&partnerID=40&md5=25139845a049556664e7967268b498e9","Although Deep Neural Networks (DNNs) have great generalization and prediction capabilities, their functioning does not allow a detailed explanation of their behavior. Opaque deep learning models are increasingly used to make important predictions in critical environments, and the danger is that they make and use predictions that cannot be justified or legitimized. Several eXplainable Artificial Intelligence (XAI) methods that separate explanations from machine learning models have emerged, but have shortcomings in faithfulness to the model actual functioning and robustness. As a result, there is a widespread agreement on the importance of endowing Deep Learning models with explanatory capabilities so that they can themselves provide an answer to why a particular prediction was made. First, we address the problem of the lack of universal criteria for XAI by formalizing what an explanation is. We also introduced a set of axioms and definitions to clarify XAI from a mathematical perspective. Finally, we present the Greybox XAI, a framework that composes a DNN and a transparent model thanks to the use of a symbolic Knowledge Base (KB). We extract a KB from the dataset and use it to train a transparent model (i.e., a logistic regression). An encoder–decoder architecture is trained on RGB images to produce an output similar to the KB used by the transparent model. Once the two models are trained independently, they are used compositionally to form an explainable predictive model. We show how this new architecture is accurate and explainable in several datasets. © 2022 Elsevier B.V.","Compositional models; Computer vision; Deep learning; Explainable artificial intelligence; Neural-symbolic learning and reasoning; Part-based object classification","Computer vision; Deep neural networks; Image classification; Knowledge based systems; Learning systems; Network architecture; Compositional models; Deep learning; Explainable artificial intelligence; Grey-box; Neural-symbolic learning and reasoning; Object classification; Part based; Part-based object classification; Symbolic learning; Symbolic reasoning; Forecasting"
"Bennett K.P., Momma M., Embrechts M.J.","MARK: A boosting algorithm for heterogeneous kernel models",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242540474&partnerID=40&md5=dfce93f8796b178c8311d7f40b0c9fb4","Support Vector Machines and other kernel methods have proven to be very effective for nonlinear inference. Practical issues are how to select the type of kernel including any parameters and how to deal with the computational issues caused by the fact that the kernel matrix grows quadratically with the data. Inspired by ensemble and boosting methods like MART, we propose the Multiple Additive Regression Kernels (MARK) algorithm to address these issues. MARK considers a large (potentially infinite) library of kernel matrices formed by different kernel functions and parameters. Using gradient boosting/column generation, MARK constructs columns of the heterogeneous kernel matrix (the base hypotheses) on the fly and then adds them into the kernel ensemble. Regularization methods such as used in SVM, kernel ridge regression, and MART, are used to prevent overfitting. We investigate how MARK is applied to heterogeneous kernel ridge regression. The resulting algorithm is simple to implement and efficient. Kernel parameter selection is handled within MARK. Sampling and ""weak"" kernels are used to further enhance the computational efficiency of the resulting additive algorithm. The user can incorporate and potentially extract domain knowledge by restricting the kernel library to interpretable kernels. MARK compares very favorably with SVM and kernel ridge regression on several benchmark datasets.",,"Algorithms; Computational methods; Data structures; Knowledge based systems; Learning systems; Regression analysis; Boosting algorithm; Heterogeneous kernel models; Multiple additive regression kernel algorithm; Support vector machines; Data mining"
"Benrimoh D., Israel S., Perlman K., Fratila R., Krause M.","Meticulous transparency—An evaluation process for an agile AI regulatory scheme","10.1007/978-3-319-92058-0_83","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049050181&doi=10.1007%2f978-3-319-92058-0_83&partnerID=40&md5=caee61fa65dda717316fd2224cea3744","Artificial intelligence (AI) poses both great potential and risk, as a rapidly developing and generally applicable technology. To ensure the ethical development and responsible use of AI, we outline a new ethical evaluation framework for usage by future regulators: Meticulous Transparency (MT). MT allows regulators to keep pace with technological progress by evaluating AI applications for their capabilities and the intentionality of developers, rather than evaluating conformity to static regulations or ethical codes of the underlying technologies themselves. MT shifts the focus of ethical evaluation from the technology itself to instead why it is being built, and potential consequences. MT assessment is reminiscent of a Research Ethics Board submission in medical research, with required explanation depending on the potential impact of the AI system. We propose the use of MT to transform AI-specific ethical quandaries into more familiar ethical questions, which society must then address. © 2018, Springer International Publishing AG, part of Springer Nature.","AI ethics; Assessment framework; Intentionality; Meticulous transparency","Intelligent systems; Transparency; Assessment framework; Ethical question; Evaluation framework; Intentionality; Medical research; Potential impacts; Regulatory schemes; Technological progress; Philosophical aspects"
"Bensch S., Holzer M., Kutrib M., Malcher A.","Input-driven stack automata","10.1007/978-3-642-33475-7_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866929892&doi=10.1007%2f978-3-642-33475-7_3&partnerID=40&md5=a586185ae2d70d7d57f1874dfceb93c3","We introduce and investigate input-driven stack automata, which are a generalization of input-driven pushdown automata that recently became popular under the name visibly pushdown automata. Basically, the idea is that the input letters uniquely determine the operations on the pushdown store. This can nicely be generalized to stack automata by further types of input letters which are responsible for moving the stack pointer up or down. While visibly pushdown languages share many desirable properties with regular languages, input-driven stack automata languages do not necessarily so. We prove that deterministic and nondeterministic input-driven stack automata have different computational power, which shows in passing that one cannot construct a deterministic input-driven stack automaton from a nondeterministic one. We study the computational capacity of these devices. Moreover, it is shown that the membership problem for nondeterministic input-driven stack automata languages is NP-complete. © 2012 IFIP International Federation for Information Processing.",,"Computational capacity; Computational power; Membership problem; NP Complete; Push-down automata; Pushdown stores; Stack pointers; Visibly pushdown languages; Computational capacity; Computational power; Membership problem; Push-down automata; Pushdown stores; Stack pointers; Visibly pushdown automaton; Visibly pushdown languages; Computer science; Artificial intelligence; Computer science; Computers; Automata theory; Automata theory"
"Ben-Shabat Y., Lindenbaum M., Fischer A.","3DmFV: Three-dimensional point cloud classification in real-time using convolutional neural networks","10.1109/LRA.2018.2850061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063309316&doi=10.1109%2fLRA.2018.2850061&partnerID=40&md5=8177161774e295e44d60b50b2b5a61b0","Modern robotic systems are often equipped with a direct three-dimensional (3-D) data acquisition device, e.g., LiDAR, which provides a rich 3-D point cloud representation of the surroundings. This representation is commonly used for obstacle avoidance and mapping. Here, we propose a new approach for using point clouds for another critical robotic capability, semantic understanding of the environment (i.e., object classification). Convolutional neural networks (CNNs), that perform extremely well for object classification in 2-D images, are not easily extendible to 3-D point clouds analysis. It is not straightforward due to point clouds' irregular format and a varying number of points. The common solution of transforming the point cloud data into a 3-D voxel grid needs to address severe accuracy versus memory size tradeoffs. In this letter, we propose a novel, intuitively interpretable, 3-D point cloud representation called 3-D modified Fisher vectors. Our representation is hybrid as it combines a coarse discrete grid structure with continuous generalized Fisher vectors. Using the grid enables us to design a new CNN architecture for real-time point cloud classification. In a series of performance analysis experiments, we demonstrate competitive results or even better than state of the art on challenging benchmark datasets while maintaining robustness to various data corruptions. © 2016 IEEE.","computer vision for other robotic applications; computer vision for transportation; Deep learning in robotics and automation; recognition","Benchmarking; Computational efficiency; Computer vision; Convolution; Data acquisition; Deep learning; Interactive computer systems; Learning systems; Metadata; Neural networks; Optical radar; Real time systems; Robotics; Robots; Semantics; 3D data acquisition; Convolutional neural network; Convolutional Neural Networks (CNN); Object classification; Performance analysis; Recognition; Robotic applications; Semantic understanding; Three dimensional displays"
"Benton A., Paul M.J., Hancock B., Dredze M.","Collective supervision of topic models for predicting surveys with social media",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007203270&partnerID=40&md5=cdb72c08c3f1f4627d877e08d50e1a3e","This paper considers survey prediction from social media. We use topic models to correlate social media messages with survey outcomes and to provide an interpretable representation of the data. Rather than rely on fully unsupervised topic models, we use existing aggregated survey data to inform the inferred topics, a class of topic model supervision referred to as collective supervision. We introduce and explore a variety of topic model variants and provide an empirical analysis, with conclusions of the most effective models for this task. © Copyright 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Surveys; Empirical analysis; Interpretable representation; Social media; Survey data; Topic model; Topic Modeling; Social networking (online)"
"Benyahia A.A., Hajjam A., Hilaire V., Hajjam M.","E-Care: Ontological architecture for telemonitoring and alerts detection","10.1109/ICTAI.2012.183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876877251&doi=10.1109%2fICTAI.2012.183&partnerID=40&md5=1edc808d7a970905a841751009f588a0","In most developed countries, life expectancy has been increasing steadily and burden of chronic disease continues to grow. The chronic diseases are responsible for increasingly growing health spending. Telemonitoring systems provide a way to monitor patients and their needs within the comfort of their own homes. In the first systems, the data were sent directly to the medical experts to be interpreted. With technological advancements, software and applications have been developed to process the data. In this paper, we will focus on e-Care platform that combines the semantic web and artificial intelligence, for telemonitoring. e-Care is based on generic ontologies to accommodate different conditions and types of sensors and data. A decision support is bases on an inference engine, this engine is used for following up the health of the patient and the detection of abnormal situations and react accordingly, by providing recommendations and informing his physician with alerts. © 2012 IEEE.","alert detection.; decision support; Ontologies; rule base inference; telemonitoring","Chronic disease; Decision supports; Developed countries; Life expectancies; Rule base; Technological advancement; Tele-monitoring; Telemonitoring systems; Application programs; Decision support systems; Diseases; Ontology; Patient monitoring; Artificial intelligence"
"Ben-Zion Z., Shany O., Admon R., Keynan N.J., Avisdris N., Balter S.R., Shalev A.Y., Liberzon I., Hendler T.","Neural Responsivity to Reward Versus Punishment Shortly After Trauma Predicts Long-Term Development of Posttraumatic Stress Symptoms","10.1016/j.bpsc.2021.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120430931&doi=10.1016%2fj.bpsc.2021.09.001&partnerID=40&md5=6066b5d391ae7da93791eee15d9e0bf3","Background: Processing negatively and positively valenced stimuli involves multiple brain regions including the amygdala and ventral striatum (VS). Posttraumatic stress disorder (PTSD) is often associated with hyperresponsivity to negatively valenced stimuli, yet recent evidence also points to deficient positive valence functioning. It is yet unclear what the relative contribution is of such opposing valence processing shortly after trauma to the development of chronic PTSD. Methods: Neurobehavioral indicators of motivational positive versus negative valence sensitivities were longitudinally assessed in 171 adults (87 females, age = 34.19 ± 11.47 years) at 1, 6, and 14 months following trauma exposure (time point 1 [TP1], TP2, and TP3, respectively). Using a gambling functional magnetic resonance imaging paradigm, amygdala and VS functionality (activity and functional connectivity with the prefrontal cortex) in response to rewards versus punishments were assessed with relation to PTSD severity at different time points. The effect of valence processing was depicted behaviorally by the amount of risk taken to maximize reward. Results: PTSD severity at TP1 was associated with greater neural functionality in the amygdala (but not in the VS) toward punishments versus rewards, and with fewer risky choices. PTSD severity at TP3 was associated with decreased neural functionality in both the VS and the amygdala toward rewards versus punishments at TP1 (but not with risky behavior). Explainable machine learning revealed the primacy of VS-biased processing, over the amygdala, in predicting PTSD severity at TP3. Conclusions: These results highlight the importance of biased neural responsivity to positive relative to negative motivational outcomes in PTSD development. Novel therapeutic strategies early after trauma may thus target both valence fronts. © 2021 Society of Biological Psychiatry","Amygdala; fMRI; Functional MRI; Negative valence system; Positive valence system; Posttraumatic stress disorder; PTSD; Ventral striatum","adult; amygdala; Article; female; functional magnetic resonance imaging; human; injury; long term care; longitudinal study; machine learning; major clinical study; male; posttraumatic stress disorder; punishment; questionnaire; reward; salience network; traffic accident; diagnostic imaging; middle aged; posttraumatic stress disorder; prefrontal cortex; young adult; Adult; Amygdala; Female; Humans; Middle Aged; Prefrontal Cortex; Punishment; Reward; Stress Disorders, Post-Traumatic; Young Adult"
"Benzmüller C., Lomfeld B.","Reasonable machines: A research manifesto","10.1007/978-3-030-58285-2_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091183119&doi=10.1007%2f978-3-030-58285-2_20&partnerID=40&md5=25bb47e81d20f3b8ae47f043e0bd99a7","Future intelligent autonomous systems (IAS) are inevitably deciding on moral and legal questions, e.g. in self-driving cars, health care or human-machine collaboration. As decision processes in most modern sub-symbolic IAS are hidden, the simple political plea for transparency, accountability and governance falls short. A sound ecosystem of trust requires ways for IAS to autonomously justify their actions, that is, to learn giving and taking reasons for their decisions. Building on social reasoning models in moral psychology and legal philosophy such an idea of »Reasonable Machines« requires novel, hybrid reasoning tools, ethico-legal ontologies and associated argumentation technology. Enabling machines to normative communication creates trust and opens new dimensions of AI application and human-machine interaction. © Springer Nature Switzerland AG 2020.","Ethico-legal governors; Pluralistic and expressive normative reasoning; Social reasoning model; Trusthworthy and explainable AI","Computer science; Computers; AI applications; Decision process; Human machine interaction; Human-machine collaboration; Hybrid reasonings; Intelligent autonomous systems; Legal questions; Social reasoning; Artificial intelligence"
"Ber W.W., Curto M., Tibihika P., Meulenbroek P., Alemayehu E., Mehnen L., Meimberg H., Sykacek P.","Identifying geographically differentiated features of Ethopian Nile tilapia (Oreochromis niloticus) morphology with machine learning","10.1371/journal.pone.0249593","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104448018&doi=10.1371%2fjournal.pone.0249593&partnerID=40&md5=9c7932bad7e6f78343e9e93eb099c6d0","Visual characteristics are among the most important features for characterizing the phenotype of biological organisms. Color and geometric properties define population phenotype and allow assessing diversity and adaptation to environmental conditions. To analyze geometric properties classical morphometrics relies on biologically relevant landmarks which are manually assigned to digital images. Assigning landmarks is tedious and error prone. Predefined landmarks may in addition miss out on information which is not obvious to the human eye. The machine learning (ML) community has recently proposed new data analysis methods which by uncovering subtle features in images obtain excellent predictive accuracy. Scientific credibility demands however that results are interpretable and hence to mitigate the black-box nature of ML methods. To overcome the black-box nature of ML we apply complementary methods and investigate internal representations with saliency maps to reliably identify location specific characteristics in images of Nile tilapia populations. Analyzing fish images which were sampled from six Ethiopian lakes reveals that deep learning improves on a conventional morphometric analysis in predictive performance. A critical assessment of established saliency maps with a novel significance test reveals however that the improvement is aided by artifacts which have no biological interpretation. More interpretable results are obtained by a Bayesian approach which allows us to identify genuine Nile tilapia body features which differ in dependence of the animals habitat. We find that automatically inferred Nile tilapia body features corroborate and expand the results of a landmark based analysis that the anterior dorsum, the fish belly, the posterior dorsal region and the caudal fin show signs of adaptation to the fish habitat. We may thus conclude that Nile tilapia show habitat specific morphotypes and that a ML analysis allows inferring novel biological knowledge in a reproducible manner. © 2021 Wo¨ber et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"article; artifact; caudal fin; data analysis; deep learning; dorsal region; habitat; lake; morphotype; nonhuman; Oreochromis niloticus; statistical significance; anatomic model; anatomy and histology; animal; Bayes theorem; cichlid; ecosystem; image processing; machine learning; phenotype; procedures; Animals; Bayes Theorem; Cichlids; Ecosystem; Image Processing, Computer-Assisted; Machine Learning; Models, Anatomic; Phenotype"
"Bera R., Goecks V.G., Gremillion G.M., Valasek J., Waytowich N.R.","PodNet: A neural network for discovery of plannable options",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085208556&partnerID=40&md5=dead1041b9138a40d71fe7863887f6eb","Learning from demonstration has been widely studied in machine learning but becomes challenging when the demonstrated trajectories are unstructured and follow different objectives. This short-paper proposes PODNet, Plannable Option Discovery Network, addressing how to segment an unstructured set of demonstrated trajectories for option discovery. This enables learning from demonstration to perform multiple tasks and plan high-level trajectories based on the discovered option labels. PODNet combines a custom categorical variational autoencoder, a recurrent option inference network, option-conditioned policy network, and option dynamics model in an end-to-end learning architecture. Due to the concurrently trained option-conditioned policy network and option dynamics model, the proposed architecture has implications in multi-task and hierarchical learning, explainable and interpretable artificial intelligence, and applications where the agent is required to learn only from observations. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Knowledge engineering; Machine learning; Springs (components); Trajectories; Dynamics modeling; Hierarchical learning; Inference network; Learning architectures; Learning from demonstration; Multiple tasks; Policy networks; Proposed architectures; Network architecture"
"Beranová L., Joachimiak M.P., Kliegr T., Rabby G., Sklenák V.","Why was this cited? Explainable machine learning applied to COVID-19 research literature","10.1007/s11192-022-04314-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127726326&doi=10.1007%2fs11192-022-04314-9&partnerID=40&md5=e3a5260786747850c987fdfa161efd8a","Multiple studies have investigated bibliometric factors predictive of the citation count a research article will receive. In this article, we go beyond bibliometric data by using a range of machine learning techniques to find patterns predictive of citation count using both article content and available metadata. As the input collection, we use the CORD-19 corpus containing research articles—mostly from biology and medicine—applicable to the COVID-19 crisis. Our study employs a combination of state-of-the-art machine learning techniques for text understanding, including embeddings-based language model BERT, several systems for detection and semantic expansion of entities: ConceptNet, Pubtator and ScispaCy. To interpret the resulting models, we use several explanation algorithms: random forest feature importance, LIME, and Shapley values. We compare the performance and comprehensibility of models obtained by “black-box” machine learning algorithms (neural networks and random forests) with models built with rule learning (CORELS, CBA), which are intrinsically explainable. Multiple rules were discovered, which referred to biomedical entities of potential interest. Of the rules with the highest lift measure, several rules pointed to dipeptidyl peptidase4 (DPP4), a known MERS-CoV receptor and a critical determinant of camel to human transmission of the camel coronavirus (MERS-CoV). Some other interesting patterns related to the type of animal investigated were found. Articles referring to bats and camels tend to draw citations, while articles referring to most other animal species related to coronavirus are lowly cited. Bat coronavirus is the only other virus from a non-human species in the betaB clade along with the SARS-CoV and SARS-CoV-2 viruses. MERS-CoV is in a sister betaC clade, also close to human SARS coronaviruses. Thus both species linked to high citation counts harbor coronaviruses which are more phylogenetically similar to human SARS viruses. On the other hand, feline (FIPV, FCOV) and canine coronaviruses (CCOV) are in the alpha coronavirus clade and more distant from the betaB clade with human SARS viruses. Other results include detection of apparent citation bias favouring authors with western sounding names. Equal performance of TF-IDF weights and binary word incidence matrix was observed, with the latter resulting in better interpretability. The best predictive performance was obtained with a “black-box” method—neural network. The rule-based models led to most insights, especially when coupled with text representation using semantic entity detection methods. Follow-up work should focus on the analysis of citation patterns in the context of phylogenetic trees, as well on patterns referring to DPP4, which is currently considered as a SARS-Cov-2 therapeutic target. © 2022, Akadémiai Kiadó, Budapest, Hungary.","Bibliometry; Citation prediction; CORD-19: COVID-19 open research dataset; Interpretability; Phylogenetic distance; SARS-CoV-2; Text analysis; Virus clades",
"Berardi L., Savic D., Giustolisi O.","Investigation of burst-prediction formulas for water distribution systems by evolutionary computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906226078&partnerID=40&md5=5ef360b3ec2706ae15c1aaee5e391373","Risk-based management and rehabilitation planning of water distribution systems require collection of company asset data and a methodology for efficient data analysis, i.e. water managers need procedures for identifying logical, non-trivial, useful and understandable patterns in data. In order to achieve this purpose, the data mining methodology named Evolutionary Polynomial Regression (EPR) is applied to investigate pipe burst data from a UK water company. Starting from a hybrid evolutionary strategy, EPR searches for patterns in data and returns symbolic expressions/models. The resulting models contain explicitly recognizable independent variables thus providing an insight into the process being modelled. This feature of EPR is used to investigate the structure of different models for burst prediction and the explicit relationships between the output set (bursts) and the input set (pipe age, pipe length, pipe diameter, etc.). These explicit relationships are useful for understanding if rehabilitation is needed and how crucial is to rehabilitate in order to meet the performance targets (e.g., number of pipe breaks). The attempt is also made to analyse and generalise formulas obtained by using some features of the water system thereby allowing development of more reliable burst prediction models for systems with scarce historical data.","Evolutionary polynomial regression; Performance indicators; Pipe bursts","Evolutionary algorithms; Water management; Evolutionary computing; Evolutionary polynomial regressions; Evolutionary strategies; Independent variables; Performance indicators; Pipe burst; Rehabilitation planning; Risk-based managements; Water distribution systems"
"Berdaliyeva A.S., Kim A.I., Seraliyeva A.M., Gassanov A.A., Dunentayev M.V.","Criminological measures to counteract corruption offences in the field of illegal gambling","10.1108/JFC-11-2021-0246","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121803119&doi=10.1108%2fJFC-11-2021-0246&partnerID=40&md5=76f923270f7a2826365de6edbfb3998c","Purpose: Despite all the anti-corruption measures and anti-corruption initiatives, people offer or accept bribes without any hesitation. As anywhere in the world, the negative consequences of corruption lead to a reduction in direct investment, increase inequality and poverty, distort and use public investment and reduce public revenues. The purpose of this article is to study the criminological measures to counteract corruption offences in the field of illegal gambling. Design/methodology/approach: The methodological basis of the study is the provisions of the theory of knowledge: the laws of dialectical materialism, philosophical categories and scientific principles of cognition of social and legal reality. Findings: Although many components of foreign state anti-corruption programmes are quite problematic to apply in modern realities in the Republic of Kazakhstan, according to legal scholars, through gradual implementation into the legislation of the Republic of Kazakhstan because of the systematic improvement by the state of the content of regulations and responsible implementation of anti-corruption strategies. In this regard, one of the conditions in the fight against corruption is actions aimed at using the best practices of countries that are similar to each other in terms of religion, habits, traditions, ethics and morality. Originality/value: Anti-corruption initiatives using information and communication technologies, such as digital public services and e-government, crowdsourcing platforms, tools for exposing, transparency portals, blockchain and artificial intelligence technologies can provide significant assistance in combating manifestations of corruption in the field of illegal gambling on the internet in the Republic of Kazakhstan. © 2020, Emerald Publishing Limited.","Anti-corruption; Corruption offences; Criminological measures; Illegal gambling",
"Berendt B., Preibusch S.","Better decision support through exploratory discrimination-aware data mining: Foundations and empirical evidence","10.1007/s10506-013-9152-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900864205&doi=10.1007%2fs10506-013-9152-0&partnerID=40&md5=79aca294c6ad7ab800106d87ade6ba6b","Decision makers in banking, insurance or employment mitigate many of their risks by telling ""good"" individuals and ""bad"" individuals apart. Laws codify societal understandings of which factors are legitimate grounds for differential treatment (and when and in which contexts) - or are considered unfair discrimination, including gender, ethnicity or age. Discrimination-aware data mining (DADM) implements the hope that information technology supporting the decision process can also keep it free from unjust grounds. However, constraining data mining to exclude a fixed enumeration of potentially discriminatory features is insufficient. We argue for complementing it with exploratory DADM, where discriminatory patterns are discovered and flagged rather than suppressed. This article discusses the relative merits of constraint-oriented and exploratory DADM from a conceptual viewpoint. In addition, we consider the case of loan applications to empirically assess the fitness of both discrimination-aware data mining approaches for two of their typical usage scenarios: prevention and detection. Using Mechanical Turk, 215 US-based participants were randomly placed in the roles of a bank clerk (discrimination prevention) or a citizen / policy advisor (detection). They were tasked to recommend or predict the approval or denial of a loan, across three experimental conditions: discrimination-unaware data mining, exploratory, and constraint-oriented DADM (eDADM resp. cDADM). The discrimination-aware tool support in the eDADM and cDADM treatments led to significantly higher proportions of correct decisions, which were also motivated more accurately. There is significant evidence that the relative advantage of discrimination-aware techniques depends on their intended usage. For users focussed on making and motivating their decisions in non-discriminatory ways, cDADM resulted in more accurate and less discriminatory results than eDADM. For users focussed on monitoring for preventing discriminatory decisions and motivating these conclusions, eDADM yielded more accurate results than cDADM. © 2014 Springer Science+Business Media Dordrecht.","Data mining for decision support; Discrimination discovery and prevention; Discrimination-aware data mining; Evaluation; Mechanical Turk; Online experiment; Responsible data mining; User studies","Decision support systems; Information technology; Motivation; Decision supports; Discrimination discovery and prevention; Evaluation; Mechanical turks; On-line experiments; User study; Data mining"
"Berendt B., Preibusch S.","Exploring discrimination: A user-centric evaluation of discrimination-aware data mining","10.1109/ICDMW.2012.109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873152534&doi=10.1109%2fICDMW.2012.109&partnerID=40&md5=c91c0942e45d3ceb9df33dc7207b58c0","Discrimination-aware data mining (DADM) aims at deriving patterns that do not discriminate on ""unjust grounds"" such as gender, ethnicity or nationality. DADM safeguards can be very helpful for decision-support applications in fields such as banking or employment. However, constraining data mining to exclude a fixed enumeration of potentially discriminatory features is too restrictive. It should be complemented by exploratory DADM. We discuss these two forms of DADM and their requirements for evaluation, and we discuss and refine our DCUBE-GUI tool as a system for exploratory DADM. In a user study administered via Mechanical Turk, we show that tools such as DCUBE-GUI can successfully assist novice users in exploring discrimination in data mining. © 2012 IEEE.","Discrimination discovery; Evaluation; Mechanical Turk; Responsible data mining; User studies","Decision support applications; Discrimination discovery; Evaluation; In-field; Mechanical turks; Novice user; User study; User-centric; Graphical user interfaces; Data mining"
"Beretta D., Monica S., Bergenti F.","Recent Neural-Symbolic Approaches to ILP Based on Templates","10.1007/978-3-031-15565-9_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140454783&doi=10.1007%2f978-3-031-15565-9_5&partnerID=40&md5=c7cc7e29aa1dc54ec48e6932ef92ed0d","Deep learning has been increasingly successful in the last few years, but its inherent limitations have recently become more evident, especially with respect to explainability and interpretability. Neural-symbolic approaches to inductive logic programming have been recently proposed to synergistically combine the advantages of inductive logic programming in terms of explainability and interpretability with the characteristic capability of deep learning to treat noisy, erroneous, and non-logical data. This paper surveys and briefly compares four relevant neural-symbolic approaches to inductive logic programming that have been proposed in the last five years and that use templates as an effective basis to learn logic programs from data. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Explainable artificial intelligence; Inductive logic programming; Neural-symbolic learning","Computer circuits; Inductive logic programming (ILP); Erroneous datum; Explainable artificial intelligence; Inductive logic; Inductive logic programming; Inherent limitations; Interpretability; Logic-programming; Neural-symbolic learning; Noisy data; Symbolic learning; Deep learning"
"Berga D.","Understanding eye movements: Psychophysics and a model of primary visual cortex","10.5565/rev/elcvia.1193","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080920947&doi=10.5565%2frev%2felcvia.1193&partnerID=40&md5=98db507a5eb33edcc4e7b78103e1a47b","Humans move their eyes in order to learn visual representations of the world. These eye movements depend on distinct factors, either by the scene that we perceive or by our own decisions. To select what is relevant to attend is part of our survival mechanisms and the way we build reality, as we constantly react both consciously and unconsciously to all the stimuli that is projected into our eyes. In this thesis [1] we try to explain (1) how we move our eyes, (2) how to build machines that understand visual information and deploy eye movements, and (3) how to make these machines understand tasks in order to decide for eye movements. (1)We provided the analysis of eye movement behavior elicited by low-level feature distinctiveness with a dataset of 230 synthetically-generated image patterns [2]. A total of 15 types of stimuli has been generated (e.g. orientation, brightness, color, size, etc.), with 7 feature contrasts for each feature category. Eyetracking data was collected from 34 participants during the viewing of the dataset, using Free-Viewing and Visual Search task instructions. Results showed that saliency is predominantly and distinctively influenced by: 1. feature type, 2. feature contrast, 3. temporality of fixations, 4. task difficulty and 5. center bias. From such dataset (SID4VAM), we have computed a benchmark of saliency models by testing performance using psychophysical patterns [3]. Model performance has been evaluated considering model inspiration and consistency with human psychophysics. Our study reveals that state-of-the-art Deep Learning saliency models do not perform well with synthetic pattern images, instead, models with Spectral/Fourier inspiration outperform others in saliency metrics and are more consistent with human psychophysical experimentation. (2) Computations in the primary visual cortex (area V1 or striate cortex) have long been hypothesized to be responsible, among several visual processing mechanisms, of bottom-up visual attention (also named saliency). In order to validate this hypothesis, images from eye tracking datasets have been processed with a biologically plausible model of V1 (named Neurodynamic Saliency Wavelet Model or NSWAM)[4]. Following Li's neurodynamic model, we define V1's lateral connections with a network of firing rate neurons, sensitive to visual features such as brightness, color, orientation and scale. Early subcortical processes (i.e. retinal and thalamic) are functionally simulated. The resulting saliency maps are generated from the model output, representing the neuronal activity of V1 projections towards brain areas involved in eye movement control. We want to pinpoint that our unified computational architecture is able to reproduce several visual processes (i.e. brightness, chromatic induction and visual discomfort) without applying any type of training or optimization and keeping the same parametrization. The model has been extended (NSWAMCM)[ 5] with an implementation of the cortical magnification function to define the retinotopical projections towards V1, processing neuronal activity for each distinct view during scene observation. Novel computational definitions of top-down inhibition (in terms of inhibition of return and selection mechanisms), are also proposed to predict attention in Free-Viewing and Visual Search conditions. Results show that our model outpeforms other biologically-inpired models of saliency prediction as well as to predict visual saccade sequences, specifically for nature and synthetic images. We also show how temporal and spatial characteristics of inhibition of return can improve prediction of saccades, as well as how distinct search strategies (in terms of feature-selective or category-specific inhibition) predict attention at distinct image contexts. (3) Although previous scanpath models have been able to efficiently predict saccades during Free- Viewing, it is well known that stimulus and task instructions can strongly affect eye movement patterns. In particular, task priming has been shown to be crucial to the deployment of eye movements, involving interactions between brain areas related to goal-directed behavior, working and long-term memory in combination with stimulus-driven eye movement neuronal correlates. In our latest study [6] we proposed an extension of the Selective Tuning Attentive Reference Fixation Controller Model based on task demands (STAR-FCT), describing novel computational definitions of Long-Term Memory, Visual Task Executive and Task Working Memory. With these modules we are able to use textual instructions in order to guide the model to attend to specific categories of objects and/or places in the scene. We have designed our memory model by processing a visual hierarchy of low- and high-level features. The relationship between the executive task instructions and the memory representations has been specified using a tree of semantic similarities between the learned features and the object category labels. Results reveal that by using this model, the resulting object localization maps and predicted saccades have a higher probability to fall inside the salient regions depending on the distinct task instructions compared to saliency. © 2019 Universitat Autonoma de Barcelona.","Attention; Eye movements; Firing rate; Freeviewing; Horizontal connections; Neural networks; Psychophysics; Saliency; visual cortex; Visual search",
"Bergamasco L., Saha S., Bovolo F., Bruzzone L.","AN EXPLAINABLE CONVOLUTIONAL AUTOENCODER MODEL for UNSUPERVISED CHANGE DETECTION","10.5194/isprs-archives-XLIII-B2-2020-1513-2020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091109339&doi=10.5194%2fisprs-archives-XLIII-B2-2020-1513-2020&partnerID=40&md5=6294c98390767679cfa749aecf4d2631","Transfer learning methods reuse a deep learning model developed for a task on another task. Such methods have been remarkably successful in a wide range of image processing applications. Following the trend, few transfer learning based methods have been proposed for unsupervised multi-temporal image analysis and change detection (CD). Inspite of their success, the transfer learning based CD methods suffer from limited explainability. In this paper, we propose an explainable convolutional autoencoder model for CD. The model is trained in: 1) an unsupervised way using, as the bi-temporal images, patches extracted from the same geographic location; 2) a greedy fashion, one encoder and decoder layer pair at a time. A number of features relevant for CD is chosen from the encoder layer. To build an explainable model, only selected features from the encoder layer is retained and the rest is discarded. Following this, another encoder and decoder layer pair is added to the model in similar fashion until convergence. We further visualize the features to better interpret the learned features. We validated the proposed method on a Landsat-8 dataset obtained in Spain. Using a set of experiments, we demonstrate the explainability and effectiveness of the proposed model. © 2020 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives.","Autoencoder; Change Detection; Deep Learning; Explainable Artificial Intelligence; Multi-temporal Analysis; Transfer Learning","Convolution; Decoding; Deep learning; Image processing; Signal encoding; Transfer learning; Change detection; Geographic location; Image processing applications; Learning-based methods; Multitemporal image analysis; Temporal images; Transfer learning methods; Unsupervised change detection; Learning systems"
"Bergamin L., Carraro T., Polato M., Aiolli F.","Novel Applications for VAE-based Anomaly Detection Systems","10.1109/IJCNN55064.2022.9892879","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140769588&doi=10.1109%2fIJCNN55064.2022.9892879&partnerID=40&md5=a416038ce556c3f2a449f87bbd5f9b5f","Deep generative modeling (DGM) is an increasingly popular approach that can create novel and unseen data, starting from a given data set. As the technology shows promising applications, many ethical issues also arise. For example, their misuse can enable disinformation campaigns and powerful phishing attempts. Research also shows different biases affect deep learning models, leading to social issues such as misrepresentation. In this work, we formulate a novel setting to deal with similar problems, showing that a repurposed anomaly detection system effectively generates novel data, avoiding generating specified unwanted data. We propose Variational Auto-encoding Binary Classifiers (V-ABC): a novel model that repurposes and extends the Auto-encoding Binary Classifier (ABC) anomaly detector using the Variational Auto-encoder (VAE). We survey the limitations of existing approaches and explore many tools to show the model's inner workings in an interpretable way. This proposal has excellent potential for generative applications: models that rely on user-generated data could automatically filter out unwanted content, such as offensive language, obscene images, and misleading information. © 2022 IEEE.","anomaly detection; deep generative model; variational auto encoder","Classification (of information); Computer vision; Deep learning; Encoding (symbols); Signal encoding; Anomaly detection; Anomaly detection systems; Auto encoders; Binary classifiers; Data set; Deep generative model; Encodings; Generative model; Novel applications; Variational auto encoder; Anomaly detection"
"Bergau D.M., Liu C., Lu H.","Prediction of human QT prolongation liability based on pre-clinical RNA expression profiles","10.1109/BIBM.2017.8217701","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046274915&doi=10.1109%2fBIBM.2017.8217701&partnerID=40&md5=a833bbc5e9ac57b8ed49775f1b140c41","Marked drug-induced prolongation of the QT interval on the electrocardiogram is associated with Torsades de Pointes (TdP), a potentially life-threatening cardiac arrhythmia. Assessment of QT prolongation liability in the drug development process is required but is time and resource intensive. Current pre-clinical safety assessments use patch clamp analysis of the Human Ether-a-Go-Go (hERG) channel, but analyses have broadened to include patch clamp analysis of other ion channels and the use of in silico models. This investigation describes a method for predicting drug-induced QT prolongation liability in humans based on an association with RNA microarray expression profiles from rat liver data, and machine learning implemented in open-source software. Recently reported hERG patch clamp sensitivities and specificities range from between 64-82% and 75-88% respectively. Classification in this study was done using drugs known to prolong the QT interval vs. those that do not, regardless of the drugs' respective indication(s), and then further sub-classified by indication which resulted in 76 sub-groups. Classifier results in this project using 10-fold cross validation had average sensitivities of 85% and specificities of 90% using all available datasets as input, and a mean sensitivity and specificity of 92% and 94%, respectively across 76 drug sub-classifications. While an association between rat liver RNA expression profiles and QT prolongation in human heart tissue does not imply that a specific genetic expression profile is responsible for the QT prolongation, these results suggest that machine learning of gene expression profiles to predict QT liability may be used as a surrogate biomarker as part of the pre-clinical cardiac safety assessment of drugs. © 2017 IEEE.","APCluster; CiPA; hERG; LQTS; QT; SVM","Artificial intelligence; Bioinformatics; Classification (of information); Electrophysiology; Forecasting; Learning systems; Open source software; Open systems; Rats; RNA; Safety engineering; 10-fold cross-validation; APCluster; Average sensitivities; CiPA; Gene expression profiles; hERG; LQTS; Microarray expressions; Gene expression"
"Berge G.T., Granmo O.-C., Tveit T.O., Goodwin M., Jiao L., Matheussen B.V.","Using the tsetlin machine to learn human-interpretable rules for high-accuracy text categorization with medical applications","10.1109/ACCESS.2019.2935416","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077769029&doi=10.1109%2fACCESS.2019.2935416&partnerID=40&md5=bf33a8d997033d71433a8c8c629d6918","Medical applications challenge today's text categorization techniques by demanding both high accuracy and ease-of-interpretation. Although deep learning has provided a leap forward in regard to accuracy, this leap comes at the sacrifice of interpretability. In this paper, we introduce a text categorization approach that leverages the recently introduced Tsetlin Machine to address this accuracy-interpretability challenge. Briefly, we represent the terms of a text as propositional variables. From these variables, we capture categories using simple propositional formulae, such as: IF ""rash"" AND ""reaction"" AND ""penicillin"" THEN Allergy. The Tsetlin Machine learns these formulae from labeled text, utilizing conjunctive clauses to represent the particular facets of each category. Therefore, also the absence of terms (negated features) can be used for categorization purposes. Our empirical comparisons with Naïve Bayes classifiers, decision trees, linear support vector machines (SVMs), random forest, long short-term memory (LSTM) neural networks, and other techniques, are quite conclusive. Using relatively simple propositional formulae, the accuracy of the Tsetlin Machine either outperforms or performs approximately on par with the best evaluated methods on both the 20 Newsgroups and IMDb datasets, as well as on a clinical dataset containing authentic electronic health records (EHRs). On average, the Tsetlin Machine delivers the best recall and precision scores across the datasets. The main merit of the proposed approach is thus its capacity for producing human-interpretable rules, while at the same time achieving acceptable accuracy. We believe that our novel approach can have a significant impact on a wide range of text analysis applications, providing a promising starting point for deeper natural language understanding with the Tsetlin Machine. © 2020 Association for Computing Machinery. All rights reserved.","Classification algorithms; Health informatics; Machine learning; Supervised learning; Text categorization; Tsetlin machine","Decision trees; Deep learning; Long short-term memory; Medical applications; Text processing; Electronic health record (EHRs); Empirical - comparisons; Interpretable rules; Linear Support Vector Machines; Natural language understanding; Propositional variables; Recall and precision; Text categorization; Support vector machines"
"Berger A.M., Berger C.R.","Data mining as a tool for research and knowledge development in nursing","10.1097/00024665-200405000-00006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-16644377260&doi=10.1097%2f00024665-200405000-00006&partnerID=40&md5=fc0cc8819dc4f87fb5e42472d233811d","The ability to collect and store data has grown at a dramatic rate in all disciplines over the past two decades. Healthcare has been no exception. The shift toward evidence-based practice and outcomes research presents significant opportunities and challenges to extract meaningful information from massive amounts of clinical data to transform it into the best available knowledge to guide nursing practice. Data mining, a step in the process of Knowledge Discovery in Databases, is a method of unearthing information from large data sets. Built upon statistical analysis, artificial intelligence, and machine learning technologies, data mining can analyze massive amounts of data and provide useful and interesting information about patterns and relationships that exist within the data that might otherwise be missed. As domain experts, nurse researchers are in ideal positions to use this proven technology to transform the information that is available in existing data repositories into useful and understandable knowledge to guide nursing practice and for active interdisciplinary collaboration and research. © 2004 Lippincott Williams & Wilkins, Inc.","Data mining; Knowledge discovery; Nursing; Nursing research","algorithm; artificial intelligence; artificial neural network; automated pattern recognition; cluster analysis; data base; decision tree; human; information retrieval; knowledge; methodology; nursing informatics; nursing research; organization and management; regression analysis; review; statistical analysis; statistical model; utilization review; Algorithms; Artificial Intelligence; Cluster Analysis; Data Interpretation, Statistical; Databases; Decision Trees; Humans; Information Storage and Retrieval; Knowledge; Models, Statistical; Neural Networks (Computer); Nursing Informatics; Nursing Research; Pattern Recognition, Automated; Regression Analysis"
"Berger F., Müller W.","Back to Basics: Explainable AI for Adaptive Serious Games","10.1007/978-3-030-88272-3_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117959825&doi=10.1007%2f978-3-030-88272-3_6&partnerID=40&md5=13c8f5cad68ba9c583cddfe927088856","The spread of AI technologies has given rise to concerns regarding the fairness, appropriateness, and neutrality of machine-made decisions. Explainable AI aims at countering this by enforcing simple or transparent solutions. Adaptive serious games themselves have long been a playground for various approaches to achieve machine-regulated adjustments for increased learner success or player satisfaction. Results have been commendable, but can not readily be complemented with explainability. Analysing 18 models of adaptivity in game-based learning and related domains, we propose a simple and explainable adaptivity model for serious games. It is designed as a rule-based, short-term decision making algorithm, proposes game progress as a reliable learning progress indicator, and adapts to both under- and over-performing learners. We present the implementation of the model in two distinct serious games, and the result of an evaluation in a controlled trial (n= 80 ), demonstrating its suitability for adaptive serious games. In conclusion, we underline the importance of designing serious games both as teaching and playing experiences, and of an iterative design process to assure the quality of the final product. © 2021, Springer Nature Switzerland AG.","Adaptivity; Explainable AI; Game-based learning","Artificial intelligence; Decision making; Iterative methods; Learning systems; Product design; Adaptive serious games; Adaptivity; AI Technologies; Decision-making algorithms; Explainable AI; Game-based Learning; Player satisfactions; Rule based; Simple++; Transparent solutions; Serious games"
"Bergh L.G., Yianatos J.B., León A.","Multivariate projection methods applied to flotation columns","10.1016/j.mineng.2004.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-17844408424&doi=10.1016%2fj.mineng.2004.12.008&partnerID=40&md5=408bac01c410e25be6023e86ad97b451","On line monitoring and diagnosis systems of process operating performance are becoming important part of industrial programs, leading to improve process operation and therefore product quality over time. In processes, such as flotation, a large number of input variables, highly correlated, are presented. These characteristics usually pose more difficulties in modelling the process for monitoring and diagnosis purposes. Multivariate statistical projection methods have been proposed to effectively deal with these situations. The application of these ideas to a column flotation process is discussed here. In this work, the detection of measurement problems in relevant variables, and the identification of the set of variables responsible for driving the process outside its normal operating region, are demonstrated. Furthermore, the use of these techniques gave considerable information for the correction of the operating problem. © 2005 Elsevier Ltd. All rights reserved.","Artificial intelligence; Column flotation; Modelling; Process control; Simulation","Artificial intelligence; Computer simulation; Distributed computer systems; Mathematical models; Online systems; Principal component analysis; Process control; Statistical methods; Column flotation; Gas velocity; Nonlinear models; Product quality; Flotation"
"Bergmann L.T.","Ethical Issues in Automated Driving—Opportunities, Dangers, and Obligations","10.1007/978-3-030-77726-5_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122349388&doi=10.1007%2f978-3-030-77726-5_5&partnerID=40&md5=77b8808582bba66458a6b6e289f37c40","Automated vehicles (AVs) not only face questions of technical feasibility but also of moral desirability. Traffic is one of the major sources of death and injury in modern society–human error causing about ninety percent of traffic accidents. Prima facie this yields a strong ethical obligation to further the development and adoption of AVs. However, moral desirability cannot be analyzed solely in terms of increased safety. Broad societal adoption of automated vehicles will entail many ethical issues. One cluster of ethical issues concerns the role of non-human entities occupying positions that usually are reserved for proper moral agents: how should AVs make decisions? And who could be held responsible for their choices? Another cluster of issues concerns the impact widespread adoption of AVs could have on society: which social groups would be negatively affected by the widespread adoption of AVs? Is society becoming too reliant on technology and which potential for abuse is entailed by this dependence? Should citizens remain free to drive vehicles themselves, though they make traffic less safe for everyone? In this paper, I advocate a cautionary position, mindful of the inevitability of technological progress and its great potential, attempting to highlight the obligation to steer this development towards an ethically acceptable trajectory. © 2022, Springer Nature Switzerland AG.","Artificial intelligence; Automated driving; Autonomous driving; AV; Ethical issues; Ethics; Technology",
"Bergmann P., Batzner K., Fauser M., Sattlegger D., Steger C.","Beyond Dents and Scratches: Logical Constraints in Unsupervised Anomaly Detection and Localization","10.1007/s11263-022-01578-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124991760&doi=10.1007%2fs11263-022-01578-9&partnerID=40&md5=b05c922c5849257793db8a0d5578d808","The unsupervised detection and localization of anomalies in natural images is an intriguing and challenging problem. Anomalies manifest themselves in very different ways and an ideal benchmark dataset for this task should contain representative examples for all of them. We find that existing datasets are biased towards local structural anomalies such as scratches, dents, or contaminations. In particular, they lack anomalies in the form of violations of logical constraints, e.g., permissible objects occurring in invalid locations. We contribute a new dataset based on industrial inspection scenarios that evenly covers both types of anomalies. We provide pixel-precise ground truth data for each anomalous region and define a generalized evaluation metric that addresses localization ambiguities that can arise for logical anomalies. Furthermore, we propose a novel algorithm that improves over the state of the art in the joint detection of structural and logical anomalies. It consists of a local and a global network branch. The first one inspects confined regions independent of their spatial locations in the input image and is primarily responsible for the detection of entirely new local structures. The second one learns a globally consistent representation of the training data through a bottleneck that enables the detection of violations of long-range dependencies, a key characteristic of many logical anomalies. We perform extensive evaluations on our new dataset to corroborate our claims. © 2022, The Author(s).","Anomaly detection; Datasets; Defect segmentation; Novelty detection; Performance metrics; Unsupervised learning","Machine learning; Anomaly detection; Anomaly localizations; Dataset; Defect segmentation; Detection and localization; Logical constraints; Novelty detection; Performance metrices; Unsupervised anomaly detection; Unsupervised detection; Anomaly detection"
"Bergsma M., Spronck P.","Adaptive spatial reasoning for turn-based strategy games",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883125094&partnerID=40&md5=17a9319e391fce67e2442d5a496ed7ab","The quality of AI opponents often leaves a lot to be desired, which poses many attractive challenges for AI researchers. In this respect, Turn-based Strategy (TBS) games are of particular interest. These games are focussed on high-level decision making, rather than low-level behavioural actions. For efficiently designing a TBS AI, in this paper we propose a game AI architecture named ADAPTA (Allocation and Decomposition Architecture for Performing Tactical AI). It is based on task decomposition using asset allocation, and promotes the use of machine learning techniques. In our research we concentrated on one of the subtasks for the ADAPTA architecture, namely the Extermination module, which is responsible for combat behaviour. Our experiments show that ADAPTA can successfully learn to outperform static opponents. It is also capable of generating AIs which defeat a variety of static tactics simultaneously. Copyright © 2008, Association for the Advancement of Artificial Intelligence.",,"Asset allocation; Game AI; Machine learning techniques; Spatial reasoning; Strategy games; Subtasks; Task decomposition; Artificial intelligence; Human computer interaction; Investments; Learning systems; Architecture"
"Bergsma M., Spronck P.","Adaptive Intelligence for Turn-based Strategy Games",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655177000&partnerID=40&md5=84837c9b973626da30a514c3fbaa3251","Computer games are an increasingly popular form of entertainment. Typically, the quality of AI opponents in computer games leaves a lot to be desired, which poses many attractive challenges for AI researchers. In this respect, Turn-based Strategy (TBS) games are of particular interest. These games are focussed on high-level decision making, rather than low-level behavioural actions. Moreover, they allow the players sufficient time to consider their moves. For efficiently designing a TBS AI, in this paper we propose a game AI architecture named ADAPTA (Allocation and Decomposition Architecture for Performing Tactical AI). It is based on task decomposition using asset allocation, and promotes the use of machine learning techniques. In our research we concentrated on one of the subtasks for the ADAPTA architecture, namely the Extermination module, which is responsible for combat behaviour. Our experiments show that ADAPTA can successfully learn to outperform static opponents. It is also capable of generating AIs which defeat a variety of static tactics simultaneously.",,"Asset allocation; Computer game; Machine learning techniques; Subtasks; Task decomposition; Artificial intelligence; Computer software; Human computer interaction; Investments; Learning systems; Architecture"
"Beri S., Kaur K.","Hybrid framework for DBSCAN algorithm using fuzzy logic","10.1109/ABLAZE.2015.7155024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941242000&doi=10.1109%2fABLAZE.2015.7155024&partnerID=40&md5=9e36c05bfd98651618fdecedf7bc3fb6","Data mining process is to obtain information from a data set and then convert it into an understandable and meaningful information for further use. DBSCAN, a density based clustering algorithm, identifies clusters of varying shape and outliers. DBSCAN is based on bivalent logic. Therefore it can only detect objects as completely belonging to a particular cluster or not wholly belonging to it. In this paper, a framework of methodology of DBSCAN algorithm with the integration of fuzzy logic is proposed. The extent to which an object belongs to a particular cluster will be determined using membership values. The improved version of DBSCAN algorithm will be the hybridization of DBSCAN algorithm with fuzzy if-then rules. © 2015 IEEE.","bivalent logic; clustering; DBSCAN","Computational methods; Computer circuits; Data mining; Fuzzy logic; Knowledge management; Object detection; Bivalent logic; clustering; Data mining process; DBSCAN; DBSCAN algorithm; Density-based clustering algorithms; Fuzzy if-then rules; Membership values; Clustering algorithms"
"Beriwal M., Cochran B.","Protecting communities from chemical warfare agents","10.1109/THS.2013.6699041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893226359&doi=10.1109%2fTHS.2013.6699041&partnerID=40&md5=755bac24c59b1e5703504f37b1cf4c18","What do you do if there is an accident involving Sarin nerve gas and you are part of the team responsible for protecting thousands of people in the path of this deadly chemical plume? Emergency operations personnel at chemical weapons stockpile sites within the continental United States know exactly what to do. They rely on WebPuff, a state-of-the-art decision support system sponsored by the U.S. Army Chemical Materials Activity (CMA)1 and developed by IEM, a security consulting firm based in North Carolina's Research Triangle Park. WebPuff is used by military and civilian jurisdictions within the Chemical Stockpile Emergency Preparedness Program (CSEPP), which is jointly managed by the U.S. Army and the Federal Emergency Management Agency (FEMA). WebPuff provides users at CSEPP sites with a suite of planning and response tools that are integrated with a unique chemical dispersion model that provides an advanced level of science on which decisions about public protection can be based. Incorporating real-time and forecast weather, topography data, and current toxicity standards, WebPuff's dispersion model provides the most realistic plume prediction in less than two minutes - enabling military installations to meet a five-minute criterion for notifying civilian jurisdictions of an impending threat. WebPuff's chemical dispersion model has been independently tested and certified by scientists at Dugway Proving Ground [1, 2]. WebPuff incorporates a shared framework for risk management among independently managed military and civil jurisdictions at each site as well as a common understanding of how to plan for and, if necessary, respond to the threat that faces communities around chemical weapons stockpile sites every day. In developing the system, CMA and IEM gained consensus from both military and civilian users on operational requirements, business rules, and detailed designs for reports and computer displays that are foundational to the system. WebPuff provides users with information that is organized - primarily through visual means - around a common understanding of the threat and a common concept of operations. Moreover, it is a cost-effective solution to emergency preparedness and response because it is built using 100% open-source technology - the customer pays no third-party license fees. The system meets Department of Defense security and interoperability requirements - military bases using the system can communicate securely and effectively with civilian emergency management organizations. As a result, WebPuff is Defense Information Assurance Certification and Accreditation Process (DIACAP) certified with a current Authority to Operate (ATO) on Army networks. To ensure interoperability with civilian jurisdictions, the system uses the Emergency Data eXchange Language (EDXL) Common Alerting Protocol (CAP) developed by the Organization for the Advancement of Structured Information Standards (OASIS). Nearly ten years after its fielding, WebPuff continues to evolve to meet emerging standards and operational concepts and CSEPP communities are still using the system. Its ability to provide trusted results quickly and to truly facilitate cooperation and collaboration among diverse organizations during disaster response has been, and continues to be, the key to its success. While WebPuff was originally designed to support preparedness for chemical weapons accidents, it represents a unique framework, models, and components that can be customized and extended for use with other hazards and other concepts of operations. © 2013 IEEE.","data visualizaiton; decision making; information management; information sharing; integrated decision support; preparedness; risk assessment; Situaitonal awareness; situational underdtanding","data visualizaiton; Information sharing; Integrated decision; preparedness; Situaitonal awareness; situational underdtanding; Accidents; Accreditation; Artificial intelligence; Chemical warfare; Civil defense; Decision making; Decision support systems; Disasters; Dispersions; Electronic data interchange; Information management; Management science; Military bases; National security; Risk assessment; Risk management; Security systems; Weather forecasting; Interoperability"
"Berka Petr, Slama Marek, Beran Hynek","Knowledge base refinement using machine learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029190669&partnerID=40&md5=35a8b9a5d552fa151fe677aabf02d9a9","Compositional knowledge base in our sense consists of a set of IF THEN rules with uncertainties expressed as weights. During consultation for a particular case, all applicable rules are combined and weights of goals (final diagnoses or recommendations) are computed. The main problem when eliciting such knowledge base from an expert is the question of `correct' weights of rules. When using neural nets for knowledge acquisition, we will obtain a hardly interpretable list of weights of connections between neurons. Our idea is, similarly to Shavlik's approach, to combine the structure of knowledge obtained from expert with weights learned from data. We choose the topology and initial settings of the NN (no. of neurons, prohibited links) according to the rules obtained from expert. Then, after learning such network, we try to interpret the weights of connections as uncertainty of the original rules. The paper shows some experimental results of this approach on a knowledge base for credit risk assessment.",,"Electric network topology; Expert systems; Knowledge acquisition; Neural networks; Risk assessment; Connections weights; Knowledge base refinement; Learning systems"
"Berkelmans G., van der Mei R., Bhulai S., Gilissen R.","Identifying socio-demographic risk factors for suicide using data on an individual level","10.1186/s12889-021-11743-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115191280&doi=10.1186%2fs12889-021-11743-3&partnerID=40&md5=b345d3ed329ca89ffed7d128061b0593","Background: Suicide is a complex issue. Due to the relative rarity of the event, studies into risk factors are regularly limited by sample size or biased samples. The aims of the study were to find risk factors for suicide that are robust to intercorrelation, and which were based on a large and unbiased sample. Methods: Using a training set of 5854 suicides and 596,416 control cases, we fit a logistic regression model and then evaluate the performance on a test set of 1425 suicides and 594,893 control cases. The data used was micro-data of Statistics Netherlands (CBS) with data on each inhabitant of the Netherlands. Results: Taking the effect of possible correlating risk factors into account, those with a higher risk for suicide are men, middle-aged people, people with low income, those living alone, the unemployed, and those with mental or physical health problems. People with a lower risk are the highly educated, those with a non-western immigration background, and those living with a partner. Conclusion: We confirmed previously known risk factors such as male gender, middle-age, and low income and found that they are risk factors that are robust to intercorrelation. We found that debt and urbanicity were mostly insignificant and found that the regional differences found in raw frequencies are mostly explained away after correction of correlating risk factors, indicating that these differences were primarily caused due to the differences in the demographic makeup of the regions. We found an AUC of 0.77, which is high for a model predicting suicide death and comparable to the performance of deep learning models but with the benefit of remaining explainable. © 2021, The Author(s).","Big data; Machine learning; Risk factors; Suicide","human; male; middle aged; migration; poverty; risk factor; statistical model; suicide; Emigration and Immigration; Humans; Logistic Models; Male; Middle Aged; Poverty; Risk Factors; Suicide"
"Berlanga F., Del Jesus M.J., González P., Herrera F., Mesonero M.","Multiobjective evolutionary induction of subgroup discovery fuzzy rules: A case study in marketing","10.1007/11790853_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746414789&doi=10.1007%2f11790853_27&partnerID=40&md5=3006cc931727964548f06b9f45f83274","This paper presents a multiobjective genetic algorithm which obtains fuzzy rules for subgroup discovery in disjunctive normal form. This kind of fuzzy rules lets us represent knowledge about patterns of interest in an explanatory and understandable form which can be used by the expert. The evolutionary algorithm follows a multiobjective approach in order to optimize in a suitable way the different quality measures used in this kind of problems. Experimental evaluation of the algorithm, applying it to a market problem studied in the University of Mondragón (Spain), shows the validity of the proposal. The application of the proposal to this problem allows us to obtain novel and valuable knowledge for the experts. © Springer-Verlag Berlin Heidelberg 2006.","Data mining; Descriptive induction; Genetic fuzzy systems; Multiobjective evolutionary algorithms; Subgroup discovery","Data mining; Evolutionary algorithms; Formal logic; Genetic algorithms; Knowledge representation; Optimization; Problem solving; Set theory; Descriptive induction; Genetic fuzzy systems; Multiobjective evolutionary algorithms; Subgroup discoveries; Fuzzy sets"
"Berlow N.E.","Probabilistic Boolean Modeling of Pre-clinical Tumor Models for Biomarker Identification in Cancer Drug Development","10.1002/cpz1.269","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118169313&doi=10.1002%2fcpz1.269&partnerID=40&md5=cf64b6c3243d4cc0ebb2e3c6a56c320b","As high-throughput sequencing experiments become more widely used in pre-clinical and clinical settings, pharmacogenetic and pharmacogenomic biomarker development plays an increasingly important role in oncology drug development pipelines and programs. Consequently, computer-based learning approaches have entered into use at multiple stages in pre-clinical and clinical pipelines. However, few approaches are available to identify interpretable and implementable biomarkers of response early in the drug development process when only small pre-clinical data packages are available. To address the need for early-stage biomarker development using pre-clinical tumor models, we have adapted the previously published Probabilistic Target Inhibitor Map (PTIM) platform to the challenge of biomarker hypothesis development, and denoted this approach the Probabilistic Target Map-Biomarker (PTM-Biomarker). In this article, we detail the history and design philosophy of PTM-Biomarker, and present two case studies using the biomarker discovery tool to illustrate its utility in guiding cancer drug development. © 2021 Wiley Periodicals LLC. © 2021 Wiley Periodicals LLC","bioinformatics; biomarker development; pre-clinical studies; probabilistic Boolean modeling","antineoplastic agent; antineoplastic agent; biological marker; Article; artificial intelligence; bioinformatics; drug development; ex vivo study; human; philosophy; probability; tumor model; validation process; drug development; neoplasm; statistical model; Antineoplastic Agents; Biomarkers; Drug Development; Humans; Models, Statistical; Neoplasms"
"Bermejo P., Gámez J.A., Puerta J.M.","A GRASP algorithm for fast hybrid (filter-wrapper) feature subset selection in high-dimensional datasets","10.1016/j.patrec.2010.12.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751648113&doi=10.1016%2fj.patrec.2010.12.016&partnerID=40&md5=01f3b3ffa17356872b7b875116084ff0","Feature subset selection is a key problem in the data-mining classification task that helps to obtain more compact and understandable models without degrading (or even improving) their performance. In this work we focus on FSS in high-dimensional datasets, that is, with a very large number of predictive attributes. In this case, standard sophisticated wrapper algorithms cannot be applied because of their complexity, and computationally lighter filter-wrapper algorithms have recently been proposed. In this work we propose a stochastic algorithm based on the GRASP meta-heuristic, with the main goal of speeding up the feature subset selection process, basically by reducing the number of wrapper evaluations to carry out. GRASP is a multi-start constructive method which constructs a solution in its first stage, and then runs an improving stage over that solution. Several instances of the proposed GRASP method are experimentally tested and compared with state-of-the-art algorithms over 12 high-dimensional datasets. The statistical analysis of the results shows that our proposal is comparable in accuracy and cardinality of the selected subset to previous algorithms, but requires significantly fewer evaluations.© 2010 Elsevier B.V. All rights reserved.","Classification; Feature selection; Filter; GRASP; High-dimensional datasets; Wrapper","Classification; Feature selection; Filter; GRASP; High-dimensional datasets; Wrapper; Algorithms; Classification (of information); Filtration; Set theory; Feature extraction"
"Bermejo-Das-Neves C., Nguyen H.-N., Poch O., Thompson J.D.","A comprehensive study of small non-frameshift insertions/deletions in proteins and prediction of their phenotypic effects by a machine learning method (KD4i)","10.1186/1471-2105-15-111","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899967710&doi=10.1186%2f1471-2105-15-111&partnerID=40&md5=ec9b1cbaba981365f0327dad1be20345","Background: Small insertion and deletion polymorphisms (Indels) are the second most common mutations in the human genome, after Single Nucleotide Polymorphisms (SNPs). Recent studies have shown that they have significant influence on genetic variation by altering human traits and can cause multiple human diseases. In particular, many Indels that occur in protein coding regions are known to impact the structure or function of the protein. A major challenge is to predict the effects of these Indels and to distinguish between deleterious and neutral variants. When an Indel occurs within a coding region, it can be either frameshifting (FS) or non-frameshifting (NFS). FS-Indels either modify the complete C-terminal region of the protein or result in premature termination of translation. NFS-Indels insert/delete multiples of three nucleotides leading to the insertion/deletion of one or more amino acids.Results: In order to study the relationships between NFS-Indels and Mendelian diseases, we characterized NFS-Indels according to numerous structural, functional and evolutionary parameters. We then used these parameters to identify specific characteristics of disease-causing and neutral NFS-Indels. Finally, we developed a new machine learning approach, KD4i, that can be used to predict the phenotypic effects of NFS-Indels.Conclusions: We demonstrate in a large-scale evaluation that the accuracy of KD4i is comparable to existing state-of-the-art methods. However, a major advantage of our approach is that we also provide the reasons for the predictions, in the form of a set of rules. The rules are interpretable by non-expert humans and they thus represent new knowledge about the relationships between the genotype and phenotypes of NFS-Indels and the causative molecular perturbations that result in the disease. © 2014 Bermejo-Das-Neves et al.; licensee BioMed Central Ltd.","Genotype-phenotype relationship; Indeuctive logic programming; Insertion/deletion; Machine learning","Forecasting; Genes; Logic programming; Nucleotides; Proteins; Evolutionary parameters; Genotype-phenotype relationship; Insertion/deletion; Machine learning approaches; Machine learning methods; Protein coding regions; Single nucleotide polymorphisms; State-of-the-art methods; Learning systems; kinesin; protein; article; artificial intelligence; genetics; human; indel mutation; phenotype; Artificial Intelligence; Humans; INDEL Mutation; Kinesin; Phenotype; Proteins"
"Bernabé-Moreno J., Wildberger K.","A fuzzy linguistic supported framework to increase Artificial Intelligence intelligibility for subject matter experts","10.1016/j.procs.2019.12.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081051191&doi=10.1016%2fj.procs.2019.12.061&partnerID=40&md5=6b48b57eb9d1dc936d8281615c31fe6b","The application of artificial intelligence (AI) techniques in the decision making processes is more widespread in the industry than ever before. Yet, one of the most critical show-stoppers is the communication gap between the machine learning (ML) models and the experts community. On one hand, the output of ML is often not intelligible for experts, in spite of the latest advances in explainable AI. On the other hand, the expert knowledge, rarely completely present in the available data, but rather in the heads of the experts, needs to be connected to the data-driven insights created by the ML model. In this paper we first identify the most critical situations with a manifest intelligibility gap and then propose a framework supported by fuzzy linguistic modelling techniques to close this gap. In addition, we present its integration into the end-to-end decision making flow, from data gathering to the execution and evaluation and we show the output of our approach with practical examples. © 2020 The Authors. Published by Elsevier B.V.","decision making; expert knowledge modelling; fuzzy linguistic modeling; intelligible AI","Artificial intelligence; Linguistics; Communication gaps; Data driven; Data gathering; Decision making process; Expert knowledge; Fuzzy linguistic modeling; Fuzzy linguistics; Subject matter experts; Decision making"
"Bernacki D.T., Bryce S.M., Bemis J.C., Dertinger S.D.","Aneugen Molecular Mechanism Assay: Proof-of-Concept with 27 Reference Chemicals","10.1093/toxsci/kfz123","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071064760&doi=10.1093%2ftoxsci%2fkfz123&partnerID=40&md5=d6e5623becb8f6b9af0b368f03adc3f0","A tiered bioassay and data analysis scheme is described for elucidating the most common molecular targets responsible for chemical-induced in vitro aneugenicity: Tubulin destabilization, tubulin stabilization, and inhibition of mitotic kinase(s). To evaluate this strategy, TK6 cells were first exposed to each of 27 presumed aneugens over a range of concentrations. After 4 and 24 h of treatment, γH2AX, p53, phospho-histone H3 (p-H3), and polyploidization biomarkers were evaluated using the MultiFlow DNA Damage Assay Kit. The assay identified 27 of 27 chemicals as genotoxic, with 25 exhibiting aneugenic signatures, 1 aneugenic and clastogenic, and 1 clastogenic. Subsequently, a newly described follow-up assay was employed to investigate the aneugenic agents' molecular targets. For these experiments, TK6 cells were exposed to each of 26 chemicals in the presence of 488 Taxol. After 4 h, cells were lysed and the liberated nuclei and mitotic chromosomes were stained with a nucleic acid dye and labeled with fluorescent antibodies against p-H3 and Ki-67. Flow cytometric analyses revealed that alterations to 488 Taxol-associated fluorescence were only observed with tubulin binders-increases in the case of tubulin stabilizers, decreases with destabilizers. Mitotic kinase inhibitors with known Aurora kinase B inhibiting activity were the only aneugens that dramatically decreased the ratio of p-H3-positive to Ki-67-positive nuclei. Unsupervised hierarchical clustering based on 488 Taxol fluorescence and p-H3: Ki-67 ratios clearly distinguished compounds with these disparate molecular mechanisms. Furthermore, a classification algorithm based on an artificial neural network was found to effectively predict molecular target, as leave-one-out cross-validation resulted in 25/26 agreement with a priori expectations. These results are encouraging, as they suggest that an adequate number of training set chemicals, in conjunction with a machine learning algorithm based on 488 Taxol, p-H3, and Ki-67 responses, can reliably elucidate the most commonly encountered aneugenic molecular targets. © 2019 The Author(s). Published by Oxford University Press on behalf of the Society of Toxicology. All rights reserved.","aneugen; flow cytometry; Mitotic kinase inhibitors; spindle poisons; TK6 cells","alisertib; aneugen; aurora B kinase; barasertib; biological marker; carbendazim; colchicine; crizotinib; danusertib; diethylstilbestrol; epothilone A; flubendazole; griseofulvin; hesperadin; histone H2AX; histone H3; ixabepilone; Ki 67 antigen; mebendazole; n [4 [6 methoxy 7 (3 morpholinopropoxy) 4 quinazolinylamino]phenyl]benzamide; nocodazole; noscapine; nucleic acid; paclitaxel; pf 3814735; protein p53; rigosertib; tozasertib; tubulin; vinblastine sulfate; aneugen; histone; Ki 67 antigen; Article; artificial neural network; cell nucleus; chromosome; controlled study; DNA damage; flow cytometry; fluorescent antibody technique; follow up; genotoxicity; genotoxicity assay; human; human cell; polyploidy; proof of concept; cell culture; machine learning; metabolism; mutagen testing; procedures; Aneugens; Cells, Cultured; DNA Damage; Histones; Humans; Ki-67 Antigen; Machine Learning; Mutagenicity Tests; Neural Networks, Computer"
"Bernard E., Peyret T., Plinet M., Contie Y., Cazaudarré T., Rouquet Y., Bernier M., Pesant S., Fabre R., Anton A., Maugis-Rabusseau C., François J.M.","The DendrisCHIP® Technology as a New, Rapid and Reliable Molecular Method for the Diagnosis of Osteoarticular Infections","10.3390/diagnostics12061353","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132294735&doi=10.3390%2fdiagnostics12061353&partnerID=40&md5=93455d246d39d384a5931b657422c466","Osteoarticular infections are major disabling diseases that can occur after orthopedic implant surgery in patients. The management of these infections is very complex and painful, requiring surgical intervention in combination with long-term antibiotic treatment. Therefore, early and accurate diagnosis of the causal pathogens is essential before formulating chemotherapeutic regimens. Although culture-based microbiology remains the most common diagnosis of osteoarticular infections, its regular failure to identify the causative pathogen as well as its long-term modus operandi motivates the development of rapid, accurate, and sufficiently comprehensive bacterial species-specific diagnostics that must be easy to use by routine clinical laboratories. Based on these criteria, we reported on the feasibility of our DendrisCHIP® technology using DendrisCHIP® OA as an innovative molecular diagnostic method to diagnose pathogen bacteria implicated in osteoarticular infections. This technology is based on the principle of microarrays in which the hybridization signals between oligoprobes and complementary labeled DNA fragments from isolates queries a database of hybridization signatures corresponding to a list of pre-established bacteria implicated in osteoarticular infections by a decision algorithm based on machine learning methods. In this way, this technology combines the advantages of a PCR-based method and next-generation sequencing (NGS) while reducing the limitations and constraints of the two latter technologies. On the one hand, DendrisCHIP® OA is more comprehensive than multiplex PCR tests as it is able to detect many more germs on a single sample. On the other hand, this method is not affected by the large number of nonclinically relevant bacteria or false positives that characterize NGS, as our DendrisCHIP® OA has been designed to date to target only a subset of 20 bacteria potentially responsible for osteoarticular infections. DendrisCHIP® OA has been compared with microbial culture on more than 300 isolates and a 40% discrepancy between the two methods was found, which could be due in part but not solely to the absence or poor identification of germs detected by microbial culture. We also demonstrated the reliability of our technology in correctly identifying bacteria in isolates by showing a convergence (i.e., same bacteria identified) with NGS superior to 55% while this convergence was only 32% between NGS and microbial culture data. Finally, we showed that our technology can provide a diagnostic result in less than one day (technically, 5 h), which is comparatively faster and less labor intensive than microbial cultures and NGS. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","biochips; bone and joint infection; in vitro multiplex diagnostic; microbial cultures; next-generation sequencing",
"Bernard Jean-Pierre, Durocher Denis","LANGAGE. An expert system for diagnosis in a real-time context",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027148505&partnerID=40&md5=bad3b419ec2f65942f1f8f791d9d9cc8","The operators at Hydro-Quebec's nine regional control centres (RCCs) are responsible for the remote control of more than 370 facilities. They must react promptly as soon as an equipment failure occurs and analyse the alarm messages displayed by the real-time SCADA system in order to diagnose the fault. To alleviate this complex task, Hydro-Quebec has developed the LANGAGE expert system to perform a continuous analysis of alarm messages, automatically detect the application of the protection or restoration control and then produce a concise, real-time diagnosis identifying the origin and consequences of the fault. Thus, diagnosis allows RCC operators to complete their analysis in a shorter period of time. This paper describes the need for these diagnoses and the application of artificial intelligence (AI) techniques to meet this need. It also discusses the constraints and problems experienced with the use of AI techniques in a real-time context.",,"Alarm systems; Artificial intelligence; Electric power systems; Real time systems; Remote control; Artificial intelligence applications; Fault diagnosis; Real time control; SCADA systems; Expert systems"
"Bernardi S., Coletti G.","A rational conditional utility model in a coherent framework","10.1007/3-540-44652-4_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149003806&doi=10.1007%2f3-540-44652-4_11&partnerID=40&md5=832e1a896f88599759d0ae1b2920f4b7","We present a decision model apt to handle preference rela- Tions among conditional acts, not necessarily satisfying transitivity and sure thing principle. We give also a characterization of preference re- lations agreeing with such a model, by means of rationality conditions interpretable in terms of betting scheme. © Springer-Verlag Berlin Heidelberg 2001.",,"Computers; Coherent frameworks; Decision modeling; Preference relation; Utility model; Artificial intelligence"
"Bernardini M., Morettini M., Romeo L., Frontoni E., Burattini L.","TyG-er: An ensemble Regression Forest approach for identification of clinical factors related to insulin resistance condition using Electronic Health Records","10.1016/j.compbiomed.2019.103358","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069559080&doi=10.1016%2fj.compbiomed.2019.103358&partnerID=40&md5=5ee338b26b7854959acd2a2a7c30837f","Background: Insulin resistance is an early-stage deterioration of Type 2 diabetes. Identification and quantification of insulin resistance requires specific blood tests; however, the triglyceride-glucose (TyG) index can provide a surrogate assessment from routine Electronic Health Record (EHR) data. Since insulin resistance is a multi-factorial condition, to improve its characterisation, this study aims to discover non-trivial clinical factors in EHR data to determine where the insulin-resistance condition is encoded. Methods: We proposed a high-interpretable Machine Learning approach (i.e., ensemble Regression Forest combined with data imputation strategies), named TyG-er. We applied three different experimental procedures to test TyG-er reliability on the Italian Federation of General Practitioners dataset, named FIMMG_obs dataset, which is publicly available and reflects the clinical use-case (i.e., not all laboratory exams are prescribed on a regular basis over time). Results: Results detected non-conventional clinical factors (i.e., uricemia, leukocytes, gamma-glutamyltransferase and protein profile) and provided novel insight into the best combination of clinical factors for detecting early glucose tolerance deterioration. The robustness of these extracted clinical factors was confirmed by the high agreement (from 0.664 to 0.911 of Lin's correlation coefficient (rc)) of the TyG-er approach among different experimental procedures. Moreover, the results of the three experimental procedures outlined the predictive power of the TyG-er approach (up to a mean absolute error of 5.68% and rc=0.666,p&lt;.05). Conclusions: The TyG-er approach is able to carry information about the identification of the TyG index, strictly correlated with the insulin-resistance condition, while extracting the most relevant non-glycemic features from routine data. © 2019","Insulin resistance; Laboratory screening; Missing values; Pattern recognition; Pre-diabetes; Random forest","Decision trees; Deterioration; Glucose; Pattern recognition; Records management; Statistical tests; Correlation coefficient; Electronic health record; Experimental procedure; Gamma-glutamyltransferase; Insulin resistance; Machine learning approaches; Missing values; Random forests; Insulin; gamma glutamyltransferase; glucose; triacylglycerol; uric acid; triacylglycerol; adult; Article; electronic health record; female; glucose tolerance; human; insulin resistance; leukocyte; machine learning; major clinical study; male; middle aged; priority journal; aged; blood; electronic health record; glucose blood level; machine learning; metabolism; non insulin dependent diabetes mellitus; Aged; Blood Glucose; Diabetes Mellitus, Type 2; Electronic Health Records; Female; Humans; Insulin Resistance; Machine Learning; Male; Middle Aged; Triglycerides"
"Bernardino M., Beiko R.","Genome-scale prediction of bacterial promoters","10.1109/CIBCB49929.2021.9562938","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126481934&doi=10.1109%2fCIBCB49929.2021.9562938&partnerID=40&md5=7c89c6426bdf748d7db927c33b081a09","Proteins are responsible for many tasks including cell growth and metabolism. Transcription, the process where genes are used as templates for the production of a messenger RNA intermediate used in the synthesis of proteins, is regulated to ensure that the cell has the appropriate response according to its current needs. An essential step in transcription is the binding of a group of proteins, collectively known as RNA polymerase, to short promoter sequences upstream of the genes to be transcribed. Automated identification of promoters and nearby regulatory sequences can help to predict which genes are likely to be active under a given set of conditions. However, promoters are short, highly variable, and belong to subclasses that sometimes overlap, making their recognition a very difficult problem. Several tools have been developed to identify promoters in DNA, but methods are generally tested on small, balanced subsets of genomic sequence, and the results may not reflect their expected performance on genomes with millions of DNA base pairs in length where only ∼1% of sequence is expected to correspond to promoters. Here we introduce Expositor, a neural-network-based method that uses different types of DNA encodings and tunable sensitivity and specificity parameters. Although the performance of Expositor on balanced datasets was comparable to that of other approaches, at the genome scale our approach finds the highest number of promoters (70% against 46%) with the smallest number of false positives. We also examined the accuracy of Expositor in distinguishing different classes of promoters, and found that misclassification between classes was consistent with the biological similarity between promoters. Expositor source code and pretrained model, and the datasets used for training and testing can be accessed at https://github.com/beiko-lab/Expositor. © 2021 IEEE.","Chromosome; DNA; Machine learning; Promoter sequence","Biosynthesis; Cell proliferation; Large dataset; Machine learning; Proteins; RNA; Transcription; 'current; Automated identification; Bacterial promoters; Condition; Messenger RNA; Performance; Promoter sequences; Regulatory sequences; RNA polymerase; Scale prediction; DNA"
"Bernard-Michel C., Douté S., Fauvel M., Gardes L., Girard S.","Machine learning techniques for the inversion of planetary hyperspectral images","10.1109/WHISPERS.2009.5289010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72049089058&doi=10.1109%2fWHISPERS.2009.5289010&partnerID=40&md5=3105d911a9679273f65c8c7ad71e4510","In this paper, the physical analysis of planetary hyperspectral images is addressed. To deal with high dimensional spaces (image cubes present 256 bands), two methods are proposed. The first method is the support vectors machines regression (SVM-R) which applies the structural risk minimization to perform a non-linear regression. Several kernels are investigated in this work. The second method is the Gaussian regularized sliced inverse regression (GRSIR). It is a two step strategy; the data are map onto a lower dimensional vector space where the regression is performed. Experimental results on simulated data sets have showed that the SVM-R is the most accurate method. However, when dealing with real data sets, the GRSIR gives the most interpretable results. © 2009 IEEE.","Gaussian regularized sliced inversion regression; Hyperspectral images; Mars surface; SVM","Dimensional vectors; Gaussians; High dimensional spaces; Hyper-spectral images; Image cube; Inverse regression; Machine learning techniques; Non-linear regression; Physical analysis; Real data sets; Simulated datasets; Structural risk minimization; Support vectors machine; Independent component analysis; Learning algorithms; Remote sensing; Signal processing; Space optics; Support vector machines; Regression analysis"
"Bernardo L.S., Quezada A., Munoz R., Maia F.M., Pereira C.R., Wu W., de Albuquerque V.H.C.","Handwritten pattern recognition for early Parkinson's disease diagnosis","10.1016/j.patrec.2019.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064211149&doi=10.1016%2fj.patrec.2019.04.003&partnerID=40&md5=d1cb7167b3a5f96097b9fa463756fd68","Parkinson's disease is a neurodegenerative disorder that affects around 10 million people in the world and is slightly more prevalent in males. It is characterized by the loss of neurons in a region of the brain known as substantia nigra. The neurons of this region are responsible for synthesizing the neurotransmitter dopamine, and a decrease in the production of this substance may cause motor symptoms, a characteristic of the disease. To obtain a definitive diagnosis, the patient's medical history is analyzed and the subject submitted to a series of clinical exams. One of these exams that can take place in the clinical environment comprises asking the patient to create a series of specific drawings. Our work is based on asking the patients to draw using a software developed for this specific purpose. The drawings will then be passed through a series of image methods to reduce noises and extract the characteristics of 11 metrics of each drawing; finally, these 11 metrics will be stored. Machine learning techniques such as Optimum-Path Forest, Support Vector Machine remove, and Naive Bayes use the dataset to search and learn of the characteristics for the process of classifying individuals distributed into two classes: sick and healthy. © 2019 Elsevier B.V.","image processing; machine learning; Parkinson's disease","Amines; Character recognition; Classification (of information); Diagnosis; Image processing; Learning systems; Machine learning; Neurophysiology; Clinical environments; Image method; Machine learning techniques; Medical history; Neurodegenerative disorders; Optimum-path forests; Parkinson's disease; Substantia nigra; Neurodegenerative diseases"
"Bernardy J.-P., Lappin S.","Assessing the Unitary RNN as an End-to-End Compositional Model of Syntax","10.4204/EPTCS.366.4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137150099&doi=10.4204%2fEPTCS.366.4&partnerID=40&md5=5b491f89eaabf0d5b7e0b384d1e6ccf3","We show that both an LSTM and a unitary-evolution recurrent neural network (URN) can achieve encouraging accuracy on two types of syntactic patterns: context-free long distance agreement, and mildly context-sensitive cross serial dependencies. This work extends recent experiments on deeply nested context-free long distance dependencies, with similar results. URNs differ from LSTMs in that they avoid non-linear activation functions, and they apply matrix multiplication to word embeddings encoded as unitary matrices. This permits them to retain all information in the processing of an input string over arbitrary distances. It also causes them to satisfy strict compositionality. URNs constitute a significant advance in the search for explainable models in deep learning applied to NLP. © Jean-Philippe Bernardy and Shalom Lappin.",,"Long short-term memory; Matrix algebra; Compositional models; Context-free; Context-sensitive; Embeddings; End to end; Long-distance dependencies; MAtrix multiplication; Nonlinear activation functions; Syntactic patterns; Unitary matrix; Syntactics"
"Bernas M., Orczyk T., Musialik J., Hartleb M., Błońska-Fajfrowska B.","Justified granulation aided noninvasive liver fibrosis classification system","10.1186/s12911-015-0181-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938804330&doi=10.1186%2fs12911-015-0181-3&partnerID=40&md5=7bdd9e2908706bb50c43b49933bd9006","Abstract Background: According to the World Health Organization 130-150 million (according to WHO) of people globally are chronically infected with hepatitis C virus. The virus is responsible for chronic hepatitis that ultimately may cause liver cirrhosis and death. The disease is progressive, however antiviral treatment may slow down or stop its development. Therefore, it is important to estimate the severity of liver fibrosis for diagnostic, therapeutic and prognostic purposes. Liver biopsy provides a high accuracy diagnosis, however it is painful and invasive procedure. Recently, we witness an outburst of non-invasive tests (biological and physical ones) aiming to define severity of liver fibrosis, but commonly used FibroTest®, according to an independent research, in some cases may have accuracy lower than 50 %. In this paper a data mining and classification technique is proposed to determine the stage of liver fibrosis using easily accessible laboratory data. Methods: Research was carried out on archival records of routine laboratory blood tests (morphology, coagulation, biochemistry, protein electrophoresis) and histopathology records of liver biopsy as a reference value. As a result, the granular model was proposed, that contains a series of intervals representing influence of separate blood attributes on liver fibrosis stage. The model determines final diagnosis for a patient using aggregation method and voting procedure. The proposed solution is robust to missing or corrupted data. Results: The results were obtained on data from 290 patients with hepatitis C virus collected over 6 years. The model has been validated using training and test data. The overall accuracy of the solution is equal to 67.9 %. The intermediate liver fibrosis stages are hard to distinguish, due to effectiveness of biopsy itself. Additionally, the method was verified against dataset obtained from 365 patients with liver disease of various etiologies. The model proved to be robust to new data. What is worth mentioning, the error rate in misclassification of the first stage and the last stage is below 6.5 % for all analyzed datasets. Conclusions: The proposed system supports the physician and defines the stage of liver fibrosis in chronic hepatitis C. The biggest advantage of the solution is a human-centric approach using intervals, which can be verified by a specialist, before giving the final decision. Moreover, it is robust to missing data. The system can be used as a powerful support tool for diagnosis in real treatment. © 2015 Bernas et al.","Classification; Granular computing; Liver fibrosis; Medical support systems","adult; aged; classification; computer assisted diagnosis; data mining; human; Internet; liver cirrhosis; medical informatics; middle aged; procedures; severity of illness index; Adult; Aged; Data Mining; Diagnosis, Computer-Assisted; Humans; Internet; Liver Cirrhosis; Medical Informatics Applications; Middle Aged; Severity of Illness Index"
"Berndsen J., Smyth B., Lawlor A.","Pace my race: Recommendations for marathon running","10.1145/3298689.3346991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073358556&doi=10.1145%2f3298689.3346991&partnerID=40&md5=995581fce08d71010938c2c73b51275c","We propose marathon running as a novel domain for recommender systems and machine learning. Using high-resolution marathon performance data from multiple marathon races (n = 7931), we build in-race recommendations for runners. We show that we can outperform the existing techniques which are currently employed for in-race fnish-time prediction, and we demonstrate how such predictions may be used to make real time recommendations to runners. The recommendations are made at critical points in the race to provide personalised guidance so the runner can adjust their race strategy. Through the association of model features and the expert domain knowledge of marathon runners we generate explainable, adaptable pacing recommendations which can guide runners to their best possible fnish time and help them avoid the potentially catastrophic efects of hitting the wall. © 2019 Association for Computing Machinery.","Marathon pacing; Recommender systems; Running; Sports analytics","Domain knowledge; High resolution; Marathon pacing; Modeling features; Novel domain; Performance data; Running; Time predictions; Recommender systems"
"Bernhard S., Al-Zoukra K., Schütte C.","Statistical parameter estimation and signal classification in cardiovascular diagnosis","10.2495/EHR110401","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865120942&doi=10.2495%2fEHR110401&partnerID=40&md5=1f63866b6eefe29e36879005a25053e2","Medical technology has seen impressive success in the past decades, generating novel clinical data at an unexpected rate. Even though numerous physiological models have been developed, their clinical application is limited. The major reason for this lies in the difficulty of finding and interpreting the model parameters, because most problems are ill-posed and do not have unique solutions. On the one hand the reason for this lies in the information deficit of the data, which is the result of finite measurement precision and contamination by artifacts and noise and on the other hand on data mining procedures that cannot sufficiently treat the statistical nature of the data. Within this work we introduce a population based parameter estimation method that is able to reveal structural parameters that can be used for patient-specific modeling. In contrast to traditional approaches this method produces a distribution of physiologically interpretable models defined by patient-specific parameters and model states. On the basis of these models we identify disease specific classes that correspond to clinical diagnoses, which enable a probabilistic assessment of human health condition on the basis of a broad patient population. In an ongoing work this technique is used to identify arterial stenosis and aneurisms from anomalous patterns in parameter space. We think that the information-based approach will provide a useful link between mathematical models and clinical diagnoses and that it will become a constituent in medicine in near future. © 2011 WIT Press.","Bayesian signal classification; Cardiovascular system identification; Multi-channel measurement; Parameter estimation; Patient-specific diagnosis; State-space model; Statistical cardiovascular system model","Arterial stenosis; Cardiovascular diagnosis; Cardiovascular system models; Clinical application; Clinical data; Clinical diagnosis; Human health; Ill posed; Measurement precision; Medical technologies; Model parameters; Model state; Multichannel measurement; Parameter estimation method; Parameter spaces; Patient population; Patient-specific modeling; Probabilistic assessments; Signal classification; State-space models; Statistical parameters; Structural parameter; Biomedical engineering; Cardiovascular system; Diagnosis; Environmental engineering; Mathematical models; Parameter estimation; Physiological models"
"Berno M., Canil M., Chiarello N., Piazzon L., Berti F., Ferrari F., Zaupa A., Ferro N., Rossi M., Susto G.A.","A machine learning-based approach for advanced monitoring of automated equipment for the entertainment industry","10.1109/MetroInd4.0IoT51437.2021.9488481","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112115672&doi=10.1109%2fMetroInd4.0IoT51437.2021.9488481&partnerID=40&md5=d6c3360d4d051f98108ad0cc9fb9e597","Machine Learning-based approaches are revolutionizing the way in which complex systems and machines are monitored and controlled. In this work, we present a smart monitoring system that combines a big data architecture with an unsupervised anomaly detection technique, targeting the automated equipment in the entertainment industry. Anomaly detection uses state-of-the-art univariate and multivariate algorithms, as well as recently proposed techniques in the field of explainable artificial intelligence, to achieve enhanced monitoring capabilities and optimize service operations. The monitoring system is here presented and tested on a real world case study, i.e., an amusement park ride. © 2021 IEEE.","Anomaly detection; Big data; Entertainment industry; Explainable artificial intelligence; Industry 4.0; Predictive maintenance; Unsupervised learning","Anomaly detection; Entertainment; Industry 4.0; Machine learning; Monitoring; Turing machines; Advanced monitoring; Amusement-park rides; Automated equipment; Data architectures; Enhanced monitoring; Service operations; Smart monitoring systems; Unsupervised anomaly detection; Internet of things"
"Berno M., Canil M., Chiarello N., Piazzon L., Berti F., Ferrari F., Zaupa A., Ferro N., Rossi M., Susto G.A.","A data management and anomaly detection solution for the entertainment industry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118765886&partnerID=40&md5=d28a5463baefc82dce34716ecd89c7ec","We present a smart monitoring system that combines a data management architecture with an unsupervised anomaly detection technique, targeting the automated equipment in the entertainment industry. Anomaly detection uses state-of-the-art univariate and multivariate algorithms, as well as recently proposed techniques in the field of explainable artificial intelligence, to achieve enhanced monitoring capabilities and optimize service operations. The monitoring system is here presented and tested on a real-world case study, i.e., an amusement park ride. © 2021 Copyright for this paper by its authors.","Anomaly detection; Data management; Entertainment industry; Predictive maintenance","Anomaly detection; Monitoring; Anomaly detection; Automated equipment; Data management architecture; Enhanced monitoring; Entertainment industry; Predictive maintenance; Smart monitoring systems; State of the art; Univariate; Unsupervised anomaly detection; Information management"
"Berns F., Lange-Hegermann M., Beecks C.","Towards Gaussian processes for automatic and interpretable anomaly detection in industry 4.0",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102926981&partnerID=40&md5=ec89f5c98357490f481c269b39d08066","Discerning unexpected from expected data patterns is the key challenge of anomaly detection. Although a multitude of solutions has been applied to this modern Industry 4.0 problem, it remains an open research issue to identify the key characteristics subjacent to an anomaly, sc. generate hypothesis as to why they appear. In recent years, machine learning models have been regarded as universal solution for a wide range of problems. While most of them suffer from non-self-explanatory representations, Gaussian Processes (GPs) deliver interpretable and robust statistical data models, which are able to cope with unreliable, noisy, or partially missing data. Thus, we regard them as a suitable solution for detecting and appropriately representing anomalies and their respective characteristics. In this position paper, we discuss the problem of automatic and interpretable anomaly detection by means of GPs. That is, we elaborate on why GPs are well suited for anomaly detection and what the current challenges are when applying these probabilistic models to large-scale production data. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Anomaly detection; Explainable machine learning; Gaussian processes; Industry 4.0","Gaussian distribution; Gaussian noise (electronic); Industry 4.0; Gaussian Processes; Key characteristics; Large scale productions; Machine learning models; Probabilistic models; Statistical data model; Suitable solutions; Universal solutions; Anomaly detection"
"Berrazeg M., Diene S.M., Drissi M., Kempf M., Richet H., Landraud L., Rolain J.-M.","Biotyping of Multidrug-Resistant Klebsiella pneumoniae Clinical Isolates from France and Algeria Using MALDI-TOF MS","10.1371/journal.pone.0061428","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876426580&doi=10.1371%2fjournal.pone.0061428&partnerID=40&md5=3d1802c1afb2f2e35be5c41c2fc65a68","Background:Klebsiella pneumoniae is one of the most important pathogens responsible for nosocomial outbreaks worldwide. Epidemiological analyses are useful in determining the extent of an outbreak and in elucidating the sources and the spread of infections. The aim of this study was to investigate the epidemiological spread of K. pneumoniae strains using a MALDI-TOF MS approach.Methods:Five hundred and thirty-five strains of K. pneumoniae were collected between January 2008 and March 2011 from hospitals in France and Algeria and were identified using MALDI-TOF. Antibiotic resistance patterns were investigated. Clinical and epidemiological data were recorded in an Excel file, including clustering obtained from the MSP dendrogram, and were analyzed using PASW Statistics software.Results:Antibiotic susceptibility and phenotypic tests of the 535 isolates showed the presence of six resistance profiles distributed unequally between the two countries. The MSP dendrogram revealed five distinct clusters according to an arbitrary cut-off at the distance level of 500. Data mining analysis of the five clusters showed that K. pneumoniae strains isolated in Algerian hospitals were significantly associated with respiratory infections and the ESBL phenotype, whereas those from French hospitals were significantly associated with urinary tract infections and the wild-type phenotype.Conclusions:MALDI-TOF was found to be a promising tool to identify and differentiate between K. pneumoniae strains according to their phenotypic properties and their epidemiological distribution. This is the first time that MALDI-TOF has been used as a rapid tool for typing K. pneumoniae clinical isolates. © 2013 Berrazeg et al.",,"amikacin; amoxicillin; amoxicillin plus clavulanic acid; ampicillin; cefalotin; cefoxitin; ceftazidime; ceftriaxone; ciprofloxacin; colistin; cotrimoxazole; gentamicin; imipenem; ticarcillin; tobramycin; Algeria; antibiotic sensitivity; article; bacterium identification; bacterium isolation; biotype; controlled study; epidemiological data; France; Klebsiella pneumoniae; matrix assisted laser desorption ionization time of flight mass spectrometry; multidrug resistance; nonhuman; phenotype; respiratory tract infection; sequence analysis; urinary tract infection; wild type; Adolescent; Adult; Aged; Aged, 80 and over; Algeria; Alleles; Anti-Bacterial Agents; Bacterial Typing Techniques; Child; Child, Preschool; Drug Resistance, Multiple, Bacterial; Female; France; Geography; Hospitals; Humans; Infant; Infant, Newborn; Klebsiella Infections; Klebsiella pneumoniae; Male; Microbial Sensitivity Tests; Middle Aged; Phenotype; Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization; Young Adult; Klebsiella pneumoniae"
"Berrebbi D., Shi J., Yan B., López-Francisco O., Amith J., Watanabe S.","Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation","10.21437/Interspeech.2022-10796","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140059971&doi=10.21437%2fInterspeech.2022-10796&partnerID=40&md5=e7bab8a221ad6c08c79c7a0acb808af8","Self-Supervised Learning (SSL) models have been successfully applied in various deep learning-based speech tasks, particularly those with a limited amount of data. However, the quality of SSL representations depends highly on the relatedness between the SSL training domain(s) and the target data domain. On the contrary, spectral feature (SF) extractors such as log Mel-filterbanks are hand-crafted non-learnable components, and could be more robust to domain shifts. The present work examines the assumption that combining non-learnable SF extractors to SSL models is an effective approach to low resource speech tasks. We propose a learnable and interpretable framework to combine SF and SSL representations. The proposed framework outperforms significantly both baseline and SSL models on Automatic Speech Recognition (ASR) and Speech Translation (ST) tasks on three low resource datasets. We additionally design a mixture of experts based combination model. This last model reveals that the relative contribution of SSL models over conventional SF extractors is very small in case of domain mismatch between SSL training set and the target language data. Copyright © 2022 ISCA.","co-Attention; Low Resource; Mixture of Experts; Self-Supervised Learning; Spectral Features","Deep learning; Mixtures; Speech communication; Speech recognition; Translation (languages); Co-attention; Data domains; Feature extractor; Learning models; Low resource; Low-resource speech recognition; Mixture of experts; Self-supervised learning; Spectral feature; Speech translation; Supervised learning"
"Berri R., Osório F.","A 3D vision system for detecting use of mobile phones while driving","10.1109/IJCNN.2018.8489093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056509508&doi=10.1109%2fIJCNN.2018.8489093&partnerID=40&md5=bf2a5da435822d3cc67bd95246e0f482","In this work, a 3D vision system has been developed using a frontal Kinect v2 sensor to monitor the driver, enabling to recognize the use of a cell phone while driving, avoiding driving risks. In fact, when cars are driven by people on phone calls, it increases between 4 and 6 times the risk of crash. The Naturalistic Driver Behavior Dataset (NDBD) was created specifically for this work and it was used to test the proposed system. The proposed solution uses two analysis of the driver's hands positions, the Short-Term (ST) and Long-Term (LT) pattern recognition subsystems, thus it could detect the cell phone usage by the driver in hand-held situations. The system has 3 levels of alarm: No alarm, lowest alarm, and highest alarm. ST detects between no alarm or some level alarm. LT is responsible for determining the risk alarm level, low or high. The classifiers are based on Machine Learning and Artificial Neural Nets (ANN), furthermore, the values set to adjust input features, neuron activation functions, and network topology/training parameters were optimized and selected using a Genetic Algorithm. The best system performance results obtained in the experiments achieved 95% of accuracy in NDBD frames. For the ST classifier, it was used length periods of 5 frames and a window of 80 or 210 frames for LT. The best results achieved obtained only 1% of 'no risk' situation having a wrong prediction (false positives with alarm activation), contributing to the driver comfort when he/she is using the system. © 2018 IEEE.",,"Accidents; Alarm systems; Behavioral research; Cellular telephones; Chemical activation; Genetic algorithms; Learning systems; Mobile phones; Pattern recognition; Statistical tests; Telecommunication links; Telephone sets; 3-D vision systems; Artificial neural net; Cell phone usages; Driver behavior; Hands positions; Network topology; Neuron activation function; System performance results; Cellular telephone systems"
"Bers J.A., Dismukes J.P., Miller L.K., Dubrovensky A.","Accelerated radical innovation: Theory and application","10.1016/j.techfore.2008.08.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57249095845&doi=10.1016%2fj.techfore.2008.08.013&partnerID=40&md5=e77796c9c316c21cc2355074c734f152","Radical innovation has been responsible for some of society's greatest advances over the past hundred years in fields as diverse as transportation, power, information technology, and medicine. But as scholars have found, it is such a long-term, chaotic, meandering, unpredictable process that promising radical innovation concepts are often never undertaken, to society's detriment. Does it need to be this way? Or can radical innovation be accelerated so that it is manageable within modern society's economic and political time horizon? This question prompted the organization of the Accelerated Radical Innovation (ARI) project five years ago. In this paper we summarize the ARI methodology as it currently stands and then report the results of an empirical verification with a radical medical innovation project that is currently under way - monochromatic X-rays for cancer diagnosis and treatment. Several conclusions were drawn. First, by and large, the ARI model tracked closely with the reality of this innovation, offering confirmation of its grounding in the real world of innovation. Second, the model offered a rationale and framework for this innovation process that could be more widely adopted. Third, the ARI model exposed critical issues whose early resolution could have accelerating the innovation cycle. Fourth, the application of a core principle of ARI, Systematic Competitive Intelligence, could have provided early warning on a competing technology that emerged suddenly. Last, the use of another core ARI concept, accelerated innovation prototyping, might help the innovator overcome the key barrier facing the innovation - the necessity of a long, expensive, high-risk clinical trial. Overall, the verification study confirms the potential of the ARI model to put the radical innovation process on a faster, lower-cost, better-managed track. © 2008 Elsevier Inc. All rights reserved.",,"Competition; Competitive intelligence; Information management; Signal filtering and prediction; Cancer diagnoses; Clinical trials; Critical issues; Early warnings; In fields; Innovation cycles; Innovation processes; Innovation projects; Prototyping; Radical innovation processes; Radical innovations; Real worlds; Time horizons; Innovation; artificial intelligence; early warning system; innovation; modeling; theoretical study"
"Bersini H., Philemotte C.","Emergent phenomena only belong to biology","10.1007/978-3-540-74913-4_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049043368&doi=10.1007%2f978-3-540-74913-4_6&partnerID=40&md5=a06ccba60b8284e268991db8ebb77aab","This rather philosophical paper discusses the necessary three ingredients which together allow a collective phenomenon to be described as ""emergent"". First the phenomenon, as usual, requires a group of agents entering in a non-linear relationship and entailing the existence of two semantic descriptions depending on the scale of observation. Second this phenomenon has to be observed by a mechanical observer instead of a human one, which has the natural capacity for temporal and/or spatial integration. Finally, for this natural observer to detect and select the collective phenomenon, it needs to do so in rewards of the adaptive value this phenomenon is responsible for. The presence of natural selection drives us to defend, with many authors, the idea that emergent phenomena can only belong to biology. After a brief philosophical plea, we present a simple and illustrative computer thought experiment in which a society of agents evolves a stigmergic collective behavior as an outcome of its greater adaptive value. The three ingredients are illustrated and discussed within this experimental context. © Springer-Verlag Berlin Heidelberg 2007.","Emergence; Natural selection","Intelligent agents; Semantics; Natural selection; Artificial intelligence"
"Bersini H.","Formalizing emergence: The natural after-life of artificial life","10.1007/1-4020-3917-4_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892050662&doi=10.1007%2f1-4020-3917-4_3&partnerID=40&md5=23369a1a426984e085565d1acf3e899a","Originally, the field of Artificial Life was born out of the frustration and isolation felt by some ""hackers"" keen on cellular automata, game of life, genetic algorithms, L-systems and other computer recreations. Fascinated by this surprising cohabitation of programming simple algorithms and the complex working of these same algorithms (this new perception of complex phenomena as emerging from simple algorithms but iterated, distributed and recursive), convinced of the interest of their works for theoreticians of biology but aware of the lack of dialogue with them, they organized a series of workshops whose desired originality was multidisciplinarity and the coming together of researchers sharing the same will to understand the mechanisms and functions characterizing living organisms. These researchers in computer science, mathematics, physics, biology, robotics, philosophy, now meet every year, alternatively in Europe and the USA. What is discussed as inherent to all living organisms, and therefore which represents the bulk of the material dealt with during these workshops, are the mechanisms of self-organization or of the ""emerging functionalities"" opposing a centralized vision of biology, the need to better balance the coupling of the studied objects with their environment opposing a solipsistic methodology still representative of a certain artificial intelligence, the compulsory passage via the mechanisms of learning and adaptation as the most simple and autonomous way to face the complexity typical of the architecture and dynamics of these systems and, finally, the study of this complexity per se. A same motto brings together all these researchers: ""some form of complexity can be faced and domesticated very simply by relying on the computer brute force"". The mascots that are most representative of artificial life are: robotic insectoids, the game of life and other cellular automata, genetic algorithms, L-systems and simulations of ecosystems. These first workshops, due to the originality of the process, created a considerable stir. They undoubtedly seemed to reach their primary target, that is to allow better communication between researchers. Today, however, a certain breathlessness is noticeable which goes not without reminding the same dying down that characterized the cybernetic and systemic trends (Alife fathers) of the forties and fifties. The multidisciplinarity although essential to the inspiration does not survive, in principle, the specialization which arises naturally as a consequence of several years of study dedicated to a same subject and which drive researchers to privilege interlocutors sharing their same narrow and deep interest. Gradually new scientific communities appear with a more focused object of study and which, either free themselves of the mother field (like genetic algorithms or cellular automata) or become connected with existing communities (like robotics, study of ecosystems, study of the origin of life, study of insects societies). As we can notice during these workshops, ""life"" resists whatever unique and narrow definition. This diversity is the de-stabilizing factor which could cause the burst of artificial life. Besides, the risk is important of a forthcoming divorce, which has already taken place in artificial intelligence, between a so called ""strong"" science which could fuse with an existing scientific tradition (cognitive science for AI and theoretical biology for artificial life) and its so called ""weak"" counterpart with a more engineering like aftertaste and leading to technological innovations (expert systems, fuzzy logic and knowledge engineering in AI, neural networks, genetic algorithms and autonomous robotics in artificial life). If the artificial life star turns into a supernovae to finally explode and leaves behind, as relics of its glorious past, one and only one scientific pulsar, more focused, firmly grounded, and, above all, perpetrating as well as possible the original enthusiasm, the best candidate I can see could be a more formalized study of the emergent phenomena. My contribution to this characterization of emergent phenomena is currently limited to two of them appearing in a large amount of biological networks: the de-stabilizing effect of frustrated connectivity and the tendency to fragment the whole network into small clusters of units showing similar behavior. Among the networks showing these two emergent properties, the attention will be paid to only two of them: Hopfield Neural Networks (HNN) and Idiotypic Immune Networks (INN). Frustrated connectivity is responsible for perturbing the equilibrium dynamics of the network and provoking ""wavering"" among alternative equilibrium regimes. When frustrated a homeostatic network exhibits oscillatory behavior while an oscillatory network falls into a new type of chaotic regime which will be designated as frustrated chaos. In HNN, there is a threshold in the degree of connectivity which marks a sharp transition into the dynamics of the network. Below this threshold, i.e. in the case of a strongly diluted connectivity, the network clusters itself into small group of oscillatory units. In IIN also, this clustering phenomenon prevails and follows very regular rules for the dimension and the distribution of the clusters. It is clear that these two properties can be regarded as emergent since in order to appear they require a specific collective configuration of the units, and in order to be detected they require a level of observation which transcends each unit taken separately. In this paper, rather than theoretical analysis, results of computer simulation are given and briefly explained to illustrate these common properties.© 2006 Springer. Printed in the Netherlands.",,
"Bertacco V., Bonkowski W.","ItHELPS: Iterative high-Accuracy error localization in post-silicon","10.1109/ICCD.2015.7357103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962381634&doi=10.1109%2fICCD.2015.7357103&partnerID=40&md5=9ef157b3d7099ce468b6d526b4c8df6b","The increasing complexity of modern digital circuits has exacerbated the challenge of verifying the functionality of these systems. To further compound the issue, shrinking time-To-market constraints place increased pressure on attaining correct devices in short amounts of time. As a result, more and more of the burden of validation has shifted to the post-silicon stage, when the first silicon prototypes of a design become available. This validation phase brings much faster test execution speeds, at the cost of a very limited ability of diagnosing bugs. To further compound the problem, intermittent failures are not uncommon, due to the physical nature of the device under validation. In this work we propose ItHELPS, a solution to identify the timing of a bug manifestation and the root signals responsible for it in industry-size complex digital designs. We employ a synergistic approach based on a machine-learning solution (DBSCAN) paired with an adaptive refinement analysis, capable of narrowing the location of a failure down to a handful of signals, possibly buried deep within the design hierarchy. We find experimentally that our approach outperforms the accuracy of prior state-of-The-Art solutions by two orders of magnitude. © 2015 IEEE.",,"Artificial intelligence; Learning systems; Silicon; Adaptive refinement; Design hierarchy; Digital designs; Intermittent failure; Orders of magnitude; Physical nature; State of the art; Validation phase; Design"
"Bertens R., Vreeken J., Siebes A.","Keeping it short and simple: Summarising complex event sequences with multivariate patterns","10.1145/2939672.2939761","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984999131&doi=10.1145%2f2939672.2939761&partnerID=40&md5=0bc26a875820bda729a7c0cde0f7ca4c","We study how to obtain concise descriptions of discrete multivariate sequential data. In particular, how to do so in terms of rich multivariate sequential patterns that can capture potentially highly interesting (cor)relations between sequences. To this end we allow our pattern language to span over the domains (alphabets) of all sequences, allow patterns to overlap temporally, as well as allow for gaps in their occurrences. We formalise our goal by the Minimum Description Length principle, by which our objective is to discover the set of patterns that provides the most succinct description of the data. To discover highquality pattern sets directly from data, we introduce DITTO, a highly efficient algorithm that approximates the ideal result very well. Experiments show that DITTO correctly discovers the patterns planted in synthetic data. Moreover, it scales favourably with the length of the data, the number of attributes, the alphabet sizes. On real data, ranging from sensor networks to annotated text, DITTO discovers easily interpretable summaries that provide clear insight in both the univariate and multivariate structure. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","MDL; Multivariate patterns; Multivariate sequences; Summarisation","Algorithms; Complex networks; Sensor networks; Minimum description length principle; Multivariate patterns; Multivariate sequences; Pattern languages; Sequential data; Sequential patterns; Summarisation; Synthetic data; Data mining"
"Berthold M.R.","Data analysis in the life sciences - Sparking ideas","10.1007/11564096_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646415346&doi=10.1007%2f11564096_1&partnerID=40&md5=40e983c2d94f49c8df4c2714bf26a2b6","Data from various areas of Life Sciences have increasingly caught the attention of data mining and machine learning researchers. Not only is the amount of data available mind-boggling but the diverse and heterogenous nature of the information is far beyond any other data analysis problem so far. In sharp contrast to classical data analysis scenarios, the life science area poses challenges of a rather different nature for mainly two reasons. Firstly, the available data stems from heterogenous information sources of varying degrees of reliability and quality and is, without the interactive, constant interpretation of a domain expert, not useful. Furthermore, predictive models are of only marginal interest to those users - instead they hope for new insights into a complex, biological system that is only partially represented within that data anyway. In this scenario, the data serves mainly to create new insights and generate new ideas that can be tested. Secondly, the notion of feature space and the accompanying measures of similarity cannot be taken for granted. Similarity measures become context dependent and it is often the case that within one analysis task several different ways of describing the objects of interest or measuring similarity between them matter. Some more recently published work in the data analysis area has started to address some of these issues. For example, data analysis in parallel universes [1], that is, the detection of patterns of interest in various different descriptor spaces at the same time, and mining of frequent, discriminative fragments in large, molecular data bases [2]. In both cases, sheer numerical performance is not the focus; it is rather the discovery of interpretable pieces of evidence that lights up new ideas in the users mind. Future work in data analysis in the life sciences needs to keep this in mind: the goal is to trigger new ideas and stimulate interesting associations. © Springer-Verlag Berlin Heidelberg 2005.",,"Data mining; Database systems; Information theory; Interactive computer systems; Learning systems; Mathematical models; Data mining; Database systems; Information science; Learning systems; Living systems studies; Mathematical models; Problem solving; Classical data analysis; Data stems; Predictive models; Data reduction; Data reduction; Interpretable pieces; Molecular data bases; Numerical performances; Parallel universes"
"Bertholdo L., da Silva C.G., de Aragão Umbuzeiro G., Camolesi Júnior L.","Classification, Association and Clustering of Water Body Data: Application to Water Quality Monitoring","10.1007/s40710-017-0261-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034568434&doi=10.1007%2fs40710-017-0261-8&partnerID=40&md5=7065dca58101a75bc51d3478e4aeb9eb","The application of advanced computational resources in the support of environmental management systems has increased in recent decades. The objective of this research is to discover useful knowledge amidst water monitoring data collected between 2005 and 2011, in a specific region of the state of São Paulo in Brazil. The research was performed in three steps: discovery of classification rules of ecotoxicity in water samples using predictive modelling; investigation of the presence of strong relationships between water quality parameters by associative analysis; and discovery of sampling sites, which are similar with respect to their quality parameters, by applying a clustering method. The knowledge discovery process employed in this work was divided into four phases: selection of study area and data, pre-processing of the selected data, mining of the pre-processed data, and interpretation and evaluation of the results. The data mining step was supported by different algorithms, each one focused on a particular issue of the research: sequential coverage algorithm for classification of ecotoxicity; Apriori algorithm for identifying associations between quality parameters, and Prim’s algorithm and pruning algorithm for clustering of water sampling sites. We observed the association of certain quality parameters to water ecotoxicity, the existence of correlations between some of the quality parameters, and the presence of homogeneous groups amidst the sampling sites. These results may provide valuable input to the field of water quality monitoring, from the technical activities, such as the laboratorial analysis of water samples, to the decision makers responsible for defining the future public policies for water resources management. © 2017, Springer International Publishing AG.","Associative analysis; Cluster analysis; Data mining; Environmental management systems; Knowledge discovery in databases; Predictive modeling; Water resources management","Cluster analysis; Data handling; Data mining; Decision making; Environmental management; Environmental management systems; Information management; Quality control; Water quality; Water resources; Associative analysis; Computational resources; Knowledge discovery in database; Knowledge discovery process; Predictive modeling; Water quality monitoring; Water quality parameters; Water resources management; Clustering algorithms; algorithm; cluster analysis; data mining; database; ecotoxicology; environmental management; environmental modeling; environmental monitoring; environmental research; resource management; social policy; water quality; water resource; Brazil"
"Bertini F., Dal Palù A., Fabiano F., Iotti E.","CARING for xAI",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138226596&partnerID=40&md5=040267204d3983947c8bb58cde926aea","Over the last few years, Artificial Intelligence (AI) has pervaded our lives. As a result, automated tools that “reason” on different scenarios have become more and more common. As this trend continues to grow, it has become necessary to ensure that newly developed tools and technologies can be safely adopted, as demonstrated by the numerous EU regulations. This is especially true when the concept of AI is intertwined with the field of medicine, where every decision may be critical. That is why, in this work, we decided to tackle the problem of automated interpretation of Computed Tomography (CT) scans using an explainable approach. In fact, while several methods based on Machine Learning (ML) are currently available, these are still outperformed by medical doctors and provide answers that cannot be traced back to a logical deduction. This paper presents CARING, a new methodology based on Answer Set Programming (ASP), which returns reliable, easy-to-program and explainable interpretations of CT scans. In particular, CARING makes use of transparent technologies in order to handle medical knowledge provided either by experts or by verified ontologies. This proof of concept shows that Logic Programming is a mature technology that can match the newest challenges in the xAI field. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)","Answer Set Programming; CT Scan; Explainable AI; Image processing; Tissue Segmentation","Artificial intelligence; Computerized tomography; Image segmentation; Answer set programming; Automated interpretation; Automated tools; Computed tomography scan; Creative Commons; EU regulations; Explainable artificial intelligence; Images processing; Tissue segmentation; Tools and technologies; Logic programming"
"Bertini J.R., Jr.","Graph embedded rules for explainable predictions in data streams","10.1016/j.neunet.2020.05.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086466731&doi=10.1016%2fj.neunet.2020.05.035&partnerID=40&md5=cc3601a7568bcc571ff95b6f6fe47ef8","Understanding the reason why a prediction has been made by a machine is crucial to grant trust to a human decision-maker. However, data mining based decision support systems are, in general, not designed to promote interpretability; instead, they are developed to improve accuracy. Interpretability becomes a more challenging issue in the context of data stream mining. Where the prediction model has to deal with enormous volumes of data gathered continuously at a fast rate and whose underlying distribution may change over time. On the one hand, the majority of the methods that address classification in a data stream are black-box models or white-box models into ensembles. Either do not provide a clear view of why a particular decision has been made. On the other hand, white-box models, such as rule-based models, do not provide acceptable accuracy to be considered in many applications. This paper proposes modeling the data as a special graph, which is built over the attribute space, and from which interpretable rules can be extracted. To overcome concept drift and enhance model accuracy, different variants of such graphs are considered within an ensemble that is updated over time. The proposed approach has shown the best overall classification results when compared to six rule-based algorithms in twelve streaming domains. © 2020 Elsevier Ltd","Attribute-based Decision Graphs; Data stream; Interpretable machine learning; Rule-based classifiers","Artificial intelligence; Data mining; Decision making; Decision support systems; Forecasting; Classification results; Data stream mining; Interpretability; Interpretable rules; Rule based algorithms; Rule-based models; Streaming domains; Underlying distribution; Data streams; algorithm; article; classifier; human; prediction; data mining; procedures; Data Mining; Neural Networks, Computer"
"Bertolini A., Riccaboni M.","Grounding the case for a European approach to the regulation of automated driving: the technology-selection effect of liability rules","10.1007/s10657-020-09671-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096371036&doi=10.1007%2fs10657-020-09671-5&partnerID=40&md5=a89f87780b96697e02749b2544002d45","In the current paper, we discuss the need for regulation at EU level of Connected and Automated Driving solutions (henceforth CAD) based on multiple considerations, namely (i) the need for uniformity of criteria across European Member States, and (ii) the impact that regulation—or the absence of it—has on the proliferation of specific technological solutions. The analysis is grounded on legal and economic considerations of possible interactions between vehicles with different levels of automation, and shows how the existing framework delays innovation. A Risk-Management Approach, identifying one sole responsible party ex ante (one-stop-shop), liable under all circumstances—pursuant to a strict, if not absolute liability rule—is to be preferred. We analyse the solution adopted by some Member States in light of those considerations and conclude that none truly corresponds to a RMA approach, and differences will also cause market fragmentation. We conclude that because legal rules determine what kind of technological application is favoured over others—and thence they are not technology-neutral—uniformity across MSs is of essential relevance, and discuss possible policy approaches to be adopted at European level. © 2020, The Author(s).","Artificial intelligence; Driverless cars; European law; Insurance; Law and technology; Liability; Product liability; Risk management",
"Bertolini R., Finch S.J., Nehm R.H.","Enhancing data pipelines for forecasting student performance: integrating feature selection with cross-validation","10.1186/s41239-021-00279-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112708121&doi=10.1186%2fs41239-021-00279-6&partnerID=40&md5=690ce40abe546e77eab35a892c58ba51","Educators seek to harness knowledge from educational corpora to improve student performance outcomes. Although prior studies have compared the efficacy of data mining methods (DMMs) in pipelines for forecasting student success, less work has focused on identifying a set of relevant features prior to model development and quantifying the stability of feature selection techniques. Pinpointing a subset of pertinent features can (1) reduce the number of variables that need to be managed by stakeholders, (2) make “black-box” algorithms more interpretable, and (3) provide greater guidance for faculty to implement targeted interventions. To that end, we introduce a methodology integrating feature selection with cross-validation and rank each feature on subsets of the training corpus. This modified pipeline was applied to forecast the performance of 3225 students in a baccalaureate science course using a set of 57 features, four DMMs, and four filter feature selection techniques. Correlation Attribute Evaluation (CAE) and Fisher’s Scoring Algorithm (FSA) achieved significantly higher Area Under the Curve (AUC) values for logistic regression (LR) and elastic net regression (GLMNET), compared to when this pipeline step was omitted. Relief Attribute Evaluation (RAE) was highly unstable and produced models with the poorest prediction performance. Borda’s method identified grade point average, number of credits taken, and performance on concept inventory assessments as the primary factors impacting predictions of student performance. We discuss the benefits of this approach when developing data pipelines for predictive modeling in undergraduate settings that are more interpretable and actionable for faculty and stakeholders. © 2021, The Author(s).","Cross-validation; Data mining; Data pipeline; Feature selection; Introductory biology",
"Bertone E., Stewart R.A., Zhang H., Bartkow M., Hacker C.","An autonomous decision support system for manganese forecasting in subtropical water reservoirs","10.1016/j.envsoft.2015.08.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939600098&doi=10.1016%2fj.envsoft.2015.08.008&partnerID=40&md5=f8f179dde6949d5587a021199f157fbb","Manganese monitoring and removal is essential for water utilities in order to avoid supplying discoloured water to consumers. Traditional manganese monitoring in water reservoirs consists of costly and time-consuming manual lake samplings and laboratory analysis. However, vertical profiling systems can automatically collect and remotely transfer a range of physical parameters that affect the manganese cycle. In this study, a manganese prediction model was developed, based on the profiler's historical data and weather forecasts. The model effectively forecasted seven-day ahead manganese concentrations in the epilimnion of Advancetown Lake (Queensland, Australia). The manganese forecasting model was then operationalised into an automatically updated decision support system with a user-friendly graphical interface that is easily accessible and interpretable by water treatment plant operators. The developed tool resulted in a reduction in traditional expensive monitoring while ensuring proactive water treatment management. © 2015 Elsevier Ltd.","Decision support system; Manganese; Prediction modelling; Statistical modelling; Water management; Water treatment","Artificial intelligence; Balloons; Decision support systems; Forecasting; Lakes; Manganese; Manganese removal (water treatment); Reservoirs (water); Water management; Water treatment; Weather forecasting; Autonomous decision; Forecasting modeling; Graphical interface; Manganese concentration; Physical parameters; Prediction modelling; Statistical modelling; Treatment management; Chemicals removal (water treatment); cost analysis; decision support system; environmental monitoring; forecasting method; manganese; numerical model; pollutant removal; prediction; reservoir; water treatment; Australia; Queensland"
"Bertossi L., Geerts F.","Data Quality and Explainable AI","10.1145/3386687","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085843620&doi=10.1145%2f3386687&partnerID=40&md5=162e2ba9f7ad06237dcbd66f822b1d81","In this work, we provide some insights and develop some ideas, with few technical details, about the role of explanations in Data Quality in the context of data-based machine learning models (ML). In this direction, there are, as expected, roles for causality, and explainable artificial intelligence. The latter area not only sheds light on the models, but also on the data that support model construction. There is also room for defining, identifying, and explaining errors in data, in particular, in ML, and also for suggesting repair actions. More generally, explanations can be used as a basis for defining dirty data in the context of ML, and measuring or quantifying them. We think dirtiness as relative to the ML task at hand, e.g., classification. © 2020 ACM.","bias; causes; fairness; Machine learning","Data processing; Information systems; Data quality; Dirty data; Machine learning models; Model construction; Technical details; Artificial intelligence"
"Bertrand A., Belloum R., Eagan J.R., Maxwell W.","How cognitive biases affect XAI-Assisted decision-making: A systematic review","10.1145/3514094.3534164","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137156399&doi=10.1145%2f3514094.3534164&partnerID=40&md5=9246c8426d4673a745ca3243c00a8a56","The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI literature, structured around XAI-Aided decision-making. We identify four main ways cognitive biases affect or are affected by XAI systems: 1) cognitive biases affect how XAI methods are designed, 2) they can distort how XAI techniques are evaluated in user studies, 3) some cognitive biases can be successfully mitigated by XAI techniques, and, on the contrary, 4) some cognitive biases can be exacerbated by XAI techniques. We construct this heuristic map through the systematic review of 37 papers-drawn from a corpus of 285-That reveal cognitive biases in XAI systems, including the explainability method and the user and task types in which they arise. We use the findings from our review to structure directions for future XAI systems to better align with people's cognitive processes. © 2022 ACM.","cognitive bias; explainability; explainable ai; human-centered ai; xai.","Behavioral research; Cognitive systems; Heuristic algorithms; Heuristic methods; AI systems; Cognitive bias; Decisions makings; Explainability; Explainable ai; Human-centered ai; Systematic Review; Technical fields; User study; Xai.; Decision making"
"Bertsimas D., Paskov A.","World-class interpretable poker","10.1007/s10994-022-06179-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131506783&doi=10.1007%2fs10994-022-06179-8&partnerID=40&md5=cdc916b7ab4997d5e5f1deeb28a0782d","We address the problem of interpretability in iterative game solving for imperfect-information games such as poker. This lack of interpretability has two main sources: first, the use of an uninterpretable feature representation, and second, the use of black box methods such as neural networks, for the fitting procedure. In this paper, we present advances on both fronts. Namely, first we propose a novel, compact, and easy-to-understand game-state feature representation for Heads-up No-limit (HUNL) Poker. Second, we make use of globally optimal decision trees, paired with a counterfactual regret minimization (CFR) self-play algorithm, to train our poker bot which produces an entirely interpretable agent. Through experiments against Slumbot, the winner of the most recent Annual Computer Poker Competition, we demonstrate that our approach yields a HUNL Poker agent that is capable of beating the Slumbot. Most exciting of all, the resulting poker bot is highly interpretable, allowing humans to learn from the novel strategies it discovers. © 2022, The Author(s).","Artificial intelligence; Deep reinforcement learning; Game solving; Interpretability; Optimal decision trees; Poker; Regret minimization","Deep learning; Forestry; Iterative methods; Reinforcement learning; Feature representation; Game solving; Imperfect information games; Interpretability; Optimal decision tree; Optimal decisions; Poker; Poker bots; Regret minimization; World class; Decision trees"
"Bertsimas D., Digalakis V., Jr.","The backbone method for ultra-high dimensional sparse machine learning","10.1007/s10994-021-06123-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123293906&doi=10.1007%2fs10994-021-06123-2&partnerID=40&md5=086ea3b137a61cae222f53c64ae86b7a","We present the backbone method, a general framework that enables sparse and interpretable supervised machine learning methods to scale to ultra-high dimensional problems. We solve sparse regression problems with 10 7 features in minutes and 10 8 features in hours, as well as decision tree problems with 10 5 features in minutes. The proposed method operates in two phases: we first determine the backbone set, consisting of potentially relevant features, by solving a number of tractable subproblems; then, we solve a reduced problem, considering only the backbone features. For the sparse regression problem, our theoretical analysis shows that, under certain assumptions and with high probability, the backbone set consists of the truly relevant features. Numerical experiments on both synthetic and real-world datasets demonstrate that our method outperforms or competes with state-of-the-art methods in ultra-high dimensional problems, and competes with optimal solutions in problems where exact methods scale, both in terms of recovering the truly relevant features and in its out-of-sample predictive performance. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.","Decision trees; Feature Selection; Mixed integer optimization; Sparse machine learning; Sparse regression; Ultra-high dimensional machine learning","Integer programming; Numerical methods; Regression analysis; Supervised learning; Features selection; High-dimensional; Higher-dimensional; Higher-dimensional problems; Mixed integer optimization; Relevant features; Sparse machine learning; Sparse regression; Ultra-high; Ultra-high dimensional machine learning; Decision trees"
"Bertsimas D., Stellato B.","The voice of optimization","10.1007/s10994-020-05893-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088087673&doi=10.1007%2fs10994-020-05893-5&partnerID=40&md5=00a98c8308682b3b19e6aac17541e288","We introduce the idea that using optimal classification trees (OCTs) and optimal classification trees with-hyperplanes (OCT-Hs), interpretable machine learning algorithms developed by Bertsimas and Dunn (Mach Learn 106(7):1039–1082, 2017), we are able to obtain insight on the strategy behind the optimal solution in continuous and mixed-integer convex optimization problem as a function of key parameters that affect the problem. In this way, optimization is not a black box anymore. Instead, we redefine optimization as a multiclass classification problem where the predictor gives insights on the logic behind the optimal solution. In other words, OCTs and OCT-Hs give optimization a voice. We show on several realistic examples that the accuracy behind our method is in the 90–100% range, while even when the predictions are not correct, the degree of suboptimality or infeasibility is very low. We compare optimal strategy predictions of OCTs and OCT-Hs and feedforward neural networks (NNs) and conclude that the performance of OCT-Hs and NNs is comparable. OCTs are somewhat weaker but often competitive. Therefore, our approach provides a novel insightful understanding of optimal strategies to solve a broad class of continuous and mixed-integer optimization problems. © 2020, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.","Interpretability; Multiclass classification; Parametric optimization; Sampling","Convex optimization; Feedforward neural networks; Learning algorithms; Machine learning; Optimal systems; Black boxes; Mixed integer; Mixed integer optimization; Multiclass classification problems; Optimal classification; Optimal solutions; Optimal strategies; Suboptimality; Integer programming"
"Bertsimas D., Margonis G.A., Huang Y., Andreatos N., Wiberg H., Ma Y., McIntyre C., Pulvirenti A., Wagner D., Van Dam J.L., Gavazzi F., Buettner S., Imai K., Stasinos G., He J., Kamphues C., Beyer K., Seeliger H., Weiss M.J., Kreis M., Cameron J.L., Wei A.C., Kornprat P., Baba H., Koerkamp B.G., Zerbi A., D'Angelica M., Wolfgang C.L.","Toward an Optimized Staging System for Pancreatic Ductal Adenocarcinoma: A Clinically Interpretable, Artificial Intelligence-Based Model","10.1200/CCI.21.00001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122904450&doi=10.1200%2fCCI.21.00001&partnerID=40&md5=dc04ce35287096a9874bd7ace0f45650","PURPOSE: The American Joint Committee on Cancer (AJCC) eighth edition schema for pancreatic ductal adenocarcinoma treats T and N stage as independent factors and uses positive lymph nodes (PLNs) to define N stage, despite data favoring lymph node ratio (LNR). We used artificial intelligence-based techniques to compare PLN with LNR and investigate interactions between tumor size and nodal status. METHODS: Patients who underwent pancreatic ductal adenocarcinoma resection between 2000 and 2017 at six institutions were identified. LNR and PLN were compared through shapley additive explanations (SHAP) analysis, with the best predictor used to define nodal status. We trained optimal classification trees (OCTs) to predict 1-year and 3-year risk of death, incorporating only tumor size and nodal status as variables. The OCTs were compared with the AJCC schema and similarly trained XGBoost models. Variable interactions were explored via SHAP. RESULTS: Two thousand eight hundred seventy-four patients comprised the derivation and 1,231 the validation cohort. SHAP identified LNR as a superior predictor. The OCTs outperformed the AJCC schema in the derivation and validation cohorts (1-year area under the curve: 0.681 v 0.603; 0.638 v 0.586, 3-year area under the curve: 0.682 v 0.639; 0.675 v 0.647, respectively) and performed comparably with the XGBoost models. We identified interactions between LNR and tumor size, suggesting that a negative prognostic factor partially overrides the effect of a concurrent favorable factor. CONCLUSION: Our findings highlight the superiority of LNR and the importance of interactions between tumor size and nodal status. These results and the potential of the OCT methodology to combine them into a powerful, visually interpretable model can help inform future staging systems. © 2022 American Society of Clinical Oncology.",,"adult; aged; area under the curve; Article; artificial intelligence; cancer classification; cancer mortality; cancer prognosis; cancer size; cancer staging; cancer surgery; cancer survival; cohort analysis; comparative study; controlled study; distal pancreatectomy; female; follow up; human; lymph node metastasis; lymph node ratio; major clinical study; male; mortality rate; overall survival; pancreatectomy; pancreatic ductal carcinoma; pancreaticoduodenectomy; perineural invasion; tumor differentiation; validation study; xgboost model; artificial intelligence; cancer staging; lymph node; pancreas carcinoma; pancreas tumor; pathology; prognosis; Artificial Intelligence; Carcinoma, Pancreatic Ductal; Humans; Lymph Nodes; Lymphatic Metastasis; Neoplasm Staging; Pancreatic Neoplasms; Prognosis"
"Bertsimas D., Orfanoudaki A., Wiberg H.","Interpretable clustering: an optimization approach","10.1007/s10994-020-05896-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089447323&doi=10.1007%2fs10994-020-05896-2&partnerID=40&md5=5b83c1f36f419a7d26629732e47d4e69","State-of-the-art clustering algorithms provide little insight into the rationale for cluster membership, limiting their interpretability. In complex real-world applications, the latter poses a barrier to machine learning adoption when experts are asked to provide detailed explanations of their algorithms’ recommendations. We present a new unsupervised learning method that leverages Mixed Integer Optimization techniques to generate interpretable tree-based clustering models. Utilizing a flexible optimization-driven framework, our algorithm approximates the globally optimal solution leading to high quality partitions of the feature space. We propose a novel method which can optimize for various clustering internal validation metrics and naturally determines the optimal number of clusters. It successfully addresses the challenge of mixed numerical and categorical data and achieves comparable or superior performance to other clustering methods on both synthetic and real-world datasets while offering significantly higher interpretability. © 2020, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.","Clustering; Interpretability; Mixed integer optimization; Unsupervised learning","Cluster analysis; Integer programming; Learning systems; Numerical methods; Unsupervised learning; Cluster memberships; Clustering methods; Mixed integer optimization; Optimal solutions; Optimization approach; Real-world datasets; State of the art; Unsupervised learning method; Clustering algorithms"
"Bertsimas D., Dunn J., Pawlowski C., Silberholz J., Weinstein A., Zhuo Y.D., Chen E., Elfiky A.A.","Applied informatics decision support tool for mortality predictions in patients with cancer","10.1200/CCI.18.00003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063397206&doi=10.1200%2fCCI.18.00003&partnerID=40&md5=00eb73655b775f82d7ac0d48cd764072","Purpose With rapidly evolving treatment options in cancer, the complexity in the clinical decisionmaking process for oncologists represents a growing challenge magnified by oncologists' disposition of intuition-based assessment of treatment risks and overall mortality. Given the unmet need for accurate prognostication with meaningful clinical rationale, we developed a highly interpretable prediction tool to identify patients with high mortality risk before the start of treatment regimens. Methods We obtained electronic health record data between 2004 and 2014 from a large national cancer center and extracted 401 predictors, including demographics, diagnosis, gene mutations, treatment history, comorbidities, resource utilization, vital signs, and laboratory test results. We built an actionable tool using novel developments in modern machine learning to predict 60-, 90- and 180-day mortality from the start of an anticancer regimen. The model was validated in unseen data against benchmark models. Results We identified 23,983 patients who initiated 46,646 anticancer treatment lines, with a median survival of 514 days. Our proposed prediction models achieved significantly higher estimation quality in unseen data (area under the curve, 0.83 to 0.86) compared with benchmark models. We identified key predictors of mortality, such as change in weight and albumin levels. The results are presented in an interactive and interpretable tool (www.oncomortality.com). Conclusion Our fully transparent prediction model was able to distinguish with high precision between highest- and lowest-risk patients. Given the rich data available in electronic health records and advances in machine learning methods, this tool can have significant implications for valuebased shared decision making at the point of care and personalized goals-of-care management to catalyze practice reforms. © 2018 American Society of Clinical Oncology.",,"adult; albumin level; area under the curve; article; cancer center; cancer survival; cancer therapy; catalysis; comorbidity; controlled study; decision support system; diagnosis; electronic health record; female; gene mutation; human; information science; laboratory test; low risk patient; machine learning; major clinical study; male; median survival time; mortality risk; prediction; risk assessment; shared decision making; vital sign"
"Betancourt C., Stomberg T.T., Edrich A.-K., Patnala A., Schultz M.G., Roscher R., Kowalski J., Stadtler S.","Global, high-resolution mapping of tropospheric ozone-explainable machine learning and impact of uncertainties","10.5194/gmd-15-4331-2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131797295&doi=10.5194%2fgmd-15-4331-2022&partnerID=40&md5=3e9864d3ecf14978477108142ec4e838","Tropospheric ozone is a toxic greenhouse gas with a highly variable spatial distribution which is challenging to map on a global scale. Here, we present a data-driven ozone-mapping workflow generating a transparent and reliable product. We map the global distribution of tropospheric ozone from sparse, irregularly placed measurement stations to a high-resolution regular grid using machine learning methods. The produced map contains the average tropospheric ozone concentration of the years 2010-2014 with a resolution of 0.1° × 0.1°. The machine learning model is trained on AQ-Bench (""air quality benchmark dataset""), a pre-compiled benchmark dataset consisting of multi-year ground-based ozone measurements combined with an abundance of high-resolution geospatial data. Going beyond standard mapping methods, this work focuses on two key aspects to increase the integrity of the produced map. Using explainable machine learning methods, we ensure that the trained machine learning model is consistent with commonly accepted knowledge about tropospheric ozone. To assess the impact of data and model uncertainties on our ozone map, we show that the machine learning model is robust against typical fluctuations in ozone values and geospatial data. By inspecting the input features, we ensure that the model is only applied in regions where it is reliable. We provide a rationale for the tools we use to conduct a thorough global analysis. The methods presented here can thus be easily transferred to other mapping applications to ensure the transparency and reliability of the maps produced. © 2022 Clara Betancourt et al.",,"machine learning; mapping method; ozone; troposphere; uncertainty analysis"
"Bethge D., Chen J., Grothe O., Elmer J., Dubrawski A.","Prognostication of neurological recovery by analyzing structural breaks in EEG data","10.1109/ICDMW.2019.00136","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078731356&doi=10.1109%2fICDMW.2019.00136&partnerID=40&md5=491807db666e75fed392c85afa983ce3","We describe an approach for unsupervised, multivariate yet interpretable structural break testing of rich electroencephalographic (EEG) data time series to perform early prediction of patient outcome after resuscitation from cardiac arrest. Few models exist that reliably determine prognosis among comatose post-arrest patients within hours of hospital admission. We present an efficient method designed to detect anomalous patterns in streaming EEG data that combines scan statistics with multiple structural break tests. Some patterns of change show non-trivial power in prognosticating patient outcomes at clinically relevant prediction horizons. Empirical evaluation of the proposed method shows its potential utility in determining cardiac arrest patient outcomes earlier and more confidently than existing alternatives. © 2019 IEEE.","Scan statistics; Streaming anomaly detection; Structural break test","Anomaly detection; Electroencephalography; Resuscitation; Electroencephalographic (EEG); Empirical evaluations; Hospital admissions; Neurological recoveries; Potential utility; Prediction horizon; Scan statistics; Structural break; Data mining"
"Betrains A., Vanderschueren S.","In reply to ‘clinical biochemistry test eliminator providing cost-effectiveness with five algorithms’: the Casablanca strategy","10.1080/17843286.2020.1763672","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085386080&doi=10.1080%2f17843286.2020.1763672&partnerID=40&md5=c62e20aa1359dc497fee66d5c977efd5","In this journal, Ataman Gönel recently demonstrated that elimination of requested unnecessary tests by means of algorithms in an artificial intelligence program may contribute to the cost-effectiveness of medicine. However, test ordering is an essential part of clinical medicine and decision-making. Interns are responsible for a modest but significant excess in laboratory utilization and underestimate their control over laboratory testing. Even in the hands of experts, rational approaches to test ordering may be subverted by the Casablanca Strategy. Establishing a differential diagnosis and ordering only tests deemed necessary reflect the attainment of expertise in the clinical decision-making process. Residents and attendings in teaching roles should not underestimate the extent to which their clinical decision-making may be mimicked. © Belgian Society of Internal Medicine and Royal Belgian Society of Laboratory Medicine (2020).","Casablanca strategy; Clinical decision-making; test-ordering behavior","adult; algorithm; artificial intelligence; clinical chemistry; clinical decision making; clinical medicine; cost effectiveness analysis; differential diagnosis; human; laboratory test; letter; mental capacity; resident; teaching; artificial intelligence; cost benefit analysis; treatment outcome; Algorithms; Artificial Intelligence; Cost-Benefit Analysis; Humans; Treatment Outcome"
"Bette A.-C., Brus P., Balazs G., Ludwig M., Knoll A.","Automated Defect Inspection in Reverse Engineering of Integrated Circuits","10.1109/WACV51458.2022.00187","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126144058&doi=10.1109%2fWACV51458.2022.00187&partnerID=40&md5=78387f484133346b42a4793fb18783ad","In the semiconductor industry, reverse engineering is used to extract information from microchips. Circuit extraction is becoming increasingly difficult due to the continuous technology shrinking. A high quality reverse engineering process is challenged by various defects coming from chip preparation and imaging errors. Currently, no automated, technology-agnostic defect inspection framework is available. To meet the requirements of the mostly manual reverse engineering process, the proposed automated frame- work needs to handle highly imbalanced data, as well as unknown and multiple defect classes. We propose a network architecture that is composed of a shared Xception- based feature extractor and multiple, individually trainable binary classification heads: the HydREnet. We evaluated our defect classifier on three challenging industrial datasets and achieved accuracies of over 85 %, even for underrepresented classes. With this framework, the manual inspection effort can be reduced down to 5 %. © 2022 IEEE.","Accountability; Deep Learning; Efficient Training and Inference Methods for Networks; Evaluation and Comparison of Vision Algorithms; Explainable AI; Fairness; Industrial Inspection Datasets; Object Detection/Recognition/Categorization; Privacy and Ethics in Vision; Vision Systems and Applications","Automation; Classification (of information); Deep learning; Defects; Inference engines; Inspection; Integrated circuits; Network architecture; Object detection; Reverse engineering; Semiconductor device manufacture; Accountability; Deep learning; Efficient training and inference method for network; Evaluation and comparison of vision algorithm; Explainable AI; Fairness; Industrial inspection dataset; Industrial inspections; Inference methods; Object detection/recognition/categorization; Objects detection; Privacy and ethic in vision; Training methods; Vision algorithms; Vision applications; Vision systems; Timing circuits"
"Betti M., Bertolotti M., Bolgeo T., Bottrighi A., Cassinari A., Maconi A., Massarino C., Pennisi M., Rava E., Roveta A.","A Preliminary Analysis of Hospitalized Covid-19 Patients in Alessandria Area: A machine learning approach","10.1109/COINS51742.2021.9524121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115439700&doi=10.1109%2fCOINS51742.2021.9524121&partnerID=40&md5=7e6c0dc74249d98875c0405bcc2772a6","In 2020, severe coronavirus 2 respiratory syndrome (SARS-Cov-2) has quickly risen, becoming a worldwide pandemic that is still ongoing nowadays. Differently from other viruses the COVID-19, responsible for SARS-Cov-2, demonstrated an unmatched capability of transmission that led towards an unprecedented challenge for the global health system. All health facilities, ranging from Hospitals to local health surveillance units, have been severely tested due to the high number of infected people. In this scenario, the use of methodologies that can improve and optimize, at any level, the management of infected patients is highly advisable. One of the goals of Artificial Intelligence in medicine is to develop advanced tools and methodologies to support patient care and to help physicians and medical work in the decision-making process. More specifically, Machine Learning (ML) methods have been successfully used to build predictive models starting from clinical patient data. In our paper, we study whether ML can be used to build prognostic models capable of predicting the potential disease outcome. In our study, we evaluate different unsupervised and supervised ML approaches using SARS-Cov-2 data collected from the ""Azienda Ospedaliera SS Antonio e Biagio e Cesare Arrigo""Hospital in Alessandria area, Italy, from 24th February to 31st October 2020. Our preliminary goal is to develop a ML model able to promptly identify patients with a high risk of fatal outcome, to steer medical doctors and clinicians towards the best management strategies. © 2021 IEEE.","Covid-19; hospitalized patients; machine learning; prognostic models","Behavioral research; Coronavirus; Decision making; Diseases; Hospital data processing; Hospitals; Machine learning; Viruses; Coronaviruses; Covid-19; Global health; Health facilities; Health surveillances; Health systems; Hospitalized patient; Machine learning approaches; Preliminary analysis; Prognostic modeling; SARS"
"Bettini C., Civitarese G., Fiori M.","Explainable Activity Recognition over Interpretable Models","10.1109/PerComWorkshops51409.2021.9430955","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107570624&doi=10.1109%2fPerComWorkshops51409.2021.9430955&partnerID=40&md5=fad812f4770fa0393e63e9aace61f9af","The majority of the approaches to sensor-based activity recognition are based on supervised machine learning. While these methods reach high recognition rates, a major challenge is to understand the rationale behind the predictions of the classifier. Indeed, those predictions may have a relevant impact on the follow-up actions taken in a smart living environment. We propose a novel approach for eXplainable Activity Recognition (XAR) based on interpretable machine learning models. We generate explanations by combining the feature values with the feature importance obtained from the underlying trained classifier. A quantitative evaluation on a real dataset of ADLs shows that our method is effective in providing explanations consistent with common knowledge. By comparing two popular ML models, our results also show that one versus one classifiers can provide better explanations in our framework. © 2021 IEEE.","activity recognition; explainable artificial intelligence; smart-homes","Intelligent buildings; Pattern recognition; Supervised learning; Ubiquitous computing; Activity recognition; Common knowledge; Feature values; Follow up; Machine learning models; Quantitative evaluation; Smart livings; Supervised machine learning; Learning systems"
"Betz L.T., Rosen M., Salokangas R.K.R., Kambeitz J.","Disentangling the impact of childhood abuse and neglect on depressive affect in adulthood: A machine learning approach in a general population sample","10.1016/j.jad.2022.07.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135135468&doi=10.1016%2fj.jad.2022.07.042&partnerID=40&md5=00129d952449f0a1736b694f1f41a340","Background: Different types of childhood maltreatment (CM) are key risk factors for psychopathology. Specifically, there is evidence for a unique role of emotional abuse in affective psychopathology in children and youth; however, its predictive power for depressive symptomatology in adulthood is still unknown. Additionally, emotional abuse encompasses several facets, but the strength of their individual contribution to depressive affect has not been examined. Method: Here, we used a machine learning (ML) approach based on Random Forests to assess the performance of domain scores and individual items from the Childhood Trauma Questionnaire (CTQ) in predicting self-reported levels of depressive affect in an adult general population sample. Models were generated in a training sample (N = 769) and validated in an independent test sample (N = 466). Using state-of-the-art methods from interpretable ML, we identified the most predictive domains and facets of CM for adult depressive affect. Results: Models based on individual CM items explained more variance in the independent test sample than models based on CM domain scores (R2 = 7.6 % vs. 6.4 %). Emotional abuse, particularly its more subjective components such as reactions to and appraisal of the abuse, emerged as the strongest predictors of adult depressive affect. Limitations: Assessment of CM was retrospective and lacked information on timing and duration. Moreover, reported rates of CM and depressive affect were comparatively low. Conclusions: Our findings corroborate the strong role of subjective experience in CM-related psychopathology across the lifespan that necessitates greater attention in research, policy, and clinical practice. © 2022 Elsevier B.V.","Adverse childhood experiences; Childhood maltreatment; Childhood trauma; Depression; Early adversity; Precision medicine","adult; aged; Article; Center for Epidemiological Studies Depression Scale; child abuse; child neglect; child sexual abuse; Childhood Trauma Questionnaire; controlled study; depression; emotional abuse; emotional neglect; female; follow up; human; longitudinal study; machine learning; male; physical abuse; population; predictive model; random forest; very elderly; adolescent; child; depression; machine learning; psychology; questionnaire; retrospective study; Adolescent; Adult; Child; Child Abuse; Depression; Humans; Machine Learning; Retrospective Studies; Surveys and Questionnaires"
"Beucher A., Rasmussen C.B., Moeslund T.B., Greve M.H.","Interpretation of Convolutional Neural Networks for Acid Sulfate Soil Classification","10.3389/fenvs.2021.809995","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124025226&doi=10.3389%2ffenvs.2021.809995&partnerID=40&md5=61df762f79bea0f8d324817e2539f847","Convolutional neural networks (CNNs) have been originally used for computer vision tasks, such as image classification. While several digital soil mapping studies have been assessing these deep learning algorithms for the prediction of soil properties, their potential for soil classification has not been explored yet. Moreover, the use of deep learning and neural networks in general has often raised concerns because of their presumed low interpretability (i.e., the black box pitfall). However, a recent and fast-developing sub-field of Artificial Intelligence (AI) called explainable AI (XAI) aims to clarify complex models such as CNNs in a systematic and interpretable manner. For example, it is possible to apply model-agnostic interpretation methods to extract interpretations from any machine learning model. In particular, SHAP (SHapley Additive exPlanations) is a method to explain individual predictions: SHAP values represent the contribution of a covariate to the final model predictions. The present study aimed at, first, evaluating the use of CNNs for the classification of potential acid sulfate soils located in the wetland areas of Jutland, Denmark (c. 6,500 km2), and second and most importantly, applying a model-agnostic interpretation method on the resulting CNN model. About 5,900 soil observations and 14 environmental covariates, including a digital elevation model and derived terrain attributes, were utilized as input data. The selected CNN model yielded slightly higher prediction accuracy than the random forest models which were using original or scaled covariates. These results can be explained by the use of a common variable selection method, namely recursive feature elimination, which was based on random forest and thus optimized the selection for this method. Notably, the SHAP method results enabled to clarify the CNN model predictions, in particular through the spatial interpretation of the most important covariates, which constitutes a crucial development for digital soil mapping. Copyright © 2022 Beucher, Rasmussen, Moeslund and Greve.","acid sulfate soils; classification; convolutional neural network; deep learning; interpretability; SHAP (SHapley Additive exPlanations); XAI (eXplainable artificial intelligence)",
"Bevilacqua V., Triggiani M., Dimatteo M., Bellantuono G., Brunetti A., Carnimeo L., Marino F., Telegrafo M., Moschetta M.","Computer assisted detection of breast lesions in magnetic resonance images","10.1007/978-3-319-42291-6_30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978833984&doi=10.1007%2f978-3-319-42291-6_30&partnerID=40&md5=79855406690ba95ae1c504d9be050dbd","Nowadays preventive screening policies and increased awareness initiatives are up surging the workload of radiologists. Due to the growing number of women undergoing first-level screening tests, systems that can make these operations faster and more effective are required. This paper presents a Computer Assisted Detection system based on medical imaging techniques and capable of labeling potentially cancerous breast lesions. This work is based on MRIs performed with morphological and dynamic sequences, obtained and classified thanks to the collaboration of the specialists from the University of Bari Aldo Moro (Italy). A first set of 60 images was acquired without Contrast Method for each patient and, subsequently, 100 more slices were taken with Contrast Method. This article formally describes the techniques adopted to segment these images and extract the most significant features from each Region of Interest (ROI). Then, the underlying architecture of the suggested Artificial Neural Network (ANN) responsible of identifying suspect lesions will be presented. We will discuss the architecture of the supervised neural network based on the algorithm named Robust Error Back Propagation, trained and optimized so to maximize the number of True Positive ROIs, i.e., the actual tumor regions. The training set, built with physicians’ help, consists of 94 lesions and 3700 regions of any interest extracted with the proposed segmentation technique. Performances of the ANN, trained using 60% of the samples, are evaluated in terms of accuracy, sensitivity and specificity indices. In conclusion, these tests show that a supervised machine learning approach to the detection of breast lesions in Magnetic Resonance Images is consistent, and shows good performance, especially from a False Negative reduction perspective. © Springer International Publishing Switzerland 2016.","Artificial neural networks; Breast lesions detection; Decision support systems; Medical image classification","Artificial intelligence; Backpropagation; Backpropagation algorithms; Computation theory; Decision support systems; Diagnosis; Image classification; Image processing; Image segmentation; Imaging techniques; Intelligent computing; Learning systems; Magnetic levitation vehicles; Magnetic resonance; Magnetic resonance imaging; Network architecture; Neural networks; Supervised learning; Breast lesion; Computer assisted detection; Error back propagation; Region of interest; Segmentation techniques; Sensitivity and specificity; Supervised machine learning; Supervised neural networks; Medical imaging"
"Bewley T., Lawry J.","TRIPLETREE: A Versatile Interpretable Representation of Black Box Agents and their Environments",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121589810&partnerID=40&md5=2990e3014e5adbfee691d652c45bd718","In explainable artificial intelligence, there is increasing interest in understanding the behaviour of autonomous agents to build trust and validate performance. Modern agent architectures, such as those trained by deep reinforcement learning, are currently so lacking in interpretable structure as to effectively be black boxes, but insights may still be gained from an external, behaviourist perspective. Inspired by conceptual spaces theory, we suggest that a versatile first step towards general understanding is to discretise the state space into convex regions, jointly capturing similarities over the agent’s action, value function and temporal dynamics within a dataset of observations. We create such a representation using a novel variant of the CART decision tree algorithm, and demonstrate how it facilitates practical understanding of black box agents through prediction, visualisation and rule-based explanation. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Autonomous agents; Data mining; Decision trees; Reinforcement learning; Agent architectures; Black boxes; Conceptual spaces; Decision-tree algorithm; Interpretable representation; Performance; Space theory; State-space; Temporal dynamics; Value functions; Deep learning"
"Bewley T., Lawry J., Richards A.","Modelling Agent Policies with Interpretable Imitation Learning","10.1007/978-3-030-73959-1_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105907394&doi=10.1007%2f978-3-030-73959-1_16&partnerID=40&md5=201fe1ec58a2b71cd5c33510db5a9b76","As we deploy autonomous agents in safety-critical domains, it becomes important to develop an understanding of their internal mechanisms and representations. We outline an approach to imitation learning for reverse-engineering black box agent policies in MDP environments, yielding simplified, interpretable models in the form of decision trees. As part of this process, we explicitly model and learn agents’ latent state representations by selecting from a large space of candidate features constructed from the Markov state. We present initial promising results from an implementation in a multi-agent traffic environment. © 2021, Springer Nature Switzerland AG.","Decision tree; Explainable artificial intelligence; Imitation learning; Interpretability; Representation learning; Traffic modelling","Decision trees; Learning systems; Multi agent systems; Reverse engineering; Safety engineering; Black boxes; Imitation learning; Large spaces; Latent state; Markov state; Multi agent; Safety-critical domain; Traffic environment; Autonomous agents"
"Bewley William L., Rosenberg David A.","AIM. An AI-based decision support system",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024631118&partnerID=40&md5=a2312cacd4407a660e9b6fb8f5090382","Decision-makers responsible for allocating resources to perform complex missions require easy access to a variety of databases, support in specifying requirements, assumptions, and constraints, an integrated system combining a variety of existing and new software to generate design options and cost-performance predictions, and assistance in evaluating model results and defining and conducting sensitivity analyses and trade studies. AIM is an example of an AI-based system that provides such support. It is an intelligent decision-support system developed for USAF Space Division space transportation system planners. It provides access to databases of requirements, constraints, and resources, and it supports 'what-if' analysis of alternative space transportation architectures. It integrates AI modules written in Lisp with Fortran costing algorithms. Functions include specification of requirements, resource selection, allocation, and evaluation. A user-friendly MMI supports use by high-level decision-makers. AIM illustrates many of the functions that should be included in a system supporting system acquisition and management of complex engineering projects, including functions appropriate for AI technology. Extensions of the system will be described which suggest additional high-leverage applications of AI technology to modeling and simulation of complex systems.",,"Computer Programming Languages; Computer Simulation; Government Program Manager; High-Level Decision-Makers; Intelligent Decision-Support System; Large Weapons System; Space Transportation; USAF; Artificial Intelligence"
"Beyazit E., Tuncel D., Yuan X., Tzeng N.-F., Wu X.","Learning interpretable representations with informative entanglements",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097339523&partnerID=40&md5=e3c5288d002b8c38005d61ac2527586d","Learning interpretable representations in an unsupervised setting is an important yet a challenging task. Existing unsupervised interpretable methods focus on extracting independent salient features from data. However they miss out the fact that the entanglement of salient features may also be informative. Acknowledging these entanglements can improve the interpretability, resulting in extraction of higher quality and a wider variety of salient features. In this paper, we propose a new method to enable Generative Adversarial Networks (GANs) to discover salient features that may be entangled in an informative manner, instead of extracting only disentangled features. Specifically, we propose a regularizer to punish the disagreement between the extracted feature interactions and a given dependency structure while training. We model these interactions using a Bayesian network, estimate the maximum likelihood parameters and calculate a negative likelihood score to measure the disagreement. Upon qualitatively and quantitatively evaluating the proposed method using both synthetic and real-world datasets, we show that our proposed regularizer guides GANs to learn representations with disentanglement scores competing with the state-of-the-art, while extracting a wider variety of salient features. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.",,"Bayesian networks; Maximum likelihood; Adversarial networks; Dependency structures; Feature interactions; Interpretability; Interpretable representation; Real-world datasets; Salient features; State of the art; Artificial intelligence"
"Beydaghi E., Rahnama M., Nasiri J.A.","Ensemble approach for metadata extraction in Persian theses","10.1109/ICSPIS51611.2020.9349571","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101736436&doi=10.1109%2fICSPIS51611.2020.9349571&partnerID=40&md5=22c392b5982d60b5735f030a61acf846","Today with increasing number of scientific articles, collecting, managing, searching and retrieving of theses are one of the most important tasks that every scientific library or organization responsible for arranging articles, deals with. Information extraction is a controversial topic that is significant for retrieving and managing scientific theses but unfortunately it is mostly concentrated on English ones. Lack of meta-data extraction methods in Persian theses led us to create a system which uses machine learning methods to extract metadata from Persian theses. First, we tried different kinds of classifier algorithm such as SVM, KNN and decision tree to obtain metadata classes. Then, we combined classifiers based on ensemble learning as an examination. According to the results, methods implemented by ensemble techniques achieve higher F1-score than single classifier algorithm. © 2020 IEEE.","Content classification; Ensemble Learning; Information extraction; Metadata extraction; Persian text processing; Scientific literature analysis","Data mining; Decision trees; Intelligent systems; Metadata; Signal processing; Trees (mathematics); Classifier algorithms; Combined classifiers; Controversial topics; Ensemble approaches; Ensemble techniques; Machine learning methods; Meta-data extractions; Scientific articles; Learning systems"
"Beykikhoshk A., Quinn T.P., Lee S.C., Tran T., Venkatesh S.","DeepTRIAGE: Interpretable and individualised biomarker scores using attention mechanism for the classification of breast cancer sub-Types","10.1186/s12920-020-0658-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080944898&doi=10.1186%2fs12920-020-0658-5&partnerID=40&md5=8c116b0012ed9b115d5ffc72e8cd2edb","Background: Breast cancer is a collection of multiple tissue pathologies, each with a distinct molecular signature that correlates with patient prognosis and response to therapy. Accurately differentiating between breast cancer sub-Types is an important part of clinical decision-making. Although this problem has been addressed using machine learning methods in the past, there remains unexplained heterogeneity within the established sub-Types that cannot be resolved by the commonly used classification algorithms. Methods: In this paper, we propose a novel deep learning architecture, called DeepTRIAGE (Deep learning for the TRactable Individualised Analysis of Gene Expression), which uses an attention mechanism to obtain personalised biomarker scores that describe how important each gene is in predicting the cancer sub-Type for each sample. We then perform a principal component analysis of these biomarker scores to visualise the sample heterogeneity, and use a linear model to test whether the major principal axes associate with known clinical phenotypes. Results: Our model not only classifies cancer sub-Types with good accuracy, but simultaneously assigns each patient their own set of interpretable and individualised biomarker scores. These personalised scores describe how important each feature is in the classification of any patient, and can be analysed post-hoc to generate new hypotheses about latent heterogeneity. Conclusions: We apply the DeepTRIAGE framework to classify the gene expression signatures of luminal A and luminal B breast cancer sub-Types, and illustrate its use for genes as well as the GO and KEGG gene sets. Using DeepTRIAGE, we calculate personalised biomarker scores that describe the most important features for classifying an individual patient as luminal A or luminal B. In doing so, DeepTRIAGE simultaneously reveals heterogeneity within the luminal A biomarker scores that significantly associate with tumour stage, placing all luminal samples along a continuum of severity. © 2020 The Author(s).","Breast cancer; Deep learning; Precision medicine; TCGA","Article; breast cancer molecular subtype; cancer classification; cancer staging; clinical decision making; cohort analysis; deep learning; disease severity; gene expression; genetic analysis; genetic association; genetic database; human; luminal A breast cancer; luminal B breast cancer; machine learning; mathematical model; phenotype; principal component analysis; priority journal; tumor suppressor gene; biological model; breast tumor; classification; female; genetics; kinetochore; RNA; transcriptome; tumor marker; Biomarkers, Tumor; Breast Neoplasms; Deep Learning; Female; Humans; Kinetochores; Models, Biological; RNA, Neoplasm; RNA-Seq; Transcriptome"
"Beyret B., Shafti A., Faisal A.A.","Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic Manipulation","10.1109/IROS40897.2019.8968488","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081160172&doi=10.1109%2fIROS40897.2019.8968488&partnerID=40&md5=677471a77505d16326279e0ab24bea72","Robotic systems are ever more capable of automation and fulfilment of complex tasks, particularly with reliance on recent advances in intelligent systems, deep learning and artificial intelligence in general. However, as robots and humans come closer together in their interactions, the matter of interpretability, or explainability of robot decision-making processes for the human grows in importance. A successful interaction and collaboration would only be possible through mutual understanding of underlying representations of the environment and the task at hand. This is currently a challenge in deep learning systems. We present a hierarchical deep reinforcement learning system, consisting of a low-level agent handling the large actions/states space of a robotic system efficiently, by following the directives of a high-level agent which is learning the high-level dynamics of the environment and task. This high-level agent forms a representation of the world and task at hand that is interpretable for a human operator. The method, which we call Dot-to-Dot, is tested on a MuJoCo-based model of the Fetch Robotics Manipulator, as well as a Shadow Hand, to test its performance. Results show efficient learning of complex actions/states spaces by the low-level agent, and an interpretable representation of the task and decision-making process learned by the high-level agent. © 2019 IEEE.",,"Concrete pavements; Decision making; Deep learning; Human robot interaction; Intelligent robots; Intelligent systems; Manipulators; Reinforcement learning; Robotics; Decision making process; Efficient learning; Hierarchical reinforcement learning; High-level dynamics; Interpretability; Interpretable representation; Mutual understanding; Robotic manipulation; Learning systems"
"Bhadriraju B., Kwon J.S.-I., Khan F.","Dynamic risk assessment in chemical process using sparse identification and deep learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106178665&partnerID=40&md5=c89935ab4a11157a3ce4d1a93552be41","The increasing technological advancements in chemical and petrochemical industries require a monitoring system that maintains both product quality and process safety. Moreover, failure in implementing a robust monitoring system can significantly impact the process reliability and lead to human and economic losses. Therefore, to promote the optimal and safe operation of the process, developing reliable fault detection and diagnosis schemes has received significant attention in the last few years. Recently, several model-based techniques have been widely used such as observer-based [1], parity equations [2], and parameter estimation [3] methods. However, these methods require mathematical models that describe the process accurately. Also, most of these methods have been developed for a particular class of nonlinear systems. On the other hand, data-driven approaches such as principal component analysis (PCA) [4] and partial least squares (PLS) [5] have been successfully used for fault detection and diagnosis. One important limitation of these approaches is that the models are trained offline and are not updated on-the-fly. Therefore, they cannot respond to frequently changing process dynamics. Another limitation is that they do not systematically evaluate the potential risk caused by the fault. It is important to have a risk-based fault detection framework that considers the probable hazards and consequences that occur due to a fault. This guides in eliminating the non-hazardous faults and taking appropriate action in the case of severe faults [6]. Specifically, quantifying risk in real-time helps in monitoring and managing operational risk. Motivated by the above considerations, we propose a robust dynamic risk assessment -based fault detection scheme that uses online adaptive sparse identification of systems (OASIS) framework [7]. The OASIS is an adaptive system identification method developed based on sparse identification of nonlinear dynamics (SINDy) [8] and deep learning. The SINDy algorithm solves a sparse regression problem to identify an interpretable and sparse model of the process using the historical data offline. But it is not feasible to directly implement SINDy for process monitoring as it is computationally expensive to solve a sparse regression problem online. Hence, a deep neural network (DNN) is trained to facilitate the applicability of SINDy for online monitoring. For offline training, we consider multiple trajectories of input-output data that represent a wide range of operating conditions and obtain multiple sparse models. Next, we train a DNN using these models identified by SINDy and their corresponding training inputs. Later, the trained DNN is used online to predict and update the process models using measurement data. At every sampling time, we estimate the process states using the model obtained from the DNN. For fault detection, we compute the residuals between model prediction and measurement values. At any time instant, if the evaluated residual exceeds the threshold, a faultis observed in the process. After fault detection, we perform risk assessment by computing the probability and severity of the detected faults. By doing so, we quantify the process risk associated at each sampling time. If the calculated risk exceeds the threshold, the fault detected is regarded to be severe. The proposed OASIS-based dynamic risk assessment method has the following advantages: 1) offering an adaptive framework for fault detection and dynamic risk assessment, 2) applicable to nonlinear systems with uncertain parameters, and 3) providing interpretable models that aid in understanding the relationship between process variables, which is useful in analyzing the propagation of faults. We demonstrate the proposed method for fault identification and risk assessment through the simulation of a floating liquefied natural gas tank and a non-isothermal continuous stirred tank reactor. Copyright © American Institute of Chemical Engineers. All rights reserved.",,"Accident prevention; Adaptive systems; Backpropagation; Deep learning; Deep neural networks; Electronic assessment; Estimation; Floating liquefied natural gas; Hazards; Least squares approximations; Liquefied natural gas; Losses; Monitoring; Nonlinear analysis; Nonlinear systems; Online systems; Petrochemicals; Process monitoring; Risk assessment; Tanks (containers); Uncertainty analysis; Continuous stirred tank reactor; Dynamic risk assessments; Fault detection and diagnosis; Fault detection and diagnosis schemes; Fault detection schemes; Identification of systems; Partial least square (PLS); Technological advancement; Fault detection"
"Bhadriraju B., Bangi M.S.F., Narasingam A., Kwon J.S.-I.","When deep learning meets sparse model identification: Operable adaptive sparse identification of systems (OASIS)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106166581&partnerID=40&md5=1bd120bdd11a01f51fb061d805e91e27","Over the past few decades, nonlinear model-based control is receiving significant attention as linear models are inadequate in describing inherently nonlinear and complex industrial processes. This requires developing methods for nonlinear model identification. Modeling using first-principles is desirable only when there is sufficient knowledge of the process available. For the processes that are complex and poorly understood, it is difficult to derive such models. Inview of this, there has been an increasing interest in data-driven system identification for prediction and control purposes. Specifically, subspace identification methods such as multi-variable output-error-state-space (MOESP) [1],numerical algorithms for subspace state-space identification (N4SID) [2], and canonical variate analysis [3] are popular in the process control domain. Although these methods are successful in identifying state-space models for various industrial applications using input-output data, they do not provide a physical understanding of the process. More importantly, for adaptive modeling applications, it is advantageous to have a model that can provide interpretability of the changing dynamics, which will help guide in evaluating the process operating conditions to take appropriate actions. Lately, sparse identification of nonlinear dynamics (SINDy) has delivered promising results for variousnonlinear processes [4]. The SINDy algorithm is based on the techniques of sparse regression and compressive sensing. It fits the input-output data to a library of candidate functions such that only the functions describing the original dynamics are identified. Thereby, using this approach, a parsimonious and interpretable model is obtained.Additionally, any prior knowledge about the process from, for example, physics and thermodynamic laws can be included in the candidate library to quicken the model identification process. Due to these reasons, there has been agrowing interest in applying SINDy to a variety of applications, which includes identifying rational nonlinearities [5],sparse learning of reaction kinetics [6], reduced-order modeling of a complex process [7], model recovery from abrupt system changes [8], and for control [9]. Despite the simplicity of SINDy algorithm, it is challenging to use SINDy for model-based control, especially at any instance of plant-model mismatch or process upset. This is because re-training the model using SINDy is computationally expensive and cannot guarantee to catch up with rapidly changing dynamics. Hence, we propose online adaptive sparse identification of systems (OASIS) framework that extends the capabilities of SINDy for accurate, automatic, and adaptive approximation of process models. The key novelty is to combine the usefulness of SINDy in discovering an interpretable model with a deep neural network (DNN) to adaptively model and control the process dynamics in real-time. The proposed method is implemented in two steps: system identification and controller design. For the system identification step, we utilize several sets of process historical data that are available for various input settings and identify their corresponding models using SINDy. Next, we train a DNN using the previously collected historical data sets and their respective SINDy models such that the DNN approximates the relationship between process data and SINDy models. We use this trained DNN to design a controller wherein the DNN updates the SINDy model by utilizing a new set of measurements at every sampling time to accurately predict the future process behavior. In this way, the OASIS method supports the application of SINDy for real-time model identification and control. We demonstrate the OASIS methodology on the model identification and control of a continuous stirred tank reactor. The closed-loop results showed that the proposed OASIS framework can be effectively used for adaptive modeling and control of nonlinear processes. Copyright © American Institute of Chemical Engineers. All rights reserved.",,"Complex networks; Control nonlinearities; Controllers; Data reduction; Deep learning; Deep neural networks; Dynamics; Learning systems; Nonlinear systems; Numerical methods; Online systems; Reaction kinetics; State space methods; Canonical variate analysis; Complex industrial process; Continuous stirred tank reactor; Identification of systems; Nonlinear model based control; Nonlinear model identification; Plant model mismatches; Subspace identification methods; Process control"
"Bhagat V., Robins B., Pallavi M.O.","Sparx - Data Preprocessing Module","10.1109/I2CT45611.2019.9033938","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083076067&doi=10.1109%2fI2CT45611.2019.9033938&partnerID=40&md5=6e2d3781d0f9ed067532dee5883b7a75","In the current scenario, there are a steady boom in Machine learning and Deep Learning areas which mainly enables us to build predictive models, CNN - Convolutional Neural Network. etc. But the most common problems that encountered are getting the right data. We have all sophisticated models, which are cutting edge, but with unclean data, it is going to be a real challenge in building an accurate model. Developers are facing the problem of handling unclean data when it is coming from multiple data sources such as Relational Databases, NoSQL, Text API's. Such problems can be solved using a common data-preprocessing platform, which is capable of taking inputs from multiple data-sources and preprocess it into a transformed dataset. Considering this problem current paper is giving a solution to get clean data, which is preprocessed. Data preprocessing is a fundamental stage of data analysis. Irrelevant, redundant or noisy and unreliable information will lead to difficulties in knowledge discovery during data analysis and data mining stage.[14][16] This paper has explained about a library named as Sparx which provides the solution for a programmer to acquire cleaned data for further analysis. Sparx is an exclusive data-preprocessing library, which involves transforming raw data into a machine-understandable format. The intention behind developing Sparx was to build a better, automated data-preprocessing library. © 2019 IEEE.","data-preprocessing; deep learning; machine learning; prediction model; python libraries; Sparx.","Convolutional neural networks; Data mining; Deep learning; Metadata; Accurate modeling; Automated data; Common datum; Cutting edges; Data preprocessing; Multiple data sources; Predictive models; Relational Database; Data handling"
"Bhagavatula C.S., Noraset T., Downey D.","TabEL: Entity linking in web tables","10.1007/978-3-319-25007-6_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952327246&doi=10.1007%2f978-3-319-25007-6_25&partnerID=40&md5=fc95c54e0d8a759de652ecd3946947b7","Web tables form a valuable source of relational data. The Web contains an estimated 154 million HTML tables of relational data, with Wikipedia alone containing 1.6 million high-quality tables. Extracting the semantics of Web tables to produce machine-understandable knowledge has become an active area of research. A key step in extracting the semantics of Web content is entity linking (EL): the task of mapping a phrase in text to its referent entity in a knowledge base (KB). In this paper we present TabEL, a new EL system for Web tables. TabEL differs from previous work by weakening the assumption that the semantics of a table can be mapped to pre-defined types and relations found in the target KB. Instead, TabEL enforces soft constraints in the form of a graphical model that assigns higher likelihood to sets of entities that tend to co-occur in Wikipedia documents and tables. In experiments, TabEL significantly reduces error when compared to current state-of-the-art table EL systems, including a 75% error reduction on Wikipedia tables and a 60% error reduction on Web tables. We also make our parsed Wikipedia table corpus and test datasets publicly available for future work. © Springer International Publishing Switzerland 2015.","Entity linking; Graphical models; Named entity disambiguation; Web tables","Data mining; Errors; Graphic methods; Knowledge based systems; Entity linking; Error reduction; GraphicaL model; Named entity disambiguations; Relational data; Soft constraint; State of the art; Web tables; Semantic Web"
"Bhakat A., Chahar N., Vijayasherly V.","Vehicle Accident Detection Alert System using IoT and Artificial Intelligence","10.1109/ASIANCON51346.2021.9544940","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117610176&doi=10.1109%2fASIANCON51346.2021.9544940&partnerID=40&md5=0d40986ec1071feda9d6595609b38176","As nations around the globe are becoming economically stronger and thus leading to more financially capable citizens, more people now own their personal vehicles. Although the road infrastructure has improved, it is still unable to cope up with the increasing population. With that, more and more road accidents are increasing. According to the Indian government, in 2019 about 151,000 people died in road accidents. In most cases, people die because they were not immediately provided medical assistance because there is no definite system that can do so. As technologies like IoT have advanced, there is now a need to develop a system that can immediately update the responsible authorities with all the relevant data on the occurrence of a road accident. This paper analyses and proposes a way IoT can be used in this regard in a way that can save thousands of lives. Along with IoT, we have incorporated machine learning methods and image processing to accurately identify a road accident. The sensors like accelerometer, gyroscope, camera, etc. provide data to a microprocessor which matches the sensor data with the machine learning model and determines if there is an accident or not and if it is, the device sends the related metrics to the server through the internet. Here, instead of using a central server topology, we have incorporated Edge computing which enables us to process requests faster locally. This further optimizes response time. Once the data is reached to an edge server, it determines the nearest hospitals, police stations by looking at the GPS data and sends a notification to them and to the registered phone number by the user. This way, it becomes a life-saving technology. © 2021 IEEE.","AWS IoT for Edge; deep learning; edge computing; emergency services; GPS sensors; Internet of things; machine learning; microprocessor; Raspberry Pi; road accident","Accidents; Deep learning; Emergency services; Global positioning system; Image processing; Internet of things; Roads and streets; Accident detections; Alert systems; AWS IoT for edge; Deep learning; Edge computing; GPS sensor; Personal vehicles; Raspberry pi; Road infrastructures; Vehicle accidents; Edge computing"
"Bhakte A., Pakkiriswamy V., Srinivasan R.","An explainable artificial intelligence based approach for interpretation of fault classification results from deep neural networks","10.1016/j.ces.2021.117373","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122261466&doi=10.1016%2fj.ces.2021.117373&partnerID=40&md5=8edca8dbc48e8b4469f486bada1dc0bd","Process monitoring is crucial to ensure operational reliability and to prevent industrial accidents. Data-driven methods have become the preferred approach for fault detection and diagnosis. Specifically, deep learning algorithms such as Deep Neural Networks (DNNs) show good potential even in complex processes. A key shortcoming of DNNs is the difficulty in interpreting their classification result. Emerging approaches from explainable Artificial Intelligence (XAI) seek to address this shortcoming. This paper proposes a method based on the Shapley value framework and its implementation using integrated gradients to identify those variables which lead a DNN to classify an input as a fault. The method estimates the marginal contribution of each variable to the DNN, averaged over the path from the baseline (in this case, the process’ normal state) to the current sample. We illustrate the resulting variable attribution using a numerical example and the benchmark Tennessee Eastman process. Our results show that the proposed methodology provides accurate, sample-specific explanations of the DNN's prediction. These can be used by the offline model developer to improve the DNN if necessary. It can also be used by the plant operator in real-time to understand the black-box DNN's predictions and decide on operational strategies. © 2022 Elsevier Ltd","Deep learning; Explainable artificial intelligence; Process monitoring; Shapley value; Tennessee Eastman","Fault detection; Game theory; Industrial hygiene; Learning algorithms; Occupational diseases; Process control; Process monitoring; Classification results; Data-driven methods; Deep learning; Explainable artificial intelligence; Fault classification; Fault detection and diagnosis; Industrial accident; Operational reliability; Shapley value; Tennessee Eastman; Deep neural networks"
"Bhalodia R., Elhabian S., Kavan L., Whitaker R.","Leveraging unsupervised image registration for discovery of landmark shape descriptor","10.1016/j.media.2021.102157","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110417574&doi=10.1016%2fj.media.2021.102157&partnerID=40&md5=09e8b86a6917dd4fc15ed850bff9f6ca","In current biological and medical research, statistical shape modeling (SSM) provides an essential framework for the characterization of anatomy/morphology. Such analysis is often driven by the identification of a relatively small number of geometrically consistent features found across the samples of a population. These features can subsequently provide information about the population shape variation. Dense correspondence models can provide ease of computation and yield an interpretable low-dimensional shape descriptor when followed by dimensionality reduction. However, automatic methods for obtaining such correspondences usually require image segmentation followed by significant preprocessing, which is taxing in terms of both computation as well as human resources. In many cases, the segmentation and subsequent processing require manual guidance and anatomy specific domain expertise. This paper proposes a self-supervised deep learning approach for discovering landmarks from images that can directly be used as a shape descriptor for subsequent analysis. We use landmark-driven image registration as the primary task to force the neural network to discover landmarks that register the images well. We also propose a regularization term that allows for robust optimization of the neural network and ensures that the landmarks uniformly span the image domain. The proposed method circumvents segmentation and preprocessing and directly produces a usable shape descriptor using just 2D or 3D images. In addition, we also propose two variants on the training loss function that allows for prior shape information to be integrated into the model. We apply this framework on several 2D and 3D datasets to obtain their shape descriptors. We analyze these shape descriptors in their efficacy of capturing shape information by performing different shape-driven applications depending on the data ranging from shape clustering to severity prediction to outcome diagnosis. © 2021 Elsevier B.V.","Image registration; Machine learning; Self-supervised learning; Statistical shape modeling","Deep learning; Diagnosis; Dimensionality reduction; Image registration; Neural networks; Object recognition; Optimization; Automatic method; Dense correspondences; Learning approach; Regularization terms; Robust optimization; Shape descriptors; Shape information; Statistical shape model; Image segmentation; article; comparative effectiveness; deep learning; dimensionality reduction; human; human experiment; image registration; image segmentation; loss of function mutation; prediction; supervised machine learning; writing; statistical model; three-dimensional imaging; Humans; Imaging, Three-Dimensional; Models, Statistical; Neural Networks, Computer"
"Bhambra P., Joachimi B., Lahav O.","Explaining deep learning of galaxy morphology with saliency mapping","10.1093/mnras/stac368","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126826172&doi=10.1093%2fmnras%2fstac368&partnerID=40&md5=5df9e8cc29f0aaae75e0d6f419ffbcd6","We successfully demonstrate the use of explainable artificial intelligence (XAI) techniques on astronomical data sets in the context of measuring galactic bar lengths. The method consists of training convolutional neural networks on human classified data from Galaxy Zoo in order to predict general galaxy morphologies, and then using SmoothGrad (a saliency mapping technique) to extract the bar for measurement by a bespoke algorithm. We contrast this to another method of using a convolutional neural network to directly predict galaxy bar lengths. These methods achieved correlation coefficients of 0.76 and 0.59, and root mean squared errors of 1.69 and 2.10 respective to human measurements. We conclude that XAI methods outperform conventional deep learning in this case, which could be reasonably explained by the larger data sets available when training the models. We suggest that our XAI method can be used to extract other galactic features (such as the bulge-to-disc ratio) without needing to collect new data sets or train new models. We also suggest that these techniques can be used to refine deep learning models as well as identify and eliminate bias within training data sets. © 2022 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society.","Galaxies: bar; Galaxies: general; Methods: data analysis; Techniques: image processing","Convolution; Convolutional neural networks; Correlation methods; Data mining; Deep learning; Image processing; Mapping; Mean square error; Morphology; Astronomical data; Classifieds; Convolutional neural network; Data set; Galaxies general; Galaxy: bar; General - galaxies; Mapping techniques; Methods. Data analysis; Techniques: image processing; Galaxies"
"Bhandari M., Shahi T.B., Siku B., Neupane A.","Explanatory classification of CXR images into COVID-19, Pneumonia and Tuberculosis using deep learning and XAI","10.1016/j.compbiomed.2022.106156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139593998&doi=10.1016%2fj.compbiomed.2022.106156&partnerID=40&md5=115f2438f85d61b0a874e230e8542957","Chest X-ray (CXR) images are considered useful to monitor and investigate a variety of pulmonary disorders such as COVID-19, Pneumonia, and Tuberculosis (TB). With recent technological advancements, such diseases may now be recognized more precisely using computer-assisted diagnostics. Without compromising the classification accuracy and better feature extraction, deep learning (DL) model to predict four different categories is proposed in this study. The proposed model is validated with publicly available datasets of 7132 chest x-ray (CXR) images. Furthermore, results are interpreted and explained using Gradient-weighted Class Activation Mapping (Grad-CAM), Local Interpretable Modelagnostic Explanation (LIME), and SHapley Additive exPlanation (SHAP) for better understandably. Initially, convolution features are extracted to collect high-level object-based information. Next, shapely values from SHAP, predictability results from LIME, and heatmap from Grad-CAM are used to explore the black-box approach of the DL model, achieving average test accuracy of 94.31 ± 1.01% and validation accuracy of 94.54 ± 1.33 for 10-fold cross validation. Finally, in order to validate the model and qualify medical risk, medical sensations of classification are taken to consolidate the explanations generated from the eXplainable Artificial Intelligence (XAI) framework. The results suggest that XAI and DL models give clinicians/medical professionals persuasive and coherent conclusions related to the detection and categorization of COVID-19, Pneumonia, and TB. © 2022 Elsevier Ltd","COVID-19; Deep learning; eXplainable AI; Grad-CAM; LIME; Pneumonia; SHAP; Tuberculosis","Cams; Deep learning; Diagnosis; Image classification; Learning systems; Lime; Activation mapping; Chest X-ray image; Deep learning; Explainable AI; Gradient-weighted class activation mapping; Local interpretable modelagnostic explanation; Pneumonia; Shapley; Shapley additive explanation; Tuberculosis; COVID-19; accuracy; Article; artificial intelligence; comparative study; coronavirus disease 2019; cross validation; deep learning; human; image analysis; pneumonia; sensitivity and specificity; thorax radiography; transfer of learning; tuberculosis"
"Bhandari N., Khare S., Walambe R., Kotecha K.","Comparison of machine learning and deep learning techniques in promoter prediction across diverse species","10.7717/PEERJ-CS.365","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101606315&doi=10.7717%2fPEERJ-CS.365&partnerID=40&md5=56d65d172d8f1e9eb1e0ebacec3d50e6","Gene promoters are the key DNA regulatory elements positioned around the transcription start sites and are responsible for regulating gene transcription process. Various alignment-based, signal-based and content-based approaches are reported for the prediction of promoters. However, since all promoter sequences do not show explicit features, the prediction performance of these techniques is poor. Therefore, many machine learning and deep learning models have been proposed for promoter prediction. In this work, we studied methods for vector encoding and promoter classification using genome sequences of three distinct higher eukaryotes viz. yeast (Saccharomyces cerevisiae), A. thaliana (plant) and human (Homo sapiens). We compared one-hot vector encoding method with frequency-based tokenization (FBT) for data pre-processing on 1-D Convolutional Neural Network (CNN) model. We found that FBT gives a shorter input dimension reducing the training time without affecting the sensitivity and specificity of classification. We employed the deep learning techniques, mainly CNN and recurrent neural network with Long Short Term Memory (LSTM) and random forest (RF) classifier for promoter classification at k-mer sizes of 2, 4 and 8. We found CNN to be superior in classification of promoters from non-promoter sequences (binary classification) as well as species-specific classification of promoter sequences (multiclass classification). In summary, the contribution of this work lies in the use of synthetic shuffled negative dataset and frequency-based tokenization for pre-processing. This study provides a comprehensive and generic framework for classification tasks in genomic applications and can be extended to various classification problems. © Copyright 2021 Bhandari et al.","CNN; Deep Learning; Frequency-based Tokenization; LSTM; Machine Learning; One-hot Encoding; Promoter Prediction; Random Forest","Binary sequences; Classification (of information); Convolutional neural networks; Decision trees; Encoding (symbols); Forecasting; Learning systems; Long short-term memory; Predictive analytics; Signal encoding; Transcription; Yeast; Binary classification; Classification tasks; Content-based approach; Genomic applications; Multi-class classification; Prediction performance; Sensitivity and specificity; Transcription start site; Deep learning"
"Bhanot G., Biehl M., Villmann T., Zühlke D.","Biomedical data analysis in translational research: Integration of expert knowledge and interpretable models",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061054591&partnerID=40&md5=d6893011d7e6ccaf607a43a99f336399","In various fields of biomedical research, the availability of electronic data has increased tremendously. Not only is the amount of disease specific data increasing, but so is its structural complexity in terms of dimensionality, multi-modality and inhomogeneity. Consequently, there is an urgent need for better coordination between bio-medical and computational researchers in order to make an impact on patient care. In any such effort, the integration of expert knowledge is essential. A careful synthesis of good analytical techniques applied to relevant medical questions would make the analysis both accurate and interpretable and facilitate transdisciplinary collaboration. This article summarizes recent challenges and introduces the contributions to this ESANN special session. © ESANN 2017 - Proceedings, 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. All rights reserved.",,"Neural networks; Biomedical data analysis; Biomedical research; Electronic data; Expert knowledge; Inhomogeneities; Multi modality; Structural complexity; Translational Research; Machine learning"
"Bhanu B., Krawiec K.","Coevolving Feature Extraction Agents for Target Recognition in SAR Images","10.1117/12.487539","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344514135&doi=10.1117%2f12.487539&partnerID=40&md5=295b496b6b871a997532af1432d9f2f0","This paper describes a novel evolutionary method for automatic induction of target recognition procedures from examples. The learning process starts with training data containing SAR images with labeled targets and consists in coevolving the population of feature extraction agents (individuals) that cooperate to build an appropriate representation of an input image. Features extracted by a team of cooperating agents are used to induce a machine learning classifier that is responsible for making the final decision of recognizing a target. Each agent contains feature extraction procedure encoded according to the principles of linear genetic programming (LGP) where an individual's genome encodes a program that is executed and tested on the training set of images for fitness calculation. The program is a sequence of calls to the library of parameterized operations, including, but not limited to, image processing, simple feature extraction, and arithmetic and logic operations. Particular calls operate on working variables that enable the program to store intermediate results and therefore design complex features. We report the results obtained when testing the proposed approach on a SAR target recognition task using the MSTAR database.","Automatic target recognition; Cooperative coevolution; Evolutionary computation; Feature synthesis; Learning in computer vision","Automatic target recognition; Computer vision; Database systems; Feature extraction; Image processing; Linear programming; Radar target recognition; Feature synthesis; Synthetic aperture radar"
"Bhanu M., Priya S., Moreira J.M., Chandra J.","ST-AGP: Spatio-Temporal aggregator predictor model for multi-step taxi-demand prediction in cities","10.1007/s10489-022-03475-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129439292&doi=10.1007%2fs10489-022-03475-7&partnerID=40&md5=edda5a83f95687ebf52374b7de031b03","Taxi demand prediction in a city is a highly demanded smart city research application for better traffic strategies formulation. It is essential for the interest of the commuters and the taxi companies both to have an accurate measure of taxi demands at different regions of a city and at varying time intervals. This reduces the cost of resources, efforts and meets the customers’ satisfaction at its best. Modern predictive models have shown the potency of Deep Neural Networks (DNN) in this domain over any traditional, statistical, or Tensor-Based predictive models in terms of accuracy. The recent DNN models using leading technologies like Convolution Neural Networks (CNN), Graph Convolution Networks (GCN), ConvLSTM, etc. are not able to efficiently capture the existing spatio-temporal characteristics in taxi demand time-series. The feature aggregation techniques in these models lack channeling and uniqueness causing less distinctive but overlapping feature space which results in a compromised prediction performance having high error propagation possibility. The present work introduces Spatio-Temporal Aggregator Predictor (ST-AGP), a DNN model which aggregates spatio-temporal features into (1) non-redundant and (2) highly distinctive feature space and in turn helps (3) reduce noise propagation for a high performing multi-step predictive model. The proposed model integrates the effective feature engineering techniques of machine learning approach with the non-linear capability of a DNN model. Consequently, the proposed model is able to use only the informative features responsible for the objective task with reduce noise propagation. Unlike, existing DNN models, ST-AGP is able to induce these qualities of feature aggregation without the use of Multi-Task Learning (MTL) approach or any additional supervised attention that existing models need for their notable performance. A considerable high-performance gain of 25 − 37% on two real-world city taxi datasets by ST-AGP over the state-of-art models on standard benchmark metrics establishes the efficacy of the proposed model over the existing ones. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Origin-destination tensor; Prediction; Spatio-temporal; Taxi-demand","Backpropagation; Benchmarking; Convolution; Customer satisfaction; Deep neural networks; Taxicabs; Tensors; Demand prediction; Feature aggregation; Feature space; Multisteps; Neural network model; Origin destination; Origin-destination tensor; Predictive models; Spatio-temporal; Taxi-demand; Forecasting"
"Bharadhwaj H.","Layer-Wise Relevance Propagation for Explainable Deep Learning Based Speech Recognition","10.1109/ISSPIT.2018.8642691","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063469350&doi=10.1109%2fISSPIT.2018.8642691&partnerID=40&md5=e9483afb47711f34e6f2c960e116532c","We develop a framework for incorporating explanations in a deep learning based speech recognition model. The most cited criticism against deep learning based methods across domains is the non-interpretability of the model. This means that the model in itself provides very less or no insight into which features of the input are most responsible for the model's predictions, Layer-wise relevance propagation is an emerging technique for explaining the predictions of deep neural networks. It has shown great success in computer vision applications, but to the best of our knowledge there has been no application of its use in a speech-recognition setup. In this paper, we develop a bi-directional GRU based speech recognition model in such a way that layer-wise relevance propagation can be suitably applied to explain the recognition task. We show through simulation results that the benefit of explainability does not compromise on the model accuracy of speech recognition. © 2018 IEEE.","Bi-directional GRU; Explainable Deep Learning; Layer-wise relevance propagation; Speech Recognition","Deep neural networks; Signal processing; Speech; Bi-directional; Computer vision applications; Interpretability; Layer-wise; Learning-based methods; Model accuracy; Show through; Speech recognition"
"Bharadhwaj H., Joshi S.","Explanations for Temporal Recommendations","10.1007/s13218-018-0560-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063481561&doi=10.1007%2fs13218-018-0560-x&partnerID=40&md5=5b6fdf6ab661584bcaed64fe03ab1183","Recommendation systems (RS) are an integral part of artificial intelligence (AI) and have become increasingly important in the growing age of commercialization in AI. Deep learning (DL) techniques for RS provide powerful latent-feature models for effective recommendation but suffer from the major drawback of being non-interpretable. In this paper we describe a framework for explainable temporal recommendations in a DL model. We consider an LSTM based Recurrent Neural Network architecture for recommendation and a neighbourhood based scheme for generating explanations in the model. We demonstrate the effectiveness of our approach through experiments on the Netflix dataset by jointly optimizing for both prediction accuracy and explainability. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.","Explainable AI; Recommendation systems; Recurrent Neural Networks","Long short-term memory; Network architecture; Commercialisation; Explainable artificial intelligence; Feature models; Integral part; Learning models; Learning techniques; Neighbourhood; Netflix; Prediction accuracy; Recurrent neural network architectures; Recommender systems"
"Bharadwaj P., Li M., Demanet L.","SymAE: An autoencoder with embedded physical symmetries for passive time-lapse monitoring","10.1190/segam2020-3423931.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102549933&doi=10.1190%2fsegam2020-3423931.1&partnerID=40&md5=53c6427a169f6f5552444716d644c769","We introduce SymAE, an auto-encoder architecture that learns to separate multichannel passive-seismic datasets into qualitatively interpretable components: one component corresponds to path-specific effects associated with subsurface properties while the other component corresponds to the spectral signature of the passive sources. This information is represented by two latent codes produced by our encoder. The novelty that enables SymAE to achieve this separation lies with the physical symmetries that are directly embedded into the architectural design of the encoder. These symmetries impose that 1. The output of the source-specific encoder is indifferent to the ordering of the receivers; and 2. the output of the path-specific encoder is indifferent to the source signatures. Our numerical experiments demonstrate that this is sufficient for achieving the intended separation. The ability to qualitatively distinguish between source- and path induced effects plays a critical role for time-lapse monitoring of visco-acoustic subsurface models where data is generated from induced passive seismic sources e.g., during CO2 injection or hydraulic fracturing. Here the problem suffers from inherent ambiguities in whether the time-lapse changes in the data should be attributed to subsurface changes such as P-wave velocity, mass density, and seismic quality factor (i.e., path effects) or because of difficulties in physically-reproducing the source wavelet (i.e. source effects). SymAE resolves these ambiguities by construction and enables reliable subsurface monitoring in these settings. We provide numerical results to show that we can accurately detect changes arising from both effects. © 2020 Society of Exploration Geophysicists.","Attenuation; Deconvolution; Machine learning; Passive imaging; Time-lapse","Machine learning; Seismic waves; Separation; Signal encoding; Source separation; Wave propagation; Attenuation; Auto encoders; Deconvolutions; Encoder architecture; Learn+; Multi channel; Passive imaging; Specific effects; Time-lapse; Time-lapse monitoring; Seismology"
"Bharadwaj Y.S.S.","Advanced Deep Learning Techniques","10.1007/978-3-030-66519-7_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111623964&doi=10.1007%2f978-3-030-66519-7_6&partnerID=40&md5=44b7e6849ff7eec9e48dd9d64ce26bc4","Artificial intelligence (AI) has been a buzz word for quite a long time now; the advancements in this field have reached its way to our pockets in form of smartphones. Google Lens, Siri, Alexa, and many other AI assistances had become part of our lives now. The key features like identifying an image or text is a necessity for such programs; in fact many investments are being poured in building better models for image recognition and contextual speed recognition through neural networks. In this chapter, the architectures of various neural networks are explored. Fundamentally, convolution neural networks aka ConvNets or CNN and recurrent neural networks or RNN are explained with a few examples and their implementation in Python. Intuitive explanation with easily understandable mathematical interpretation can be seen in this chapter. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Artificial intelligence; Autoencoders; CNN; DCGAN; Deep learning; GRU; LSTM; Python; RNN; TensorFlow","Character recognition; Convolutional neural networks; Image recognition; Convolution neural network; In-buildings; Key feature; Learning techniques; Recurrent neural networks"
"Bharath Raj N., Subramanian A., Ravichandran K., Venkateswaran N.","Exploring Techniques to Improve Activity Recognition using Human Pose Skeletons","10.1109/WACVW50321.2020.9096918","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085928696&doi=10.1109%2fWACVW50321.2020.9096918&partnerID=40&md5=b6cdaa3ea4113cfa54e640330017eea0","Human pose skeletons provide an explainable representation of the orientation of a person. Neural network architectures such as OpenPose can estimate the 2D human pose skeletons of people present in an image with good accuracy. Naturally, the human pose is a very attractive choice as a representation for building systems aimed at human activity recognition. However, raw pose keypoint representations suffer from various problems such as variance to translation and scale of the input images. Keypoints are also often missed by the pose estimation framework. These, and other factors lead to poor generalization and learning of networks that may be trained directly on these raw representations. This paper introduces various methods aimed at building a robust representation for training models related to activity recognition tasks, such as the usage of handcrafted features extracted from poses with the intent of introducing scale and translation invariance. Additionally, the usage of train-time techniques such as keypoint dropout are explored to facilitate better learning of models. Finally, we conduct an ablation study comparing the performance of deep learning models trained on raw keypoint representation and handcrafted features whilst incorporating our train-time techniques to quantify the effectiveness of our introduced methods over raw representations. © 2020 IEEE.",,"Computer vision; Deep learning; Musculoskeletal system; Network architecture; Neural networks; Activity recognition; Building systems; Human activity recognition; Input image; Learning models; Pose estimation; Training model; Translation invariance; Learning systems"
"Bharathi M., Wang Z., Guo B., Balraj B., Li Q., Shuai J., Guo D.","Memristors: Understanding, Utilization and Upgradation for Neuromorphic Computing","10.1142/S1793292020300054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096174859&doi=10.1142%2fS1793292020300054&partnerID=40&md5=78b73a9f0ac4e2b559d71153c40704c2","The next generation of artificial intelligence systems is generally governed by a new electronic element called memristor. Memristor-based computational system is responsible for confronting memory wall issues in conventional system architecture in the big data era. Complementary Metal Oxide Semiconductor (CMOS) compatibility, nonvolatility and scalability are the important properties of memristor for designing such computing architecture. However, some of the concerns, such as analogue switching and stochasticity, need to be addressed for the use of memristor in novel architecture. Here, we reviewed a number of important scientific works on memristor materials, electrical performance and their integration. In addition, strategies to address the challenges of memristor integration in neuromorphic computing are also being investigated. © 2020 World Scientific Publishing Company.","in-memory computing; Memristor; neuromorphic computing; synapse plasticity; von Neumann architecture","Artificial intelligence; CMOS integrated circuits; Computer architecture; Metals; MOS devices; Oxide semiconductors; Artificial intelligence systems; Complementary metal oxide semiconductors; Computational system; Computing architecture; Conventional systems; Electrical performance; Electronic elements; Neuromorphic computing; Memristors"
"Bharati V.","An Efficient Edge Deep Learning Computer Vision System to Prevent Sudden Infant Death Syndrome","10.1109/SMARTCOMP52413.2021.00061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117613320&doi=10.1109%2fSMARTCOMP52413.2021.00061&partnerID=40&md5=61b7e15dc6ce105b573a33cf03830707","Sudden Infant Death Syndrome (SIDS) causes infants under one year of age to die inexplicably. One of the most important external factors, also called an ""outside stressor,""that is responsible for Sudden Infant Death Syndrome (SIDS), is the sleeping position of the baby. According to past research, the risk of SIDS increases when the baby sleeps facedown on the stomach. We propose a Convolutional Neural Network (CNN) based computer-vision system that estimates the sleeping pose of the baby and alerts caregivers on their mobile phones within a few seconds of the baby moving to the hazardous facedown sleeping position. The system has a low computational load and a low memory footprint. This characteristic allows the system to be embedded in low power edge devices such as certain baby monitors. Processing at the edge also alleviates privacy concerns with regards to sending images into the network. We experimented with various numbers of convolutional processing units and dense layers as well as the number of convolutional kernels to arrive at the optimal production configuration. We observed a consistently high accuracy of detection of infant sleeping position changes from turning to facedown positions with a potential towards even higher accuracies with caregiver feedback for model retraining. Therefore, this system is a viable candidate for consideration as a non-intrusive solution to assist in preventing SIDS. © 2021 IEEE.","convolutional neural network; edge deep learning; image classification; posture estimation; SIDS; Sudden Infant Death Syndrome","Computer vision; Convolution; Deep learning; Image classification; Sleep research; Computer vision system; Convolutional neural network; Edge deep learning; External factors; High-accuracy; Images classification; Network-based; Posture estimation; Sudden infant death syndromes; Convolutional neural networks"
"Bhardwaj A., Iyer S., Jalan Y., Subramanian L.","Learning Pollution Maps from Mobile Phone Images",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137856620&partnerID=40&md5=0458fa84c6980bc3ff491d49ef5c0801","Air pollution monitoring and management is one of the key challenges for urban sectors, especially in developing countries. Measuring pollution levels requires significant investment in reliable and durable instrumentation and subsequent maintenance. On the other hand, there have been many attempts by researchers to develop image-based pollution measurement models, which have shown significant results and established the feasibility of the idea. But, taking image-level models to a city-level system presents new challenges, which include scarcity of high-quality annotated data and a high amount of label noise. In this paper, we present a low-cost, end-to-end system for learning pollution maps using images captured through a mobile phone. We demonstrate our system for parts of New Delhi and Ghaziabad. We use transfer learning to overcome the problem of data scarcity. We investigate the effects of label noise in detail and introduce the metric of in-interval accuracy to evaluate our models in presence of noise. We use distributed averaging to learn pollution maps and mitigate the effects of noise to some extent. We also develop haze-based interpretable models which have comparable performance to mainstream models. With only 382 images from Delhi and Ghaziabad and single-scene dataset from Beijing and Shanghai, we are able to achieve a mean absolute error of 44 µg/m3in PM2.5 concentration on a test set of 267 images and an in-interval accuracy of 67% on predictions. Going further, we learn pollution maps with a mean absolute error as low as 35 µg/m3and in-interval accuracy as high as 74% significantly mitigating the image models' error. We also show that the noise in pollution labels emerging from unreliable sensing instrumentation forms a significant barrier to the realization of an ideal air pollution monitoring system. Our code-base can be found at https://github.com/ankitbha/pollution with images. © 2022 International Joint Conferences on Artificial Intelligence. All rights reserved.",,"Air pollution; Artificial intelligence; Developing countries; Statistical tests; Air pollution monitoring; Image-based; Learn+; Management IS; Mean absolute error; Measurement model; Monitoring and management; Pollution level; Pollution management; Pollution measurement; Cellular telephones"
"Bhardwaj A., Khanna P., Kumar S., Pragya","Generative Model for NLP Applications based on Component Extraction","10.1016/j.procs.2020.03.391","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084473201&doi=10.1016%2fj.procs.2020.03.391&partnerID=40&md5=af58b306b1d4ad8189c8cd793e4965eb","People all around the world speak so many different languages, but a Computer System or any other Computerized Machine only understands a single language i.e. binary language (1s and 0s) This system or a process that converts human language to computer understandable language is known as Natural Language Processing (NLP), though various diversified models have suggested so far, yet the need for a generative predictive model which can optimize depending upon the nature of problem being addressed is still an area of research under work. The paper presents a Generative Model for NLP Applications based on significant components extracted from Case Studies. The generative model is a single platform for diversified areas of NLP that can address specific problems relating to read text, hear speech, interpret it, measure sentiment and determine which parts are important. This is achieved by process of elimination once the relevant components are identified. Single platform provides same model generating and reproducing optimized solutions and addressing different issues. © 2020 The Authors. Published by Elsevier B.V.","Behavior signal processing; Natural Language Processing; Requirement Processing; Stroke based classification","Artificial intelligence; Binary languages; Component extraction; Generative model; NAtural language processing; Optimized solutions; Predictive modeling; Relevant components; Specific problems; Natural language processing systems"
"Bhardwaj P., Kelleher J., Costabello L., O'Sullivan D.","Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127393463&partnerID=40&md5=d242936c9234d2b8297eaedfff53d1a6","Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the security vulnerabilities that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training time to cause model failure at test time. To select adversarial deletions, we propose to use the model-agnostic instance attribution methods from Interpretable Machine Learning, which identify the training instances that are most influential to a neural model's predictions on test instances. We use these influential triples as adversarial deletions. We further propose a heuristic method to replace one of the two entities in each influential triple to generate adversarial additions. Our experiments show that the proposed strategies outperform the state-of-art data poisoning attacks on KGE models and improve the MRR degradation due to the attacks by up to 62% over the baselines. © 2021 Association for Computational Linguistics",,"Computational linguistics; Heuristic methods; Knowledge graph; Graph embeddings; Knowledge graphs; Link prediction; Machine-learning; Modeling failures; Neural modelling; Poisoning attacks; Security vulnerabilities; Test time; Training time; Graph embeddings"
"Bhardwaj R., Datta D.","Consensus Algorithm","10.1007/978-3-030-38677-1_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096483007&doi=10.1007%2f978-3-030-38677-1_5&partnerID=40&md5=ebf271c934415eb9f61e7339252ebaac","Consensus algorithm in general is framed as a decision-making process where a group of people express their individual opinions to construct the decision which provides a best estimate of a process or system. Each member of the group expresses their opinion to support the decisions taken for a course of action. In simple terms, it is just a method to decide any event to occur within a group. Every one present in the group can suggest an idea, but the majority will be in favor of the one that helps them the most. Others have to deal with this decision whether they liked it or not. Byzantine Fault Tolerance (BFT), a problem of Byzantine General, is a system with a particular event of failure. One can experience best the aforementioned situation (BFT) with a distributed computer system. Many times, there can be malfunctioning consensus systems. These components are responsible for the further conflicting information. Consensus systems can only work successfully if all the elements work in harmony. However, if even one of the components in this system malfunctions the whole system could break down. These Blockchain consensus models are just the way to reach an agreement. However, there can’t be any decentralized system without common consensus algorithms. It won’t even matter whether the nodes trust each other or not. They will have to go by certain principles and reach a collective agreement. In order to do that, it is required to check out all the Consensus algorithms. It can be stated that versatility of blockchain networks is due to consensus algorithms. However, blockchain consensus algorithm may have pros and cons which can always alter the perfection of the algorithm. © 2020, Springer Nature Switzerland AG.","Artificial Intelligence (AI); Blockchain; Cognitive Intelligence (CI); Consensus Algorithm (CA); Human Intelligence (HI); Peer-to-peer network (P2P)","Behavioral research; Decision making; Distributed computer systems; Fault tolerance; Peer to peer networks; Artificial intelligence; Block-chain; Cognitive intelligence; Consensus algorithm; Consensus algorithms; Human intelligence; Network P2P; Peer-to-peer network (P2P); Peer-to-peer networks; Blockchain"
"Bhargava D., Gupta L.K.","Explainable AI in Neural Networks Using Shapley Values","10.1007/978-981-19-1476-8_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128198881&doi=10.1007%2f978-981-19-1476-8_5&partnerID=40&md5=9aaa46fc66a7dfc9a31f5e6c66f0ca0c","Deep neural networks (DNNs) often outperform other machine learning models in terms of accuracy. However, due to the complex structure of deep neural networks, it is difficult to comprehend and trust the decisions made by them. Even though DNNs are highly accurate, they cannot be adopted easily in domains such as medical and finance, where trust and transparency are non-negotiable. Explainable Artificial Intelligence (XAI) is the field of study in AI to understand the machine learning models and to interpret their decisions. In this paper, we propose feature attribution-based explainable methods to interpret deep neural networks using a game theory concept known as Shapley values. We also discuss how to introduce interpretability in existing deep learning model systems non-intrusively, making the transition from “black box” to interpretable deep neural networks. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Explainable AI; Feature attribution; Interpretable deep learning; Shapley values",
"Bhargavi K., Shiva S.G.","Man-in-The-Middle attack Explainer for Fog computing using Soft Actor Critic Q-Learning Approach","10.1109/AIIoT54504.2022.9817151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134878564&doi=10.1109%2fAIIoT54504.2022.9817151&partnerID=40&md5=0465b9063581f8751bdbf30404fde586","Because of exponential growth in the availability of large number of Internet of Things (IoT) devices there is an increase in the latency of IoT applications that is managed by performing computation on edge devices/fog nodes. Man-in-The-Middle (MitM) attack is very much common in fog computing as the Fog computing architecture is vulnerable to MitM attack because of the positioning of fog nodes between cloud and end devices. Several machine learning approaches are designed and developed in literature for detection of MitM attacks in fog computing but they lack interpretability/explainability feature. In this paper a novel interpretable Q-learning algorithm with soft actor critic approach is designed for detecting MitM attacks in Fog computing with proper reasoning. Entropy regularized reinforcement learning is performed at each time step which prevents the loss during of every Q-function during approximation of the target. The attack detection policies formulated are of high quality as it satisfies the quality assurance metrics of robustness and correctness the conduct of the proposed interpretable Q-learning framework is encouraging towards the metrics like latency, attack detection time, energy consumption, and accuracy. copy; 2022 IEEE. © 2022 IEEE.","Explainer; Fog computing; Man-in-The-Middle; Q-learning; Soft actor critic","Computer architecture; Energy utilization; Fog; Internet of things; Learning algorithms; Quality assurance; Reinforcement learning; Actor critic; Attack detection; Computing architecture; End-devices; Explainer; Exponential growth; Man in the middle; Q-learning; Q-learning approach; Soft actor critic; Fog computing"
"Bhargavi R., Dayal H.S., Sankpal K.","Emotion Classification Using Single-Channel EEG",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085501789&partnerID=40&md5=d039233fcf333e2613337a8f605c155d","The discovery of EEG signal in 1929 by the German psychiatrist Hans Berger changed the way we understand the structure and functioning of brain.The interaction between a machine and computer is increasing day by day and the need of the hour is to develop a Brain Computer Interface(BCI) which can help the humanity.EEG Emotion recognition system can be used to predict the emotions felt by disabled people.Studies have found that corticolimbic Theta electroencephalographic (EEG) oscillation is responsible for the emotions that one feels. Alpha, Theta, Beta and Delta sub-bands of EEG play a major role in brains emotion processing. The goal of this study is to identify emotions from an EEG data collected from a Single Channel EEG headset.13 subjects of varying age participated in the EEG experiment which were shown videos that helped in evoking three emotional states: neutral, calm and fear. After each video the participants were asked to rate the video on the basis of SAM Model and Valence-Arousal scale.The method used is based on Digital Signal Processing Techniques in order to remove arte-facts,clubbed with machine learning in order to design a system for predicting emotions using EEG signal.Stationary Wavelet Transform (SWT) with haar wavelet at level 6 decomposition with Garrote Thresholding is used to clean the signal and remove the noise. Higuchi Fractal Dimension is also calculated and added as one of the features and is found to have increase the classification accuracy due to its ability to identify the patterns from the data.Experimental results show that EEG based emotion classification can predict emotions with an average of 76% in case of pure EEG signal and 85% in case of EEG signals with Valence-Arousal scale using Recurrent Neural Networks. © 2019 Galgotias University.","Deep Neural Network; Higuchi Fractal Dimension; Recurrent Neural Network; Savitzky Golay Filter; Short time Fourier Transform; Statinary Wavelet Transform; Support Vector Machine","Brain computer interface; Classification (of information); Digital signal processing; Electroencephalography; Forecasting; Fractal dimension; Recurrent neural networks; Wavelet decomposition; Classification accuracy; Digital signal processing techniques; Electroencephalographic (EEG); Emotion classification; Emotion processing; Emotion recognition; Higuchi fractal dimension; Single channel eeg; Biomedical signal processing"
"Bhasuran B., Subramanian D., Natarajan J.","Text mining and network analysis to find functional associations of genes in high altitude diseases","10.1016/j.compbiolchem.2018.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046740572&doi=10.1016%2fj.compbiolchem.2018.05.002&partnerID=40&md5=b9094b5dfddce1e1fdd1b9174f48dbb1","Background and objectives: Travel to elevations above 2500 m is associated with the risk of developing one or more forms of acute altitude illness such as acute mountain sickness (AMS), high altitude cerebral edema (HACE) or high altitude pulmonary edema (HAPE). Our work aims to identify the functional association of genes involved in high altitude diseases. Method: In this work we identified the gene networks responsible for high altitude diseases by using the principle of gene co-occurrence statistics from literature and network analysis. First, we mined the literature data from PubMed on high-altitude diseases, and extracted the co-occurring gene pairs. Next, based on their co-occurrence frequency, gene pairs were ranked. Finally, a gene association network was created using statistical measures to explore potential relationships. Results: Network analysis results revealed that EPO, ACE, IL6 and TNF are the top five genes that were found to co-occur with 20 or more genes, while the association between EPAS1 and EGLN1 genes is strongly substantiated. Conclusion: The network constructed from this study proposes a large number of genes that work in-toto in high altitude conditions. Overall, the result provides a good reference for further study of the genetic relationships in high altitude diseases. © 2018 Elsevier Ltd","Gene co-occurrence; High altitude diseases; Network analysis; Text mining","Data mining; Diseases; Electric network analysis; Acute mountain sickness; Co-occurrence; Co-occurrence statistics; Functional associations; Gene associations; Genetic relationships; Statistical measures; Text mining; Genes; altitude disease; brain edema; data mining; gene regulatory network; genetics; human; pulmonary hypertension; Altitude Sickness; Brain Edema; Data Mining; Gene Regulatory Networks; Humans; Hypertension, Pulmonary"
"Bhat H.S.","Learning and interpreting potentials for classical hamiltonian systems","10.1007/978-3-030-43823-4_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083737960&doi=10.1007%2f978-3-030-43823-4_19&partnerID=40&md5=ca97950012b718020e5d839efcd5d21a","We consider the problem of learning an interpretable potential energy function from a Hamiltonian system’s trajectories. We address this problem for classical, separable Hamiltonian systems. Our approach first constructs a neural network model of the potential and then applies an equation discovery technique to extract from the neural potential a closed-form algebraic expression. We demonstrate this approach for several systems, including oscillators, a central force problem, and a problem of two charged particles in a classical Coulomb potential. Through these test problems, we show close agreement between learned neural potentials, the interpreted potentials we obtain after training, and the ground truth. In particular, for the central force problem, we show that our approach learns the correct effective potential, a reduced-order model of the system. © Springer Nature Switzerland AG 2020.","Equation discovery; Hamiltonian systems; Neural networks","Charged particles; Electric fields; Machine learning; Potential energy functions; Algebraic expression; Central force problems; Effective potentials; Equation discovery; Hamiltonian systems; Neural network model; Reduced order models; Separable Hamiltonians; Hamiltonians"
"Bhat N., Saggu N., Pragati, Kumar S.","Generating visible spectrum images from thermal infrared using conditional generative adversarial Networks","10.1109/ICCES48766.2020.09137895","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091340232&doi=10.1109%2fICCES48766.2020.09137895&partnerID=40&md5=e5eaf57076e473bca756154d31b717a1","Thermal Infrared cameras have a unique set of capabilities that enables them to see and capture images in situations where vision is severely obscured, including dark, shade, and foggy conditions. Conversion of TIR image to visible spectrum makes them more interpretable, but automating transformation is a challenging task as it requires estimation of both chrominance and luminance for each pixel, posing the need for a more complex model that is capable of capturing higher-level semantics of a TIR image to generate a perceptually realistic RGB image.The use of Conditional Generative Adversarial Networks (cGANs) is proposed with a combination of content and adversarial losses to tackle this problem. The proposed cGAN uses a U-Net auto-encoder network as generator, and a patch based discriminator for adversarial training. The cGAN learns the most effective loss function during training, producing state-of-the-art perceptually realistic outputs. Qualitative and quantitative analyses show that our approach outperforms existing methods. © 2020 IEEE.","Conditional Generative Adversarial Networks; Deep Learning; Image to Image Transformation; Thermal Infrared Imaging","Semantics; Adversarial networks; Capture images; Foggy conditions; Qualitative and quantitative analysis; State of the art; Thermal infrared; Thermal infrared cameras; Visible spectra; Infrared radiation"
"Bhatawdekar R.M., Kainthola A., Pandey V.H.R., Nath S.T., Mohamad E.T.","Recent Developments in Machine Learning and Flyrock Prediction","10.1007/978-981-16-9770-8_39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132785652&doi=10.1007%2f978-981-16-9770-8_39&partnerID=40&md5=24a02ac543b889f7676be48bf1d86ac4","The blasting techniques are employed in mining and underground works to loosen the rock mass and ease the excavation. The blasting practices are economical and swifter in terms of their engineering application, however, they are of major environmental and safety concerns. The major issues related to blasting are flyrock, air over pressure, and ground vibrations etc. The rock fragments of rockmass are thrown outward after blasting, which can be threat to workers and machineries involved in the work, and sometimes nearby human settlements can be its victim. Therefore, an accurate prediction of the flyrock distance is the needed by mining practitioners. Earlier, experts have developed several empirical methods based on certain known parameters to assess flyrock distance. However, with time they become irrelevant and were easily replaced with advanced machine learning algorithm. The present study reviews some of these latest publications (2019–2021) examining flyrocks through artificial intelligent technique. The study incorporates types of machine learning models employed, input parameters used and number of datasets supporting the models. The input parameters were further classified according to rock-mass properties, blast design at site, and explosives responsible for blasting. Moreover, to compare the reliability of the model coefficient of correlation of the testing data of the all the documented model were evaluated. Rock density, rock mass rating and Shmidt hammer rebound number (SHRN) were found to be uncertain parameters. Artificial Neural Network (ANN) and other hybrid models for prediction of flyrock were compared. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Blasting; Flyrock prediction; Machine learning; Optimization algorithms","Blasting; Explosives; Learning algorithms; Machine learning; Machinery; Neural networks; Rock mechanics; Rocks; Uncertainty analysis; Engineering applications; Environmental concerns; Fly-rocks; Flyrock prediction; Input parameter; Machine-learning; Optimization algorithms; Overpressure; Rock-mass; Safety concerns; Forecasting"
"Bhatia A., Hagras H.","A Time Series Based Explainable Interval Type-2 Fuzzy Logic System","10.1109/FUZZ-IEEE55066.2022.9882556","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138768897&doi=10.1109%2fFUZZ-IEEE55066.2022.9882556&partnerID=40&md5=ac5f6bb3f4e694119b6aa7fb1e1a3704","Existing approaches to time series prediction using fuzzy techniques do not consider time periods in a human understandable format. The current time series based fuzzy approaches lead to large feature spaces where the individual features could be prone to noise in the individual observations. This paper presents a Time Series based Interval Type 2 Fuzzy Logic System (TS-IT2FLS) where the time periods are considered in a fuzzy manner with easily understandable linguistic labels. We have performed several experiments using various data sets where the proposed TS-IT2FLS achieved 29% higher ROC-AUC score compared to other non deep-learning approaches while our suggested approach is more explainable compared to existing approaches. © 2022 IEEE.","Explainable; FRBS; Fuzzy; Time Series; XAI","Computer circuits; Deep learning; Fuzzy logic; 'current; Explainable; FRBS; Fuzzy; Fuzzy techniques; Interval type-2 fuzzy logic systems; Time series prediction; Time-periods; Times series; XAI; Time series"
"Bhatia A., Garg V., Haves P., Pudi V.","Explainable Clustering Using Hyper-Rectangles for Building Energy Simulation Data","10.1088/1755-1315/238/1/012068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063374384&doi=10.1088%2f1755-1315%2f238%2f1%2f012068&partnerID=40&md5=311108fa722d11778ea06ea9ed7be8a0","Clustering has become a very popular machine learning technique for identifying groups of data points with common features in a set of data points. In several applications, there is a need to explain the clusters so that the user can understand the underlying commonalities. One such application is in the area of building energy simulation. There is a need to cluster solutions obtained by parametric energy simulation runs and explain the characteristics of each cluster for human consumption. This paper demonstrates how the axis-aligned hyper-rectangles based clustering, on building energy simulation data, can help identify clusters and describe the governing rules for each cluster. We are calling these rules design strategies. Instead of the distance-based clustering methods that are unable to extract simple rules from the underlying commonalities in each cluster, this method is able to overcome this limitation. This method is applied to identify design strategies from a parametric run of a simple five-zone rectangular building model. Based on a user-given threshold, low energy solutions are selected for clustering. Each axis-aligned hyper-rectangle cluster is a unique design strategy that can be easily communicated to the user. © 2019 Institute of Physics Publishing. All rights reserved.",,"Buildings; Geometry; Learning systems; Based clustering; Building energy simulations; Design strategies; Energy simulation; Energy solutions; Human consumption; Machine learning techniques; Rectangular Buildings; Structural design"
"Bhatia S., Alojail M., Sengan S., Dadheech P.","An efficient modular framework for automatic LIONC classification of MedIMG using unified medical language","10.3389/fpubh.2022.926229","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136544407&doi=10.3389%2ffpubh.2022.926229&partnerID=40&md5=924b7a27fa46c1e82473ced6b5a111bb","Handwritten prescriptions and radiological reports: doctors use handwritten prescriptions and radiological reports to give drugs to patients who have illnesses, injuries, or other problems. Clinical text data, like physician prescription visuals and radiology reports, should be labelled with specific information such as disease type, features, and anatomical location for more effective use. The semantic annotation of vast collections of biological and biomedical texts, like scientific papers, medical reports, and general practitioner observations, has lately been examined by doctors and scientists. By identifying and disambiguating references to biomedical concepts in texts, medical semantics annotators could generate such annotations automatically. For Medical Images (MedIMG), we provide a methodology for learning an effective holistic representation (handwritten word pictures as well as radiology reports). Deep Learning (DL) methods have recently gained much interest for their capacity to achieve expert-level accuracy in automated MedIMG analysis. We discovered that tasks requiring significant responsive fields are ideal for downscaled input images that are qualitatively verified by examining functional, responsive areas and class activating maps for training models. This article focuses on the following contributions: (a) Information Extraction from Narrative MedImages, (b) Automatic categorisation on image resolution with an impact on MedIMG, and (c) Hybrid Model to Predictions of Named Entity Recognition utilising RNN + LSTM + GRM that perform admirably in every trainee for every input purpose. At the same time, supplying understandable scale weight implies that such multi-scale structures are also crucial for extracting information from high-resolution MedIMG. A portion of the reports (30%) are manually evaluated by trained physicians, while the rest were automatically categorised using deep supervised training models based on attention mechanisms and supplied with test reports. MetaMapLite proved recall and precision, but also an F1-score equivalent for primary biomedicine text search techniques and medical text examination on many databases of MedIMG. In addition to implementing as well as getting the requirements for MedIMG, the article explores the quality of medical data by using DL techniques for reaching large-scale labelled clinical data and also the significance of their real-time efforts in the biomedical study that have played an instrumental role in its extramural diffusion and global appeal. Copyright © 2022 Bhatia, Alojail, Sengan and Dadheech.","accuracy; deep learning; LIONC; MedIMG; natural language processing","factual database; human; information retrieval; language; natural language processing; semantics; Databases, Factual; Humans; Information Storage and Retrieval; Language; Natural Language Processing; Semantics"
"Bhatia S., Alsuwailam R.I., Roy D.G., Mashat A.","Improved Multimedia Object Processing for the Internet of Vehicles","10.3390/s22114133","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130851087&doi=10.3390%2fs22114133&partnerID=40&md5=9e0a7990c290ced1e1f5ae549b2b2890","The combination of edge computing and deep learning helps make intelligent edge devices that can make several conditional decisions using comparatively secured and fast machine learning algorithms. An automated car that acts as the data-source node of an intelligent Internet of vehicles or IoV system is one of these examples. Our motivation is to obtain more accurate and rapid object detection using the intelligent cameras of a smart car. The competent supervision camera of the smart automobile model utilizes multimedia data for real-time automation in real-time threat detection. The corresponding comprehensive network combines cooperative multimedia data processing, Internet of Things (IoT) fact handling, validation, computation, precise detection, and decision making. These actions confront real-time delays during data offloading to the cloud and synchronizing with the other nodes. The proposed model follows a cooperative machine learning technique, distributes the computational load by slicing real-time object data among analogous intelligent Internet of Things nodes, and parallel vision processing between connective edge clusters. As a result, the system increases the computational rate and improves accuracy through responsible resource utilization and active–passive learning. We achieved low latency and higher accuracy for object identification through real-time multimedia data objectification. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","cooperative learning; edge intelligence; industrial Internet of Things; multimedia object processing","Cameras; Data handling; Decision making; Deep learning; Learning algorithms; Multimedia systems; Object detection; Conditional decisions; Cooperative learning; Edge computing; Edge intelligence; Machine learning algorithms; Multimedia data; Multimedia object; Multimedium object processing; Object processing; Real- time; Internet of things; algorithm; automation; multimedia; Algorithms; Automation; Internet of Things; Multimedia"
"Bhatt A., Ongsakul W., Nimal Madhu M., Singh J.G.","Sliding window approach with first-order differencing for very short-term solar irradiance forecasting using deep learning models","10.1016/j.seta.2021.101864","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121397008&doi=10.1016%2fj.seta.2021.101864&partnerID=40&md5=6f71ab68411712a534f0f8e18f2fa544","The intermittent behavior of solar is usually responsible for creating uncertainty while generating power. By implementing suitable forecasting techniques for solar irradiance (SI), we can overcome this intermittency which can be helpful in the economic load dispatch as well as to control, manage and optimize the power generation in the microgrid. This paper presents three deep learning (DL) models to forecast SI from 1 step (15-minute) to 6 steps (1 h 30 min) ahead. By implementing the sliding window technique, the input variables are converted into 12 steps lag datasets to train the model whereas outputs are transformed by first-order differencing. A total dataset of 18,277 from average 15-min interval global SI collected during January 1, 2016, to January 6, 2017, from the Asian Institute of Technology (AIT) Metrological station is used to train and evaluate the performance of the DL models. Based on the obtained results from different evaluation parameters such as maximum absolute error, confidence interval (CI), linear regression plot, mean absolute error (MAE), root mean square error (RMSE), mean absolute percentage error (MAPE), and R squared, we found that the deep hybrid model consists of convolutional neural network-long short term memory (CNN-LSTM) can outperform during multistep forecasting. The findings of the present work suggest that the proposed deep hybrid LSTM–CNN model is a reliable alternative for very short-term SI prediction due to its high predictive accuracy. © 2021","Deep neural networks; First-order differencing; Multi-step solar irradiance prediction; Sliding window technique","Electric load dispatching; Errors; Forecasting; Long short-term memory; Mean square error; Microgrids; Multilayer neural networks; Solar radiation; First order; First-order differencing; Intermittent behaviors; Learning models; Multi-step solar irradiance prediction; Multisteps; Sliding Window; Sliding window techniques; Solar irradiances; Uncertainty; Deep neural networks; computer simulation; error analysis; forecasting method; numerical model; regression analysis; uncertainty analysis"
"Bhatt A., Roberts R., Chen X., Li T., Connor S., Hatim Q., Mikailov M., Tong W., Liu Z.","DICE: A Drug Indication Classification and Encyclopedia for AI-Based Indication Extraction","10.3389/frai.2021.711467","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117879800&doi=10.3389%2ffrai.2021.711467&partnerID=40&md5=f8f29fabe48c6ddbf112124bbf3e9774","Drug labeling contains an ‘INDICATIONS AND USAGE’ that provides vital information to support clinical decision making and regulatory management. Effective extraction of drug indication information from free-text based resources could facilitate drug repositioning projects and help collect real-world evidence in support of secondary use of approved medicines. To enable AI-powered language models for the extraction of drug indication information, we used manual reading and curation to develop a Drug Indication Classification and Encyclopedia (DICE) based on FDA approved human prescription drug labeling. A DICE scheme with 7,231 sentences categorized into five classes (indications, contradictions, side effects, usage instructions, and clinical observations) was developed. To further elucidate the utility of the DICE, we developed nine different AI-based classifiers for the prediction of indications based on the developed DICE to comprehensively assess their performance. We found that the transformer-based language models yielded an average MCC of 0.887, outperforming the word embedding-based Bidirectional long short-term memory (BiLSTM) models (0.862) with a 2.82% improvement on the test set. The best classifiers were also used to extract drug indication information in DrugBank and achieved a high enrichment rate (>0.930) for this task. We found that domain-specific training could provide more explainable models without performance sacrifices and better generalization for external validation datasets. Altogether, the proposed DICE could be a standard resource for the development and evaluation of task-specific AI-powered, natural language processing (NLP) models. © Copyright © 2021 Bhatt, Roberts, Chen, Li, Connor, Hatim, Mikailov, Tong and Liu.","artificial intelligence; deep learning; drug indication; natural language processing; transformers",
"Bhatt P., Liu J., Gong Y., Wang J., Guo Y.","Emerging Artificial Intelligence-Empowered mHealth: Scoping Review","10.2196/35053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131772500&doi=10.2196%2f35053&partnerID=40&md5=f716405712a6e3482118f2ba6f2f3c13","Background: Artificial intelligence (AI) has revolutionized health care delivery in recent years. There is an increase in research for advanced AI techniques, such as deep learning, to build predictive models for the early detection of diseases. Such predictive models leverage mobile health (mHealth) data from wearable sensors and smartphones to discover novel ways for detecting and managing chronic diseases and mental health conditions. Objective: Currently, little is known about the use of AI-powered mHealth (AIM) settings. Therefore, this scoping review aims to map current research on the emerging use of AIM for managing diseases and promoting health. Our objective is to synthesize research in AIM models that have increasingly been used for health care delivery in the last 2 years. Methods: Using Arksey and O'Malley's 5-point framework for conducting scoping reviews, we reviewed AIM literature from the past 2 years in the fields of biomedical technology, AI, and information systems. We searched 3 databases, PubsOnline at INFORMS, e-journal archive at MIS Quarterly, and Association for Computing Machinery (ACM) Digital Library using keywords such as ""mobile healthcare,"" ""wearable medical sensors,"" ""smartphones"", and ""AI."" We included AIM articles and excluded technical articles focused only on AI models. We also used the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) technique for identifying articles that represent a comprehensive view of current research in the AIM domain. Results: We screened 108 articles focusing on developing AIM models for ensuring better health care delivery, detecting diseases early, and diagnosing chronic health conditions, and 37 articles were eligible for inclusion, with 31 of the 37 articles being published last year (76%). Of the included articles, 9 studied AI models to detect serious mental health issues, such as depression and suicidal tendencies, and chronic health conditions, such as sleep apnea and diabetes. Several articles discussed the application of AIM models for remote patient monitoring and disease management. The considered primary health concerns belonged to 3 categories: mental health, physical health, and health promotion and wellness. Moreover, 14 of the 37 articles used AIM applications to research physical health, representing 38% of the total studies. Finally, 28 out of the 37 (76%) studies used proprietary data sets rather than public data sets. We found a lack of research in addressing chronic mental health issues and a lack of publicly available data sets for AIM research. Conclusions: The application of AIM models for disease detection and management is a growing research domain. These models provide accurate predictions for enabling preventive care on a broader scale in the health care domain. Given the ever-increasing need for remote disease management during the pandemic, recent AI techniques, such as federated learning and explainable AI, can act as a catalyst for increasing the adoption of AIM and enabling secure data sharing across the health care industry. © 2022 JMIR Publications. All rights reserved.","artificial intelligence; machine learning; mobile health units; review literature as topic; telemedicine","artificial intelligence; health care delivery; human; pandemic; procedures; smartphone; telemedicine; Artificial Intelligence; Delivery of Health Care; Humans; Pandemics; Smartphone; Telemedicine"
"Bhatt R., Gaw D., Meystel A.","Learning in a multiresolutional conceptual framework",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024183171&partnerID=40&md5=3dee039219f21ef7b0b0c2f46e69cd97","A real-time system for the control of an autonomous vehicle consisting of a nested hierarchy of control modules is discussed. The proposed intelligent controller has nonhomogeneous knowledge representation and a neural-network-based decision-making system operating in real time. The focus is on the Pilot module, which provides the real-time guidance of the system. It is responsible for the generation and tracking of dynamically feasible trajectories which follow the planned path given by the upper level (Navigator) and avoid local obstacles. Control of a complex system (mobile robot) is facilitated by the use of a feed-forward neural network. How such an approach addresses constant response time of decision-making (control) and online learning and adaptability is discussed. Dealing with constraints is done via a multiresolutional system of dynamic avoidance regions, which are analogous to the concept of potential field but require much simpler representation and computational procedures.",,"Artificial Intelligence; Control Systems; Robots, Industrial--Mobile; Autonomous Vehicle; Decision-Making; Dynamic Avoidance Regions; Feed-Forward Neural Network; Intelligent Controller; Multiresolutional Conceptual Framework; Systems Science and Cybernetics"
"Bhatt U., Xiang A., Sharma S., Weller A., Taly A., Jia Y., Ghosh J., Puri R., Moura J.M.F., Eckersley P.","Explainable machine learning in deployment","10.1145/3351095.3375624","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079672069&doi=10.1145%2f3351095.3375624&partnerID=40&md5=3911ffd8909c05a71e02b0fb79f10d8d","Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability. © 2020 Copyright held by the owner/author(s).","Deployed systems; Explainability; Machine learning; Qualitative study; Transparency","Learning systems; Transparency; Deployed systems; End users; Explainability; Modeling behavior; Qualitative study; Training data; Machine learning"
"Bhatt U.","Maintaining the humanity of our models",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100424999&partnerID=40&md5=18fce0725979e135e4581870b153e5e3","Artificial intelligence (AI) and machine learning (ML) have been major research interests in computer science for the better part of the last few decades. However, all too recently, both AI and ML have rapidly grown to be media frenzies, pressuring companies and researchers to claim they use these technologies. As ML continues to percolate into the layman's life, we, as computer scientists and machine learning researchers, are responsible for ensuring we clearly convey the extent of our work and the humanity of our models. In our current discussion, we limit ourselves to the following three important aspects that are needed to regularize ML for mass adoption: a standard for model interpretability, a consideration for human bias in data, and an understanding of a model’s societal effects. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Computer scientists; Human bias; Interpretability; Mass adoption; Research interests; Machine learning"
"Bhattachargee C.K., Sikder N., Hasan M.T., Nahid A.-A.","Finger Movement Classification Based on Statistical and Frequency Features Extracted from Surface EMG Signals","10.1109/IC4ME247184.2019.9036671","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082994047&doi=10.1109%2fIC4ME247184.2019.9036671&partnerID=40&md5=fb50fba7cb60f60afd5cd35423fcfdb2","Anatomization of EMG signals is one of the building blocks of modern prostheses. As the goal is to build robotic arms whose functions are identical to the natural ones, EMG signals produced from various hand gestures and finger movements have received much attention in recent times. Surface EMG signals collected from the upper hand muscles show specific patterns for a particular finger movement, which is also true for combined (more than one) finger movements. Utilizing Digital Signal Processing (DSP), and Machine Learning (ML) techniques this paper proposes a novel method to distinguish among various EMG signals generated from ten different hand gestures. To reduce complexity and make the signals more understandable to the algorithm statistical and frequency features were extracted from the raw EMG signals and used for classification. In order to prove the effectiveness of the method, it was tested on a practical EMG dataset and the results of the experiments are presented. © 2019 IEEE.","EMG; ensemble learning; feature extraction; FFT; finger movement classification; machine learning","Digital signal processing; Fast Fourier transforms; Feature extraction; Learning systems; Machine learning; Motion analysis; Building blockes; Digital signal processing (DSP); Ensemble learning; Finger movements; Frequency features; Hand gesture; Hand muscles; Surface EMG; Biomedical signal processing"
"Bhattacharjee K., Petzold L.","What Drives Consumer Choices? Mining Aspects and Opinions on Large Scale Review Data Using Distributed Representation of Words","10.1109/ICDMW.2016.0133","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015210541&doi=10.1109%2fICDMW.2016.0133&partnerID=40&md5=3e9cf968f9cd72ec6d719f5723102ec3","With the increasing popularity of online review sites, developing methods to mine and analyze information contained in the vast amounts of noisy user-generated reviews becomes a necessity. In this work, we develop a method to uncover the various aspects of a product or service reviewed by a user, and the opinions associated with them, in an automated fashion. We use the neural network model Word2Vec to build a vector space representation of a large corpus of user-generated, online restaurant reviews, and harness these distributed representations for aspect-based sentiment analysis. User generated text data is intrinsically noisy, with misspellings, informal language, and digressions. Because of the many variations in spelling and expression, the data is also very sparse. Despite these inherent challenges we are able to represent the reviews by key drivers of consumer sentiment, allowing for highly accurate sentiment prediction using a method that is both scalable and human interpretable. © 2016 IEEE.",,"Data mining; Sentiment analysis; Vector spaces; Consumer choice; Distributed representation; Highly accurate; Neural network model; Online reviews; Restaurant reviews; User-generated; Vector space representation; Digital storage"
"Bhattacharjee N.","Automated Dental Cavity Detection System Using Deep Learning and Explainable AI",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134634841&partnerID=40&md5=b1446f6ed408a4a2948bb2f0031a7b6f","Impacting over 3.9 billion people, dental cavities requires a trained dentist for diagnosis. Unfortunately, barriers such as dentophobia, limited dentist availability, and lack of dental insurance prevent millions from receiving care. To address this, an Artificial Intelligence system was developed that detects cavity presence on photographs and visually explains the rationale behind each diagnosis. While previous systems only detected cavities on one extracted tooth showing one tooth surface, this study's system detects cavities on photographs showing multiple teeth and four tooth surfaces. For training, 506 de-identified images from online sources and consenting human participants were collected. Using curriculum learning, a ResNet-27 architecture proved to be most optimal after achieving 82.8% accuracy and 1.0 in sensitivity. Visual explanations for the system's diagnoses were also generated using Local Interpretable Model Agnostic Explanation. This system can explain its diagnoses to users in an understandable manner, which is a crucial skill employed by dentists. ©2022 AMIA - All rights reserved.",,"artificial intelligence; human; phobia; Artificial Intelligence; Deep Learning; Humans; Phobic Disorders"
"Bhattacharjee P., Kar S.P., Rout N.K.","Sleep and Sedentary Behavior Analysis from Physiological Signals using Machine Learning","10.1109/ICIMIA48430.2020.9074883","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084525419&doi=10.1109%2fICIMIA48430.2020.9074883&partnerID=40&md5=7e16ffc5ef1181b790229b2e41de38be","In this modern digital world with rapidly changing technologies, personal health, i.e., physical as well as mental health is affected by sedentary behavior. In order to monitor the human sedentary behavior and its analysis requires some knowledge on physical as well as mental disciplines along with some measures of physical exercise as well. The purpose of this paper is to analyze the sedentary, sleep, or the rest behavior of a healthy adult person with the help of the physical activity (PA). The data has been collected from a open data source named as 'Replication Data for: Classification of sleep, sedentary behaviour, and physical activity using commercial wearable devices' published on 2019 by Harvard University. The data comparison has been made with various machine learning algorithms. Accuracy has been obtained for each case and the case with the best possible accuracy has been considered for monitoring and analysis purpose. With this kind of analysis it is easy to establish an understandable relationship between parameters that affect the sleep and sedentary behavior with the physiological signals obtained from the commercial wearable devices. The interrelationship will help to scrutinize the hazards from this kind of sedentary behaviour. In such cases, necessary awareness can be build up among the society. The problem can be detected prior to any mishap. Also some detection methods can be applied to reduce such threat and required solution can be suggested as well. © 2020 IEEE.","Accuracy; Classifier; Confusion Matrix; Energy Expenditure; K-Score; Machine Learning Algorithm; MET; Physical Activity; Sedentary Behavior","Behavioral research; Learning algorithms; Open Data; Physiological models; Physiology; Sleep research; Wearable technology; Behavior analysis; Detection methods; Harvard University; Monitoring and analysis; Physical activity; Physical exercise; Physiological signals; Wearable devices; Machine learning"
"Bhattacharjee S., Islam M.J., Abedzadeh S.","Robust Anomaly based Attack Detection in Smart Grids under Data Poisoning Attacks","10.1145/3494107.3522778","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134416737&doi=10.1145%2f3494107.3522778&partnerID=40&md5=5c9429c04b49856a7c38f833bb670119","Anomaly-based attack detection methods are often used to detect data integrity or data falsification attacks in advanced metering infrastructure (AMI) of smart grids. However, there is a lack of studies on the effect of data poisoning attacks against the anomaly based attack detectors that depend on some form of machine learning. In this paper, we introduce some data poisoning attack strategies against anomaly-based attack detectors in smart metering infrastructure and show its impact. Specifically, we propose a whitebox and black box approach to poisoning attacks. Then, we propose modifications to improve the robustness of previous anomaly detection algorithms by modifying certain design choices for learning the thresholds for the anomaly detector. Specifically, we offer theoretical insights and experimental proof to explain why and when they mitigate data poisoning. These design choices include both the regression type and the loss function choice. We measure attack mitigation performance with two NIST specified metrics for CPS systems in the test set using a real smart metering dataset. Finally, we offer recommendations on energy utility's best anomaly detector design choices under varying attack parameters. © 2022 ACM.","adversarial machine learning; anomaly detection; data poisoning attack; interpretable ml based security; smart grid; time series","Advanced metering infrastructures; Anomaly detection; Electric power transmission networks; Machine learning; Statistical tests; Adversarial machine learning; Anomaly detection; Attack detection; Data poisoning attack; Interpretable ml based security; Machine-learning; Poisoning attacks; Smart grid; Smart metering; Times series; Smart power grids"
"Bhattacharjee S., Hwang Y.-B., Ikromjanov K., Sumon R.I., Kim H.-C., Choi H.-K.","An Explainable Computer Vision in Histopathology: Techniques for Interpreting Black Box Model","10.1109/ICAIIC54071.2022.9722656","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127689793&doi=10.1109%2fICAIIC54071.2022.9722656&partnerID=40&md5=6c946aefe7da330d32952a84dab1bca9","Computer vision is a field of artificial intelligence (AI) that is being used increasingly in histopathology to identify pathologies in slide images with a high degree of accuracy. In this paper, we focus on the different interpreting techniques of explainable computer vision (XCV). Analysis of histopathology images is a challenging task, and specialized knowledge is mandatory to make AI decisions. To carry out this analysis, a deep learning model has been used to classify and differentiate the scoring (i.e., benign and malignant) of Prostate cancer (PCa). However, the AI models are complex and opaque, and it is important to understand model decision-making. Therefore, to address this problem, we present three techniques for accountability and transparency of the model, namely Activation Layer Visualization (ALV), Local Interpretable Model-Agnostic Explanation (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class Activation Mapping (Grad-CAM). XCV is AI in which the results of the black-box model can be understood by humans. The robustness of our model has been confirmed by using an external test dataset including 100 histopathology images. The model performance has been evaluated using the receiver operating characteristic (ROC) curve. © 2022 IEEE.","artificial intelligence; black box; explainable computer vision; histopathology; prostate cancer","Chemical activation; Computer vision; Decision making; Deep learning; Urology; Black box modelling; Black boxes; Explainable computer vision; High degree of accuracy; Histopathology; Intelligence decision; Learning models; Prostate cancers; Specialized knowledge; Task knowledge; Diseases"
"Bhattacharjya D., Gao T., Subramanian D.","Ordinal Historical Dependence in Graphical Event Models with Tree Representations",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130092383&partnerID=40&md5=be3fad8cbee629b34cbc2c25eaf18194","Graphical event models are representations that capture process independence between different types of events in multivariate temporal point processes. The literature consists of various parametric models and approaches to learn them from multivariate event stream data. Since these models are interpretable, they are often able to provide beneficial insights about event dynamics. In this paper, we show how to compactly model the situation where the order of occurrences of an event's causes in some recent historical time interval impacts its occurrence rate; this sort of historical dependence is common in several real-world applications. To overcome the practical challenge of parameter explosion due to the number of potential orders that is super-exponential in the number of parents, we introduce a novel graphical event model based on a parametric tree representation for capturing ordinal historical dependence. We present an approach to learn such a model from data, demonstrating that the proposed model fits several real-world datasets better than relevant baselines. We also showcase the potential advantages of such a model to an analyst during the process of knowledge discovery. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved",,"Artificial intelligence; Trees (mathematics); Capture process; Event dynamics; Event model; Event streams; Learn+; Parametric approach; Parametric models; Point process; Stream data; Tree representation; Forestry"
"Bhattacharjya D., Subramanian D., Gao T.","State variable effects in graphical event models",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097345716&partnerID=40&md5=995a02552c11240d3f9761dcaa21508c","Many real-world domains involve co-evolving relationships between events, such as meals and exercise, and time-varying random variables, such as a patient's blood glucose levels. In this paper, we propose a general framework for modeling joint temporal dynamics involving continuous time transitions of discrete state variables and irregular arrivals of events over the timeline. We show how conditional Markov processes (as represented by continuous time Bayesian networks) and multivariate point processes (as represented by graphical event models) are among various processes that are covered by the framework. We introduce and compare two simple and interpretable yet practical joint models within the framework with relevant baselines on simulated and real-world datasets, using a graph search algorithm for learning. The experiments highlight the importance of jointly modeling event arrivals and state variable transitions to better fit joint temporal datasets, and the framework opens up possibilities for models involving even more complex dynamics whenever suitable. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.",,"Bayesian networks; Continuous time systems; Graph algorithms; Markov processes; Blood glucose level; Complex dynamics; Continuous-time; Graph search algorithm; Real world domain; Real-world datasets; State variables; Temporal dynamics; Artificial intelligence"
"Bhattacharya P., Obaidat M.S., Savaliya D., Sanghavi S., Tanwar S., Sadaun B.","Metaverse assisted Telesurgery in Healthcare 5.0: An interplay of Blockchain and Explainable AI","10.1109/CITS55221.2022.9832978","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136105938&doi=10.1109%2fCITS55221.2022.9832978&partnerID=40&md5=f774a038d3e66a50894de94710880d98","Smart healthcare has transitioned towards health-care 5.0, which allows ambient tracking of patients, emotive telemedicine, telesurgery, wellness monitoring, virtual clinics, and personalized care. Thus, the metaverse is a potential tool to leverage digital connectivity via improved healthcare experience in virtual environments. However, despite its potential benefits, patient's sensitive information is captured, and digital avatars are created that interact with healthcare stakeholders for connected virtual care. As metaverse components are decentralized, blockchain (BC) is a potential solution to induce transparency and immutability in stored transactions on metaverse. For clinical decision support in BC-Assisted metaverse enabled, Healthcare 5.0, accurate and interpretable diagnosis is critical. Thus, explainable AI (xAI) forms another critical component that provides trust in the healthcare informatics front. A dual solution of trusted informatics is possible via the interplay of BC and xAI in metaverse-enabled Healthcare 5.0. The article investigates the interplay through a proposed telesurgical scheme between patients, virtual hospitals, and doctors. Next, we discuss the potential challenges and present an experimental use-case of the benefits of our proposed architecture over traditional telesurgery systems. © 2022 IEEE.","Blockchain; Deep Learning; Explainable AI; Healthcare 5.0; Metaverse; Telesurgery","Decision support systems; Deep learning; Diagnosis; Hospitals; Medical informatics; Ambients; Block-chain; Deep learning; Digital connectivity; Explainable AI; Healthcare 5.0; Metaverses; Potential benefits; Potential tool; Telesurgery; Blockchain"
"Bhattacharya S., Hossain M.M., Juyal R., Sharma N., Pradhan K.B., Singh A.","Role of public health ethics for responsible use of artificial intelligence technologies","10.4103/ijcm.IJCM_62_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107574646&doi=10.4103%2fijcm.IJCM_62_20&partnerID=40&md5=170df97702b2e3af2f7662aa9b41f8cc","Recent advancements in artificial intelligence (AI) technologies have shown promising success in optimizing health-care processes and improvising health services research and practice leading to better health outcomes. However, the role of public health ethics in the era of AI is not widely evaluated. This article aims to describe the responsible approach to AI design, development, and use from a public health perspective. This responsible approach should focus on the collective well-being of humankind and incorporate ethical principles and societal values. Such approaches are important because AI concerns and impacts the health and well-being of all of us collectively. Rather than limiting such discourses at the individual level, ethical considerations regarding AI systems should be analyzed enlarge, considering the complex socio-technological reality around the world. © 2021 Indian Journal of Community Medicine | Published by Wolters Kluwer - Medknow","Artificial intelligence; Ethics; Humane values; Public health; Responsibility","article; artificial intelligence; ethics; health services research; outcome assessment; public health; responsibility; wellbeing"
"Bhattacharyya R., Bhattacharyya A.","Smart grid demand-side management by machine learning","10.1007/978-981-15-2188-1_50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084116808&doi=10.1007%2f978-981-15-2188-1_50&partnerID=40&md5=028ab8f0c0d482ab34d98bad16c2d1ac","The smart grid is a complex electrical network system comprising of different subsystems at different levels of aggregation. It facilitates a bidirectional information flow among all the actors such as producers of electricity, end users of electrical energy, transmission and distribution system operators (TSO/DSO), and demand response (DR) aggregators. Smart grid contains smart meters that send user statistics to the server. Accurate forecasting of the electricity usage is required in order to take controlled actions to balance the supply and demand of electricity. This forecasting can be achieved using machine learning-based predictive models. This paper deals with the forecasting of short-term and mid-term load for the grid entity using machine learning. A predictive system is designed using machine learning techniques in order to process the smart meter data which in turn is used as the training data for the model. The outcomes are then shown with data and results to make it more understandable by the reader. © Springer Nature Singapore Pte Ltd. 2020.","Bidirectional energy flow; Electrical network; Forecasting; Machine learning; Predictive model; Smart grid; Smart meter","Demand side management; Economics; Electric power transmission networks; Electric utilities; Forecasting; Machine learning; Smart meters; Electrical energy; Electrical networks; Electricity usage; Information flows; Machine learning techniques; Predictive models; Predictive systems; Transmission and distribution systems; Smart power grids"
"Bhattacharyya S., Koehler G.J.","Learning by objectives for adaptive shop-floor scheduling","10.1111/j.1540-5915.1998.tb01580.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032388104&doi=10.1111%2fj.1540-5915.1998.tb01580.x&partnerID=40&md5=42f098e02d4e4bd674c969f1c805ca7a","Effective production scheduling requires consideration of the dynamics and unpredictability of the manufacturing environment. An automated learning scheme, utilizing genetic search, is proposed for adaptive control in typical decentralized factory-floor decision making. A high-level knowledge representation for modeling production environments is developed, with facilities for genetic learning within this scheme. A multiagent framework is used, with individual agents being responsible for the dispatch decision making at different workstations. Learning is with respect to stated objectives, and given the diversity of scheduling goals, the efficacy of the designed learning scheme is judged through its response under different objectives. The behavior of the genetic learning scheme is analyzed and simulation studies help compare how learning under different objectives impacts certain aggregate measures of system performance.","Genetic algorithms; Intelligent decision support; Machine learning; Production scheduling",
"Bhattarai B., Granmo O.-C., Jiao L.","A Tsetlin Machine Framework for Universal Outlier and Novelty Detection","10.1007/978-3-031-10161-8_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135021383&doi=10.1007%2f978-3-031-10161-8_14&partnerID=40&md5=47059ab6ba2a30357d2c7e2c23ee01d9","Outlier and novelty detection are two of the most active study areas where a huge amount of research effort has been made over the past decades. Although there are several well-known outlier and novelty detection methods, it is difficult to find one that can effectively and simultaneously deal with both tasks across different data types. When studied in detail, outliers and novelties exhibit different characteristics. In this paper, we introduce a universal Tsetlin Machine (TM) framework for novelty and outlier detection. The framework consists of a TM generator and a machine learning classifier. To this end, we enhance the vanilla TM with a generator to produce a novelty score. The generator consists of the conjunctive clauses of the TM, which are used to form a representative pattern of a given input. We demonstrate that the clauses provide a succinct interpretable description of the trained input and that our scoring mechanism enables us to discern outlier and novel input. Empirically, we evaluate our TM framework on nine outlier datasets, five novelty tasks, and a one-class classification setup. In all experiments, we were able to either outperform or closely match state-of-the-art methods, with the added benefit of an interpretable propositional logic-based representation. © 2022, Springer Nature Switzerland AG.","Interpretable; Novelty detection; One-class classification; Outlier detection; Tsetlin Machine","Classification (of information); Data handling; Formal logic; Learning systems; Machine learning; Statistics; Datatypes; Detection methods; Interpretable; Machine-learning; Novelty detection; One-class Classification; Outlier Detection; Research efforts; Study areas; Tsetlin machine; Anomaly detection"
"Bhattarai B., Granmo O.-C., Jiao L.","Measuring the novelty of natural language text using the conjunctive clauses of a tsetlin machine text classifier",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102467038&partnerID=40&md5=730a57745ef1aaf08a3dac2edcb4adf6","Most supervised text classification approaches assume a closed world, counting on all classes being present in the data at training time. This assumption can lead to unpredictable behaviour during operation, whenever novel, previously unseen, classes appear. Although deep learning-based methods have recently been used for novelty detection, they are challenging to interpret due to their black-box nature. This paper addresses interpretable open-world text classification, where the trained classifier must deal with novel classes during operation. To this end, we extend the recently introduced Tsetlin machine (TM) with a novelty scoring mechanism. The mechanism uses the conjunctive clauses of the TM to measure to what degree a text matches the classes covered by the training data. We demonstrate that the clauses provide a succinct interpretable description of known topics, and that our scoring mechanism makes it possible to discern novel topics from the known ones. Empirically, our TM-based approach outperforms seven other novelty detection schemes on three out of five datasets, and performs second and third best on the remaining, with the added benefit of an interpretable propositional logic-based representation. © 2021 by SCITEPRESS - Science and Technology Publications, Lda.","Deep learning; Interpretable; Novelty detection; Tsetlin machine","Classification (of information); Deep learning; Formal logic; Learning-based methods; Natural language text; Novelty detection; Propositional logic; Text classification; Text classifiers; Training data; Training time; Text processing"
"Bhattarai G., Olaoye D., Mou B., Correll J.C., Shi A.","Mapping and selection of downy mildew resistance in spinach cv. whale by low coverage whole genome sequencing","10.3389/fpls.2022.1012923","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140443636&doi=10.3389%2ffpls.2022.1012923&partnerID=40&md5=03c9a391a22ce4eb4e07dfd2163df2ad","Spinach (Spinacia oleracea) is a popular leafy vegetable crop and commercial production is centered in California and Arizona in the US. The oomycete Peronospora effusa causes the most important disease in spinach, downy mildew. A total of nineteen races of P. effusa are known, with more than 15 documented in the last three decades, and the regular emergence of new races is continually overcoming the genetic resistance to the pathogen. This study aimed to finely map the downy mildew resistance locus RPF3 in spinach, identify single nucleotide polymorphism (SNP) markers associated with the resistance, refine the candidate genes responsible for the resistance, and evaluate the prediction performance using multiple machine learning genomic prediction (GP) methods. Segregating progeny population developed from a cross of resistant cultivar Whale and susceptible cultivar Viroflay to race 5 of P. effusa was inoculated under greenhouse conditions to determine downy mildew disease response across the panel. The progeny panel and the parents were resequenced at low coverage (1x) to identify genome wide SNP markers. Association analysis was performed using disease response phenotype data and SNP markers in TASSEL, GAPIT, and GENESIS programs and mapped the race 5 resistance loci (RPF3) to 1.25 and 2.73 Mb of Monoe-Viroflay chromosome 3 with the associated SNP in the 1.25 Mb region was 0.9 Kb from the NBS-LRR gene SOV3g001250. The RPF3 locus in the 1.22-1.23 Mb region of Sp75 chromosome 3 is 2.41-3.65 Kb from the gene Spo12821 annotated as NBS-LRR disease resistance protein. This study extended our understanding of the genetic basis of downy mildew resistance in spinach cultivar Whale and mapped the RPF3 resistance loci close to the NBS-LRR gene providing a target to pursue functional validation. Three SNP markers efficiently selected resistance based on multiple genomic selection (GS) models. The results from this study have added new genomic resources, generated an informed basis of the RPF3 locus resistant to spinach downy mildew pathogen, and developed markers and prediction methods to select resistant lines. Copyright © 2022 Bhattarai, Olaoye, Mou, Correll and Shi.","breeding; candidate gene; disease resistance; downy mildew; GWAS; mapping; oomycete; spinach",
"Bhatti A., Behinaein B., Rodenburg D., Hungler P., Etemad A.","Attentive Cross-modal Connections for Deep Multimodal Wearable-based Emotion Recognition","10.1109/ACIIW52867.2021.9666360","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124987446&doi=10.1109%2fACIIW52867.2021.9666360&partnerID=40&md5=c27fba009432d5a730b52c755e3a9c0f","Classification of human emotions can play an essential role in the design and improvement of human-machine systems. While individual biological signals such as Electrocardiogram (ECG) and Electrodermal Activity (EDA) have been widely used for emotion recognition with machine learning methods, multimodal approaches generally fuse extracted features or final classification/regression results to boost performance. To enhance multimodal learning, we present a novel attentive cross-modal connection to share information between convolutional neural networks responsible for learning individual modalities. Specifically, these connections improve emotion classification by sharing intermediate representations among EDA and ECG and apply attention weights to the shared information, thus learning more effective multimodal embeddings. We perform experiments on the WESAD dataset to identify the best configuration of the proposed method for emotion classification. Our experiments show that the proposed approach is capable of learning strong multimodal representations and outperforms a number of baselines methods. © 2021 IEEE.","Affective Computing; Attention Mechanism; Multimodal Representation Learning","Biomedical signal processing; Classification (of information); Convolutional neural networks; Deep learning; Speech recognition; Wearable technology; Affective Computing; Attention mechanisms; Cross-modal; Electrodermal activity; Emotion classification; Emotion recognition; Human emotion; Human-machine systems; Multi-modal; Multimodal representation learning; Electrocardiography"
"Bhoi B., Vyawahare P., Avhad P., Patil N.","Data duplication avoidance in larger database","10.1109/ICIIECS.2017.8276031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046966506&doi=10.1109%2fICIIECS.2017.8276031&partnerID=40&md5=c5e89356d7e43cf4235220751bef2591","The database is a collection of interconnected data which is managed and retrieved in an efficient manner. The concept of the larger database is large scale database which will consist of a large number of data can be stored in the database system. Duplication of data is a technique for minimizing storage needs by eliminating inordinate data. Data avoidance is rejection of risk, the action that can negatively effect on a larger database system. Duplicate detection is a problem of material or stuff in many kinds of application including user relationship management, confidential information management or data mining. Duplicate detection is a method of detecting or observation of all cases multiple demonstrations in the real-world application. In the existing system, the duplication of data is checked on the basis of string which checks character by character, so it is time-consuming and it occupied more memory. The proposed system is implemented on Hadoop which handle larger database. It consists detection of duplicate data based on the multiple attributes. In our system, we used data pre-processing is data mining technique that consists transformed row data in the understandable format. We applied Parallel Progressive Sorted Neighbourhood Method & Map Reduce algorithm on this data to get a clean database. Map reduce programming allows for the processing of such large data in a completely safe and cost-effective manner. It will provide more manageable space and efficient handling of data. © 2017 IEEE.","Attribute; Data Pre-processing; Large Database; Map-reduce; PPSNM","Cost effectiveness; Data handling; Data mining; Database systems; Digital storage; Embedded systems; Information management; Attribute; Data preprocessing; Large database; Map-reduce; PPSNM; Data communication systems"
"Bhoj N., Dwivedi A.R., Tripathi A., Pandey B.","LSTM Powered Identification of Clickbait Content on Entertainment and News Websites","10.1109/CICN51697.2021.9574677","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119254720&doi=10.1109%2fCICN51697.2021.9574677&partnerID=40&md5=29e44c2b1195e3f5dcd7249f513a730c","Clickbait content on online platforms, is exaggerating content that doesn't deliver what it promises. The main motive of such content is to mislead the reader to 'click' on them. These are widely responsible for delivering false information to the user and damaging their online experience. Many online creators deliberately use them to get more views and generate more revenue. In light of potential difficulties created by clickbait content, this paper aims to create a clickbait detection model for entertainment and news websites utilizing the power of the machine and deep learning models. Empirical results of our experiments indicate that LSTM models are best suited for identifying clickbait content containing text by achieving an accuracy of 95.031 % which is 1.138 times greater than the Random Forest and 1.709 times greater than the Naive Bayes model. © 2021 IEEE.","clickbait identification; component; deep learning; LSTM; machine learning; random forest","Bayesian networks; Long short-term memory; Websites; Clickbait identification; Component; Deep learning; Detection models; Learning models; LSTM; News websites; Online platforms; Power; Random forests; Decision trees"
"Bhuiyan M.A.M., Sahi R.K., Islam M.R., Mahmud S.","Machine learning techniques applied to predict tropospheric ozone in a semi-arid climate region","10.3390/math9222901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119610296&doi=10.3390%2fmath9222901&partnerID=40&md5=6f01023c18215caf2c7caa1f15700ec7","In the last decade, ground-level ozone exposure has led to a significant increase in environmental and health risks. Thus, it is essential to measure and monitor atmospheric ozone concentration levels. Specifically, recent improvements in machine learning (ML) processes, based on statistical modeling, have provided a better approach to solving these risks. In this study, we compare Naive Bayes, K-Nearest Neighbors, Decision Tree, Stochastic Gradient Descent, and Extreme Gradient Boosting (XGBoost) algorithms and their ensemble technique to classify ground-level ozone concentration in the El Paso-Juarez area. As El Paso-Juarez is a non-attainment city, the concentrations of several air pollutants and meteorological parameters were analyzed. We found that the ensemble (soft voting classifier) of algorithms used in this paper provide high classification accuracy (94.55%) for the ozone dataset. Furthermore, variables that are highly responsible for the high ozone concentration such as Nitrogen Oxide (NOx), Wind Speed and Gust, and Solar radiation have been discovered. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","El Paso-Juarez; Machine learning; Semi-arid climate; Tropospheric ozone",
"Bhuiyan M.N.Q., Rahut S.K., Tanvir R.A., Ripon S.","Automatic acute lymphoblastic leukemia detection and comparative analysis from images","10.1109/CoDIT.2019.8820299","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072836514&doi=10.1109%2fCoDIT.2019.8820299&partnerID=40&md5=f85d6738c2d493621b67be7f774723e0","In this era, surrounded by numerous technologies, medical sector has seen a lot of advancement through implementing various autonomous systems to identify different types of diseases. In this paper, a framework for identification of Acute Lymphoblastic Leukemia from the microscopic image of white blood cell is proposed. Microscopic images are at first carefully preprocessed to prepare them for classification. In addition, four different machine learning algorithms, namely, Random Forest (RF), Support Vector Machine (SVM), Logistic Regression (LR), and Decision Tree (DT) are applied and respective results are analyzed to provide a comparison between these algorithms in terms of different performance metrics. After a thorough comparison, it is observed that the SVM works well to classify and identify the Acute Lymphoblastic cell which is responsible for Leukemia cancer. © 2019 IEEE.",,"Blood; Decision trees; Learning algorithms; Machine learning; Support vector machines; Acute lymphoblastic leukemia; Autonomous systems; Comparative analysis; Logistic regressions; Microscopic image; Performance metrics; Random forests; White blood cells; Diseases"
"Bhuiyan M.T.H., Khan I.M., Jony S.S.R., Robinson R., Nguyen U.-S.D.T., Keellings D., Rahman M.S., Haque U.","The disproportionate impact of COVID-19 among undocumented immigrants and racial minorities in the US","10.3390/ijerph182312708","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120359615&doi=10.3390%2fijerph182312708&partnerID=40&md5=f9aeab3848cbc60f086ed53fed8b215d","Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus responsible for coronavirus disease 2019 (COVID-19), has had an unprecedented effect, especially among under-resourced minority communities. Surveillance of those at high risk is critical for preventing and controlling the pandemic. We must better understand the relationships between COVID-19-related cases or deaths and characteristics in our most vulnerable population that put them at risk to target COVID-19 prevention and management efforts. Population characteristics strongly related to United States (US) county-level data on COVID-19 cases and deaths during all stages of the pandemic were identified from the onset of the epidemic and included county-level socio-demographic and comorbidities data, as well as daily meteorological modeled observation data from the North American Regional Reanalysis (NARR), and the NARR high spatial resolution model to assess the environment. Advanced machine learning (ML) approaches were used to identify outbreaks (geographic clusters of COVID-19) and included spatiotemporal risk factors and COVID-19 vaccination efforts, especially among vulnerable and underserved communities. COVID-19 outcomes were found to be negatively associated with the number of people vaccinated and positively associated with age, the prevalence of cardiovascular disease, diabetes, and the minority population. There was also a strong positive correlation between unauthorized immigrants and the prevalence of COVID-19 cases and deaths. Meteorological variables were also investigated, but correlations with COVID-19 were relatively weak. Our findings suggest that COVID-19 has had a disproportionate impact across the US population among vulnerable and minority communities. Findings also emphasize the importance of vaccinations and tailored public health initiatives (e.g., mask mandates, vaccination) to reduce the spread of COVID-19 and the number of COVID-19 related deaths across all populations. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","COVID-19; Environment; Unauthorized; USA; Vaccine","COVID-19; health risk; immigrant; minority group; race; vaccination; vaccine; adult; age; aged; Article; cardiovascular disease; clinical outcome; comorbidity; coronavirus disease 2019; correlational study; death; demography; diabetes mellitus; disease association; ethnic difference; groups by age; health impact assessment; high risk population; human; machine learning; medically underserved; middle aged; minority group; mortality risk; pandemic; prediction; predictor variable; prevalence; racial minority; risk factor; undocumented immigrant; United States; vaccination; very elderly; vulnerable population; epidemiology; United States; Coronavirus; SARS coronavirus; COVID-19; COVID-19 Vaccines; Ethnic and Racial Minorities; Humans; SARS-CoV-2; Undocumented Immigrants; United States"
"Bhuiyan T., Carney R.M., Chellappan S.","Artificial intelligence versus natural selection: Using computer vision techniques to classify bees and bee mimics","10.1016/j.isci.2022.104924","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138522271&doi=10.1016%2fj.isci.2022.104924&partnerID=40&md5=e81eebb42e7e0894a8b683d0454e2068","Many groups of stingless insects have independently evolved mimicry of bees to fool would-be predators. To investigate this mimicry, we trained artificial intelligence (AI) algorithms—specifically, computer vision—to classify citizen scientist images of bees, bumble bees, and diverse bee mimics. For detecting bees and bumble bees, our models achieved accuracies of 91.71% and 88.86%, respectively. As a proxy for a natural predator, our models were poorest in detecting bee mimics that exhibit both aggressive and defensive mimicry. Using the explainable AI method of class activation maps, we validated that our models learn from appropriate components within the image, which in turn provided anatomical insights. Our t-SNE plot yielded perfect within-group clustering, as well as between-group clustering that grossly replicated the phylogeny. Ultimately, the transdisciplinary approaches herein can enhance global citizen science efforts as well as investigations of mimicry and morphology of bees and other insects. © 2022 The Authors","Artificial intelligence; Bioinformatics; Computing methodology; Entomology; Zoology",
"Bhullar G., Khullar A., Kumar A., Sharma A., Pannu H.S., Malhi A.","Time series sentiment analysis (SA) of relief operations using social media (SM) platform for efficient resource management","10.1016/j.ijdrr.2022.102979","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129202534&doi=10.1016%2fj.ijdrr.2022.102979&partnerID=40&md5=f07a34be19ea6240ef4688573afb5ea2","The ease of access to the internet has sparked a worldwide interest in SM in recent years. The possibilities for the use of SM as a potential source in improving the management of disasters are increasing. Although there is an existing research ongoing with the analysis of the usage of SM during disasters, but none of the works have exploited the use of explainable artificial intelligence (XAI) for the validation of the transparency of the ML models. The contribution of this research is two-fold: Firstly, ML based time series analysis for relief operations using social media information with situational information gathered using Twitter from the users and resource providers and secondly, XAI has been used to increase the transparency and understand-ability of model decisions. For the case study, public dataset from Nepal, Italy earthquakes, COVID-19 dataset along with originally collected Twitter dataset has been considered. The performance of the extreme gradient boosting (xgboost) model is relatively superior than other techniques with 10-fold training mean accuracy of 87.17%. Thus, the experiments conclude the possibility of automation for the time series analysis for optimal relief operation management to serve the victims in the most efficient way and to control legal and administration implications. © 2022 Elsevier Ltd","Deep learning (DL); Hazard relief analysis; Machine learning (ML); Resource management; Sentiment analysis (SA); Social media (SM); Time series",
"Bhuvaneshwari M., Kanaga E.G.M., Anitha J., Raimond K., George S.T.","A comprehensive review on deep learning techniques for a BCI-based communication system","10.1016/B978-0-12-821633-0.00013-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123656727&doi=10.1016%2fB978-0-12-821633-0.00013-1&partnerID=40&md5=a6178d3c6685fd6a0155494e05e2e5e4","Dysarthria is a condition in which the muscles used for speech get weakened due to neurological disorders such as facial paralysis, stroke, and brain injury. Owing to this, patients suffering from this disease could not convey their basic needs to their caretakers. A recent era of research empowers this issue using a brain-computer interface (BCI). BCI is a recent breakthrough invention in the field of neuroscience that interprets the human brain signal into machine-understandable instruction. The main objective of this chapter is to provide a systematic study on various deep learning and machine learning techniques that could be used to process electroencephalography (EEG) signals to develop communication tools for paralytic people as well as future work in this field of study. The main advantage of using deep learning techniques is the reduced computation cost by avoiding the need for feature extraction from electrophysiological signals. The earlier literature on deep learning states that it does not work well with small datasets and may not be suitable for an EEG dataset taken from few subjects for any healthcare-related analysis. Even though deep learning has been widely used with various data analytics, it is less explored with EEG data. On the contrary, recent studies have proven that deep learning provides better performance even on a dataset with few samples. Hence, this chapter explores the feasibility of using deep learning and machine learning techniques for EEG-based communication systems. It also discusses future research directions and challenges in detail to enable the researchers in this domain to explore further. © 2021 Elsevier Inc. All rights reserved.","Brain-computer interface; Communication system; Deep learning; Neurological disorder",
"Bhuyan H.K., Chakraborty C.","Explainable Machine Learning for Data Extraction Across Computational Social System","10.1109/TCSS.2022.3164993","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129669469&doi=10.1109%2fTCSS.2022.3164993&partnerID=40&md5=d93df5f949b0e8ba59533ca08f693f43","This article addresses the explainable machine learning for data extraction on diverse datasets. In many cases, individual or specific approaches have been developed for feature selection (FS) on a certain dataset, but collecting the diversity dataset and demonstrating it through different FS methods are challenging. Thus, this article proposed multiapproaches for FS with the classification of diverse datasets. The proposed framework is developed using various methods, such as extendable particle swarm optimization (PSO), global and local searching, feature ranking, feature clustering, computational cost-based FS, and multiobjective optimization. We effectively used these methods in our proposed work in a single-setting framework. We focused on three essential computational items in our framework: classification accuracy, selected features, and computational times. Due to the diverse dataset, few methods have been considered challenging during computational evaluation for classification accuracy with test cost. We tried to manage the classification accuracy based on total cost and high accuracy with less cost. The proposed framework is experimented with the above methods and analyzed through comparative results on diversity datasets. For example, when regular parameter values are in the range of 2&#x207B;&#x00B9;&#x00B3;-2&#x207B;&#x2076;, the evaluation result affects all items, i.e., decreasing during this range; other values do not affect results. We used thresholds ranging from 0.6 to 0.9 for highly correlated feature pairs as per the support vector machine (SVM) method for recursive feature elimination. IEEE","Classification; computational cost; Computational efficiency; computational social system; Costs; data extraction; Data mining; explainable machine learning; Feature extraction; multiobjective optimization.; Particle swarm optimization; Support vector machines; Testing","Classification (of information); Computational efficiency; Data mining; Extraction; Feature extraction; Multiobjective optimization; Statistical tests; Support vector machines; Computational costs; Computational social system; Data extraction; Explainable machine learning; Features extraction; Features selection; Multi-objectives optimization; Multiobjective optimization.; Social systems; Support vectors machine; Particle swarm optimization (PSO)"
"Bhuyan M.S.I., Pe'er I., Sohel Rahman M.","SICaRiO: Short indel call filtering with boosting","10.1093/bib/bbaa238","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112130396&doi=10.1093%2fbib%2fbbaa238&partnerID=40&md5=1613e6b7e96147cad78ea90d67405af5","Despite impressive improvement in the next-generation sequencing technology, reliable detection of indels is still a difficult endeavour. Recognition of true indels is of prime importance in many applications, such as personalized health care, disease genomics and population genetics. Recently, advanced machine learning techniques have been successfully applied to classification problems with large-scale data. In this paper, we present SICaRiO, a gradient boosting classifier for the reliable detection of true indels, trained with the gold-standard dataset from 'Genome in a Bottle' (GIAB) consortium. Our filtering scheme significantly improves the performance of each variant calling pipeline used in GIAB and beyond. SICaRiO uses genomic features that can be computed from publicly available resources, i.e. it does not require sequencing pipeline-specific information (e.g. read depth). This study also sheds lights on prior genomic contexts responsible for the erroneous calling of indels made by sequencing pipelines. We have compared prediction difficulty for three categories of indels over different sequencing pipelines. We have also ranked genomic features according to their predictivity in determining false positives. © 2020 The Author(s) 2020. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.","genomic context; gradient tree boosting; indel filtering; variant filtering","high throughput sequencing; indel mutation; machine learning; nucleic acid database; software; Databases, Nucleic Acid; High-Throughput Nucleotide Sequencing; INDEL Mutation; Machine Learning; Software"
"Bi C., Zhou S., Liu X., Zhu Y., Yu J., Zhang X., Shi M., Wu R., He H., Zhan C., Lin Y., Shen B.","NDDRF: A risk factor knowledgebase for personalized prevention of neurodegenerative diseases","10.1016/j.jare.2021.06.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109424686&doi=10.1016%2fj.jare.2021.06.015&partnerID=40&md5=9cc7fdedd5a0a963b43a47b6e190e8fa","Introduction: Neurodegenerative diseases (NDDs) are a series of chronic diseases, which are associated with progressive loss of neuronal structure or function. The complex etiologies of the NDDs remain unclear, thus the prevention and early diagnosis of NDDs are critical to reducing the mortality and morbidity of these diseases. Objectives: To provide a systematic understanding of the heterogeneity of the risk factors associated with different NDDs (pan-neurodegenerative diseases or pan-NDDs), the knowledgebase is established to facilitate the personalized and knowledge-guided diagnosis, prevention and prediction of NDDs. Methods: Before data collection, the medical, life science and informatics experts as well as the potential users of the database were consulted and discussed for the scope of data and the classification of risk factors. The PubMed database was used as the resource of the data and knowledge extraction. Risk factors of NDDs were manually collected from literature published between 1975 and 2020. Results: The comprehensive risk factors database for NDDs (NDDRF) was established including 998 single or combined risk factors, 2293 records and 1071 articles relevant to the 14 most common NDDs. The single risk factors are classified into 3 categories, i.e. epidemiological factors (469), genetic factors (324) and biochemical factors (153). Among all the factors, 179 factors are positive and protective, while 880 factors have negative influence for NDDs. The knowledgebase is available at http://sysbio.org.cn/NDDRF/. Conclusion: NDDRF provides the structured information and knowledge resource on risk factors of NDDs. It could benefit the future systematic and personalized investigation of pan-NDDs genesis and progression. Meanwhile it may be used for the future explainable artificial intelligence modeling for smart diagnosis and prevention of NDDs. © 2022","Diagnosis and prevention; Knowledge base; Neurodegenerative diseases; Protective factor; Risk factor","artificial intelligence; degenerative disease; genetics; human; knowledge base; risk factor; Artificial Intelligence; Humans; Knowledge Bases; Neurodegenerative Diseases; Risk Factors"
"Bi J., Liu M., Hu H., Dai J.","Image captioning based on dependency syntax [基于依存句法的图像描述文本生成]","10.13700/j.bh.1001-5965.2020.0443","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104278680&doi=10.13700%2fj.bh.1001-5965.2020.0443&partnerID=40&md5=ee37abd78033e99f8e132d69da5c65d7","Current image captioning model can automatically apply the part-of-speech sequences and syntactic trees to make the generated text in line with grammar. However, the above-mentioned models generally generate the simple sentences. There is no groundbreaking work in language models promoting the interpretability of deep learning models. The dependency syntax is integrated into the deep learning model to supervise the image captioning, which can make deep learning models more interpretable. An image structure attention mechanism, which recognizes the relationship between image regions based on the dependency syntax, is applied to compute the visual relations and obtain the features. The fusion of image region relation features and image region features, and the word embedding are employed into Long Short-Term Memory (LSTM) to generate the image captions. In testing, the content keywords of the testing and training image datasets are produced due to the content overlap of two images, and the dependency syntax template corresponding to the test image can be indirectly extracted. According to the dependency syntax template, the diverse descriptions can be generated. Experiment resultsverify the capacity of the proposed model to improve the diversity of generated captions and syntax complexity and indicate that the dependency syntax can enhance the interpretability of deep learning model. © 2021, Editorial Board of JBUAA. All right reserved.","Content overlap; Dependency syntax; Image captioning; Image structure attention; Interpretability of deep learning model","Deep learning; Learning systems; Syntactics; Attention mechanisms; Image captioning; Image Structures; Interpretability; Language model; Learning models; Syntactic trees; Training image; Long short-term memory"
"Bi K.-W., Wei X.-G., Qin X.-X., Li B.","BTK Has Potential to Be a Prognostic Factor for Lung Adenocarcinoma and an Indicator for Tumor Microenvironment Remodeling: A Study Based on TCGA Data Mining","10.3389/fonc.2020.00424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083996595&doi=10.3389%2ffonc.2020.00424&partnerID=40&md5=1778aa6c6415138fdce268004fde6a99","Tumor microenvironment (TME) plays a crucial role in the initiation and progression of lung adenocarcinoma (LUAD); however, there is still a challenge in understanding the dynamic modulation of the immune and stromal components in TME. In the presented study, we applied CIBERSORT and ESTIMATE computational methods to calculate the proportion of tumor-infiltrating immune cell (TIC) and the amount of immune and stromal components in 551 LUAD cases from The Cancer Genome Atlas (TCGA) database. The differentially expressed genes (DEGs) were analyzed by COX regression analysis and protein–protein interaction (PPI) network construction. Then, Bruton tyrosine kinase (BTK) was determined as a predictive factor by the intersection analysis of univariate COX and PPI. Further analysis revealed that BTK expression was negatively correlated with the clinical pathologic characteristics (clinical stage, distant metastasis) and positively correlated with the survival of LUAD patients. Gene Set Enrichment Analysis (GSEA) showed that the genes in the high-expression BTK group were mainly enriched in immune-related activities. In the low-expression BTK group, the genes were enriched in metabolic pathways. CIBERSORT analysis for the proportion of TICs revealed that B-cell memory and CD8+ T cells were positively correlated with BTK expression, suggesting that BTK might be responsible for the preservation of immune-dominant status for TME. Thus, the levels of BTK might be useful for outlining the prognosis of LUAD patients and especially be a clue that the status of TME transition from immune-dominant to metabolic activity, which offered an extra insight for therapeutics of LUAD. © Copyright © 2020 Bi, Wei, Qin and Li.","BTK; CIBERSORT; ESTIMATE; lung adenocarcinoma; tumor microenvironment; tumor-infiltrating immune cells","Bruton tyrosine kinase; ibrutinib; lithium ion; transcriptome; Article; cancer prognosis; cancer staging; cancer survival; CD8+ T lymphocyte; controlled study; data mining; down regulation; human; human tissue; immune-related gene; intracellular signaling; lung adenocarcinoma; natural killer cell; overall survival; oxidative phosphorylation; protein protein interaction; survival rate; T lymphocyte activation; transcriptome sequencing; tumor microenvironment; upregulation"
"Bi X., Zhao J.","A novel orthogonal self-attentive variational autoencoder method for interpretable chemical process fault detection and identification","10.1016/j.psep.2021.10.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118855475&doi=10.1016%2fj.psep.2021.10.036&partnerID=40&md5=3358760305318a4258d9491b4189645d","Industrial processes are becoming increasingly large and complex, thus introducing potential safety risks and requiring an effective approach to maintain safe production. Intelligent process monitoring is critical to prevent losses and avoid casualties in modern industry. As the digitalization of process industry deepens, data-driven methods offer an exciting avenue to address the demands for monitoring complex systems. Nevertheless, many of these methods still suffer from low accuracy and slow response. Besides, most black-box models based on deep learning can only predict the existence of faults, but cannot provide further interpretable analysis, which greatly confines their usage in decision-critical scenarios. In this paper, we propose a novel orthogonal self-attentive variational autoencoder (OSAVA) model for process monitoring, consisting of two components, orthogonal attention (OA) and variational self-attentive autoencoder (VSAE). Specifically, OA is utilized to extract the correlations between different variables and the temporal dependency among different timesteps; VSAE is trained to detect faults through a reconstruction-based method, which employs self-attention mechanisms to comprehensively consider information from all timesteps and enhance detection performance. By jointly leveraging these two models, the OSAVA model can effectively perform fault detection and identification tasks simultaneously and deliver interpretable results. Finally, extensive evaluation on the Tennessee Eastman process (TEP) demonstrates that the proposed OSAVA-based fault detection and identification method shows promising fault detection rate as well as low detection delay and can provide interpretable identification of the abnormal variables, compared with representative statistical methods and state-of-the-art deep learning methods. © 2021 The Institution of Chemical Engineers","Fault detection; Fault identification; Orthogonal attention; Process monitoring; Variational self-attentive autoencoder","Accident prevention; Chemical detection; Deep learning; Petroleum reservoir evaluation; Process control; Process monitoring; Auto encoders; Chemical process; Fault detection and identification; Fault identifications; Faults detection; Industrial processs; Orthogonal attention; Process fault detection; Time step; Variational self-attentive autoencoder; Fault detection"
"Bi Y., Xiang D., Ge Z., Li F., Jia C., Song J.","An Interpretable Prediction Model for Identifying N7-Methylguanosine Sites Based on XGBoost and SHAP","10.1016/j.omtn.2020.08.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092004921&doi=10.1016%2fj.omtn.2020.08.022&partnerID=40&md5=de3b8aca75e016236e4bb07ee55c3f55","Recent studies have increasingly shown that the chemical modification of mRNA plays an important role in the regulation of gene expression. N7-methylguanosine (m7G) is a type of positively-charged mRNA modification that plays an essential role for efficient gene expression and cell viability. However, the research on m7G has received little attention to date. Bioinformatics tools can be applied as auxiliary methods to identify m7G sites in transcriptomes. In this study, we develop a novel interpretable machine learning-based approach termed XG-m7G for the differentiation of m7G sites using the XGBoost algorithm and six different types of sequence-encoding schemes. Both 10-fold and jackknife cross-validation tests indicate that XG-m7G outperforms iRNA-m7G. Moreover, using the powerful SHAP algorithm, this new framework also provides desirable interpretations of the model performance and highlights the most important features for identifying m7G sites. XG-m7G is anticipated to serve as a useful tool and guide for researchers in their future studies of mRNA modification sites. © 2020 The Author(s)N7-methylguanosine modification of mRNA plays a key role in the regulation of gene expression. In this study, we developed a computational model termed XG-m7G based on XGBoost and SHAP to identify N7-methylguanosine sites from sequence information. This will help the discovery of N7-methylguansine and its genetic function. © 2020 The Author(s)","ENAC; feature selection; m7G; machine learning; model interpretation; N7-Methylguanosine; prediction; SCPseDNC; SHAP; XGBoost","7 methylguanosine; messenger RNA; algorithm; Article; cell differentiation; jackknife test; machine learning; predictive value; priority journal; sequence analysis"
"Bi Z., Zhang C.W.J., Wu C., Li L.","New digital triad (DT-II) concept for lifecycle information integration of sustainable manufacturing systems","10.1016/j.jii.2021.100316","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121397304&doi=10.1016%2fj.jii.2021.100316&partnerID=40&md5=d141ed797bd62ae4b2f29b5a1e2e0e38","A system paradigm is a typical pattern or model of enterprise architectures (EAs) that describes constitutive system elements and their relations in achieving missions and goals of enterprises. A well-defined system paradigm and EA help enterprises in adopting appropriate manufacturing resources and technologies, optimizing plans, schedules, and controls of production lines, and coping with complexity, changes, and uncertainties of business processes cost-effectively. It is understandable that a system paradigm should be evolved along with the availability and advancement of advanced manufacturing technologies; on the other hand, new system paradigms are mostly inspired by some successful manufacturing applications of new information technologies (ITs). One of the recently developed system paradigms is Digital Manufacturing (DM) where Digital Twin (DT-I) is used to describe the interactions of virtual and physical entities. This paper discusses the concepts of DM and DT-I and rationalizes the relations of DM and DT-I; in particular, the origin and evolution of DT-I are explored in details to identify its limitations to be used as an EA for DM. It is our finding that existing IT concepts show their limitations in supporting smooth transitions when systems have to be reconfigured in dealing with long-term changes in Sustainable Manufacturing, and this is evidenced by the fact of the trend of the shortened lifespans of modern enterprises even with the aids of rapidly developed digital technologies in decades. To overcome these limitations, a new concept so called Digital Triad (DT-II) is coined and the Internet of Digital Triad Things (IoDTT) is proposed as an information integration (II) solution for digital manufacturing enterprises, and their application is illustrated through a reconfigurable robotic system example. The rationales and significances of DT-II and IoDTT as well as future research directions of DM are summarized as a conclusion. © 2021 Elsevier Inc.","Big data analytics (BDA); Blockchain technology (BCT); Cloud computing (CC); Cyber-physical system (CPS); Digital manufacturing (DM); Digital triad (DT-II); Digital twin (DT-I); Enterprise architecture (EA); Information integration (II); Internet of digital triad things (IoDTT); Internet of Things (IoT); Machine learning (ML)","Big data; Blockchain; Computer architecture; Data Analytics; E-learning; Embedded systems; Engineering education; Integration; Internet of things; Life cycle; Machine learning; Manufacture; Search engines; Big data analytic; Block-chain; Blockchain technology; Cloud computing; Cloud-computing; Cybe-physical system; Digital manufacturing; Digital triad (DT-information integration); Digital twin; Enterprise Architecture; Information integration; Internet of digital triad thing; Internet of thing; Machine learning; Information retrieval"
"Białek J., Bujalski W., Wojdan K., Guzek M., Kurek T.","Dataset level explanation of heat demand forecasting ANN with SHAP","10.1016/j.energy.2022.125075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138562012&doi=10.1016%2fj.energy.2022.125075&partnerID=40&md5=421d3f7697abacaf7085436ee1e6c65e","This paper aims to provide a thorough guide on how to analyze complex energy demand forecasting models with Shapley Additive exPlanations (SHAP) in order to build trust in their predictions and understand the model and SHAP limitations based on selected real-world use case. There are only few examples described in the literature in energy industry and they present very basic usage. This study fills the gap for the class of energy (heat, electric, gas) demand predicting models by showing step by step, top-down analysis of state-of-the-art, deep neural network model predicting total heat demand (hot water and room heating) in Warsaw District Heating Network – the largest district heating network in EU. The paper shows how SHAP can be successfully used on demand forecasting models to provide practical, easily interpretable insights on inner workings of these models which can be used to assess their reliability and plan further development. © 2022 Elsevier Ltd","Contents; Explainable AI; Heat demand forecasting; Machine learning; Shapley additive explanations","Deep neural networks; District heating; Forecasting; Heating equipment; Content; Demand forecasting; District heating networks; Explainable AI; Forecasting models; Heat demand forecasting; Heat demands; Machine-learning; Shapley; Shapley additive explanation; Additives; artificial neural network; data set; energy; heating; Warsaw [Mazowieckie]"
"Bian J., Zhang D., Liu T.-Y., Ding Y., Liu W.","Investor-imitator: A framework for trading knowledge extraction","10.1145/3219819.3220113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051506812&doi=10.1145%2f3219819.3220113&partnerID=40&md5=08f30476d332e9e69862355ec7ba76e4","Stock trading is a popular investment approach in real world. However, since lacking enough domain knowledge and experience, it is very difficult for common investors to analyze thousands of stocks manually. Algorithmic investment provides another rational way to formulate human knowledge as a trading agent. However, it still requires well-built knowledge and experience to design effective trading algorithms in such a volatile market. Fortunately, various kinds of historical trading records are easy to obtain in this big-data era, it is invaluable of us to extract the trading knowledge hidden in the data to help people make better decisions. In this paper, we propose a reinforcement learning driven Investor-Imitator framework to formalize the trading knowledge, by imitating an investor's behavior with a set of logic descriptors. In particular, to instantiate specific logic descriptors, we introduce the Rank-Invest model that can keep the diversity of logic descriptors by learning to optimize different evaluation metrics. In the experiment, we first simulate three types of investors, representing different degrees of information disclosure we may meet in real market. By learning towards these investors, we can tell the inherent trading logic of the target investor with the Investor-Imitator empirically, and the extracted interpretable knowledge can help us better understand and construct trading portfolios. Experimental results in this paper sufficiently demonstrate the designed purpose of Investor-Imitator, it makes the Investor-Imitator an applicable and meaningful intelligent trading framework in financial investment research. © 2018 Association for Computing Machinery.","Data Mining; Finance; Financial Investment; Reinforcement Learning","Big data; Commerce; Computer circuits; Electronic trading; Finance; Investments; Reinforcement learning; Domain knowledge; Evaluation metrics; Financial investments; Human knowledge; Information disclosure; Knowledge and experience; Knowledge extraction; Volatile markets; Data mining"
"Bian Y., Yang J.-X., Zhao X.-H., Zhang X.-L., Han T.-S.","Research on Influencing Factors of Reverse Riding Risk Behavior of Shared E-bike Based on Trajectory Data [基于轨迹数据的共享电动自行车逆行风险行为影响因素研究]","10.19721/j.cnki.1001-7372.2021.12.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124519052&doi=10.19721%2fj.cnki.1001-7372.2021.12.020&partnerID=40&md5=5f3440c435586ebe6e010f6ec59ac2ec","In order to improve the traffic safety problems caused by electric bicycles, this paper studies the relationship between reverse riding risk behavior and its influencing factors. Based on the GPS trajectory data of shared E-Bike in Furong district, Changsha City, the accurate identification of reverse riding behavior is realized. The machine learning model CatBoost and interpretable machine learning framework SHAP are used to extract and analyze the influencing factors of reverse riding behavior from the aspects of road conditions, traffic conditions, land use attributes, etc. The results show that: ① CatBoost model can effectively predict the reverse riding frequency of road sections and extract the important influencing factors of reverse riding behavior, mainly including travel time, public transport facilities, land use attributes, road conditions and traffic conditions. ② In terms of travel time, reverse riding is more likely to occur on weekdays and morning &amp; evening peak hours; In terms of public transport facilities and land use attributes, the number of bus stops and subway station exits, and the number of restaurants, companies, shopping and other facilities around the roads present a non-linear influence relationship to the reverse riding frequency. In a certain range, the number of facilities has a positive effect on the reverse riding behavior. In terms of road conditions, reverse riding is less likely to happen with road crossing intervals of 50~400 m. But reverse riding is more likely to happen when there are no physical separation facilities in the bicycle lane or with road crossing intervals of 400~600 m. And the effect is unstable when the intervals is wider than 600 m. In terms of bicycle lane, the reverse riding probability with guardrail separation is lower, while the probability with green belt separation is higher. In terms of traffic conditions, when the riding speed and acceleration are too low or too high, it is negatively related to the reverse riding behavior. When the riding speed is between 6~16 km•h-1 and the acceleration is between 0.3~1.0 m•s-2, it is positively related to the reverse riding behavior. This research can provide an effective technical support for the identification of shared E-Bike risk riding behavior, as well as for the management of non-motor vehicle traffic safety. © 2021, Editorial Department of China Journal of Highway and Transport. All right reserved.","CatBoost model; Interpretable machine learning; Reverse riding influencing factor; Shared E-Bike; Traffic engineering; Trajectory data","Accident prevention; Behavioral research; Machine learning; Motor transportation; Roads and streets; Separation; Sporting goods; Subway stations; Traffic control; Travel time; Catboost model; Interpretable machine learning; Reverse riding influencing factor; Risk behaviour; Road condition; Shared E-bike; Traffic conditions; Traffic Engineering; Trajectories datum; Travel-time; Bicycles"
"Bianchi C.","Context of utterance and intended context","10.1007/3-540-44607-9_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876448236&doi=10.1007%2f3-540-44607-9_6&partnerID=40&md5=9e2e4417ad85426e84079a603bd37f49","In this paper I expose and criticise the distinction between pure indexicals and demonstratives, held by David Kaplan and John Perry. I oppose the context of material production of the utterance to the ""intended context"" (the context of interpretation, i.e. the context the speaker indicates as semantically relevant): this opposition introduces an intentional feature into the interpretation of pure indexicals. As far as the indexical I is concerned, I maintain that we must distinguish between the material producer of the utterance containing I and the ""intended agent of the context"" - i.e. the individual designated by the material producer as the responsible for the utterance. © Springer-Verlag Berlin Heidelberg 2001.",,"Computer science; Computers; Artificial intelligence"
"Bianchi F., Tarocco P., Castellini A., Farinelli A.","Convolutional Neural Network and Stochastic Variational Gaussian Process for Heating Load Forecasting","10.1007/978-3-030-64583-0_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101252931&doi=10.1007%2f978-3-030-64583-0_23&partnerID=40&md5=e1d7dbcae961933e49a109a5c0a6edfd","Heating load forecasting is a key task for operational planning in district heating networks. In this work we present two advanced models for this purpose, namely a Convolutional Neural Network (CNN) and a Stochastic Variational Gaussian Process (SVGP). Both models are extensions of an autoregressive linear model available in the literature. The CNN outperforms the linear model in terms of 48-h prediction accuracy and its parameters are interpretable. The SVGP has performance comparable to the linear model but it intrinsically deals with prediction uncertainty, hence it provides both load estimations and confidence intervals. Models and performance are analyzed and compared on a real dataset of heating load collected in an Italian network. © 2020, Springer Nature Switzerland AG.","Convolutional Neural Networks; Heating load forecasting; Model interpretability; Smart grids; Stochastic variational Gaussian processes","Convolution; Data Science; District heating; Electric power plant loads; Forecasting; Gaussian distribution; Gaussian noise (electronic); Machine learning; Stochastic models; Stochastic systems; Zoning; Auto-regressive; Confidence interval; District heating networks; Gaussian Processes; Load estimation; Operational planning; Prediction accuracy; Prediction uncertainty; Convolutional neural networks"
"Bianchi F.M., Maiorino E., Livi L., Rizzi A., Sadeghian A.","An agent-based algorithm exploiting multiple local dissimilarities for clusters mining and knowledge discovery","10.1007/s00500-015-1876-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944537369&doi=10.1007%2fs00500-015-1876-1&partnerID=40&md5=82e783ea4507d28ec25074989940f90c","We propose a multi-agent algorithm able to automatically discover relevant regularities in a given dataset, determining at the same time the set of configurations of the adopted parametric dissimilarity measure that yield compact and separated clusters. Each agent operates independently by performing a Markovian random walk on a weighted graph representation of the input dataset. Such a weighted graph representation is induced by a specific parameter configuration of the dissimilarity measure adopted by an agent for the search. During its lifetime, each agent evaluates different parameter configurations and takes decisions autonomously for one cluster at a time. Results show that the algorithm is able to discover parameter configurations that yield a consistent and interpretable collection of clusters. Moreover, we demonstrate that our algorithm shows comparable performances with other similar state-of-the-art algorithms when facing specific clustering problems. Notably, we compare our method with respect to several graph-based clustering algorithms and a well-known subspace search method. © 2015, Springer-Verlag Berlin Heidelberg.","Agent-based algorithms; Clustering; Data mining; Graph conductance; Knowledge discovery; Local dissimilarity measure; Random walk","Algorithms; Autonomous agents; Data mining; Graphic methods; Multi agent systems; Parameter estimation; Random processes; Software agents; Agent based; Clustering; Clustering problems; Dissimilarity measures; Graph-based clustering; Multi-agent algorithms; Random Walk; State-of-the-art algorithms; Clustering algorithms"
"Bianchi F.M., Scardapane S., Rizzi A., Uncini A., Sadeghian A.","Granular Computing Techniques for Classification and Semantic Characterization of Structured Data","10.1007/s12559-015-9369-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952054873&doi=10.1007%2fs12559-015-9369-1&partnerID=40&md5=176f9e11f438cca7827077a92b759ca8","We propose a system able to synthesize automatically a classification model and a set of interpretable decision rules defined over a set of symbols, corresponding to frequent substructures of the input dataset. Given a preprocessing procedure which maps every input element into a fully labeled graph, the system solves the classification problem in the graph domain. The extracted rules are then able to characterize semantically the classes of the problem at hand. The structured data that we consider in this paper are images coming from classification datasets: they represent an effective proving ground for studying the ability of the system to extract interpretable classification rules. For this particular input domain, the preprocessing procedure is based on a flexible segmentation algorithm whose behavior is defined by a set of parameters. The core inference engine uses a parametric graph edit dissimilarity measure. A genetic algorithm is in charge of selecting suitable values for the parameters, in order to synthesize a classification model based on interpretable rules which maximize the generalization capability of the model. Decision rules are defined over a set of information granules in the graph domain, identified by a frequent substructures miner. We compare the system with two other state-of-the-art graph classifiers, evidencing both its main strengths and limits. © 2015, Springer Science+Business Media New York.","Automatic semantic interpretation; Evolutionary optimization; Frequent substructures miner; Granular computing; Graph classification; Graph matching; Watershed segmentation","Algorithms; Classification (of information); Data mining; Genetic algorithms; Granular computing; Image matching; Image segmentation; Information granules; Miners; Optimization; Parameter estimation; Pattern matching; Semantics; Evolutionary optimizations; Graph classification; Graph matchings; Semantic interpretation; Watershed segmentation; Image classification"
"Bianchi Reinaldo A.C., Rillo Anna H.R.C.","Purposive computer vision system: A multi-agent approach",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031340994&partnerID=40&md5=b9d7bc6bf0a089892fcb3f7d779a3d6d","This paper describes a purposive computer vision system for visually guided tasks and a Multi-Agent architecture used to model it. In this architecture, the vision system's purpose is decomposed into a set of behaviors, which are translated into specific tasks. Purpose, behaviors and tasks, as well as the relationship among them, are modeled using a Multi-Agent approach: purpose is modeled by a society of autonomous agents, which communicate through a common language, each one responsible for a specific visually guided behavior; tasks are represented by basic agents, organized in a hierarchical structure. A description of a system that is being implemented as a testbed for the architecture is given, with some details on the implementation of the agents and its communication methods. Finally, a brief discussion about the use of the basic agents is done and research directions are proposed.",,"Artificial intelligence; Computer architecture; Computer simulation; Hierarchical systems; Robots; Purposive computer vision systems; Computer vision"
"Bianchi Reinaldo A.C., Rillo Anna H.R.C.","Distributed control architecture for a purposive computer vision system",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030422190&partnerID=40&md5=c3252685d43377fe70926f7c57d10da9","In this paper we present a distributed control architecture engaged in purposive computer vision tasks. In this context, the vision system's purpose is translated into a set of behaviors, which are decomposed in specific tasks. A Multi-Agent approach is used to model purpose, behaviors, tasks and the relationship among them. Purpose is modeled by a society of autonomous agents, each one responsible for a specific visually guided behavior. Tasks are represented by basic agents, organized in a hierarchical structure with autonomous agents on the top. As a testbed, the architecture of a specific system for visually guided assembly is presented, and a distributed implementation is described.",,"Artificial intelligence; Computer vision; Distributed parameter control systems; Hierarchical systems; Pattern recognition systems; Autonomous agents; Basic agents; Multiagent systems; Purposive computer vision; Specific tasks; Computer architecture"
"Biass S., Jenkins S.F., Aeberhard W.H., Delmelle P., Wilson T.","Insights into the vulnerability of vegetation to tephra fallouts from interpretable machine learning and big Earth observation data","10.5194/nhess-22-2829-2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140322025&doi=10.5194%2fnhess-22-2829-2022&partnerID=40&md5=77cec81ba07f837a9c12f30b173c48ca","Although the generally high fertility of volcanic soils is often seen as an opportunity, short-term consequences of eruptions on natural and cultivated vegetation are likely to be negative. The empirical knowledge obtained from post-event impact assessments provides crucial insights into the range of parameters controlling impact and recovery of vegetation, but their limited coverage in time and space offers a limited sample of all possible eruptive and environmental conditions. Consequently, vegetation vulnerability remains largely unconstrained, thus impeding quantitative risk analyses. Here, we explore how cloud-based big Earth observation data, remote sensing and interpretable machine learning (ML) can provide a large-scale alternative to identify the nature of, and infer relationships between, drivers controlling vegetation impact and recovery. We present a methodology developed using Google Earth Engine to systematically revisit the impact of past eruptions and constrain critical hazard and vulnerability parameters. Its application to the impact associated with the tephra fallout from the 2011 eruption of Cordón Caulle volcano (Chile) reveals its ability to capture different impact states as a function of hazard and environmental parameters and highlights feedbacks and thresholds controlling impact and recovery of both natural and cultivated vegetation. We therefore conclude that big Earth observation (EO) data and machine learning complement existing impact datasets and open the way to a new type of dynamic and large-scale vulnerability models. Copyright: © 2022 Sébastien Biass et al.",,
"Bibal A., Bouadi T., Frénay B., Galárraga L., Oramas J.","AIMLAI'20: Third Workshop on Advances in Interpretable Machine Learning and Artificial Intelligence","10.1145/3340531.3414071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095863828&doi=10.1145%2f3340531.3414071&partnerID=40&md5=18c36140f2f1480cf95a0418d2084c7f","The Third Workshop on ""Advances in Interpretable Machine Learning and Artificial Intelligence"" (AIMLAI) presents contributions in the fields of (i) interpretable ML and AI, i.e., algorithms that are natively interpretable, and (ii) interpretability modules, i.e., explanation layers on top of black-box models, also called post-hoc interpretability. AIMLAI encourages interdisciplinary collaborations with particular emphasis in knowledge management, infovis, human computer interaction and psychology. It also welcomes applied research for use cases where interpretability matters. © 2020 Owner/Author.","explainable ai; interpretability; machine learning","Human computer interaction; Knowledge management; Applied research; Black-box model; Interdisciplinary collaborations; Interpretability; Machine learning"
"Bibal A., Marion R., Frénay B.","Finding the most interpretable MDS rotation for sparse linear models based on external features",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061840537&partnerID=40&md5=9761b83cfbdf1e4e0851d2ae2b5ef188","One approach to interpreting multidimensional scaling (MDS) embeddings is to estimate a linear relationship between the MDS dimensions and a set of external features. However, because MDS only preserves distances between instances, the MDS embedding is invariant to rotation. As a result, the weights characterizing this linear relationship are arbitrary and difficult to interpret. This paper proposes a procedure for selecting the most pertinent rotation for interpreting a 2D MDS embedding. © ESANN 2018 - Proceedings, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning.",,"Machine learning; Neural networks; External features; Linear relationships; Multi-dimensional scaling; Embeddings"
"Bibault J.-E., Hancock S., Buyyounouski M.K., Bagshaw H., Leppert J.T., Liao J.C., Xing L.","Development and validation of an interpretable artificial intelligence model to predict 10-year prostate cancer mortality","10.3390/cancers13123064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108095159&doi=10.3390%2fcancers13123064&partnerID=40&md5=53a3d0d6e8ecd16edf3ad576b9746b3a","Prostate cancer treatment strategies are guided by risk-stratification. This stratification can be difficult in some patients with known comorbidities. New models are needed to guide strategies and determine which patients are at risk of prostate cancer mortality. This article presents a gradient-boosting model to predict the risk of prostate cancer mortality within 10 years after a cancer diagnosis, and to provide an interpretable prediction. This work uses prospective data from the PLCO Cancer Screening and selected patients who were diagnosed with prostate cancer. During follow-up, 8776 patients were diagnosed with prostate cancer. The dataset was randomly split into a training (n = 7021) and testing (n = 1755) dataset. Accuracy was 0.98 (±0.01), and the area under the receiver operating characteristic was 0.80 (±0.04). This model can be used to support informed decision-making in prostate cancer treatment. AI interpretability provides a novel understanding of the predictions to the users. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence; Machine learning; Prediction; Prostate cancer","prostate specific antigen; accuracy; adult; alcohol consumption; arthritis; Article; artificial intelligence; body height; body weight; bronchitis; cancer diagnosis; cancer mortality; cancer patient; cancer screening; cancer therapy; cerebrovascular accident; comorbidity; computer assisted tomography; computer model; controlled study; current smoker; decision making; diabetes mellitus; emphysema; female; follow up; Gleason score; heart infarction; human; hypertension; incidence; liver disease; machine learning; major clinical study; male; medical history; mobile application; mortality; nocturia; nomogram; osteoporosis; physical activity; prediction; predictive value; prospective study; prostate biopsy; prostate cancer; prostate volume; prostatitis; receiver operating characteristic; sensitivity and specificity; smoking; socioeconomics; training; transrectal ultrasonography; validation process"
"Bibault J.-E., Chang D.T., Xing L.","Development and validation of a model to predict survival in colorectal cancer using a gradient-boosted machine","10.1136/gutjnl-2020-321799","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091448955&doi=10.1136%2fgutjnl-2020-321799&partnerID=40&md5=82f7bafbdf5a47870196ff5eae7de391","Objective The success of treatment planning relies critically on our ability to predict the potential benefit of a therapy. In colorectal cancer (CRC), several nomograms are available to predict different outcomes based on the use of tumour specific features. Our objective is to provide an accurate and explainable prediction of the risk to die within 10 years after CRC diagnosis, by incorporating the tumour features and the patient medical and demographic information. Design In the prostate, lung, colorectal and ovarian cancer screening (PLCO) Trial, participants (n=154 900) were randomised to screening with flexible sigmoidoscopy, with a repeat screening at 3 or 5 years, or to usual care. We selected patients who were diagnosed with CRC during the follow-up to train a gradient-boosted model to predict the risk to die within 10 years after CRC diagnosis. Using Shapley values, we determined the 20 most relevant features and provided explanation to prediction. Results During the follow-up, 2359 patients were diagnosed with CRC. Median follow-up was 16.8 years (14.4-18.9) for mortality. In total, 686 patients (29%) died from CRC during the follow-up. The dataset was randomly split into a training (n=1887) and a testing (n=472) dataset. The area under the receiver operating characteristic was 0.84 (±0.04) and accuracy was 0.83 (±0.04) with a 0.5 classification threshold. The model is available online for research use. Conclusions We trained and validated a model with prospective data from a large multicentre cohort of patients. The model has high predictive performances at the individual scale. It could be used to discuss treatment strategies. © 2021 Author(s).","Colorectal cancer","alcohol; antineoplastic agent; cholesterol; adjuvant therapy; adult; age; aged; alcohol consumption; area under the curve; arthritis; Article; body height; body weight; bronchitis; cancer grading; cancer prognosis; cancer radiotherapy; cancer screening; cancer staging; cancer survival; cerebrovascular accident; colorectal cancer; diabetes mellitus; diagnostic test accuracy study; emphysema; family history; female; follow up; gradient boosting machine; heart infarction; human; hypertension; intestine resection; liver disease; machine learning; major clinical study; male; mortality risk; osteoporosis; physical activity; prediction; priority journal; receiver operating characteristic; sigmoidoscopy; smoking habit; social status; treatment planning; tumor localization; artificial intelligence; clinical trial; colorectal tumor; controlled study; middle aged; mortality; multicenter study; predictive value; prospective study; randomized controlled trial; risk assessment; sigmoidoscopy; survival analysis; Aged; Artificial Intelligence; Colorectal Neoplasms; Female; Humans; Male; Middle Aged; Predictive Value of Tests; Prospective Studies; Risk Assessment; Sigmoidoscopy; Survival Analysis"
"Bibi M., Hanif M.K., Sarwar M.U., Khan M.I., Khan S.Z., Shikali Shivachi C., Anees A.","Monitoring Population Phenology of Asian Citrus Psyllid Using Deep Learning","10.1155/2021/4644213","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122849641&doi=10.1155%2f2021%2f4644213&partnerID=40&md5=b79484718a00a001f5d93fa73a3b0526","Asian citrus psyllid, Diaphorina citri Kuwayama (Liviidae: Hemiptera) is a menacing and notorious pest of citrus plants. It vectors a phloem vessel-dwelling bacterium Candidatus Liberibacter asiaticus, which is a causative pathogen of the serious citrus disease known as Huanglongbing. Huanglongbing disease is a major bottleneck in the export of citrus fruits from Pakistan. It is being responsible for huge citrus economic losses globally. In the current study, several prediction models were developed based on regression algorithms of machine learning to monitor different phenological stages of Asian citrus psyllid to predict its population about different abiotic variables (average maximum temperature, average minimum temperature, average weekly temperature, average weekly relative humidity, and average weekly rainfall) and biotic variable (host plant phenological patterns) in citrus-growing regions of Pakistan. The pest prediction models can be used for proper applications of pesticides only when needed for reducing the environmental and cost impacts of pesticides. Pearson's correlation analysis was performed to find the relationship between different predictor (abiotic and biotic) variables and pest infestation rate on citrus plants. Multiple linear regression, random forest regressor, and deep neural network approaches were compared to predict population dynamics of Asian citrus psyllid. In comparison with other regression techniques, a deep neural network-based prediction model resulted in the least root mean squared error values while predicting egg, nymph, and adult populations. © 2021 Maria Bibi et al.",,"Citrus fruits; Correlation methods; Decision trees; Deep neural networks; Forecasting; Losses; Mean square error; Multiple linear regression; 'current; Biotics; Citrus plant; Diaphorina citri; Economic loss; Hemiptera; Huanglongbing; Pakistan; Prediction modelling; Psyllid; Pesticides"
"Bicego M., Lovato P., Debona M., Guzzo F., Assfalg M.","Mining NMR Spectroscopy Using Topic Models","10.1109/ICPR.2018.8545883","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059770603&doi=10.1109%2fICPR.2018.8545883&partnerID=40&md5=cf1a9ef2f9e57e9a99db6612314bfef6","Pattern Recognition techniques have been successfully exploited for the biomedical analysis of NMR spectra. In this context, it is crucial to derive a suitable representation for the data: Among others, a successful line of research exploits the Bag of Words representation (called here 'Bag of Peaks'). However, despite its success, the Bag of Peaks paradigm has not been fully explored: For example, appropriate probabilistic models (such as topic models) can further distill the information contained in the Bag of Words, allowing for more interpretable and accurate solutions for the task-at-hand. This paper is aimed at filling this gap, by investigating the usefulness of topic models in the analysis of NMR spectra. In particular, we first introduce an unsupervised approach, based on topic models, that performs soft biclustering of NMR spectra-this kind of unsupervised analysis being new in the NMR literature. Second, we show that descriptors extracted from topic models can be successfully employed for classification of NMR samples: Compared to the original Bag of Words, we prove that our descriptors provide higher accuracies. Finally, we perform an empirical evaluation involving a complex dataset of spectra derived from fruits, and two datasets of medical NMR spectra: Our analysis confirms the suitability of such models in the NMR spectra analysis. © 2018 IEEE.",,"Nuclear magnetic resonance spectroscopy; Pattern recognition; Bag of words; Bi-clustering; Biomedical analysis; Empirical evaluations; Pattern recognition techniques; Probabilistic models; Unsupervised analysis; Unsupervised approaches; Data mining"
"Bicego M., Lovato P., Perina A., Fasoli M., Delledonne M., Pezzotti M., Polverari A., Murino V.","Investigating topic models' capabilities,in expression microarray data classification","10.1109/TCBB.2012.121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875203429&doi=10.1109%2fTCBB.2012.121&partnerID=40&md5=2528431717568fae695cc63819fc04d0","In recent years a particular class of probabilistic graphical models-called topic models-has proven to represent an useful and interpretable tool for understanding and mining microarray data. In this context, such models have been almost only applied in the clustering scenario, whereas the classification task has been disregarded by researchers. In this paper, we thoroughly investigate the use of topic models for classification of microarray data, starting from ideas proposed in other fields (e.g., computer vision). A classification scheme is proposed, based on highly interpretable features extracted from topic models, resulting in a hybrid generative-discriminative approach; an extensive experimental evaluation, involving 10 different literature benchmarks, confirms the suitability of the topic models for classifying expression microarray data. © 2012 IEEE.","Expression microarray; Hybrid generative discriminative approaches; Topic models","Classification scheme; Classification tasks; Discriminative approach; Experimental evaluation; Expression microarray; Microarray data; Topic model; Biotechnology; Genetic engineering; Classification (of information); article; Bayes theorem; biology; data mining; factual database; methodology; microarray analysis; semantics; statistical model; Bayes Theorem; Computational Biology; Data Mining; Databases, Factual; Microarray Analysis; Models, Statistical; Semantics"
"Bichindaritz I., Kansu E., Sullivan K.M.","Case-based reasoning in care-partner: Gathering evidence for evidence-based medical practice","10.1007/BFb0056345","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958671410&doi=10.1007%2fBFb0056345&partnerID=40&md5=9a0be72942485ed951ccc307a4d1df23","This paper presents the CARE-PARTNER system. Functionally, it offers via the WWW knowledge-support assistance to clinicians responsible for the long-term follow-up of stem-cell post-transplant patient care. CAREPARTNER aims at implementing the concept of evidence-based medical practice, which recommends the practice of medicine based on proven and validated knowledge. From an artificial intelligence viewpoint, it proposes a multimodal reasoning framework for the cooperation of case-based reasoning, rule-based reasoning and information retrieval to solve problems. The role of case-based reasoning is presented in this paper as the collection of evidence for evidence-based medical practice. Case-based reasoning permits to refine and complete the knowledge of the system. It enhances the system by conferring an ability to learn from experience, and thus improve results over time. © Springer-Verlag Berlin Heidelberg 1998.",,"Artificial intelligence; Medicine; Problem solving; Stem cells; Evidence-based; Knowledge supports; Long-term follow-up; Medical practice; Multi-modal; Patient care; Reasoning framework; Rule based reasoning; Case based reasoning"
"Bichler A., Neumaier A., Hofmann T.","A tree-based statistical classification algorithm (CHAID) for identifying variables responsible for the occurrence of faecal indicator bacteria during waterworks operations","10.1016/j.jhydrol.2014.08.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906736007&doi=10.1016%2fj.jhydrol.2014.08.013&partnerID=40&md5=4ef430527c01800ec5d32051391ed0c8","Microbial contamination of groundwater used for drinking water can affect public health and is of major concern to local water authorities and water suppliers. Potential hazards need to be identified in order to protect raw water resources. We propose a non-parametric data mining technique for exploring the presence of total coliforms (TC) in a groundwater abstraction well and its relationship to readily available, continuous time series of hydrometric monitoring parameters (seven year records of precipitation, river water levels, and groundwater heads). The original monitoring parameters were used to create an extensive generic dataset of explanatory variables by considering different accumulation or averaging periods, as well as temporal offsets of the explanatory variables. A classification tree based on the Chi-Squared Automatic Interaction Detection (CHAID) recursive partitioning algorithm revealed statistically significant relationships between precipitation and the presence of TC in both a production well and a nearby monitoring well. Different secondary explanatory variables were identified for the two wells. Elevated water levels and short-term water table fluctuations in the nearby river were found to be associated with TC in the observation well. The presence of TC in the production well was found to relate to elevated groundwater heads and fluctuations in groundwater levels. The generic variables created proved useful for increasing significance levels. The tree-based model was used to predict the occurrence of TC on the basis of hydrometric variables. © 2014 Elsevier B.V.","CHAID; Classification tree; Drinking water; Faecal indicator bacteria; Groundwater quality; Total coliforms","CHAID; Classification trees; Faecal indicator bacteria; Ground-water qualities; Total coliforms; Potable water; algorithm; data mining; data set; drinking water; groundwater; monitoring system; public health; river water; time series; time series analysis"
"Bicknell J.W., Krebs W.G.","Detecting botnet signals using process mining","10.1007/s10588-020-09320-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095591602&doi=10.1007%2fs10588-020-09320-x&partnerID=40&md5=fcd0463ca61d72bf9a8c3cbc570f4d13","Detecting and elucidating botnets is an active area of research. Using explainable, highly scalable Apache Spark-based artificial intelligence, process mining technologies are presented which illuminate bot activity within terrorist Twitter data. A derived hidden Markov model suggests that bot logic uses information camouflage in order to disguise intentions similar to World War II Nazi propagandists and Soviet-era practitioners of information warfare enhanced with reflexive control. A future effort is presented which strings together best of breed techniques into a composite classification algorithm in order to improve continually the discovery of malicious accounts, understand cross-platform weaponized botnet dynamics, and model adversarial information warfare campaigns recursively. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Cognitive security; Information warfare; Misinformation; Process mining; Reflexive control; Social media","Artificial intelligence; Classification (of information); Data mining; Hidden Markov models; Military operations; Sodium compounds; Active area; Botnets; Classification algorithm; Cross-platform; Information warfare; Process mining; World war II; Botnet"
"Biddle E., Agostinelli E., Bellamy M., Amaba B., Flower C.","Trusted AI-driven Pipe Specification Advisor: Augmenting the Engineer with knowledge developed over the past 40 years",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137179471&partnerID=40&md5=e12b85a6a128a10671e3585991fc801e","Defining complex piping specifications is a profitable business for ABB. However, key engineers with years of experience are approaching retirement, leaving a knowledge chasm. Can Artificial Intelligence (AI) be trained to capture and learn this expertise to help advise and augment the knowledge of junior engineers? A cross-functional team of scientists, manufacturing experts, and engineers produce Piping Specifications for ABB clients' new and existing projects. They are based on deep technical expertise which engineers have built over four decades of client engagements. Our AI solution unlocks and ingests this knowledge into a relational database. It uses an unsupervised association machine learning model (MLM) to understand what pipes and components were used in previous specifications based on various attributes. Given the constraints (i.e., fluid type, material, temperature, etc.), the engineer is presented with several recommendations detailing pipe configurations (i.e., the pipe standard, thickness, flanges, reducers, etc.) that have been most used in similar settings. The MLM is inherently interpretable following Explainable AI and the recommendations are presented in a human-centric, evidence-based web application which is hosted on an open hybrid multi-cloud platform. The AI solution supports ABB to capture and curate the wealth of knowledge that has been developed over several decades. This allows ABB to maintain and expand the piping specification offering with reduced costs while augmenting many future engineers. This solution is problem agnostic and has been built in a flexible and reusable manner, applicable to any industry where knowledge retention brings a crucial competitive advantage. © 2022 IISE Annual Conference and Expo 2022. All rights reserved.","Artificial Intelligence; Explainable AI; Industrial Systems Engineering; Knowledge Retention; Machine Learning","Competition; Computer software reusability; Engineers; Machine components; Machine learning; Cross-functional teams; Explainable artificial intelligence; Industrial system engineering; Industrial systems; Knowledge retention; Learn+; Machine learning models; Machine-learning; Pipe specifications; Technical expertise; Specifications"
"Bidstrup M., Kørnøv L., Partidário M.R.","Cumulative effects in strategic environmental assessment: The influence of plan boundaries","10.1016/j.eiar.2015.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952324905&doi=10.1016%2fj.eiar.2015.12.003&partnerID=40&md5=a2bc39b03c1890182fd05fc6b2bdbe4e","Cumulative effects (CE) assessment is lacking quality in impact assessment (IA) worldwide. It has been argued that the strategic environmental assessment (SEA) provides a suitable IA framework for addressing CE because it is applied to developments with broad boundaries, but few have tested this claim. Through a case study on the Danish mining sector, this article explores how plan boundaries influence the analytical boundaries applied for assessing CE in SEA. The case was studied through document analysis in combination with semi-structured group interviews of the responsible planners, who also serve as SEA practitioners. It was found that CE are to some extent assessed and managed implicitly throughout the planning process. However, this is through a focus on lowering the cumulative stress of mining rather than the cumulative stress on and capacity of the receiving environment. Plan boundaries do influence CE assessment, though all boundaries are not equally influential. The geographical and time boundaries of the Danish mining plans are broad or flexible enough to accommodate a meaningful assessment of CE, but the topical boundary is restrictive. The study indicates that collaboration among planning authorities and legally appointed CE leadership may facilitate better practice on CE assessment in sector-specific SEA contexts. However, most pressing is the need for relating assessment to the receiving environment as opposed to solely the stress of a proposed plan. © 2015 Elsevier Inc.",,"Data mining; Environmental impact; Cumulative effects; Document analysis; Impact assessments; Mining sector; Planning authorities; Planning process; Semi-structured; Strategic environmental assessments; Environmental impact assessments; environmental impact assessment; environmental planning; mining industry; planning process; strategic approach; Denmark"
"Biecek P.","Dalex: Explainers for complex predictive models in R",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060515890&partnerID=40&md5=f8cb493b5ce34cb2db3fc778f4d30d13","Predictive modeling is invaded by elastic, yet complex methods such as neural networks or ensembles (model stacking, boosting or bagging). Such methods are usually described by a large number of parameters or hyper parameters - a price that one needs to pay for elasticity. The very number of parameters makes models hard to understand. This paper describes a consistent collection of explainers for predictive models, a.k.a. black boxes. Each explainer is a technique for exploration of a black box model. Presented approaches are model-agnostic, what means that they extract useful information from any predictive method irrespective of its internal structure. Each explainer is linked with a specific aspect of a model. Some are useful in decomposing predictions, some serve better in understanding performance, while others are useful in understanding importance and conditional responses of a particular variable. Every explainer presented here works for a single model or for a collection of models. In the latter case, models can be compared against each other. Such comparison helps to find strengths and weaknesses of different models and gives additional tools for model validation. Presented explainers are implemented in the DALEX package for R. They are based on a uniform standardized grammar of model exploration which may be easily extended. © 2018 Przemyslaw Biecek.","Explainable artificial intelligence; Interpretable machine learning; Model visualization; Predictive modelling","Artificial intelligence; Learning systems; Black-box model; Internal structure; Model validation; Model visualization; Predictive methods; Predictive modeling; Predictive modelling; Predictive models; Complex networks"
